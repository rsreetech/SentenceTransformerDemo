{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo of Sentence Transforemers(https://www.sbert.net/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('allenai/scibert_scivocab_uncased')\n",
    "model.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.4579\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.1759\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9283\n"
     ]
    }
   ],
   "source": [
    "# Two lists of sentences\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading function taken from: https://www.kaggle.com/foolofatook/zero-shot-classification-with-huggingface-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = r'd:/arxivDS/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\"\"\" Using `yield` to load the JSON file in a loop to prevent Python memory issues if JSON is loaded directly\"\"\"\n",
    "\n",
    "def get_metadata():\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata()\n",
    "ids = []\n",
    "titles = []\n",
    "abstracts = []\n",
    "categories = []\n",
    "\n",
    "for paper in metadata:\n",
    "    metaDict = json.loads(paper)\n",
    "    try:\n",
    "        try:\n",
    "            year = int(metaDict['journal-ref'][-4:])    ### Example Format: \"Phys.Rev.D76:013009,2007\"\n",
    "        except:\n",
    "            year = int(metaDict['journal-ref'][-5:-1])    ### Example Format: \"Phys.Rev.D76:013009,(2007)\"\n",
    "        if(year == 2020):\n",
    "            ids.append(metaDict['id'])\n",
    "            titles.append(metaDict['title'])\n",
    "            abstracts.append(metaDict['abstract'])\n",
    "            categories.append(metaDict['categories'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26558\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'id' : ids,'title' : titles,'abstract' : abstracts, 'categories' : categories})\n",
    "\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0712.1975</td>\n",
       "      <td>Reentrant spin glass transition in LuFe2O4</td>\n",
       "      <td>We have carried out a comprehensive investig...</td>\n",
       "      <td>cond-mat.str-el cond-mat.mtrl-sci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0804.3104</td>\n",
       "      <td>Teichm\\\"uller Structures and Dual Geometric Gi...</td>\n",
       "      <td>The Gibbs measure theory for smooth potentia...</td>\n",
       "      <td>math.DS math.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0810.5491</td>\n",
       "      <td>Nonequilibrium phase transition in a spreading...</td>\n",
       "      <td>We consider a nonequilibrium process on a ti...</td>\n",
       "      <td>cond-mat.stat-mech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0902.3288</td>\n",
       "      <td>Origin and evolution of cosmic accelerators - ...</td>\n",
       "      <td>One of the most tantalizing questions in ast...</td>\n",
       "      <td>astro-ph.CO astro-ph.HE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0908.2605</td>\n",
       "      <td>A use of geometric calculus to reduce Berezin ...</td>\n",
       "      <td>Berezin integration of functions of anticomm...</td>\n",
       "      <td>gr-qc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0712.1975         Reentrant spin glass transition in LuFe2O4   \n",
       "1  0804.3104  Teichm\\\"uller Structures and Dual Geometric Gi...   \n",
       "2  0810.5491  Nonequilibrium phase transition in a spreading...   \n",
       "3  0902.3288  Origin and evolution of cosmic accelerators - ...   \n",
       "4  0908.2605  A use of geometric calculus to reduce Berezin ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    We have carried out a comprehensive investig...   \n",
       "1    The Gibbs measure theory for smooth potentia...   \n",
       "2    We consider a nonequilibrium process on a ti...   \n",
       "3    One of the most tantalizing questions in ast...   \n",
       "4    Berezin integration of functions of anticomm...   \n",
       "\n",
       "                          categories  \n",
       "0  cond-mat.str-el cond-mat.mtrl-sci  \n",
       "1                    math.DS math.CV  \n",
       "2                 cond-mat.stat-mech  \n",
       "3            astro-ph.CO astro-ph.HE  \n",
       "4                              gr-qc  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond-mat.str-el cond-mat.mtrl-sci\n",
      "math.DS math.CV\n",
      "cond-mat.stat-mech\n",
      "astro-ph.CO astro-ph.HE\n",
      "gr-qc\n",
      "astro-ph.IM astro-ph.EP\n",
      "cond-mat.str-el cond-mat.mes-hall\n",
      "cond-mat.dis-nn cs.DM math.CO\n",
      "math.DS\n",
      "astro-ph.IM astro-ph.CO\n",
      "hep-ph hep-lat hep-th\n",
      "math-ph math.MP\n",
      "physics.soc-ph cond-mat.dis-nn cs.SI\n",
      "quant-ph\n",
      "cond-mat.soft\n",
      "physics.plasm-ph\n",
      "nucl-ex\n",
      "math.SG\n",
      "math.CT\n",
      "math.ST stat.TH\n",
      "math.NT\n",
      "cond-mat.mtrl-sci\n",
      "nucl-th nucl-ex\n",
      "cond-mat.stat-mech cond-mat.dis-nn\n",
      "math.CO\n",
      "math-ph cond-mat.mtrl-sci hep-th math.MP quant-ph\n",
      "gr-qc hep-th\n",
      "cond-mat.mes-hall\n",
      "math-ph hep-th math.MP\n",
      "math.QA math.CO math.CT\n",
      "math.CA math-ph math.MP\n",
      "math.OC math.PR\n",
      "cs.IT math.AG math.IT\n",
      "gr-qc hep-th quant-ph\n",
      "hep-ph cond-mat.str-el hep-th\n",
      "cs.IT math.IT math.PR quant-ph\n",
      "cs.DS\n",
      "nucl-th math-ph math.MP quant-ph\n",
      "math.RA math.AC\n",
      "cond-mat.supr-con cond-mat.str-el\n",
      "cond-mat.quant-gas\n",
      "math.FA math.CO\n",
      "hep-ex nucl-ex\n",
      "quant-ph physics.ed-ph\n",
      "astro-ph.GA\n",
      "math.AG math.DG math.RT\n",
      "math.AP math.PR\n",
      "math.DG math-ph math.AP math.MP\n",
      "cond-mat.mtrl-sci cond-mat.str-el\n",
      "math-ph math.AP math.MP math.PR math.QA\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con quant-ph\n",
      "q-bio.PE nlin.CD physics.bio-ph\n",
      "physics.gen-ph astro-ph.GA\n",
      "math.DG\n",
      "math.CO cs.DS math.SP\n",
      "nucl-ex hep-ph nucl-th\n",
      "cs.CL cs.LG math.OC\n",
      "nucl-th hep-ph\n",
      "math.SG math-ph math.AG math.DG math.MP\n",
      "cond-mat.soft cond-mat.stat-mech\n",
      "cs.GT\n",
      "gr-qc astro-ph.HE hep-ph hep-th physics.flu-dyn\n",
      "math-ph gr-qc hep-th math.MP quant-ph\n",
      "physics.optics\n",
      "cs.IR cs.CL cs.CV\n",
      "q-bio.PE nlin.CD nlin.SI physics.bio-ph\n",
      "math.DG math.CO\n",
      "astro-ph.HE hep-ph\n",
      "math.DS math.GR\n",
      "math.PR math-ph math.MP\n",
      "quant-ph math-ph math.MP\n",
      "physics.optics cs.NA math.NA nlin.PS nlin.SI\n",
      "q-fin.MF\n",
      "cs.CE cs.ET q-bio.MN\n",
      "math.RA math.NT\n",
      "math.CA cs.SY math.DS math.OC q-bio.CB\n",
      "quant-ph gr-qc hep-th\n",
      "q-fin.GN q-fin.EC q-fin.MF q-fin.ST\n",
      "physics.chem-ph cond-mat.other physics.ins-det quant-ph\n",
      "quant-ph cond-mat.stat-mech math-ph math.MP\n",
      "math.GT\n",
      "math.NT math.CO\n",
      "hep-ph\n",
      "stat.ME q-bio.MN stat.ML\n",
      "stat.CO\n",
      "cond-mat.str-el cond-mat.supr-con\n",
      "cs.CG cs.DS math.CO\n",
      "math.GT math.AG math.GR\n",
      "cs.LO\n",
      "math.AG\n",
      "quant-ph cs.CR\n",
      "math.PR math.FA\n",
      "astro-ph.CO astro-ph.GA hep-th\n",
      "astro-ph.CO gr-qc\n",
      "math.AG math.AC\n",
      "physics.ed-ph\n",
      "stat.ME\n",
      "cond-mat.supr-con\n",
      "math.RT math.RA\n",
      "q-bio.PE math.PR\n",
      "cond-mat.str-el hep-th math-ph math.GT math.MP quant-ph\n",
      "math.CO math.NT\n",
      "math.DG math-ph math.MP math.OA\n",
      "quant-ph cs.IT gr-qc hep-th math.IT\n",
      "physics.gen-ph\n",
      "math.DG math.SP\n",
      "math.DG math-ph math.MP\n",
      "cond-mat.str-el\n",
      "math.KT math.AG math.AT\n",
      "cond-mat.dis-nn physics.soc-ph\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.flu-dyn\n",
      "cs.CV cs.SY\n",
      "math.RT math.PR math.RA\n",
      "cs.CR\n",
      "math-ph math.AP math.CO math.MP math.QA math.RT\n",
      "math.NA cs.NA\n",
      "cs.CV cs.SY eess.SY\n",
      "cs.SI physics.soc-ph\n",
      "cond-mat.quant-gas cond-mat.str-el\n",
      "q-fin.RM physics.soc-ph\n",
      "stat.ME cs.AI stat.ML\n",
      "gr-qc hep-th math-ph math.MP\n",
      "quant-ph cond-mat.mes-hall\n",
      "q-fin.CP cs.NA math.NA\n",
      "physics.atom-ph quant-ph\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "math.CT math.AT\n",
      "q-fin.RM\n",
      "cs.SY\n",
      "math.PR\n",
      "stat.CO stat.ML\n",
      "math.RA\n",
      "cs.SI cond-mat.stat-mech nlin.AO physics.soc-ph stat.ME\n",
      "math-ph math.AG math.MP math.NT\n",
      "q-bio.NC math.DS\n",
      "quant-ph astro-ph.CO hep-ph\n",
      "cs.DS math.OC physics.comp-ph physics.med-ph\n",
      "math.PR math-ph math.CV math.MP\n",
      "cs.CY\n",
      "math.CA\n",
      "math.OC cs.IT cs.SY math.IT\n",
      "math.CO cs.DM math.PR\n",
      "q-fin.PM\n",
      "math.AG math.CO math.OC\n",
      "math.AP\n",
      "math.CV\n",
      "math.AT cs.CG math.CT\n",
      "cs.CG cs.RO math.MG\n",
      "q-fin.GN\n",
      "math.QA\n",
      "cs.LG cs.AI stat.ML\n",
      "math.QA hep-th math.RA\n",
      "gr-qc quant-ph\n",
      "hep-ph nucl-th\n",
      "math.OC\n",
      "stat.CO stat.AP stat.ME\n",
      "cs.CV eess.IV\n",
      "math.ST stat.ME stat.TH\n",
      "cs.IT math.IT quant-ph\n",
      "math.PR q-fin.RM\n",
      "nucl-th cond-mat.quant-gas cond-mat.stat-mech hep-ph nucl-ex\n",
      "nlin.CG math-ph math.MP nlin.PS\n",
      "math.RT math.GR math.RA\n",
      "physics.bio-ph\n",
      "cs.IT cond-mat.dis-nn math-ph math.IT math.MP\n",
      "math.AT hep-th math-ph math.GT math.MP math.QA\n",
      "math.OC q-fin.MF\n",
      "physics.soc-ph cond-mat.stat-mech physics.data-an\n",
      "math.OC cs.SY physics.ins-det\n",
      "cs.CL\n",
      "cs.IT math.IT\n",
      "math-ph cond-mat.str-el hep-th math.MP\n",
      "astro-ph.SR astro-ph.EP\n",
      "math-ph math.CO math.MP math.PR\n",
      "quant-ph cs.NE cs.SY\n",
      "math.CO math.PR math.RT\n",
      "math.AC\n",
      "cs.OH quant-ph\n",
      "math.QA math.KT math.RA\n",
      "quant-ph cs.IT math.IT\n",
      "q-fin.EC cs.SI econ.TH\n",
      "hep-th\n",
      "cs.LG cs.CV\n",
      "math.AT math.GT math.KT math.RT\n",
      "cond-mat.dis-nn cond-mat.stat-mech\n",
      "quant-ph cond-mat.stat-mech\n",
      "cs.DL cond-mat.stat-mech physics.data-an stat.AP\n",
      "math.PR math.CO\n",
      "cs.RO cs.AI\n",
      "math.OC cs.LG cs.NA math.NA\n",
      "math.LO math.CO\n",
      "physics.flu-dyn\n",
      "quant-ph math.OC\n",
      "physics.gen-ph math-ph math.MP\n",
      "physics.soc-ph\n",
      "stat.CO econ.EM stat.AP stat.ME\n",
      "physics.optics quant-ph\n",
      "stat.OT\n",
      "astro-ph.CO hep-ph\n",
      "math.AP gr-qc math-ph math.MP\n",
      "physics.app-ph cond-mat.mtrl-sci\n",
      "cond-mat.other cond-mat.mtrl-sci\n",
      "quant-ph cs.LO\n",
      "math.AG math.AT math.CV\n",
      "q-fin.MF q-fin.PM q-fin.ST stat.AP\n",
      "quant-ph cs.CC cs.DS\n",
      "math.RT math.QA math.RA\n",
      "math.AP math.CA math.SP\n",
      "stat.AP q-bio.NC\n",
      "gr-qc math.AP\n",
      "math.ST cs.SI stat.TH\n",
      "physics.class-ph\n",
      "physics.bio-ph q-bio.QM\n",
      "math.NA\n",
      "cs.CV\n",
      "hep-th gr-qc\n",
      "cond-mat.supr-con cond-mat.quant-gas\n",
      "astro-ph.HE gr-qc\n",
      "math-ph math.MP quant-ph\n",
      "stat.AP physics.soc-ph\n",
      "quant-ph hep-th math-ph math.MP math.OA\n",
      "physics.optics nlin.PS\n",
      "stat.CO math.PR\n",
      "q-fin.RM math.OC\n",
      "math.LO\n",
      "cs.CR cs.NI\n",
      "cs.RO\n",
      "quant-ph cs.CR cs.DC\n",
      "math.NT hep-th\n",
      "cond-mat.stat-mech hep-th math-ph math.MP quant-ph\n",
      "physics.bio-ph cond-mat.soft\n",
      "math.AP math.DG\n",
      "math.OA\n",
      "cond-mat.quant-gas quant-ph\n",
      "math.DG math.AP math.MG\n",
      "math-ph math.MP math.QA\n",
      "math.CO math.MG\n",
      "cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "math.CO math.OA math.PR\n",
      "physics.gen-ph hep-th\n",
      "math.ST cs.LG stat.ML stat.TH\n",
      "gr-qc math-ph math.AP math.MP\n",
      "physics.ins-det physics.optics quant-ph\n",
      "physics.comp-ph cond-mat.stat-mech cond-mat.str-el physics.app-ph quant-ph\n",
      "math.DG math-ph math.AP math.MP math.SP\n",
      "math-ph math.CO math.MP\n",
      "hep-ex astro-ph.SR\n",
      "hep-th gr-qc math-ph math.DG math.MP\n",
      "cs.MM cs.CV\n",
      "physics.comp-ph cond-mat.stat-mech cond-mat.str-el\n",
      "math.GT math.AT math.MG\n",
      "physics.atom-ph\n",
      "math.DS math.CO math.MG\n",
      "math.DS cs.RO math.OC\n",
      "astro-ph.CO\n",
      "cond-mat.mes-hall math-ph math.MP nucl-th quant-ph\n",
      "math.OA math.KT math.QA\n",
      "nlin.CD math.AP math.DS math.NA\n",
      "math.AG math.AT math.RA\n",
      "nucl-th\n",
      "cs.CG math.DG\n",
      "math.RT\n",
      "math.DS math.PR\n",
      "cond-mat.soft physics.flu-dyn\n",
      "cs.CC cs.DS math.CO\n",
      "gr-qc astro-ph.IM physics.ins-det\n",
      "q-bio.PE\n",
      "cond-mat.dis-nn physics.optics\n",
      "hep-ph quant-ph\n",
      "physics.plasm-ph physics.acc-ph\n",
      "math.RT math.NT\n",
      "quant-ph cond-mat.stat-mech hep-th math-ph math.MP\n",
      "physics.chem-ph physics.atom-ph\n",
      "astro-ph.EP\n",
      "math.FA math.OA math.SP\n",
      "physics.flu-dyn stat.AP\n",
      "cs.NI cs.CR\n",
      "physics.acc-ph\n",
      "stat.ML cs.CV cs.DS cs.LG math.ST stat.TH\n",
      "math-ph math.MP nlin.PS\n",
      "math-ph cond-mat.mes-hall math.MP\n",
      "math.NT math.AG\n",
      "math.AP gr-qc math-ph math.DG math.MP\n",
      "q-bio.BM\n",
      "cs.LG\n",
      "astro-ph.IM physics.ins-det\n",
      "cs.SY cs.CR\n",
      "math.DG math.CV\n",
      "math.GT math.NT\n",
      "q-fin.MF math.PR\n",
      "cs.DS q-bio.QM\n",
      "math-ph cond-mat.stat-mech math.MP math.PR math.ST stat.TH\n",
      "q-bio.NC physics.bio-ph\n",
      "quant-ph cond-mat.quant-gas cond-mat.str-el hep-th\n",
      "math.FA\n",
      "cond-mat.stat-mech math-ph math.MP math.PR\n",
      "cond-mat.dis-nn cond-mat.stat-mech nlin.CD\n",
      "gr-qc cond-mat.stat-mech hep-th math-ph math.MP\n",
      "math.AT math.KT\n",
      "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH\n",
      "hep-ph hep-ex nucl-ex\n",
      "nucl-th hep-lat hep-th\n",
      "astro-ph.IM\n",
      "math.NA cs.NA cs.SY eess.SY\n",
      "quant-ph cond-mat.other cs.IT math.IT\n",
      "quant-ph physics.hist-ph\n",
      "quant-ph math-ph math.FA math.MP math.OA\n",
      "math.CT math.RA\n",
      "cond-mat.supr-con physics.optics\n",
      "physics.app-ph\n",
      "cs.NE\n",
      "cs.GR cs.SI\n",
      "math.ST math.PR stat.ME stat.TH\n",
      "cs.FL math.GR\n",
      "physics.ins-det cond-mat.mes-hall physics.app-ph physics.optics\n",
      "hep-th cond-mat.str-el\n",
      "nucl-th hep-ph nucl-ex\n",
      "physics.comp-ph\n",
      "math.MG\n",
      "physics.soc-ph nlin.AO\n",
      "cs.CE stat.CO stat.ME\n",
      "hep-ph hep-ex nucl-ex nucl-th\n",
      "cs.SE\n",
      "astro-ph.GA astro-ph.HE\n",
      "math.OC cs.NA math.NA\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mes-hall cond-mat.str-el quant-ph\n",
      "stat.ML cs.AI\n",
      "quant-ph physics.optics\n",
      "math.QA hep-th math-ph math.MP\n",
      "astro-ph.GA astro-ph.CO astro-ph.IM\n",
      "gr-qc astro-ph.HE\n",
      "cond-mat.quant-gas cond-mat.supr-con hep-lat nucl-th\n",
      "cs.DS cs.DM cs.NE math.PR\n",
      "cond-mat.str-el cond-mat.stat-mech\n",
      "cs.SY math.OC\n",
      "cs.IT cs.DC math.IT\n",
      "stat.ME math.ST stat.TH\n",
      "cond-mat.str-el hep-th\n",
      "physics.flu-dyn nlin.CD physics.ao-ph\n",
      "cs.MS\n",
      "quant-ph cond-mat.str-el hep-th\n",
      "cond-mat.dis-nn nlin.AO physics.soc-ph\n",
      "stat.ME cs.LG stat.AP stat.ML\n",
      "physics.comp-ph cs.CE math.NA\n",
      "physics.hist-ph\n",
      "math.ST math.AC math.PR stat.TH\n",
      "cs.HC cs.AI cs.CY stat.ML\n",
      "math.OC cs.CC\n",
      "astro-ph.IM astro-ph.CO hep-ex physics.ins-det\n",
      "q-fin.TR\n",
      "astro-ph.SR\n",
      "math.AG math.CV\n",
      "math.RT math.LO\n",
      "eess.SP\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech\n",
      "eess.IV cs.CV eess.SP\n",
      "physics.bio-ph cond-mat.soft q-bio.CB\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall\n",
      "quant-ph hep-ex physics.atom-ph\n",
      "math.AP math.DS math.FA\n",
      "cond-mat.str-el cond-mat.other\n",
      "math-ph math.MP nlin.SI\n",
      "math.QA math.CT math.RT\n",
      "stat.ML cond-mat.dis-nn cond-mat.stat-mech physics.data-an\n",
      "quant-ph cs.ET\n",
      "math.RT math.AC math.RA\n",
      "stat.ML cs.LG q-bio.NC\n",
      "stat.ME stat.AP stat.CO stat.ML\n",
      "math.CO cs.SI physics.data-an physics.soc-ph\n",
      "cond-mat.str-el quant-ph\n",
      "math.CO cs.DM\n",
      "cs.MA cs.SY math.OC\n",
      "physics.bio-ph q-bio.CB\n",
      "physics.soc-ph cond-mat.stat-mech cs.GT econ.EM math-ph math.MP q-bio.PE\n",
      "math.DS math.AT math.DG\n",
      "math.GR cs.FL\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con\n",
      "quant-ph cond-mat.mes-hall physics.atom-ph physics.optics\n",
      "math.ST stat.AP stat.TH\n",
      "physics.soc-ph cs.CY cs.SI\n",
      "astro-ph.HE\n",
      "cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "cs.IR cs.SI\n",
      "cond-mat.soft cond-mat.mtrl-sci\n",
      "quant-ph cs.DS cs.LG stat.ML\n",
      "cs.GT math.CO math.PR\n",
      "math.GR math.CO math.PR\n",
      "q-bio.MN\n",
      "cs.DS cs.DM math.CO\n",
      "cond-mat.quant-gas cond-mat.str-el quant-ph\n",
      "quant-ph cond-mat.mes-hall gr-qc\n",
      "math.CO math.CA\n",
      "math.AT\n",
      "physics.soc-ph math.HO stat.AP\n",
      "cs.CE\n",
      "cs.CC cs.DM\n",
      "q-fin.RM cs.CE\n",
      "math.OA quant-ph\n",
      "math.CT math.KT\n",
      "cs.GT econ.TH\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mes-hall\n",
      "physics.data-an cs.IT math.IT\n",
      "quant-ph cs.CC\n",
      "math.CO math.AC\n",
      "math.RT math.LO math.RA\n",
      "cs.NE cs.CV\n",
      "quant-ph cs.IT math-ph math.FA math.IT math.MP\n",
      "stat.ML cs.LG\n",
      "astro-ph.HE nucl-th\n",
      "cs.LG stat.ML\n",
      "cs.DS cs.CC\n",
      "cs.IR cs.AI cs.MM\n",
      "q-bio.PE math.RA math.ST stat.TH\n",
      "math.GT math.AG math.DS math.RT\n",
      "physics.bio-ph q-bio.PE\n",
      "cond-mat.stat-mech hep-th math-ph math.MP\n",
      "math.GT math.CO math.QA\n",
      "cs.DB\n",
      "math-ph hep-th math.AP math.DG math.MP\n",
      "quant-ph cond-mat.str-el\n",
      "quant-ph math.CO\n",
      "cond-mat.quant-gas cond-mat.mes-hall quant-ph\n",
      "math.GT math.AT math.RT\n",
      "hep-th cond-mat.str-el math-ph math.MP quant-ph\n",
      "physics.soc-ph cond-mat.stat-mech physics.app-ph\n",
      "math.GR math.CO\n",
      "quant-ph cond-mat.other physics.optics\n",
      "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP\n",
      "cs.CC\n",
      "quant-ph math.OA\n",
      "cs.LO cs.CC\n",
      "stat.AP\n",
      "cs.AI\n",
      "math.DG gr-qc\n",
      "quant-ph cs.LG\n",
      "physics.ins-det cond-mat.supr-con eess.IV eess.SP physics.app-ph\n",
      "math.AT math.CT\n",
      "hep-th cond-mat.stat-mech math-ph math.MP\n",
      "hep-lat hep-ph\n",
      "cs.GT cs.NI\n",
      "physics.data-an hep-ex physics.comp-ph\n",
      "q-bio.OT\n",
      "hep-th quant-ph\n",
      "math.MG math.CA math.CV\n",
      "hep-th math.DG\n",
      "quant-ph gr-qc\n",
      "quant-ph math.RT\n",
      "quant-ph hep-th math-ph math.MP\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.supr-con\n",
      "math-ph math.AP math.MP\n",
      "math.AP math-ph math.MP math.PR\n",
      "math.MG math.CO\n",
      "physics.app-ph cond-mat.mtrl-sci physics.optics\n",
      "q-bio.PE cond-mat.dis-nn cond-mat.stat-mech physics.bio-ph\n",
      "physics.soc-ph cs.SI\n",
      "cs.CR cs.AI cs.NI\n",
      "math.RT math-ph math.MP\n",
      "math.CO math.PR\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.quant-gas\n",
      "astro-ph.CO astro-ph.GA hep-ph hep-th\n",
      "math.QA math.AG math.RA\n",
      "cond-mat.quant-gas physics.optics\n",
      "gr-qc physics.flu-dyn\n",
      "math.AG math.DS\n",
      "cond-mat.str-el hep-th math-ph math.MP math.QA quant-ph\n",
      "cond-mat.str-el math-ph math.MP\n",
      "physics.med-ph physics.bio-ph physics.optics\n",
      "math.DS astro-ph.EP math-ph math.MP\n",
      "cond-mat.dis-nn cond-mat.soft\n",
      "math.AC math.NT\n",
      "cs.PL\n",
      "physics.optics cond-mat.mes-hall physics.app-ph\n",
      "hep-ex physics.ins-det\n",
      "quant-ph cond-mat.mes-hall cs.CR cs.ET\n",
      "quant-ph cond-mat.dis-nn cs.AI cs.LG\n",
      "math.RA math.KT math.OA\n",
      "physics.atm-clus\n",
      "hep-ph astro-ph.CO astro-ph.HE\n",
      "cs.DS cs.DM\n",
      "math-ph hep-th math.AG math.CO math.MP nlin.SI\n",
      "cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph quant-ph\n",
      "math.CA math-ph math.MP math.SP\n",
      "quant-ph cond-mat.quant-gas\n",
      "physics.geo-ph\n",
      "cond-mat.str-el cond-mat.quant-gas\n",
      "cs.GT cond-mat.stat-mech physics.soc-ph\n",
      "cond-mat.mtrl-sci physics.comp-ph\n",
      "quant-ph cond-mat.str-el hep-lat hep-th\n",
      "econ.EM\n",
      "cond-mat.mes-hall quant-ph\n",
      "hep-th hep-ph\n",
      "hep-th cond-mat.stat-mech gr-qc\n",
      "cs.CL cs.CY\n",
      "physics.ed-ph physics.optics\n",
      "math.DS math-ph math.FA math.MP math.SP nlin.CD\n",
      "cond-mat.mtrl-sci physics.app-ph physics.chem-ph\n",
      "cond-mat.dis-nn physics.optics quant-ph\n",
      "math.CO math.AG math.AT\n",
      "nucl-th astro-ph.HE hep-ph\n",
      "cond-mat.str-el cond-mat.mes-hall math-ph math.MP\n",
      "nlin.AO cs.MA nlin.CD\n",
      "physics.space-ph\n",
      "math.AG math.DG\n",
      "physics.comp-ph math-ph math.MP math.NA\n",
      "math.AP math-ph math.MP math.SP\n",
      "math.AP math.DG math.FA\n",
      "cs.IT cs.DC cs.LG math.IT\n",
      "math.OC math.NA\n",
      "hep-lat hep-ex hep-ph\n",
      "cs.ET cs.NE quant-ph\n",
      "cond-mat.str-el physics.comp-ph\n",
      "cs.SY math-ph math.MP math.OC quant-ph\n",
      "cond-mat.str-el cond-mat.stat-mech cond-mat.supr-con physics.comp-ph\n",
      "cond-mat.supr-con quant-ph\n",
      "physics.flu-dyn astro-ph.CO gr-qc physics.comp-ph\n",
      "quant-ph cs.DS\n",
      "hep-th math-ph math.MP math.QA\n",
      "nlin.CD math.DS\n",
      "math-ph math.MP math.PR\n",
      "hep-lat hep-ex hep-ph nucl-ex nucl-th\n",
      "physics.acc-ph physics.ins-det physics.med-ph\n",
      "stat.AP cs.AI cs.GT\n",
      "quant-ph hep-th\n",
      "cs.CE cs.NA math.NA physics.comp-ph physics.optics\n",
      "quant-ph cond-mat.quant-gas physics.atom-ph\n",
      "math.AG math.NT\n",
      "hep-th hep-ph nucl-th\n",
      "quant-ph gr-qc physics.space-ph\n",
      "math.DS math.OC nlin.CD\n",
      "hep-ph cs.LG hep-ex\n",
      "cond-mat.mes-hall hep-th\n",
      "quant-ph cs.LG physics.data-an\n",
      "math.RA math.CT math.GN\n",
      "physics.atom-ph cond-mat.quant-gas quant-ph\n",
      "math.OC cs.LG\n",
      "math.NA math.DG\n",
      "physics.ao-ph\n",
      "econ.TH\n",
      "physics.data-an cs.CV cs.LG hep-ex\n",
      "stat.ML cs.LG econ.EM math.OC math.ST stat.TH\n",
      "math.PR cond-mat.stat-mech math-ph math.MP\n",
      "cond-mat.dis-nn cs.LG stat.ML\n",
      "quant-ph cond-mat.other cs.CC\n",
      "hep-th math.AG\n",
      "math.DS math.AT math.PR\n",
      "math-ph hep-th math.MP math.QA math.SG\n",
      "astro-ph.CO gr-qc hep-ph hep-th\n",
      "physics.ed-ph physics.pop-ph\n",
      "math.AP math-ph math.MP math.OC math.PR\n",
      "quant-ph cond-mat.quant-gas physics.atom-ph physics.optics\n",
      "stat.AP cs.CE\n",
      "cond-mat.str-el cond-mat.mes-hall math-ph math.MP quant-ph\n",
      "physics.flu-dyn physics.comp-ph\n",
      "math.AP cond-mat.mtrl-sci math-ph math.MP\n",
      "cs.IT cs.SY eess.SY math.IT\n",
      "quant-ph cond-mat.stat-mech hep-th\n",
      "physics.plasm-ph astro-ph.GA astro-ph.HE\n",
      "math.OC cs.GT cs.SY\n",
      "physics.app-ph physics.optics\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph\n",
      "physics.bio-ph cond-mat.stat-mech q-bio.BM\n",
      "cond-mat.dis-nn\n",
      "physics.ins-det physics.med-ph\n",
      "cs.SI cs.LG physics.soc-ph\n",
      "math.SG math.AG\n",
      "physics.plasm-ph astro-ph.CO astro-ph.HE physics.flu-dyn\n",
      "math.OA math.GR\n",
      "physics.comp-ph cond-mat.mtrl-sci cs.LG\n",
      "cond-mat.mes-hall physics.optics\n",
      "physics.optics physics.app-ph\n",
      "cond-mat.stat-mech quant-ph\n",
      "math.NA cs.LG\n",
      "math.HO q-bio.OT\n",
      "math.OC math.ST q-bio.MN stat.TH\n",
      "math.GT math.QA\n",
      "q-bio.QM q-bio.NC\n",
      "math.DS cs.SY eess.SP nlin.CD\n",
      "math.CO cs.IT math.IT\n",
      "quant-ph cond-mat.stat-mech nlin.CD\n",
      "math.QA math.AG\n",
      "math.GT math.AT math.CO math.DG math.GR\n",
      "hep-ph hep-ex\n",
      "math.OA math.LO quant-ph\n",
      "cs.GT cs.CC cs.MA\n",
      "math.AG quant-ph\n",
      "math.AT math.RT\n",
      "math.PR cs.LO math.CT math.FA\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech hep-th\n",
      "econ.EM econ.GN q-fin.EC\n",
      "nlin.PS nlin.SI\n",
      "math-ph math.DS math.MP\n",
      "econ.EM stat.ME\n",
      "math.MG math-ph math.AP math.MP\n",
      "hep-th math.QA\n",
      "math.DG math.GT\n",
      "quant-ph cs.DS math.OC\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el quant-ph\n",
      "physics.soc-ph cs.CY cs.MA\n",
      "physics.ins-det physics.plasm-ph\n",
      "physics.chem-ph\n",
      "math-ph cond-mat.quant-gas cond-mat.str-el math.MP\n",
      "math.NT math.AG math.PR\n",
      "q-fin.ST\n",
      "quant-ph cs.SY\n",
      "math.OC cs.CV math.ST stat.TH\n",
      "econ.EM econ.GN q-fin.EC stat.ME\n",
      "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.str-el\n",
      "cond-mat.quant-gas cond-mat.mes-hall\n",
      "stat.CO cond-mat.stat-mech math.PR\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.quant-gas\n",
      "physics.hist-ph hep-th physics.class-ph\n",
      "math.CO math.QA\n",
      "quant-ph cond-mat.mes-hall cond-mat.supr-con\n",
      "gr-qc astro-ph.CO hep-th\n",
      "cs.LG cs.CR stat.ML\n",
      "cond-mat.supr-con cond-mat.mes-hall\n",
      "math.GT math.CV\n",
      "quant-ph cond-mat.quant-gas cond-mat.str-el physics.atom-ph\n",
      "cond-mat.soft cond-mat.stat-mech q-bio.CB\n",
      "physics.class-ph cond-mat.mes-hall math-ph math.MP physics.optics quant-ph\n",
      "math.OC cs.LG cs.NA\n",
      "stat.ME stat.AP\n",
      "quant-ph cond-mat.mes-hall physics.optics\n",
      "quant-ph physics.acc-ph\n",
      "cs.NI\n",
      "math.AG math.RT math.SG\n",
      "cs.LG cs.NA math.NA stat.ML\n",
      "cs.DM math.CO\n",
      "hep-th astro-ph.HE gr-qc\n",
      "astro-ph.IM physics.hist-ph\n",
      "astro-ph.GA astro-ph.CO\n",
      "cs.CV physics.bio-ph physics.med-ph q-bio.QM\n",
      "cs.IR cs.CL\n",
      "quant-ph cs.IT math-ph math.IT math.MP\n",
      "cond-mat.mes-hall physics.class-ph physics.optics\n",
      "cs.LG cs.RO stat.ML\n",
      "quant-ph math-ph math.FA math.MP\n",
      "quant-ph cond-mat.mes-hall cond-mat.quant-gas cond-mat.stat-mech\n",
      "hep-th cond-mat.str-el hep-lat hep-ph quant-ph\n",
      "math.ST stat.CO stat.ME stat.ML stat.TH\n",
      "cs.SY cond-mat.stat-mech nlin.AO physics.soc-ph\n",
      "quant-ph cond-mat.supr-con\n",
      "hep-th cond-mat.str-el gr-qc\n",
      "physics.optics physics.bio-ph\n",
      "math.AP math.DS math.OC\n",
      "hep-th math-ph math.MP\n",
      "physics.comp-ph cond-mat.str-el\n",
      "q-bio.QM\n",
      "nlin.SI math-ph math.AG math.MP\n",
      "hep-th math.RT\n",
      "cond-mat.str-el cond-mat.supr-con hep-th\n",
      "math.KT\n",
      "quant-ph cond-mat.other hep-th math-ph math.MP\n",
      "q-bio.SC cond-mat.stat-mech\n",
      "gr-qc hep-ex hep-th\n",
      "cond-mat.mes-hall gr-qc hep-th\n",
      "physics.app-ph quant-ph\n",
      "hep-ph nucl-ex nucl-th\n",
      "physics.bio-ph cond-mat.soft q-bio.BM\n",
      "cond-mat.mes-hall hep-ph\n",
      "quant-ph cond-mat.supr-con physics.acc-ph\n",
      "cond-mat.other\n",
      "math.OC cs.SY\n",
      "physics.comp-ph cond-mat.mtrl-sci physics.data-an\n",
      "stat.AP econ.GN q-fin.EC\n",
      "cs.IT cs.DM math.IT\n",
      "quant-ph cond-mat.str-el hep-th math-ph math.MP\n",
      "physics.ins-det\n",
      "math.FA math.SP\n",
      "cond-mat.quant-gas gr-qc hep-th\n",
      "math.AT stat.ML\n",
      "cs.LG cs.CE cs.NA math.AP math.NA stat.ML\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
      "quant-ph cond-mat.mes-hall physics.app-ph physics.ins-det\n",
      "quant-ph cond-mat.dis-nn physics.chem-ph physics.optics\n",
      "cs.SY cs.AI math.OC\n",
      "cs.DC\n",
      "physics.atom-ph gr-qc quant-ph\n",
      "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
      "cond-mat.stat-mech math-ph math.MP\n",
      "math.GR math.KT\n",
      "hep-ph hep-th\n",
      "math-ph math.MP math.OA math.QA\n",
      "cond-mat.stat-mech cond-mat.dis-nn cs.LG hep-th\n",
      "physics.soc-ph q-fin.GN\n",
      "cond-mat.dis-nn cs.ET\n",
      "econ.GN q-fin.EC\n",
      "math.CT math.AG\n",
      "astro-ph.CO hep-th\n",
      "astro-ph.HE hep-ex hep-ph\n",
      "cs.CV cs.LG stat.ML\n",
      "quant-ph math.PR\n",
      "math.AP math.OC\n",
      "hep-th cond-mat.quant-gas cond-mat.str-el\n",
      "physics.soc-ph cond-mat.stat-mech cs.MA cs.SI nlin.AO\n",
      "hep-ph hep-ex physics.comp-ph stat.ML\n",
      "nucl-ex astro-ph.HE nucl-th\n",
      "astro-ph.CO gr-qc hep-ph\n",
      "math.AP math-ph math.CA math.FA math.MP\n",
      "quant-ph physics.atom-ph physics.optics\n",
      "math.GT math.AG math.DS\n",
      "cs.HC cs.AI\n",
      "cond-mat.mes-hall physics.atom-ph physics.optics quant-ph\n",
      "quant-ph cond-mat.quant-gas cond-mat.stat-mech\n",
      "cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
      "math.CV math.AG\n",
      "physics.data-an\n",
      "physics.gen-ph gr-qc hep-th\n",
      "cs.CL cs.AI cs.LG cs.NE\n",
      "eess.IV\n",
      "physics.plasm-ph physics.acc-ph physics.app-ph\n",
      "cond-mat.str-el hep-th math-ph math.MP\n",
      "stat.AP cs.CY\n",
      "math.PR cs.LG cs.NA math.NA\n",
      "hep-lat cond-mat.stat-mech\n",
      "physics.optics cond-mat.mes-hall\n",
      "cs.GT cs.MA math.PR\n",
      "physics.optics physics.chem-ph\n",
      "math.QA math.OA\n",
      "physics.comp-ph quant-ph\n",
      "cond-mat.mes-hall cond-mat.quant-gas\n",
      "gr-qc math-ph math.AP math.DG math.MP\n",
      "hep-th gr-qc quant-ph\n",
      "stat.ML cs.LG math.OC\n",
      "cs.IT math.IT math.PR math.ST stat.TH\n",
      "cs.AR cs.AI cs.CV cs.LG\n",
      "quant-ph cond-mat.stat-mech cond-mat.str-el\n",
      "physics.ao-ph cs.CV cs.LG\n",
      "hep-th math.GR\n",
      "cond-mat.mes-hall cond-mat.str-el\n",
      "q-bio.TO physics.bio-ph q-bio.CB\n",
      "stat.ML cs.LG math.ST stat.ME stat.TH\n",
      "quant-ph cs.CR cs.IT math.IT\n",
      "math.NA cs.NA math.PR\n",
      "q-bio.NC\n",
      "stat.CO astro-ph.IM gr-qc stat.ME\n",
      "cond-mat.stat-mech cond-mat.quant-gas quant-ph\n",
      "math.AG cs.CC\n",
      "cond-mat.mtrl-sci physics.class-ph\n",
      "math.OC physics.comp-ph\n",
      "cs.ET\n",
      "math-ph math.DS math.MP math.PR\n",
      "cond-mat.mtrl-sci physics.optics\n",
      "astro-ph.HE astro-ph.CO\n",
      "quant-ph cond-mat.stat-mech physics.chem-ph\n",
      "physics.optics physics.med-ph\n",
      "physics.data-an stat.ML\n",
      "hep-lat hep-ph nucl-ex nucl-th\n",
      "eess.SP nlin.CD\n",
      "hep-ph astro-ph.HE hep-th nucl-th\n",
      "stat.CO cs.NA math.NA stat.ME\n",
      "quant-ph cond-mat.mes-hall cs.AI cs.ET cs.LG\n",
      "physics.space-ph astro-ph.EP\n",
      "gr-qc cond-mat.quant-gas physics.flu-dyn\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.str-el\n",
      "math-ph math.MP math.OA quant-ph\n",
      "q-fin.GN cs.GT cs.IT math.IT math.ST stat.TH\n",
      "gr-qc cond-mat.other hep-th\n",
      "quant-ph cond-mat.dis-nn cond-mat.str-el\n",
      "cs.DS math.CO\n",
      "gr-qc hep-ph hep-th\n",
      "cs.GT cs.LG cs.SI stat.ML\n",
      "hep-ph astro-ph.CO hep-th\n",
      "hep-ph hep-ex nucl-ex nucl-th physics.data-an\n",
      "cs.LG math.CA\n",
      "cond-mat.stat-mech cond-mat.soft\n",
      "math.NA cs.NA stat.CO\n",
      "q-bio.PE physics.bio-ph q-bio.CB\n",
      "q-bio.QM physics.bio-ph\n",
      "math.OC cs.CE\n",
      "math.QA math-ph math.CT math.MP\n",
      "physics.soc-ph math.OC\n",
      "cond-mat.str-el hep-lat hep-th\n",
      "cond-mat.stat-mech cond-mat.soft math.ST physics.data-an stat.TH\n",
      "hep-th cond-mat.stat-mech\n",
      "gr-qc hep-th math-ph math.DG math.MP\n",
      "math.NT math.DS math.MG\n",
      "math.GR math.GT math.KT\n",
      "physics.comp-ph cs.NA math.NA physics.optics\n",
      "math-ph cond-mat.dis-nn math.MP\n",
      "cs.SI cs.LG stat.ML\n",
      "cs.NA math.FA math.NA\n",
      "hep-th gr-qc hep-ph\n",
      "hep-th astro-ph.HE cond-mat.str-el gr-qc\n",
      "math.QA cond-mat.str-el hep-th\n",
      "quant-ph cond-mat.other cond-mat.quant-gas cond-mat.str-el\n",
      "q-bio.PE physics.bio-ph physics.soc-ph\n",
      "math-ph hep-th math.CO math.MP math.RA\n",
      "physics.chem-ph cond-mat.str-el\n",
      "physics.app-ph cond-mat.supr-con\n",
      "quant-ph cond-mat.mes-hall physics.atom-ph\n",
      "math.CV math.CA\n",
      "hep-th cond-mat.supr-con\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el quant-ph\n",
      "nucl-th astro-ph.HE nucl-ex\n",
      "math.NA cs.NA math.AP math.OC\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph\n",
      "physics.ins-det hep-ex nucl-ex\n",
      "math.CA cs.CV\n",
      "math.AC math.AG\n",
      "cs.CG cs.CV math.NA physics.med-ph q-bio.QM\n",
      "quant-ph cond-mat.mes-hall cond-mat.quant-gas\n",
      "math.GR math.RT\n",
      "quant-ph physics.atom-ph\n",
      "math.NA cs.NA physics.class-ph\n",
      "stat.ML cs.LG cs.SI stat.AP\n",
      "physics.ed-ph cond-mat.stat-mech\n",
      "math.RT math.CO\n",
      "q-bio.CB q-bio.TO\n",
      "nlin.AO\n",
      "physics.soc-ph cond-mat.stat-mech\n",
      "eess.SP cs.HC cs.NE\n",
      "nlin.CD\n",
      "cond-mat.mes-hall physics.app-ph\n",
      "physics.soc-ph cs.SI nlin.AO\n",
      "hep-lat hep-th math-ph math.MP\n",
      "hep-th cond-mat.stat-mech quant-ph\n",
      "physics.chem-ph physics.comp-ph\n",
      "hep-th nlin.SI\n",
      "math.MG cs.CG\n",
      "gr-qc math.DG\n",
      "cs.IT cs.AI math.IT stat.ML\n",
      "astro-ph.CO astro-ph.HE hep-ph\n",
      "nlin.PS\n",
      "cond-mat.mtrl-sci cond-mat.soft\n",
      "math.CA math.FA\n",
      "hep-lat hep-th quant-ph\n",
      "q-bio.QM cond-mat.stat-mech physics.bio-ph\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph\n",
      "cs.DL cs.SI\n",
      "hep-ph hep-ex nucl-th\n",
      "cs.DC cs.LG q-bio.QM\n",
      "quant-ph math.LO\n",
      "physics.chem-ph cond-mat.mtrl-sci\n",
      "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
      "hep-ph astro-ph.CO hep-ex nucl-ex physics.data-an\n",
      "math.GR math.GT\n",
      "math.OC math.DS math.SP\n",
      "cs.DS cs.DM math.OC\n",
      "cond-mat.soft cond-mat.stat-mech physics.flu-dyn\n",
      "quant-ph cond-mat.dis-nn\n",
      "quant-ph cond-mat.dis-nn cs.AI cs.DS\n",
      "physics.soc-ph gr-qc\n",
      "gr-qc astro-ph.CO astro-ph.GA\n",
      "hep-ph astro-ph.HE\n",
      "math.QA math.GR\n",
      "cs.LG cs.AI cs.MA stat.ML\n",
      "astro-ph.CO hep-ex hep-ph\n",
      "math-ph math.DS math.FA math.MP\n",
      "hep-th cond-mat.str-el math-ph math.AT math.GT math.MP\n",
      "hep-th cond-mat.str-el hep-lat hep-ph math.AT\n",
      "astro-ph.GA astro-ph.CO hep-ph\n",
      "cond-mat.mes-hall cond-mat.supr-con physics.optics\n",
      "astro-ph.SR physics.flu-dyn\n",
      "physics.soc-ph cond-mat.soft\n",
      "cond-mat.mtrl-sci cond-mat.other physics.optics\n",
      "cond-mat.soft cond-mat.stat-mech physics.bio-ph\n",
      "cs.DM\n",
      "math.FA math.AP\n",
      "cs.CV cs.CG q-bio.QM\n",
      "cs.SI cs.LG\n",
      "math.DG math.DS\n",
      "cond-mat.mes-hall cs.LG\n",
      "gr-qc astro-ph.IM\n",
      "math.OC cs.SY eess.SY q-fin.MF\n",
      "cond-mat.mes-hall physics.app-ph quant-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.supr-con\n",
      "cs.NE cs.AI cs.LG\n",
      "cond-mat.mtrl-sci physics.geo-ph\n",
      "math.NA math.OC\n",
      "stat.CO cs.LG stat.ML\n",
      "cond-mat.mes-hall physics.optics quant-ph\n",
      "cs.NE hep-lat physics.comp-ph\n",
      "hep-lat hep-ph hep-th\n",
      "physics.soc-ph math.CO math.OC\n",
      "physics.data-an hep-ph physics.comp-ph\n",
      "math-ph math.CA math.MP quant-ph\n",
      "hep-ex hep-ph\n",
      "physics.plasm-ph hep-ph hep-th\n",
      "cs.NA physics.comp-ph\n",
      "cs.MM\n",
      "astro-ph.SR astro-ph.HE physics.flu-dyn\n",
      "math.AG math.CO\n",
      "cond-mat.str-el hep-lat hep-th quant-ph\n",
      "math.DG math.AP\n",
      "math.FA math.AP math.CA\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.supr-con nlin.SI\n",
      "cs.AI cs.CE\n",
      "nucl-ex hep-ex nucl-th\n",
      "cond-mat.mes-hall cond-mat.str-el hep-th\n",
      "stat.ML cs.LG math.ST stat.TH\n",
      "astro-ph.SR astro-ph.IM\n",
      "cs.DC cs.CR\n",
      "hep-ph hep-lat\n",
      "q-bio.NC q-bio.QM\n",
      "cond-mat.mtrl-sci cond-mat.supr-con\n",
      "cond-mat.quant-gas nlin.PS\n",
      "cs.CV cs.AI\n",
      "math.QA cond-mat.str-el math-ph math.MP quant-ph\n",
      "stat.ML cs.LG eess.SP\n",
      "cond-mat.str-el cond-mat.mtrl-sci quant-ph\n",
      "math.QA math.GT\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el\n",
      "cs.GT cs.MA\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.optics\n",
      "gr-qc astro-ph.HE hep-th\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.supr-con\n",
      "cond-mat.quant-gas physics.atom-ph\n",
      "physics.plasm-ph nucl-th\n",
      "math.QA math.RA math.RT\n",
      "cs.LG cs.CV stat.ML\n",
      "math.PR math.NA\n",
      "physics.ins-det cond-mat.supr-con\n",
      "physics.bio-ph cond-mat.soft q-bio.SC\n",
      "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
      "math-ph math.CO math.MP math.PR nlin.SI\n",
      "cs.SD eess.AS\n",
      "cs.NA math.NA\n",
      "nlin.AO cond-mat.soft\n",
      "cs.CY cs.LG stat.ML\n",
      "cond-mat.quant-gas physics.atm-clus quant-ph\n",
      "math-ph cond-mat.quant-gas math.MP\n",
      "cs.LG cs.AI cs.RO stat.ML\n",
      "quant-ph physics.atom-ph physics.data-an\n",
      "cs.LG stat.AP stat.ML\n",
      "cs.CV cs.LG\n",
      "cond-mat.mes-hall cond-mat.dis-nn\n",
      "hep-ex\n",
      "cs.MM cs.HC\n",
      "cs.DL cs.CY\n",
      "cond-mat.mes-hall cond-mat.supr-con\n",
      "math.CO math.AG\n",
      "q-fin.TR math.OC q-fin.MF\n",
      "astro-ph.CO gr-qc hep-th\n",
      "stat.ME stat.ML\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.str-el\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.mes-hall\n",
      "physics.app-ph cond-mat.mtrl-sci quant-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
      "cs.LG physics.chem-ph stat.ML\n",
      "cs.CL cs.AI cs.LG stat.ML\n",
      "cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
      "physics.atom-ph cond-mat.other\n",
      "cond-mat.stat-mech physics.comp-ph\n",
      "math.CA math.SP\n",
      "math.FA math.CA math.PR\n",
      "physics.bio-ph cond-mat.soft cond-mat.stat-mech\n",
      "cond-mat.str-el cond-mat.stat-mech math-ph math.MP\n",
      "physics.app-ph cond-mat.other cs.ET\n",
      "physics.optics physics.atom-ph physics.ins-det quant-ph\n",
      "physics.acc-ph hep-ex physics.ins-det\n",
      "math.QA math.RA\n",
      "quant-ph cs.LO math.CO\n",
      "stat.ME stat.CO\n",
      "q-fin.CP\n",
      "physics.soc-ph cond-mat.dis-nn\n",
      "cs.LG physics.app-ph stat.ML\n",
      "math.GN\n",
      "cond-mat.dis-nn cond-mat.str-el cs.LG\n",
      "math.CV math.CO\n",
      "nucl-ex hep-ex\n",
      "quant-ph math-ph math.MP math.PR\n",
      "cs.SC\n",
      "quant-ph gr-qc math-ph math.MP\n",
      "physics.app-ph cond-mat.mes-hall\n",
      "nlin.AO math.DS nlin.PS physics.bio-ph\n",
      "physics.med-ph\n",
      "math.DG math.AP math.MG math.OC\n",
      "cond-mat.stat-mech nlin.CD quant-ph\n",
      "math-ph cond-mat.stat-mech math.MP math.PR\n",
      "physics.app-ph cond-mat.mtrl-sci physics.comp-ph\n",
      "quant-ph cond-mat.other\n",
      "stat.OT quant-ph\n",
      "physics.app-ph nlin.CD\n",
      "cond-mat.quant-gas nucl-th physics.atom-ph\n",
      "nucl-ex nucl-th\n",
      "cond-mat.mes-hall cond-mat.other quant-ph\n",
      "astro-ph.SR astro-ph.GA astro-ph.HE\n",
      "math.CV math.PR math.RA\n",
      "physics.data-an nucl-ex\n",
      "hep-ph astro-ph.CO physics.atom-ph\n",
      "gr-qc astro-ph.HE astro-ph.IM hep-ph\n",
      "hep-ex cs.CV cs.LG physics.data-an\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.stat-mech\n",
      "hep-ph cs.CV hep-ex\n",
      "cond-mat.soft physics.optics\n",
      "cs.GR cs.CG\n",
      "quant-ph cond-mat.quant-gas cs.LG\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech\n",
      "cond-mat.quant-gas cond-mat.stat-mech hep-lat quant-ph\n",
      "cs.RO cs.AI cs.CL cs.CV cs.LG\n",
      "cond-mat.stat-mech math-ph math.MP physics.class-ph\n",
      "gr-qc astro-ph.CO\n",
      "physics.flu-dyn math.AP math.DS\n",
      "cond-mat.mes-hall math-ph math.MP physics.ao-ph physics.flu-dyn\n",
      "math.ST cs.NA math.NA stat.ML stat.TH\n",
      "physics.geo-ph cs.LG eess.SP\n",
      "quant-ph physics.app-ph physics.optics\n",
      "quant-ph cs.GT cs.LG physics.comp-ph\n",
      "astro-ph.CO astro-ph.GA\n",
      "quant-ph physics.chem-ph\n",
      "math.OC econ.GN q-fin.EC\n",
      "physics.atom-ph cond-mat.quant-gas physics.optics quant-ph\n",
      "quant-ph cond-mat.stat-mech cond-mat.str-el hep-th\n",
      "math.ST stat.ML stat.TH\n",
      "quant-ph cond-mat.stat-mech cond-mat.str-el hep-lat math-ph math.MP\n",
      "math.AG math.GN\n",
      "physics.ins-det astro-ph.IM nucl-ex\n",
      "cs.AI cs.CL cs.CV\n",
      "hep-ph astro-ph.IM hep-ex quant-ph\n",
      "cond-mat.str-el physics.data-an\n",
      "cond-mat.dis-nn cond-mat.stat-mech hep-th\n",
      "nlin.AO cond-mat.dis-nn\n",
      "cs.NI cs.SI math.OC\n",
      "physics.atom-ph physics.ins-det physics.optics\n",
      "cond-mat.mes-hall physics.app-ph physics.ins-det\n",
      "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el hep-lat quant-ph\n",
      "cond-mat.dis-nn cond-mat.soft cond-mat.stat-mech\n",
      "quant-ph math.CT\n",
      "math.OC cs.SY math.DS\n",
      "math.AP math-ph math.CA math.MP math.SP\n",
      "stat.ML cs.LG stat.CO\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.optics quant-ph\n",
      "eess.IV cs.CV\n",
      "math-ph hep-th math.CT math.DG math.MP\n",
      "cs.AI cs.CY cs.LG stat.ML\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.stat-mech quant-ph\n",
      "cs.CR cs.AR\n",
      "math.CO math.RT\n",
      "eess.SP cs.NE physics.optics\n",
      "cond-mat.supr-con cond-mat.dis-nn cond-mat.quant-gas quant-ph\n",
      "gr-qc astro-ph.CO hep-ph\n",
      "math.CO math.HO\n",
      "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el hep-th\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el\n",
      "physics.app-ph cond-mat.mes-hall physics.optics\n",
      "hep-ex cs.CV\n",
      "cond-mat.str-el cond-mat.mtrl-sci physics.app-ph\n",
      "physics.optics cond-mat.dis-nn\n",
      "physics.plasm-ph physics.acc-ph physics.comp-ph\n",
      "cs.SI cs.CY physics.data-an\n",
      "quant-ph math-ph math.MP physics.optics\n",
      "math-ph cond-mat.dis-nn hep-th math.KT math.MP\n",
      "q-fin.MF q-fin.PM\n",
      "quant-ph cond-mat.other cs.DM hep-lat math-ph math.MP\n",
      "cs.IT math.FA math.IT math.PR\n",
      "physics.data-an hep-ex nucl-ex\n",
      "math-ph hep-th math.MP math.OA\n",
      "quant-ph cond-mat.mtrl-sci physics.optics\n",
      "stat.ML cs.LG stat.ME\n",
      "stat.AP nlin.CD\n",
      "physics.acc-ph physics.comp-ph\n",
      "physics.class-ph cond-mat.stat-mech\n",
      "physics.soc-ph cond-mat.stat-mech cs.IT math.IT physics.data-an\n",
      "cs.CV cs.AI cs.LG\n",
      "cond-mat.mes-hall cond-mat.str-el cond-mat.supr-con\n",
      "physics.med-ph physics.comp-ph\n",
      "hep-ph hep-lat nucl-th\n",
      "physics.chem-ph physics.optics\n",
      "math.AP math.MG math.OC\n",
      "math.CV math.PR\n",
      "math.OC math.CA math.ST stat.TH\n",
      "math.CA math.CO\n",
      "cond-mat.mtrl-sci nlin.PS\n",
      "astro-ph.HE hep-ph nucl-th\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph physics.comp-ph\n",
      "gr-qc physics.data-an\n",
      "nucl-ex physics.ins-det\n",
      "cs.LG cs.IT eess.SP math.IT stat.ML\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall quant-ph\n",
      "gr-qc hep-th math-ph math.MP quant-ph\n",
      "math.AP math.CA\n",
      "astro-ph.CO gr-qc physics.optics\n",
      "math.PR cond-mat.stat-mech math-ph math.CV math.MP\n",
      "math.DG math.NA math.RA\n",
      "cond-mat.dis-nn cond-mat.supr-con\n",
      "cond-mat.str-el cond-mat.stat-mech hep-th\n",
      "cs.DL cs.CY physics.soc-ph\n",
      "cs.LG cond-mat.str-el quant-ph stat.ML\n",
      "gr-qc math-ph math.MP\n",
      "astro-ph.IM astro-ph.GA\n",
      "cond-mat.dis-nn cond-mat.mes-hall cond-mat.str-el\n",
      "physics.ins-det gr-qc\n",
      "q-bio.PE cs.LG q-bio.QM stat.ML\n",
      "q-bio.PE q-bio.CB\n",
      "math.AP math.FA\n",
      "cond-mat.mtrl-sci physics.app-ph\n",
      "hep-th cond-mat.mes-hall hep-lat\n",
      "cond-mat.dis-nn cond-mat.stat-mech cs.LG\n",
      "quant-ph physics.comp-ph\n",
      "hep-ph astro-ph.GA\n",
      "cond-mat.str-el math.CT math.QA\n",
      "cs.CG cs.DC cs.GR math.CV math.DG\n",
      "stat.ML cs.CV cs.LG\n",
      "math.CA math.CO math.CV\n",
      "astro-ph.HE astro-ph.SR\n",
      "nlin.CD cond-mat.stat-mech\n",
      "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "stat.ML cs.IT cs.LG math.IT\n",
      "cond-mat.other cond-mat.quant-gas hep-ph hep-th physics.atom-ph\n",
      "math.CA math.MG\n",
      "physics.comp-ph physics.app-ph\n",
      "math.AC math.CO\n",
      "cond-mat.stat-mech cond-mat.soft hep-th\n",
      "eess.SP physics.app-ph\n",
      "cond-mat.mes-hall physics.data-an quant-ph\n",
      "nlin.PS cond-mat.soft cond-mat.stat-mech\n",
      "cs.LG eess.SP stat.ML\n",
      "math.HO\n",
      "physics.ins-det hep-ex\n",
      "math.CA cs.LG\n",
      "cond-mat.soft physics.bio-ph\n",
      "math.FA math.OA\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.quant-gas\n",
      "cond-mat.str-el cond-mat.quant-gas quant-ph\n",
      "physics.comp-ph cond-mat.soft\n",
      "nucl-th astro-ph.HE\n",
      "stat.CO cs.DS math.CO stat.AP\n",
      "physics.bio-ph cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
      "cond-mat.stat-mech physics.bio-ph\n",
      "gr-qc astro-ph.GA\n",
      "cs.LG math.OC stat.ML\n",
      "stat.AP stat.CO\n",
      "hep-ph astro-ph.CO gr-qc\n",
      "cs.GR math.NA\n",
      "physics.class-ph cond-mat.quant-gas math-ph math.MP\n",
      "astro-ph.HE astro-ph.SR hep-ph\n",
      "cond-mat.stat-mech hep-th quant-ph\n",
      "hep-ph astro-ph.CO\n",
      "quant-ph cond-mat.mes-hall cond-mat.stat-mech\n",
      "physics.flu-dyn cs.NA math.NA\n",
      "cs.SI cs.LG stat.AP stat.ML\n",
      "math.SP cs.DM\n",
      "hep-th astro-ph.CO gr-qc\n",
      "quant-ph physics.atm-clus\n",
      "quant-ph cond-mat.mes-hall cs.CC\n",
      "cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
      "cs.LG cs.CV cs.NE physics.comp-ph quant-ph\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el\n",
      "physics.comp-ph cond-mat.mtrl-sci cs.NA math.NA\n",
      "quant-ph cond-mat.dis-nn nlin.CD\n",
      "q-bio.BM cs.LG\n",
      "nucl-ex astro-ph.IM\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el\n",
      "math-ph hep-th math.MP quant-ph\n",
      "physics.acc-ph cond-mat.supr-con\n",
      "astro-ph.HE astro-ph.CO astro-ph.IM gr-qc\n",
      "physics.optics nlin.AO nlin.PS\n",
      "cond-mat.soft cond-mat.dis-nn\n",
      "cs.DS cs.LO\n",
      "cond-mat.mes-hall cond-mat.soft\n",
      "cs.FL cs.CE\n",
      "cs.DS cs.IR\n",
      "cs.SY eess.SP\n",
      "physics.plasm-ph hep-ph\n",
      "cs.LG eess.IV stat.ML\n",
      "math.PR cs.NA math.NA\n",
      "astro-ph.CO hep-ph nucl-th\n",
      "astro-ph.GA hep-ph\n",
      "math.FA cs.NA math.NA\n",
      "stat.ML cs.CV cs.LG stat.CO\n",
      "hep-ph astro-ph.HE gr-qc hep-th nucl-th\n",
      "cs.CV cs.AI cs.LG cs.RO\n",
      "math.AG math.GR\n",
      "physics.atom-ph physics.optics\n",
      "physics.acc-ph physics.app-ph physics.ins-det physics.med-ph\n",
      "cond-mat.quant-gas cond-mat.stat-mech\n",
      "cs.CR cs.AI cs.CC cs.LG\n",
      "cond-mat.dis-nn cond-mat.quant-gas quant-ph\n",
      "cs.PF cs.DC\n",
      "stat.AP q-bio.GN q-bio.QM\n",
      "physics.soc-ph cs.SI physics.data-an\n",
      "physics.optics math-ph math.MP\n",
      "hep-th math-ph math.AT math.DG math.MP\n",
      "eess.IV physics.optics\n",
      "q-bio.TO q-bio.PE\n",
      "quant-ph cond-mat.stat-mech cs.IT hep-th math-ph math.IT math.MP\n",
      "quant-ph hep-lat hep-ph nucl-th\n",
      "quant-ph math.SG\n",
      "cond-mat.quant-gas cond-mat.mtrl-sci nucl-th physics.comp-ph\n",
      "quant-ph cs.LG stat.ML\n",
      "stat.AP cs.LG stat.ML\n",
      "astro-ph.CO hep-ph hep-th\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.soft cond-mat.str-el\n",
      "cond-mat.stat-mech cond-mat.str-el\n",
      "hep-ph astro-ph.HE astro-ph.SR hep-ex\n",
      "cs.SE cs.AI\n",
      "physics.soc-ph q-bio.NC\n",
      "cs.IR\n",
      "hep-ph hep-ex hep-th quant-ph\n",
      "cond-mat.mes-hall cond-mat.stat-mech hep-th quant-ph\n",
      "math.DG math.AP math.OC\n",
      "econ.TH cs.LO math.CT\n",
      "math.AP math-ph math.MP\n",
      "q-fin.RM stat.ME\n",
      "cond-mat.dis-nn nlin.CD\n",
      "quant-ph cond-mat.mes-hall cond-mat.other math-ph math.MP\n",
      "cs.MM eess.IV\n",
      "math.RT math.CO math.RA\n",
      "cond-mat.str-el cond-mat.mtrl-sci physics.comp-ph\n",
      "q-bio.PE physics.bio-ph\n",
      "physics.soc-ph q-bio.PE\n",
      "physics.soc-ph q-fin.ST\n",
      "cond-mat.stat-mech math-ph math.MP quant-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn quant-ph\n",
      "cs.RO cs.LG cs.SY\n",
      "cond-mat.stat-mech cond-mat.dis-nn nlin.AO physics.data-an\n",
      "eess.SY cs.SY\n",
      "quant-ph cs.CC physics.comp-ph\n",
      "physics.soc-ph stat.AP\n",
      "cs.SY cs.LG math.OC\n",
      "hep-ph astro-ph.HE nucl-th\n",
      "math.OA math.FA\n",
      "physics.optics nlin.CD\n",
      "cs.LG cs.HC stat.ML\n",
      "physics.data-an nlin.AO\n",
      "cond-mat.mes-hall cond-mat.supr-con quant-ph\n",
      "cs.DC cs.HC cs.SE\n",
      "cs.LO math.CT math.LO\n",
      "quant-ph cond-mat.other math-ph math.MP physics.comp-ph\n",
      "nlin.AO cond-mat.dis-nn math.DS q-bio.NC\n",
      "gr-qc astro-ph.CO quant-ph\n",
      "physics.app-ph cond-mat.str-el\n",
      "cs.SY quant-ph\n",
      "physics.bio-ph q-bio.SC\n",
      "cond-mat.stat-mech gr-qc hep-th math-ph math.MP\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el\n",
      "physics.flu-dyn nlin.CD\n",
      "cs.IR cs.AI\n",
      "cs.IR cs.LG\n",
      "cond-mat.mes-hall cond-mat.stat-mech\n",
      "quant-ph cs.CC cs.IT math.IT\n",
      "quant-ph cond-mat.mes-hall cond-mat.stat-mech math-ph math.MP\n",
      "quant-ph cond-mat.other math-ph math.MP\n",
      "q-bio.QM math.AT\n",
      "cond-mat.quant-gas gr-qc\n",
      "cond-mat.str-el cond-mat.supr-con quant-ph\n",
      "astro-ph.HE astro-ph.SR gr-qc\n",
      "cs.DM cs.DS\n",
      "math.OC econ.TH math.DS\n",
      "cs.SI\n",
      "cond-mat.stat-mech cond-mat.str-el hep-th\n",
      "cond-mat.stat-mech hep-th\n",
      "cs.SY cs.RO\n",
      "cs.SD cs.SY eess.AS\n",
      "quant-ph cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el\n",
      "quant-ph math.AT\n",
      "cs.CY cs.SI physics.soc-ph\n",
      "eess.IV cs.NA\n",
      "quant-ph cond-mat.mes-hall physics.chem-ph\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "cs.CL cs.AI cs.HC cs.LG\n",
      "cond-mat.dis-nn physics.comp-ph\n",
      "cs.AI cs.LG cs.MA cs.NE cs.PL\n",
      "math.GR\n",
      "physics.comp-ph cs.LG hep-lat hep-ph\n",
      "cond-mat.mes-hall cond-mat.quant-gas physics.optics\n",
      "math.AP cond-mat.mes-hall math-ph math.MP nlin.PS\n",
      "eess.SY cs.SY math.OC\n",
      "eess.SP cs.IT cs.NA math.IT math.NA math.OC\n",
      "hep-ph astro-ph.CO astro-ph.HE gr-qc\n",
      "cond-mat.mtrl-sci cs.CV eess.IV\n",
      "cs.MA cs.LG cs.SI math.DS stat.ML\n",
      "cond-mat.str-el hep-th math-ph math.MP math.QA\n",
      "gr-qc cond-mat.mes-hall hep-th\n",
      "cs.DC eess.SP\n",
      "quant-ph physics.app-ph\n",
      "cond-mat.quant-gas nucl-th\n",
      "nucl-th hep-ex hep-lat hep-ph nucl-ex\n",
      "cs.AI cs.NE cs.SE\n",
      "nucl-th cond-mat.quant-gas nucl-ex\n",
      "cs.LG cs.CC cs.DM physics.data-an stat.ML\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech cond-mat.supr-con\n",
      "nlin.CD physics.soc-ph\n",
      "cs.LG math.AT math.MG stat.ML\n",
      "cond-mat.stat-mech cond-mat.dis-nn math-ph math.MP math.PR nlin.SI\n",
      "astro-ph.GA astro-ph.CO physics.plasm-ph\n",
      "physics.optics cond-mat.mes-hall quant-ph\n",
      "physics.geo-ph nlin.CD\n",
      "physics.flu-dyn physics.ao-ph physics.data-an physics.geo-ph\n",
      "stat.ME stat.CO stat.ML\n",
      "physics.comp-ph physics.bio-ph\n",
      "physics.atom-ph nlin.CD\n",
      "cond-mat.str-el cond-mat.dis-nn quant-ph\n",
      "eess.IV physics.med-ph physics.optics\n",
      "physics.comp-ph physics.flu-dyn\n",
      "cs.FL math.NT\n",
      "cs.GR cs.CV cs.HC\n",
      "physics.flu-dyn cond-mat.soft physics.bio-ph\n",
      "q-bio.MN q-bio.SC\n",
      "cond-mat.quant-gas cond-mat.mes-hall physics.optics quant-ph\n",
      "hep-lat\n",
      "physics.bio-ph nlin.PS\n",
      "physics.optics physics.app-ph physics.atom-ph quant-ph\n",
      "cs.NI cs.ET\n",
      "cs.LG cs.DC stat.ML\n",
      "physics.soc-ph cond-mat.stat-mech math.PR\n",
      "cond-mat.stat-mech physics.bio-ph q-bio.MN\n",
      "cs.DS math.NT stat.CO\n",
      "cond-mat.quant-gas physics.optics quant-ph\n",
      "physics.comp-ph cond-mat.mes-hall\n",
      "astro-ph.EP physics.flu-dyn physics.geo-ph\n",
      "physics.bio-ph q-bio.TO\n",
      "physics.acc-ph physics.optics\n",
      "cond-mat.str-el hep-th quant-ph\n",
      "cs.LG cs.CY stat.ML\n",
      "stat.ME stat.AP stat.CO\n",
      "cs.SI stat.AP\n",
      "math.GM\n",
      "physics.ins-det physics.atom-ph\n",
      "cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "math.OC cs.NA math.DS math.NA\n",
      "quant-ph nucl-th\n",
      "quant-ph cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
      "cond-mat.quant-gas physics.atom-ph physics.flu-dyn\n",
      "physics.comp-ph physics.plasm-ph\n",
      "physics.atom-ph hep-ex\n",
      "cond-mat.soft physics.app-ph\n",
      "hep-th astro-ph.CO\n",
      "hep-ph astro-ph.HE hep-ex\n",
      "hep-th cond-mat.str-el math-ph math.AT math.MP\n",
      "physics.atm-clus physics.atom-ph physics.optics\n",
      "math.CV math.DG\n",
      "physics.comp-ph cond-mat.mes-hall physics.ins-det\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn\n",
      "physics.flu-dyn physics.geo-ph\n",
      "math.FA math.AP math.OC\n",
      "cond-mat.quant-gas nlin.PS physics.atom-ph\n",
      "astro-ph.IM physics.comp-ph physics.flu-dyn\n",
      "cond-mat.dis-nn q-bio.NC\n",
      "q-bio.NC cond-mat.dis-nn physics.soc-ph\n",
      "gr-qc hep-ph\n",
      "cs.AI cs.CL cs.LG\n",
      "cond-mat.str-el cond-mat.dis-nn\n",
      "cond-mat.mes-hall cond-mat.str-el quant-ph\n",
      "cond-mat.quant-gas cond-mat.other cond-mat.str-el\n",
      "math.QA math.AT\n",
      "cs.CY cs.LG cs.NE\n",
      "cs.LG cond-mat.dis-nn stat.ML\n",
      "nlin.PS cond-mat.mes-hall physics.app-ph\n",
      "cond-mat.soft physics.chem-ph physics.flu-dyn\n",
      "cond-mat.stat-mech nlin.AO\n",
      "math.CV math.AP math.DG\n",
      "physics.comp-ph cs.AI cs.LG hep-th\n",
      "cs.CV cs.AI cs.CR cs.LG\n",
      "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "nlin.SI\n",
      "quant-ph math.NA\n",
      "physics.ins-det cs.LG hep-ex\n",
      "physics.ins-det cond-mat.mtrl-sci\n",
      "astro-ph.HE astro-ph.GA hep-ph\n",
      "cond-mat.quant-gas cond-mat.other\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.soft\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.supr-con quant-ph\n",
      "physics.flu-dyn physics.app-ph\n",
      "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG\n",
      "cond-mat.mtrl-sci cond-mat.str-el quant-ph\n",
      "math.AG math.CV math.DG\n",
      "quant-ph physics.ins-det physics.optics\n",
      "eess.SP cs.LG stat.ML\n",
      "q-bio.QM q-bio.MN\n",
      "cond-mat.stat-mech math.PR\n",
      "hep-lat quant-ph\n",
      "cond-mat.dis-nn cond-mat.mes-hall quant-ph\n",
      "q-bio.NC physics.data-an q-bio.QM\n",
      "cs.CG cs.LG math.AT stat.ML\n",
      "cs.CV cs.LG cs.NE\n",
      "cs.MA\n",
      "q-bio.CB\n",
      "cs.DB cs.DC cs.DS\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.class-ph physics.comp-ph\n",
      "hep-th nlin.CD quant-ph\n",
      "eess.SP physics.ins-det\n",
      "cond-mat.str-el hep-lat\n",
      "quant-ph cond-mat.mes-hall physics.app-ph\n",
      "nucl-th astro-ph.HE cond-mat.supr-con hep-th\n",
      "eess.IV cs.CV cs.LG\n",
      "cs.LG physics.chem-ph quant-ph stat.ML\n",
      "hep-ph astro-ph.CO cond-mat.mtrl-sci hep-ex\n",
      "eess.IV math.OC\n",
      "physics.comp-ph cond-mat.stat-mech\n",
      "stat.CO econ.EM stat.ME\n",
      "cond-mat.dis-nn quant-ph\n",
      "math.AP math-ph math.MP nlin.SI\n",
      "cond-mat.soft nlin.PS\n",
      "physics.soc-ph nlin.CG\n",
      "cond-mat.mtrl-sci physics.app-ph physics.space-ph\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph q-bio.QM\n",
      "q-bio.NC cond-mat.dis-nn nlin.AO\n",
      "hep-th cond-mat.supr-con nucl-th\n",
      "hep-ph hep-ex hep-lat\n",
      "eess.SP physics.data-an physics.flu-dyn\n",
      "math-ph hep-th math.AT math.MP\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft cs.LG\n",
      "stat.ML cs.HC cs.LG\n",
      "physics.comp-ph cond-mat.mtrl-sci physics.flu-dyn\n",
      "physics.comp-ph astro-ph.CO astro-ph.IM cs.NA math.NA\n",
      "cs.LG cs.AI cs.MA cs.NE stat.ML\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci\n",
      "cs.LG cs.CV cs.NE\n",
      "quant-ph hep-ph hep-th math-ph math.MP physics.atom-ph\n",
      "astro-ph.IM astro-ph.HE\n",
      "nucl-th astro-ph.HE cond-mat.quant-gas\n",
      "gr-qc cond-mat.stat-mech hep-th quant-ph\n",
      "cond-mat.stat-mech cond-mat.str-el physics.comp-ph quant-ph\n",
      "quant-ph cond-mat.str-el physics.comp-ph\n",
      "cs.CV cs.SI\n",
      "cs.NE math.OC\n",
      "stat.ML cs.CL cs.IR cs.LG\n",
      "physics.bio-ph cond-mat.stat-mech q-bio.SC\n",
      "physics.optics eess.IV physics.bio-ph\n",
      "nlin.AO cond-mat.stat-mech\n",
      "quant-ph cond-mat.quant-gas cond-mat.str-el\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el hep-th quant-ph\n",
      "hep-ex hep-ph nucl-th\n",
      "cs.RO cs.CV\n",
      "nlin.CG\n",
      "hep-th cond-mat.other\n",
      "physics.atom-ph physics.optics quant-ph\n",
      "cs.RO math.AT\n",
      "cond-mat.soft physics.bio-ph physics.comp-ph physics.flu-dyn\n",
      "math.RA math.CT math.QA\n",
      "physics.optics physics.atom-ph physics.chem-ph quant-ph\n",
      "physics.optics cond-mat.mtrl-sci quant-ph\n",
      "math.QA math-ph math.MP\n",
      "math.DS nlin.CD physics.flu-dyn\n",
      "cond-mat.str-el cond-mat.stat-mech quant-ph\n",
      "cond-mat.supr-con physics.acc-ph\n",
      "math-ph cond-mat.str-el hep-th math.AT math.MP quant-ph\n",
      "q-bio.PE physics.soc-ph\n",
      "cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con\n",
      "quant-ph astro-ph.CO gr-qc hep-th\n",
      "physics.optics physics.plasm-ph quant-ph\n",
      "cond-mat.quant-gas cond-mat.supr-con\n",
      "cs.SI stat.ME\n",
      "cond-mat.supr-con cond-mat.soft\n",
      "physics.comp-ph cond-mat.other\n",
      "physics.atom-ph cond-mat.mtrl-sci\n",
      "hep-ph astro-ph.CO astro-ph.HE hep-th\n",
      "quant-ph cond-mat.stat-mech gr-qc hep-th\n",
      "cs.RO cs.SY eess.SY\n",
      "hep-th gr-qc physics.bio-ph\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el quant-ph\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall math-ph math.MP\n",
      "math.NA cs.CE cs.NA\n",
      "q-bio.PE cond-mat.stat-mech\n",
      "nlin.AO nlin.CD physics.bio-ph q-bio.NC\n",
      "physics.class-ph cond-mat.mes-hall\n",
      "cond-mat.mtrl-sci cond-mat.other\n",
      "astro-ph.SR astro-ph.HE\n",
      "physics.ins-det physics.geo-ph\n",
      "cond-mat.mtrl-sci physics.app-ph physics.optics\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.other\n",
      "cs.LG quant-ph stat.ML\n",
      "cond-mat.supr-con cond-mat.soft nlin.AO\n",
      "cond-mat.stat-mech physics.soc-ph\n",
      "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech\n",
      "math.CO math.OA math.QA\n",
      "cs.CR cs.GT\n",
      "cond-mat.other quant-ph\n",
      "physics.data-an astro-ph.HE astro-ph.IM hep-ph stat.ME\n",
      "cs.CV cs.NA math.NA\n",
      "math.NA cond-mat.stat-mech cs.NA math.PR\n",
      "cs.DL cs.LG physics.hist-ph physics.soc-ph quant-ph\n",
      "astro-ph.HE physics.comp-ph\n",
      "cs.DM math.DG\n",
      "physics.atom-ph physics.atm-clus physics.optics quant-ph\n",
      "quant-ph cond-mat.str-el cs.DS physics.chem-ph\n",
      "hep-ph astro-ph.CO hep-ex\n",
      "q-bio.NC cond-mat.dis-nn quant-ph\n",
      "quant-ph nlin.CD\n",
      "cond-mat.str-el cond-mat.quant-gas math-ph math.MP\n",
      "physics.soc-ph cs.MA\n",
      "physics.optics cond-mat.mtrl-sci\n",
      "cond-mat.quant-gas cond-mat.soft cond-mat.stat-mech physics.flu-dyn\n",
      "cond-mat.mes-hall cond-mat.other\n",
      "cond-mat.stat-mech physics.bio-ph q-bio.SC\n",
      "physics.flu-dyn math-ph math.MP physics.plasm-ph\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci quant-ph\n",
      "cs.LG math.DS nlin.CD stat.ML\n",
      "stat.CO stat.ME stat.ML\n",
      "q-bio.MN physics.bio-ph\n",
      "physics.optics cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech physics.chem-ph\n",
      "hep-lat hep-th\n",
      "cs.CR cs.LG stat.ML\n",
      "physics.comp-ph cond-mat.mtrl-sci\n",
      "cs.GT cs.AI cs.LG\n",
      "physics.flu-dyn cond-mat.soft\n",
      "cond-mat.mtrl-sci physics.chem-ph\n",
      "quant-ph cond-mat.dis-nn cond-mat.str-el cs.LG\n",
      "physics.optics cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.atom-ph\n",
      "hep-th math-ph math.MP math.RT\n",
      "cs.DS quant-ph\n",
      "cond-mat.mes-hall physics.chem-ph\n",
      "math.FA math.CO quant-ph\n",
      "nlin.AO cond-mat.dis-nn cond-mat.stat-mech math.DS\n",
      "physics.optics physics.acc-ph\n",
      "cond-mat.stat-mech cs.NA math.NA physics.soc-ph\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.str-el\n",
      "physics.optics physics.atom-ph quant-ph\n",
      "nucl-th hep-ex hep-ph\n",
      "cond-mat.stat-mech nlin.CD\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other cond-mat.str-el\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG\n",
      "cond-mat.dis-nn physics.data-an\n",
      "cond-mat.str-el cond-mat.quant-gas hep-th quant-ph\n",
      "physics.comp-ph cond-mat.mtrl-sci physics.chem-ph\n",
      "cond-mat.str-el cond-mat.stat-mech hep-th math-ph math.MP\n",
      "nlin.AO cs.NE q-bio.NC\n",
      "cond-mat.stat-mech cond-mat.mes-hall\n",
      "hep-th gr-qc math-ph math.MP\n",
      "hep-ex hep-ph nucl-ex physics.data-an\n",
      "physics.flu-dyn cond-mat.dis-nn cond-mat.soft\n",
      "cs.CV cs.CL cs.HC cs.LG stat.ML\n",
      "physics.bio-ph physics.app-ph\n",
      "math.OC stat.AP\n",
      "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
      "q-bio.BM cond-mat.stat-mech physics.bio-ph\n",
      "physics.comp-ph astro-ph.CO gr-qc\n",
      "math.CO math.GR\n",
      "cs.NE cs.LG\n",
      "hep-ph astro-ph.CO hep-ex nucl-ex nucl-th\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.supr-con\n",
      "hep-th cond-mat.mtrl-sci gr-qc\n",
      "q-bio.NC cond-mat.dis-nn\n",
      "physics.optics physics.app-ph physics.comp-ph\n",
      "cond-mat.mtrl-sci hep-ex\n",
      "cond-mat.stat-mech math.PR stat.OT\n",
      "quant-ph physics.chem-ph physics.comp-ph\n",
      "q-bio.PE cond-mat.dis-nn cond-mat.stat-mech\n",
      "physics.med-ph cs.CE physics.comp-ph q-bio.QM\n",
      "physics.app-ph cs.ET\n",
      "cond-mat.dis-nn cs.IT math.IT\n",
      "cond-mat.other physics.flu-dyn\n",
      "math.FA math.CO math.MG\n",
      "cond-mat.quant-gas cond-mat.stat-mech hep-th quant-ph\n",
      "physics.plasm-ph physics.optics\n",
      "hep-ex physics.atom-ph\n",
      "math-ph math.FA math.MP quant-ph\n",
      "physics.plasm-ph astro-ph.SR\n",
      "cond-mat.stat-mech physics.class-ph\n",
      "eess.IV cs.LG stat.AP stat.ML\n",
      "cs.NI cs.SY eess.SY\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mes-hall quant-ph\n",
      "astro-ph.IM gr-qc\n",
      "physics.space-ph astro-ph.SR physics.plasm-ph\n",
      "math.FA math.CA\n",
      "hep-ph astro-ph.CO hep-th math-ph math.MP\n",
      "astro-ph.CO hep-ph physics.atom-ph\n",
      "math.KT math.AT math.MG\n",
      "physics.ao-ph physics.flu-dyn\n",
      "cs.DS cs.CV cs.LG\n",
      "cond-mat.supr-con cond-mat.mes-hall quant-ph\n",
      "cs.LG cs.CR cs.CV stat.ML\n",
      "physics.bio-ph physics.flu-dyn\n",
      "eess.IV cs.LG physics.med-ph\n",
      "physics.data-an cs.LG\n",
      "physics.optics cond-mat.quant-gas\n",
      "physics.atom-ph physics.chem-ph\n",
      "astro-ph.CO astro-ph.GA hep-ph physics.flu-dyn\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "quant-ph cond-mat.dis-nn cond-mat.mes-hall\n",
      "q-bio.QM q-bio.GN q-bio.MN q-bio.SC\n",
      "nucl-th astro-ph.HE cond-mat.quant-gas hep-lat quant-ph\n",
      "hep-th cond-mat.str-el nlin.PS physics.optics\n",
      "physics.atom-ph cond-mat.quant-gas\n",
      "physics.class-ph physics.app-ph physics.optics\n",
      "gr-qc astro-ph.CO hep-ph hep-th\n",
      "stat.ML cs.LG eess.SP physics.comp-ph\n",
      "astro-ph.IM astro-ph.GA astro-ph.HE gr-qc hep-th\n",
      "astro-ph.CO astro-ph.IM hep-ph\n",
      "cond-mat.mtrl-sci physics.comp-ph physics.optics quant-ph\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.geo-ph\n",
      "physics.app-ph physics.chem-ph\n",
      "cond-mat.quant-gas math-ph math.MP quant-ph\n",
      "physics.optics cond-mat.mes-hall physics.comp-ph\n",
      "cs.DS cs.CC cs.CG cs.IR\n",
      "math.CT math.OA\n",
      "hep-ph gr-qc hep-th\n",
      "q-fin.ST cond-mat.stat-mech physics.soc-ph\n",
      "eess.IV cs.CR cs.MM\n",
      "eess.IV cs.MM\n",
      "physics.optics cond-mat.mtrl-sci physics.app-ph\n",
      "q-bio.PE cond-mat.stat-mech nlin.AO physics.soc-ph\n",
      "physics.flu-dyn astro-ph.SR physics.plasm-ph\n",
      "math.RA math.RT\n",
      "cond-mat.str-el physics.optics\n",
      "hep-th math-ph math.AT math.MP\n",
      "cond-mat.stat-mech cond-mat.supr-con\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.class-ph physics.flu-dyn\n",
      "cs.LO cs.PL\n",
      "physics.comp-ph cs.LG physics.data-an stat.ML\n",
      "quant-ph cs.NI\n",
      "astro-ph.CO astro-ph.IM hep-ex physics.ins-det\n",
      "physics.chem-ph cond-mat.other physics.optics quant-ph\n",
      "physics.app-ph physics.bio-ph physics.med-ph quant-ph\n",
      "astro-ph.GA hep-ph stat.ML\n",
      "q-fin.MF cs.LG q-fin.CP stat.ML\n",
      "astro-ph.CO astro-ph.GA gr-qc hep-ph\n",
      "quant-ph cond-mat.str-el math-ph math.MP math.QA\n",
      "cond-mat.stat-mech cond-mat.mes-hall quant-ph\n",
      "physics.space-ph physics.plasm-ph\n",
      "cs.LG cs.CG cs.DS quant-ph stat.ML\n",
      "cond-mat.mes-hall physics.class-ph\n",
      "cs.CV stat.ML\n",
      "math.CA math.AP\n",
      "physics.space-ph astro-ph.EP physics.ao-ph\n",
      "cond-mat.stat-mech astro-ph.CO\n",
      "nucl-th hep-ph physics.atom-ph\n",
      "math.NA astro-ph.IM cs.NA physics.comp-ph\n",
      "q-bio.QM math.PR q-bio.SC\n",
      "math-ph math.MP physics.data-an stat.AP\n",
      "math-ph cond-mat.stat-mech math.MP physics.comp-ph\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el physics.atom-ph quant-ph\n",
      "q-bio.QM eess.IV\n",
      "cond-mat.mtrl-sci quant-ph\n",
      "cond-mat.str-el physics.ins-det quant-ph\n",
      "q-fin.MF q-fin.GN\n",
      "hep-ph hep-th nucl-th\n",
      "physics.optics physics.comp-ph\n",
      "quant-ph cs.CC math.CO\n",
      "cs.SI math.OC physics.soc-ph\n",
      "math-ph cond-mat.str-el hep-th math.MP nlin.SI\n",
      "cs.ET cond-mat.mtrl-sci\n",
      "cond-mat.dis-nn cond-mat.str-el quant-ph\n",
      "cond-mat.str-el math-ph math.MP quant-ph\n",
      "hep-ph hep-ex stat.ML\n",
      "hep-th cond-mat.mes-hall cond-mat.quant-gas hep-ph\n",
      "physics.class-ph cs.ET\n",
      "cs.NI cs.LG\n",
      "physics.flu-dyn math.DS nlin.CD physics.ao-ph\n",
      "quant-ph cond-mat.other math-ph math.MP nlin.PS\n",
      "quant-ph cond-mat.stat-mech physics.hist-ph\n",
      "math.MG math.CA math.NT\n",
      "math.AG math.CO math.GT math.MG math.OC\n",
      "q-bio.NC cs.HC eess.SP\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas quant-ph\n",
      "physics.bio-ph cond-mat.stat-mech q-bio.PE q-bio.QM\n",
      "math-ph hep-th math.MP math.RT\n",
      "physics.app-ph physics.bio-ph physics.chem-ph physics.med-ph\n",
      "math.OC cs.SY eess.SY\n",
      "physics.optics physics.ins-det\n",
      "physics.flu-dyn nlin.CD physics.app-ph physics.geo-ph\n",
      "eess.SP cs.LG\n",
      "math.NA cs.CC cs.NA\n",
      "cs.AI cs.NE math.OC\n",
      "hep-th cond-mat.str-el gr-qc quant-ph\n",
      "quant-ph cond-mat.mes-hall cond-mat.str-el\n",
      "cs.ET cond-mat.dis-nn cond-mat.mes-hall\n",
      "cond-mat.mtrl-sci hep-lat math-ph math.MP\n",
      "physics.atom-ph hep-ex nucl-ex\n",
      "physics.class-ph physics.comp-ph physics.optics\n",
      "physics.optics cond-mat.mes-hall cond-mat.soft\n",
      "cond-mat.supr-con cond-mat.mtrl-sci\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other cond-mat.supr-con quant-ph\n",
      "quant-ph cond-mat.dis-nn cond-mat.mes-hall cond-mat.quant-gas\n",
      "nlin.PS cond-mat.mtrl-sci math-ph math.MP\n",
      "cond-mat.dis-nn cond-mat.str-el\n",
      "cs.MS cs.DS cs.SE physics.comp-ph\n",
      "eess.IV cs.CV cs.LG stat.ML\n",
      "physics.flu-dyn physics.ao-ph\n",
      "math.GT math.GR\n",
      "nlin.PS physics.bio-ph q-bio.CB\n",
      "stat.ML cond-mat.str-el cs.LG quant-ph\n",
      "cond-mat.stat-mech cond-mat.str-el hep-th quant-ph\n",
      "nucl-th hep-th\n",
      "cs.DB cs.CR\n",
      "math-ph math.MP math.SP\n",
      "cs.HC cs.CV\n",
      "quant-ph cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
      "q-fin.GN physics.soc-ph q-fin.RM\n",
      "physics.flu-dyn cond-mat.str-el hep-th\n",
      "eess.IV cs.LG stat.ML\n",
      "eess.AS\n",
      "physics.soc-ph cond-mat.stat-mech cs.SI math.DS nlin.AO\n",
      "nlin.CD math.DS physics.comp-ph\n",
      "physics.comp-ph astro-ph.CO astro-ph.IM\n",
      "cond-mat.supr-con cond-mat.dis-nn cond-mat.str-el\n",
      "cs.NI cs.CR cs.DC\n",
      "cond-mat.stat-mech physics.geo-ph\n",
      "quant-ph cs.SY eess.SY\n",
      "stat.CO cs.LG math.OC\n",
      "q-bio.MN cond-mat.stat-mech physics.bio-ph\n",
      "nlin.AO cond-mat.dis-nn nlin.CD\n",
      "cond-mat.dis-nn cond-mat.quant-gas\n",
      "cond-mat.quant-gas cond-mat.supr-con nucl-th\n",
      "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el physics.optics quant-ph\n",
      "hep-ph nucl-th physics.atom-ph\n",
      "cs.SI physics.soc-ph stat.ML\n",
      "cond-mat.str-el cond-mat.quant-gas cond-mat.supr-con physics.atom-ph quant-ph\n",
      "cond-mat.mes-hall physics.chem-ph quant-ph\n",
      "math.AP math-ph math.DG math.FA math.MP\n",
      "quant-ph cs.DS cs.LG\n",
      "quant-ph cond-mat.quant-gas cond-mat.stat-mech nlin.AO\n",
      "nucl-th astro-ph.HE hep-ph hep-th\n",
      "nlin.AO nlin.CD\n",
      "cond-mat.quant-gas nlin.PS quant-ph\n",
      "physics.app-ph cond-mat.mes-hall quant-ph\n",
      "quant-ph gr-qc physics.ins-det\n",
      "physics.soc-ph physics.data-an\n",
      "physics.bio-ph q-bio.MN\n",
      "nucl-th astro-ph.HE gr-qc\n",
      "hep-th cond-mat.mes-hall gr-qc quant-ph\n",
      "physics.app-ph cond-mat.mtrl-sci physics.ins-det\n",
      "cs.HC cs.CY cs.LG\n",
      "hep-th hep-lat hep-ph\n",
      "physics.plasm-ph nlin.CD\n",
      "physics.med-ph physics.app-ph\n",
      "cs.CV cs.RO\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.comp-ph\n",
      "hep-lat cond-mat.str-el hep-th quant-ph\n",
      "physics.flu-dyn astro-ph.SR nlin.CD physics.ao-ph\n",
      "gr-qc astro-ph.IM hep-ex hep-ph physics.atom-ph\n",
      "math.NA cs.CC cs.CV cs.NA\n",
      "astro-ph.HE astro-ph.CO gr-qc\n",
      "hep-lat cond-mat.stat-mech hep-ph nucl-th\n",
      "cond-mat.str-el cond-mat.dis-nn hep-th\n",
      "hep-th astro-ph.GA\n",
      "astro-ph.GA gr-qc\n",
      "cs.SI eess.SP\n",
      "physics.atom-ph physics.atm-clus quant-ph\n",
      "physics.atom-ph cond-mat.dis-nn\n",
      "math.DS cs.NA math.NA\n",
      "q-bio.TO cond-mat.stat-mech\n",
      "physics.optics physics.app-ph quant-ph\n",
      "physics.ins-det physics.optics\n",
      "math.OC cs.LG cs.SY eess.SY\n",
      "cond-mat.other physics.data-an\n",
      "nucl-th astro-ph.HE astro-ph.SR hep-ph\n",
      "math-ph gr-qc hep-th math.MP\n",
      "cond-mat.quant-gas nlin.CD\n",
      "quant-ph cond-mat.other gr-qc\n",
      "nucl-th hep-ex nucl-ex\n",
      "nucl-th hep-ex hep-ph nucl-ex\n",
      "quant-ph hep-ph\n",
      "math.NA cs.NA physics.comp-ph\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.ins-det quant-ph\n",
      "physics.comp-ph physics.atom-ph physics.chem-ph\n",
      "physics.soc-ph cs.CY\n",
      "cond-mat.str-el hep-th math-ph math.MP quant-ph\n",
      "cond-mat.soft physics.geo-ph\n",
      "cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el\n",
      "physics.soc-ph econ.GN q-fin.EC\n",
      "cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con\n",
      "eess.SP cs.CC\n",
      "quant-ph cs.NA math-ph math.MP math.NA\n",
      "astro-ph.HE astro-ph.IM\n",
      "nlin.PS physics.flu-dyn\n",
      "quant-ph hep-lat hep-ph nucl-th physics.atom-ph\n",
      "astro-ph.HE gr-qc hep-ph hep-th nucl-th\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el quant-ph\n",
      "cond-mat.supr-con cond-mat.dis-nn cond-mat.mes-hall\n",
      "astro-ph.IM physics.atom-ph\n",
      "physics.app-ph physics.optics quant-ph\n",
      "cond-mat.quant-gas cond-mat.mes-hall physics.app-ph physics.optics quant-ph\n",
      "cs.HC cs.CR cs.CY\n",
      "nucl-ex hep-ex hep-ph\n",
      "physics.comp-ph cs.LG cs.SY eess.SY math.DS stat.ML\n",
      "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI stat.ML\n",
      "physics.flu-dyn physics.comp-ph physics.plasm-ph\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.stat-mech\n",
      "quant-ph cond-mat.quant-gas cond-mat.stat-mech math-ph math.MP\n",
      "hep-lat hep-ph nucl-th\n",
      "astro-ph.CO astro-ph.IM\n",
      "physics.plasm-ph physics.comp-ph\n",
      "cond-mat.quant-gas nucl-th physics.atm-clus\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.str-el hep-th quant-ph\n",
      "cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech\n",
      "cs.CV cs.LG eess.IV stat.ML\n",
      "nucl-th astro-ph.HE astro-ph.SR\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph\n",
      "math.DS math-ph math.MP nlin.SI physics.class-ph\n",
      "math-ph math.AP math.FA math.MP math.PR\n",
      "nucl-th astro-ph.HE gr-qc nucl-ex\n",
      "gr-qc astro-ph.GA hep-th\n",
      "physics.ed-ph astro-ph.CO\n",
      "cond-mat.mes-hall physics.comp-ph\n",
      "math.RA math.MG\n",
      "cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
      "math.OC cs.CC quant-ph\n",
      "nlin.PS math.DS physics.optics\n",
      "cond-mat.dis-nn math-ph math.MP\n",
      "cond-mat.str-el cond-mat.quant-gas hep-th\n",
      "cond-mat.soft cond-mat.mes-hall\n",
      "cs.GR\n",
      "nucl-th hep-lat hep-ph nucl-ex\n",
      "astro-ph.IM physics.ao-ph physics.space-ph\n",
      "physics.optics physics.app-ph physics.ins-det\n",
      "cond-mat.stat-mech physics.optics quant-ph\n",
      "cs.LG cs.GT math.OC stat.ML\n",
      "cond-mat.quant-gas physics.atom-ph physics.comp-ph\n",
      "hep-ph physics.plasm-ph\n",
      "cs.CC math.CO\n",
      "gr-qc astro-ph.CO astro-ph.HE\n",
      "cond-mat.stat-mech physics.flu-dyn\n",
      "math.AG cs.SC\n",
      "hep-ph hep-th quant-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft\n",
      "q-bio.NC cs.AI cs.LG\n",
      "physics.optics physics.ins-det quant-ph\n",
      "cond-mat.supr-con nucl-th\n",
      "physics.flu-dyn cond-mat.stat-mech nlin.CD\n",
      "nucl-th hep-ph nucl-ex quant-ph\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.str-el quant-ph\n",
      "cs.AI cs.LG\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.app-ph\n",
      "q-bio.SC cond-mat.soft physics.bio-ph\n",
      "physics.atom-ph physics.app-ph\n",
      "math.ST cs.IT math.IT stat.OT stat.TH\n",
      "eess.IV cs.CV q-bio.QM\n",
      "hep-ph physics.plasm-ph quant-ph\n",
      "cond-mat.stat-mech gr-qc hep-th quant-ph\n",
      "math.ST cs.LO math.CT math.PR stat.TH\n",
      "cs.NE q-bio.PE\n",
      "astro-ph.IM astro-ph.GA astro-ph.HE gr-qc\n",
      "hep-th hep-ph nucl-th quant-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn cs.SI physics.soc-ph q-bio.PE\n",
      "physics.ins-det physics.app-ph\n",
      "hep-th math-ph math.CO math.MP\n",
      "physics.plasm-ph cond-mat.soft\n",
      "cond-mat.stat-mech nlin.SI\n",
      "math.NA cs.NA physics.flu-dyn\n",
      "physics.atom-ph cond-mat.quant-gas physics.chem-ph quant-ph\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas physics.comp-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.quant-gas cond-mat.str-el quant-ph\n",
      "hep-th cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "cond-mat.dis-nn cond-mat.stat-mech nlin.CD physics.comp-ph\n",
      "physics.plasm-ph cond-mat.dis-nn physics.chem-ph\n",
      "cs.CG math.CO\n",
      "physics.class-ph gr-qc hep-th\n",
      "nucl-th cond-mat.stat-mech\n",
      "physics.ins-det nucl-ex\n",
      "physics.ins-det physics.ed-ph physics.optics\n",
      "eess.SY cs.SY eess.SP physics.ins-det\n",
      "cond-mat.str-el cond-mat.mes-hall hep-th\n",
      "stat.ML cs.CR cs.DC cs.LG eess.SP\n",
      "cond-mat.mes-hall physics.app-ph physics.optics quant-ph\n",
      "physics.plasm-ph physics.flu-dyn\n",
      "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el quant-ph\n",
      "astro-ph.CO cond-mat.stat-mech\n",
      "cond-mat.soft cond-mat.other physics.class-ph physics.geo-ph\n",
      "astro-ph.IM astro-ph.HE hep-ex physics.ins-det\n",
      "astro-ph.HE astro-ph.CO hep-ph\n",
      "quant-ph cond-mat.stat-mech math.DS nlin.CD\n",
      "cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP\n",
      "math.PR math-ph math.CA math.MP\n",
      "physics.flu-dyn cs.LG cs.SY eess.SY math.OC\n",
      "cond-mat.quant-gas cond-mat.supr-con physics.atom-ph\n",
      "q-bio.BM cond-mat.soft\n",
      "cond-mat.str-el cond-mat.stat-mech cond-mat.supr-con hep-th\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech\n",
      "hep-th math.NT\n",
      "physics.comp-ph cond-mat.dis-nn quant-ph\n",
      "cond-mat.soft cond-mat.stat-mech physics.atm-clus\n",
      "physics.chem-ph cond-mat.str-el physics.comp-ph\n",
      "quant-ph math-ph math.MP math.SP\n",
      "q-fin.MF econ.TH math.OC q-fin.PM\n",
      "hep-ex hep-ph nucl-ex\n",
      "physics.chem-ph physics.atm-clus\n",
      "math.NA cs.NA math.DS math.OC\n",
      "astro-ph.HE gr-qc hep-ph nucl-th\n",
      "hep-ph astro-ph.HE gr-qc hep-lat nucl-th\n",
      "math-ph math.AP math.MP quant-ph\n",
      "hep-lat hep-ph nucl-th quant-ph\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.class-ph\n",
      "cond-mat.mes-hall cond-mat.str-el physics.optics quant-ph\n",
      "nlin.PS cond-mat.other\n",
      "hep-lat hep-ex hep-ph nucl-th\n",
      "cond-mat.other cond-mat.supr-con\n",
      "cs.CE physics.flu-dyn\n",
      "cond-mat.str-el hep-th physics.comp-ph\n",
      "astro-ph.CO astro-ph.SR\n",
      "quant-ph cond-mat.other cs.DS\n",
      "quant-ph cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech math-ph math.MP\n",
      "nlin.AO physics.class-ph\n",
      "hep-ph hep-ex hep-lat hep-th\n",
      "cond-mat.stat-mech cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
      "quant-ph cond-mat.quant-gas math-ph math.MP\n",
      "math.GT math.CO\n",
      "physics.flu-dyn math.AP\n",
      "cond-mat.quant-gas physics.atom-ph physics.chem-ph\n",
      "physics.soc-ph cs.DM\n",
      "cs.NE q-bio.QM\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.stat-mech\n",
      "cs.SI cs.CR physics.soc-ph\n",
      "physics.comp-ph cond-mat.stat-mech physics.chem-ph\n",
      "hep-lat nucl-th\n",
      "cond-mat.quant-gas cond-mat.str-el physics.atom-ph\n",
      "physics.optics physics.acc-ph quant-ph\n",
      "hep-th astro-ph.CO gr-qc hep-ph\n",
      "physics.optics cond-mat.other\n",
      "cs.CR cs.CL cs.LG stat.ML\n",
      "quant-ph cs.CR physics.optics\n",
      "astro-ph.IM astro-ph.SR\n",
      "quant-ph cond-mat.dis-nn cond-mat.quant-gas\n",
      "cond-mat.str-el cond-mat.quant-gas math-ph math.MP quant-ph\n",
      "cs.IT eess.SP math.IT\n",
      "astro-ph.HE astro-ph.IM astro-ph.SR\n",
      "math.CT math.GN math.LO math.RA\n",
      "nlin.CD cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
      "quant-ph cs.IT hep-th math-ph math.IT math.MP\n",
      "nlin.CD physics.optics\n",
      "physics.bio-ph cond-mat.stat-mech q-bio.QM\n",
      "cond-mat.quant-gas physics.atom-ph physics.optics quant-ph\n",
      "math.OC cs.CV\n",
      "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph quant-ph\n",
      "cs.DC cs.LG\n",
      "physics.plasm-ph physics.space-ph\n",
      "eess.SP cs.LG physics.optics quant-ph\n",
      "hep-ex physics.geo-ph\n",
      "quant-ph physics.ins-det\n",
      "quant-ph cond-mat.other physics.app-ph\n",
      "physics.chem-ph quant-ph\n",
      "physics.chem-ph cs.LG physics.comp-ph\n",
      "physics.optics eess.SP\n",
      "cond-mat.str-el physics.chem-ph physics.comp-ph\n",
      "nlin.CD quant-ph\n",
      "cond-mat.supr-con physics.comp-ph\n",
      "q-bio.NC cs.ET\n",
      "cond-mat.mtrl-sci cond-mat.supr-con physics.comp-ph\n",
      "cs.RO cs.HC\n",
      "hep-ph hep-ex physics.data-an\n",
      "astro-ph.IM gr-qc physics.optics\n",
      "quant-ph cond-mat.mes-hall nlin.CD\n",
      "physics.flu-dyn astro-ph.SR physics.ao-ph physics.geo-ph\n",
      "math-ph math.MP math.RT\n",
      "cond-mat.mes-hall cond-mat.other physics.app-ph\n",
      "math.NA cs.NA econ.GN math.OC q-fin.EC\n",
      "eess.SP cs.SI nlin.CD\n",
      "cs.LG cs.NE stat.ML\n",
      "cond-mat.stat-mech math-ph math.MP nlin.SI\n",
      "cond-mat.stat-mech cond-mat.dis-nn physics.comp-ph\n",
      "math.AT math.GR math.GT\n",
      "cond-mat.mes-hall physics.app-ph physics.optics\n",
      "astro-ph.HE gr-qc nucl-th\n",
      "cond-mat.soft cond-mat.supr-con\n",
      "quant-ph cond-mat.mes-hall physics.atom-ph physics.chem-ph\n",
      "math.DG math-ph math.GT math.MP math.QA\n",
      "hep-ph gr-qc\n",
      "stat.ML cs.LG physics.data-an\n",
      "cs.LG cs.CR cs.MA stat.ML\n",
      "cond-mat.mes-hall cond-mat.stat-mech cond-mat.supr-con\n",
      "math.NA astro-ph.CO astro-ph.EP astro-ph.GA cs.NA physics.comp-ph\n",
      "cs.LG cs.AI\n",
      "cond-mat.stat-mech cond-mat.str-el physics.comp-ph\n",
      "physics.optics cond-mat.mes-hall cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "cond-mat.mes-hall physics.med-ph quant-ph\n",
      "physics.flu-dyn math-ph math.AP math.MP\n",
      "math.AP physics.comp-ph\n",
      "nlin.PS quant-ph\n",
      "cond-mat.quant-gas hep-ph quant-ph\n",
      "q-bio.QM cs.CV cs.LG eess.IV stat.ML\n",
      "cond-mat.mes-hall cond-mat.stat-mech hep-th\n",
      "gr-qc astro-ph.CO math-ph math.MP\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con\n",
      "cs.RO cs.CV eess.IV\n",
      "cond-mat.mes-hall astro-ph.IM cond-mat.mtrl-sci cond-mat.supr-con quant-ph\n",
      "quant-ph cond-mat.quant-gas physics.optics\n",
      "math-ph cond-mat.mes-hall math.CO math.MP math.PR math.SP\n",
      "cs.RO cs.NA math.NA\n",
      "cs.GR cs.HC\n",
      "astro-ph.HE astro-ph.GA gr-qc\n",
      "nlin.AO math.DS math.GN q-bio.PE\n",
      "cond-mat.supr-con cond-mat.str-el hep-th\n",
      "cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
      "physics.acc-ph physics.data-an\n",
      "gr-qc astro-ph.IM stat.ML\n",
      "nlin.CD cond-mat.stat-mech math.CO math.OC\n",
      "physics.bio-ph nlin.AO\n",
      "hep-ph cond-mat.quant-gas hep-lat\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.soft\n",
      "quant-ph cond-mat.quant-gas nlin.CD\n",
      "physics.comp-ph cs.NA math.NA\n",
      "hep-th math-ph math.DG math.MP\n",
      "nucl-th cond-mat.mes-hall\n",
      "physics.ins-det cond-mat.mtrl-sci quant-ph\n",
      "physics.chem-ph cond-mat.soft\n",
      "cond-mat.soft physics.comp-ph\n",
      "math-ph math.CA math.CV math.MP\n",
      "cond-mat.mtrl-sci math.AG\n",
      "cond-mat.stat-mech cs.IT math.IT nlin.AO q-bio.SC\n",
      "cond-mat.soft physics.chem-ph\n",
      "hep-ph astro-ph.CO astro-ph.HE astro-ph.SR\n",
      "math.ST stat.OT stat.TH\n",
      "eess.IV cs.NA math.NA\n",
      "eess.SY cs.SY eess.SP\n",
      "math-ph cond-mat.stat-mech math.MP\n",
      "cond-mat.quant-gas nlin.SI quant-ph\n",
      "cond-mat.str-el cond-mat.quant-gas hep-lat hep-th\n",
      "cond-mat.stat-mech hep-th nlin.CD quant-ph\n",
      "cond-mat.str-el physics.chem-ph quant-ph\n",
      "nucl-th cond-mat.str-el hep-ph\n",
      "stat.ML cs.LG cs.SI physics.data-an q-bio.MN\n",
      "cond-mat.stat-mech physics.atom-ph\n",
      "cs.RO cs.DS\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci hep-th\n",
      "cond-mat.soft physics.class-ph physics.geo-ph\n",
      "physics.plasm-ph astro-ph.SR cond-mat.mes-hall physics.flu-dyn\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.quant-gas\n",
      "math-ph gr-qc math.MP\n",
      "hep-th astro-ph.CO hep-ph\n",
      "cond-mat.quant-gas cond-mat.dis-nn\n",
      "math.QA math.RT\n",
      "quant-ph nlin.AO physics.data-an\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft physics.bio-ph physics.soc-ph\n",
      "cs.MA cs.SY eess.SY\n",
      "gr-qc physics.comp-ph\n",
      "quant-ph stat.ML\n",
      "physics.comp-ph cond-mat.stat-mech physics.flu-dyn\n",
      "cond-mat.supr-con cond-mat.stat-mech\n",
      "stat.ME q-bio.QM stat.AP\n",
      "cond-mat.str-el physics.chem-ph\n",
      "cs.CV cs.RO math.OC\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.quant-gas physics.atom-ph\n",
      "cond-mat.stat-mech physics.atom-ph quant-ph\n",
      "physics.soc-ph cond-mat.stat-mech nlin.AO\n",
      "gr-qc hep-th physics.class-ph\n",
      "astro-ph.EP astro-ph.SR\n",
      "physics.soc-ph q-bio.PE q-bio.QM\n",
      "astro-ph.HE gr-qc hep-th\n",
      "cond-mat.str-el gr-qc hep-ph\n",
      "math.RA math.CO\n",
      "physics.optics cond-mat.mes-hall physics.ins-det quant-ph\n",
      "eess.SP physics.optics\n",
      "cond-mat.quant-gas cond-mat.mes-hall physics.atom-ph quant-ph\n",
      "physics.flu-dyn physics.app-ph physics.plasm-ph\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.chem-ph q-bio.SC\n",
      "hep-ph hep-ex physics.atom-ph\n",
      "cond-mat.dis-nn cond-mat.mes-hall\n",
      "quant-ph cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph\n",
      "math.SG math-ph math.DG math.MP math.RT\n",
      "math.PR q-bio.QM q-bio.SC\n",
      "cond-mat.mes-hall cond-mat.quant-gas cond-mat.supr-con quant-ph\n",
      "gr-qc astro-ph.CO hep-th physics.data-an\n",
      "cs.RO cs.LG cs.MA cs.NE\n",
      "cs.DC cs.LG hep-ex\n",
      "cond-mat.mtrl-sci hep-ph\n",
      "cond-mat.dis-nn cond-mat.stat-mech physics.soc-ph\n",
      "cs.RO cs.AI cs.LG\n",
      "astro-ph.SR astro-ph.HE astro-ph.IM hep-ex\n",
      "cs.AI q-bio.QM\n",
      "math.DS math.AG math.SG\n",
      "math-ph math.MP math.OA math.SP quant-ph\n",
      "q-bio.MN cond-mat.stat-mech\n",
      "hep-ph astro-ph.CO astro-ph.EP\n",
      "quant-ph physics.app-ph physics.optics stat.AP\n",
      "math.OC math.DS\n",
      "eess.IV cs.CV cs.LG q-bio.QM q-bio.TO\n",
      "astro-ph.IM astro-ph.GA cs.CV\n",
      "physics.atom-ph cond-mat.quant-gas physics.ins-det\n",
      "gr-qc astro-ph.HE hep-ph\n",
      "physics.bio-ph eess.IV\n",
      "cond-mat.str-el cond-mat.other quant-ph\n",
      "physics.comp-ph physics.atom-ph\n",
      "cond-mat.str-el physics.atom-ph\n",
      "nucl-th cond-mat.mes-hall hep-ph\n",
      "cs.NE cs.LG physics.chem-ph physics.comp-ph\n",
      "hep-ph astro-ph.EP hep-ex\n",
      "cond-mat.dis-nn cs.LG quant-ph\n",
      "physics.optics astro-ph.IM\n",
      "physics.comp-ph cond-mat.mes-hall physics.optics\n",
      "cs.RO cs.LG\n",
      "physics.chem-ph physics.comp-ph quant-ph\n",
      "math-ph math.CV math.MP math.PR\n",
      "hep-th cond-mat.mes-hall cond-mat.str-el nucl-th\n",
      "q-bio.GN stat.AP stat.ME\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el hep-lat hep-th\n",
      "cond-mat.stat-mech math-ph math.MP physics.data-an\n",
      "physics.app-ph physics.plasm-ph\n",
      "physics.atom-ph cond-mat.dis-nn quant-ph\n",
      "hep-ph hep-lat hep-th nucl-th\n",
      "cond-mat.str-el cond-mat.mtrl-sci physics.chem-ph\n",
      "cond-mat.soft physics.chem-ph physics.optics\n",
      "hep-ph astro-ph.HE gr-qc hep-th\n",
      "astro-ph.HE physics.plasm-ph\n",
      "gr-qc math.NT\n",
      "stat.CO stat.AP\n",
      "astro-ph.CO astro-ph.GA hep-ph\n",
      "physics.geo-ph nlin.PS\n",
      "nlin.AO nlin.PS\n",
      "cond-mat.supr-con physics.app-ph\n",
      "quant-ph cond-mat.mtrl-sci\n",
      "physics.app-ph cond-mat.dis-nn cond-mat.mtrl-sci physics.atom-ph physics.optics\n",
      "econ.TH nlin.AO\n",
      "cs.LO cs.LG cs.PL\n",
      "cond-mat.dis-nn cond-mat.mes-hall physics.optics\n",
      "hep-ph hep-ex hep-lat nucl-th\n",
      "astro-ph.CO astro-ph.IM gr-qc\n",
      "physics.atm-clus physics.chem-ph\n",
      "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.quant-gas cond-mat.str-el quant-ph\n",
      "astro-ph.CO astro-ph.HE astro-ph.IM hep-ph physics.data-an\n",
      "cond-mat.stat-mech cond-mat.dis-nn hep-th quant-ph\n",
      "cond-mat.stat-mech cs.LG physics.comp-ph stat.ML\n",
      "gr-qc hep-th math.DG\n",
      "physics.bio-ph cond-mat.stat-mech\n",
      "physics.comp-ph cond-mat.str-el physics.chem-ph\n",
      "math.GR math.NT math.RT\n",
      "eess.IV cs.CV physics.med-ph\n",
      "physics.flu-dyn cond-mat.other\n",
      "physics.plasm-ph hep-ph nlin.PS\n",
      "nlin.PS physics.class-ph\n",
      "cond-mat.other physics.class-ph\n",
      "cs.RO eess.SP\n",
      "hep-ph astro-ph.CO gr-qc hep-th physics.atom-ph\n",
      "physics.ao-ph cond-mat.stat-mech nlin.CD physics.flu-dyn physics.geo-ph\n",
      "math.OC cs.SY eess.SY physics.space-ph\n",
      "math.NA cs.NA math.AP math.FA\n",
      "gr-qc astro-ph.GA astro-ph.HE\n",
      "math.SP\n",
      "eess.SP cs.LG eess.IV stat.ML\n",
      "nlin.AO q-bio.PE\n",
      "cond-mat.soft cond-mat.stat-mech physics.comp-ph physics.flu-dyn\n",
      "hep-th gr-qc math-ph math.MP quant-ph\n",
      "cond-mat.soft cond-mat.other\n",
      "physics.soc-ph nlin.AO physics.data-an\n",
      "math.GR math.DS\n",
      "hep-ph astro-ph.CO astro-ph.SR hep-ex\n",
      "cond-mat.str-el cond-mat.soft cond-mat.stat-mech hep-th\n",
      "cond-mat.dis-nn cond-mat.quant-gas cond-mat.str-el\n",
      "physics.soc-ph cond-mat.stat-mech cs.SI math.CO nlin.AO\n",
      "cs.NE cs.LG physics.geo-ph\n",
      "cond-mat.mes-hall physics.plasm-ph\n",
      "cs.IT math-ph math.IT math.MP math.ST stat.TH\n",
      "nlin.PS physics.optics\n",
      "cond-mat.quant-gas hep-lat nucl-th\n",
      "q-bio.PE q-bio.MN\n",
      "cond-mat.stat-mech hep-lat quant-ph\n",
      "math-ph math.MP physics.class-ph\n",
      "math.PR math.ST stat.CO stat.TH\n",
      "math.AT math.AC\n",
      "hep-ph astro-ph.HE gr-qc\n",
      "quant-ph cond-mat.stat-mech math-ph math.MP nlin.CD\n",
      "cond-mat.stat-mech astro-ph.GA\n",
      "physics.flu-dyn physics.bio-ph\n",
      "cs.CC cs.CG cs.DM math.CO physics.soc-ph\n",
      "math-ph hep-th math.CA math.MP\n",
      "cond-mat.str-el cond-mat.stat-mech hep-th quant-ph\n",
      "hep-ph astro-ph.HE hep-lat nucl-th\n",
      "cs.SI physics.data-an stat.AP\n",
      "physics.flu-dyn math.OC stat.ML\n",
      "hep-ph cond-mat.mtrl-sci hep-ex\n",
      "stat.CO cs.NA math.NA\n",
      "quant-ph physics.atom-ph physics.chem-ph physics.optics\n",
      "quant-ph cond-mat.quant-gas physics.flu-dyn\n",
      "cond-mat.str-el cond-mat.quant-gas physics.atom-ph\n",
      "cond-mat.stat-mech cond-mat.str-el math-ph math.MP quant-ph\n",
      "stat.AP cs.LG econ.EM\n",
      "cond-mat.soft physics.bio-ph physics.comp-ph\n",
      "hep-th cond-mat.str-el math-ph math.MP nlin.SI\n",
      "cond-mat.soft cond-mat.stat-mech physics.chem-ph\n",
      "gr-qc cond-mat.stat-mech math-ph math.MP\n",
      "cond-mat.mes-hall physics.comp-ph quant-ph\n",
      "physics.soc-ph physics.geo-ph\n",
      "cs.RO cs.LG stat.ML\n",
      "physics.hist-ph physics.space-ph\n",
      "hep-ph hep-ex hep-th\n",
      "cond-mat.mes-hall cond-mat.str-el math-ph math.MP physics.optics quant-ph\n",
      "quant-ph hep-ph hep-th\n",
      "math.NT math.AT math.CT math.GR math.KT\n",
      "cs.CL cs.AI\n",
      "hep-ph hep-ex hep-th nucl-th\n",
      "math.OC cs.LG stat.ML\n",
      "cs.PF cs.SY eess.SY physics.space-ph\n",
      "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.soft cond-mat.str-el physics.ins-det\n",
      "math.AP math-ph math.DG math.MP math.SP\n",
      "math-ph cond-mat.quant-gas cond-mat.stat-mech math.MP\n",
      "physics.comp-ph cond-mat.dis-nn physics.chem-ph\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
      "cond-mat.soft physics.class-ph\n",
      "physics.chem-ph cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "cs.IT math-ph math.IT math.MP quant-ph\n",
      "physics.bio-ph physics.optics\n",
      "quant-ph math-ph math.MP physics.comp-ph\n",
      "nlin.CD astro-ph.HE\n",
      "astro-ph.SR astro-ph.GA\n",
      "hep-th math-ph math.MP nlin.PS\n",
      "nucl-th physics.comp-ph\n",
      "quant-ph stat.AP\n",
      "astro-ph.GA gr-qc hep-ph physics.hist-ph\n",
      "physics.soc-ph cond-mat.stat-mech cs.GT\n",
      "cs.LG math.ST stat.ML stat.TH\n",
      "cond-mat.soft physics.bio-ph physics.chem-ph\n",
      "nlin.CD physics.data-an\n",
      "cs.CV cs.GR\n",
      "astro-ph.CO astro-ph.GA gr-qc hep-th\n",
      "cs.CY eess.SP\n",
      "hep-ph astro-ph.CO gr-qc hep-th\n",
      "physics.soc-ph cond-mat.stat-mech cs.SI\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.supr-con hep-th quant-ph\n",
      "physics.atom-ph physics.plasm-ph\n",
      "math-ph cond-mat.stat-mech math.MP quant-ph\n",
      "cond-mat.supr-con physics.ins-det\n",
      "hep-th cond-mat.soft cond-mat.str-el\n",
      "cs.LG cs.NE cs.SE stat.ML\n",
      "physics.optics eess.IV\n",
      "astro-ph.GA cond-mat.stat-mech gr-qc\n",
      "physics.bio-ph cond-mat.dis-nn\n",
      "cs.CG cond-mat.mtrl-sci\n",
      "math.OC cs.DC cs.LG eess.SP stat.ML\n",
      "cs.HC cs.LG\n",
      "hep-th cond-mat.stat-mech math-ph math.MP math.QA\n",
      "physics.comp-ph cs.CE cs.NA math.NA\n",
      "physics.app-ph cond-mat.soft physics.flu-dyn\n",
      "astro-ph.SR astro-ph.GA physics.flu-dyn\n",
      "cond-mat.stat-mech cs.CC physics.soc-ph\n",
      "cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el\n",
      "cs.DS cs.DB cs.IR cs.LG\n",
      "astro-ph.HE astro-ph.GA\n",
      "astro-ph.SR astro-ph.EP astro-ph.GA\n",
      "hep-ph astro-ph.CO astro-ph.GA\n",
      "cond-mat.stat-mech hep-ph\n",
      "physics.ins-det physics.data-an\n",
      "cond-mat.quant-gas cond-mat.mtrl-sci\n",
      "cond-mat.stat-mech cs.GT physics.data-an\n",
      "cs.HC\n",
      "physics.data-an physics.soc-ph stat.CO\n",
      "cs.CV math.OC\n",
      "math-ph cond-mat.stat-mech math.CO math.MP\n",
      "cs.CV cs.LG cs.RO\n",
      "q-bio.PE math.DS physics.soc-ph\n",
      "nlin.PS math-ph math.MP physics.optics\n",
      "math.CA math.PR\n",
      "math.NA cs.LG cs.NA cs.NE stat.ML\n",
      "astro-ph.GA astro-ph.SR\n",
      "cond-mat.soft cond-mat.mtrl-sci nlin.PS physics.flu-dyn\n",
      "cond-mat.mes-hall physics.atom-ph\n",
      "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech cs.CG\n",
      "math-ph cond-mat.stat-mech hep-lat math.MP\n",
      "nlin.AO quant-ph\n",
      "physics.class-ph cond-mat.mes-hall math-ph math.MP\n",
      "math.RA math.GR math.GT math.QA math.RT\n",
      "cond-mat.stat-mech cond-mat.str-el hep-lat\n",
      "math-ph cond-mat.dis-nn hep-th math.MP nlin.CD quant-ph\n",
      "physics.app-ph cond-mat.mtrl-sci physics.comp-ph physics.data-an physics.ins-det\n",
      "astro-ph.IM hep-ph quant-ph\n",
      "physics.chem-ph cond-mat.stat-mech\n",
      "hep-ph physics.atom-ph\n",
      "quant-ph cond-mat.quant-gas hep-lat nucl-th\n",
      "hep-lat astro-ph.CO hep-ph hep-th\n",
      "astro-ph.GA astro-ph.IM\n",
      "cond-mat.mtrl-sci nlin.PS physics.comp-ph\n",
      "physics.flu-dyn math-ph math.MP nlin.PS nlin.SI\n",
      "cs.IT eess.SP math.IT math.OC\n",
      "gr-qc hep-th nlin.CD quant-ph\n",
      "quant-ph cond-mat.str-el hep-lat\n",
      "physics.ins-det cond-mat.mes-hall\n",
      "astro-ph.IM physics.class-ph\n",
      "physics.comp-ph physics.data-an\n",
      "quant-ph cond-mat.other cond-mat.quant-gas\n",
      "q-bio.BM cond-mat.soft physics.bio-ph\n",
      "physics.acc-ph physics.plasm-ph\n",
      "cs.CY cs.SI\n",
      "physics.acc-ph nlin.SI\n",
      "hep-ex astro-ph.CO physics.ins-det\n",
      "astro-ph.GA hep-ex\n",
      "physics.geo-ph physics.ao-ph physics.space-ph\n",
      "astro-ph.SR astro-ph.EP astro-ph.IM\n",
      "eess.IV cs.CV cs.LG eess.SP physics.med-ph\n",
      "q-fin.ST cs.AI cs.CE cs.LG\n",
      "astro-ph.IM astro-ph.EP physics.ins-det\n",
      "cs.LG cs.AI cs.HC stat.ML\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP\n",
      "physics.app-ph cond-mat.mes-hall physics.flu-dyn\n",
      "eess.AS cs.SD eess.SP\n",
      "cs.NE cs.CV cs.LG q-bio.NC\n",
      "math.OC cs.LG cs.NA math.NA stat.ML\n",
      "cond-mat.stat-mech cond-mat.quant-gas\n",
      "hep-th cond-mat.dis-nn math-ph math.MP quant-ph\n",
      "cond-mat.str-el cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.stat-mech\n",
      "hep-lat cond-mat.str-el hep-th math.DG\n",
      "cond-mat.supr-con cond-mat.str-el physics.optics\n",
      "astro-ph.SR physics.ao-ph physics.flu-dyn\n",
      "hep-th nlin.PS\n",
      "cs.DC q-bio.QM\n",
      "cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML\n",
      "cond-mat.other physics.optics\n",
      "cond-mat.mes-hall nlin.CD\n",
      "hep-th cond-mat.stat-mech cond-mat.str-el hep-ph\n",
      "astro-ph.CO nlin.PS physics.plasm-ph\n",
      "cond-mat.str-el physics.comp-ph quant-ph\n",
      "astro-ph.SR physics.plasm-ph\n",
      "gr-qc astro-ph.HE physics.comp-ph\n",
      "cs.CR cs.LG\n",
      "astro-ph.CO astro-ph.GA gr-qc hep-ph hep-th\n",
      "stat.CO cs.NE\n",
      "nlin.CD physics.class-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn math-ph math.MP\n",
      "cond-mat.soft physics.bio-ph physics.flu-dyn\n",
      "math.SP math-ph math.MP quant-ph\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con quant-ph\n",
      "hep-ph cond-mat.mtrl-sci\n",
      "cond-mat.supr-con cond-mat.stat-mech cond-mat.str-el\n",
      "hep-th cond-mat.stat-mech gr-qc quant-ph\n",
      "quant-ph cond-mat.stat-mech physics.atom-ph\n",
      "hep-lat physics.comp-ph\n",
      "nucl-th cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con\n",
      "physics.gen-ph hep-ph\n",
      "quant-ph cond-mat.stat-mech stat.ML\n",
      "astro-ph.EP astro-ph.GA\n",
      "cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "cs.SI cs.IR\n",
      "cond-mat.mtrl-sci cs.LG physics.comp-ph\n",
      "hep-ph nucl-ex\n",
      "math.NA cs.LG cs.NA\n",
      "eess.AS cs.LG cs.SD\n",
      "cs.CR cs.MM\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.quant-gas\n",
      "quant-ph cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
      "quant-ph cond-mat.other physics.atom-ph\n",
      "quant-ph physics.plasm-ph\n",
      "eess.IV eess.SP\n",
      "hep-th hep-ph math-ph math.MP\n",
      "quant-ph hep-ex\n",
      "cs.LG cs.AI quant-ph stat.ML\n",
      "q-bio.GN\n",
      "physics.plasm-ph math-ph math.MP\n",
      "physics.geo-ph physics.ao-ph physics.bio-ph\n",
      "math.PR q-bio.QM\n",
      "quant-ph cond-mat.mes-hall gr-qc hep-th math-ph math.MP\n",
      "physics.data-an nlin.CD\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.mtrl-sci physics.class-ph\n",
      "physics.ed-ph physics.ins-det\n",
      "nucl-th nucl-ex stat.ML\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.soft physics.class-ph\n",
      "quant-ph cond-mat.mes-hall nlin.AO\n",
      "eess.IV cond-mat.mtrl-sci\n",
      "cond-mat.soft cs.LG physics.chem-ph\n",
      "cond-mat.stat-mech math-ph math.MP nlin.CD quant-ph\n",
      "math.CO math.AT\n",
      "astro-ph.HE astro-ph.IM hep-ex\n",
      "cond-mat.stat-mech hep-th nlin.SI\n",
      "quant-ph cond-mat.str-el physics.chem-ph\n",
      "stat.CO cs.SY eess.SY\n",
      "math.FA math.CV\n",
      "math-ph cond-mat.soft math.MP\n",
      "nlin.SI math-ph math.MP\n",
      "hep-ex hep-ph physics.ins-det\n",
      "nucl-th hep-ph hep-th nucl-ex\n",
      "cond-mat.str-el hep-lat math-ph math.MP\n",
      "cond-mat.stat-mech cs.LG stat.ML\n",
      "math.RT math.AG\n",
      "cs.CR cs.DC cs.LG\n",
      "physics.bio-ph cond-mat.stat-mech q-bio.MN\n",
      "hep-th cond-mat.stat-mech cond-mat.str-el hep-lat hep-ph\n",
      "cs.DM math.ST stat.TH\n",
      "physics.atom-ph cond-mat.quant-gas physics.chem-ph\n",
      "hep-th cond-mat.str-el math.AT\n",
      "cond-mat.stat-mech physics.bio-ph q-bio.BM\n",
      "q-bio.TO physics.bio-ph\n",
      "cond-mat.quant-gas physics.atm-clus physics.atom-ph physics.optics quant-ph\n",
      "cs.CG\n",
      "cs.CL cs.AI cs.LG\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.flu-dyn physics.plasm-ph\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech hep-th math-ph math.MP\n",
      "nucl-ex hep-ph\n",
      "math.AG math.KT\n",
      "quant-ph cs.NA math.NA\n",
      "cs.LG cs.CV cs.IT math.IT stat.ML\n",
      "cond-mat.str-el hep-ph hep-th\n",
      "hep-th cond-mat.str-el hep-ph math.AT\n",
      "hep-th gr-qc physics.flu-dyn\n",
      "cond-mat.str-el cond-mat.other cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.mes-hall cond-mat.str-el\n",
      "quant-ph cond-mat.quant-gas physics.atom-ph physics.chem-ph\n",
      "cond-mat.stat-mech cs.SI physics.soc-ph stat.ML\n",
      "cond-mat.quant-gas nucl-th quant-ph\n",
      "cond-mat.stat-mech physics.gen-ph\n",
      "quant-ph cond-mat.mtrl-sci cond-mat.str-el\n",
      "physics.app-ph cond-mat.other cs.NA math.NA physics.comp-ph\n",
      "gr-qc nlin.CD\n",
      "astro-ph.CO physics.atom-ph\n",
      "astro-ph.IM eess.IV stat.ME stat.ML\n",
      "hep-th cond-mat.stat-mech cond-mat.str-el\n",
      "astro-ph.GA astro-ph.CO astro-ph.SR\n",
      "math.KT math.AT\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.flu-dyn q-bio.TO\n",
      "physics.ins-det hep-ex hep-ph nucl-ex\n",
      "physics.chem-ph cond-mat.stat-mech cs.CE math.PR\n",
      "hep-ph cond-mat.str-el hep-th nucl-th\n",
      "hep-ph hep-ex hep-lat nucl-ex nucl-th\n",
      "cs.LG math.DS stat.CO stat.ME stat.ML\n",
      "q-bio.QM cs.LG stat.ML\n",
      "quant-ph cond-mat.stat-mech cs.IT math.IT\n",
      "physics.plasm-ph cond-mat.mes-hall physics.flu-dyn\n",
      "cs.LG eess.IV physics.med-ph stat.ML\n",
      "cs.DC cs.DB cs.LG stat.ML\n",
      "stat.ML cs.AI cs.LG cs.MA physics.data-an\n",
      "cond-mat.str-el hep-th math.QA quant-ph\n",
      "gr-qc stat.ML\n",
      "cond-mat.supr-con cond-mat.other physics.flu-dyn\n",
      "physics.soc-ph cond-mat.stat-mech cs.MA\n",
      "gr-qc hep-th nucl-th physics.flu-dyn\n",
      "cond-mat.stat-mech cond-mat.mes-hall math-ph math.MP\n",
      "cs.LG cs.DS math.OC math.PR stat.ML\n",
      "astro-ph.HE hep-th\n",
      "q-bio.PE cs.SI math.CO\n",
      "cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
      "cond-mat.str-el physics.app-ph\n",
      "hep-ph cond-mat.supr-con hep-th math-ph math.MP quant-ph\n",
      "q-bio.PE cond-mat.stat-mech physics.bio-ph\n",
      "cs.CL cs.LG\n",
      "cs.CL cs.IR\n",
      "physics.space-ph astro-ph.EP astro-ph.SR\n",
      "q-bio.CB cond-mat.stat-mech math.PR\n",
      "cs.CE cs.LG stat.ML\n",
      "cs.IT cs.SI math.IT physics.data-an q-bio.NC\n",
      "nlin.PS cond-mat.quant-gas\n",
      "physics.app-ph physics.data-an\n",
      "gr-qc astro-ph.IM physics.optics\n",
      "cs.CC cs.CR\n",
      "cs.CR cs.DC cs.NI cs.SI cs.SY eess.SY\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.soft cond-mat.supr-con\n",
      "math.DS physics.comp-ph physics.flu-dyn\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph physics.comp-ph\n",
      "hep-th cond-mat.str-el quant-ph\n",
      "physics.optics cs.SY eess.SY physics.class-ph quant-ph\n",
      "cond-mat.mtrl-sci physics.ins-det\n",
      "hep-th astro-ph.HE gr-qc math-ph math.MP\n",
      "physics.optics cond-mat.mes-hall physics.class-ph\n",
      "math.CA math.OC\n",
      "cond-mat.soft physics.plasm-ph\n",
      "math-ph cond-mat.stat-mech math.CO math.DS math.MP\n",
      "cond-mat.quant-gas cond-mat.supr-con quant-ph\n",
      "cs.CE cs.DC cs.PF q-bio.GN\n",
      "physics.data-an physics.med-ph q-bio.QM\n",
      "physics.app-ph physics.comp-ph\n",
      "eess.IV cs.CV cs.LG physics.med-ph\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.other\n",
      "quant-ph cond-mat.stat-mech physics.bio-ph\n",
      "hep-ph hep-th math.CO\n",
      "hep-ph cond-mat.quant-gas cond-mat.stat-mech\n",
      "physics.chem-ph cond-mat.mes-hall physics.comp-ph\n",
      "cs.PL cs.LO\n",
      "physics.hist-ph physics.class-ph\n",
      "physics.comp-ph cond-mat.stat-mech quant-ph\n",
      "cond-mat.supr-con cond-mat.mtrl-sci physics.app-ph\n",
      "nucl-ex nlin.AO physics.atom-ph physics.hist-ph physics.pop-ph\n",
      "nlin.AO math.OC physics.soc-ph\n",
      "math-ph math.MP physics.chem-ph\n",
      "math.SP math.FA\n",
      "quant-ph physics.bio-ph physics.chem-ph\n",
      "gr-qc physics.optics quant-ph\n",
      "physics.ins-det astro-ph.IM hep-ph\n",
      "cs.DL\n",
      "nucl-th physics.comp-ph physics.data-an\n",
      "cond-mat.dis-nn cond-mat.mtrl-sci\n",
      "cond-mat.soft astro-ph.HE cond-mat.mtrl-sci nucl-th\n",
      "physics.optics cond-mat.mes-hall cond-mat.quant-gas nlin.PS\n",
      "math.OC math.DS nlin.CD\n",
      "cond-mat.stat-mech nlin.AO q-bio.NC\n",
      "gr-qc astro-ph.GA astro-ph.HE hep-th\n",
      "cs.GT cs.CC cs.DS\n",
      "hep-ph gr-qc hep-ex\n",
      "nucl-th hep-lat hep-ph\n",
      "astro-ph.CO astro-ph.GA gr-qc\n",
      "hep-th cond-mat.str-el math-ph math.MP\n",
      "astro-ph.CO astro-ph.IM physics.ins-det\n",
      "cond-mat.str-el cond-mat.stat-mech math-ph math.MP quant-ph\n",
      "quant-ph physics.app-ph physics.atom-ph physics.optics\n",
      "physics.app-ph physics.flu-dyn\n",
      "quant-ph gr-qc physics.optics\n",
      "nlin.PS cond-mat.stat-mech\n",
      "gr-qc astro-ph.SR\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.optics\n",
      "astro-ph.EP astro-ph.IM\n",
      "cs.CL cs.IR cs.LG\n",
      "nlin.CD math.DS physics.chem-ph\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.stat-mech\n",
      "hep-th cond-mat.stat-mech cond-mat.str-el nucl-th\n",
      "quant-ph hep-lat stat.ML\n",
      "hep-th math.DG math.SG\n",
      "nucl-th astro-ph.SR\n",
      "physics.bio-ph cond-mat.soft q-bio.TO\n",
      "physics.atom-ph physics.app-ph quant-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci hep-th\n",
      "cs.LG cs.IR stat.ML\n",
      "cs.CV cs.CR cs.LG\n",
      "quant-ph cs.CR math.FA math.PR\n",
      "physics.chem-ph cond-mat.other\n",
      "math.RA math-ph math.MP\n",
      "gr-qc astro-ph.HE nucl-th\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci\n",
      "cs.PL cs.DC\n",
      "cond-mat.stat-mech cond-mat.mtrl-sci\n",
      "nucl-th astro-ph.CO\n",
      "nucl-th cond-mat.supr-con hep-ph\n",
      "hep-lat cond-mat.mes-hall cs.LG hep-th\n",
      "stat.ML cond-mat.dis-nn cs.LG cs.NE q-bio.NC\n",
      "astro-ph.CO hep-ex hep-ph nucl-ex physics.ins-det\n",
      "cond-mat.dis-nn math.DS nlin.AO nlin.CD nlin.PS\n",
      "physics.optics physics.atom-ph\n",
      "math.AP math-ph math.MP physics.flu-dyn\n",
      "physics.plasm-ph astro-ph.SR physics.space-ph\n",
      "eess.IV cs.AI cs.CV cs.LG\n",
      "cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "cond-mat.supr-con cond-mat.quant-gas cond-mat.str-el\n",
      "cs.DC cs.LG math.OC\n",
      "physics.app-ph physics.ins-det physics.optics\n",
      "cs.DS cs.DB\n",
      "hep-lat math-ph math.MP\n",
      "hep-ph hep-ex quant-ph\n",
      "hep-th math.GT math.QA\n",
      "physics.optics physics.data-an\n",
      "cond-mat.stat-mech hep-th math-ph math.MP nlin.PS\n",
      "astro-ph.IM cs.LG\n",
      "cond-mat.soft physics.bio-ph q-bio.BM\n",
      "cond-mat.soft physics.bio-ph q-bio.CB\n",
      "astro-ph.EP physics.ao-ph physics.geo-ph\n",
      "cs.SD cs.LG eess.AS stat.ML\n",
      "q-fin.ST cs.CE econ.EM\n",
      "physics.comp-ph cond-mat.soft cond-mat.stat-mech physics.bio-ph\n",
      "cond-mat.quant-gas cond-mat.dis-nn nlin.CD quant-ph\n",
      "cs.CV cs.LG eess.IV\n",
      "gr-qc astro-ph.IM cs.LG physics.data-an physics.ins-det\n",
      "physics.ins-det physics.class-ph physics.flu-dyn\n",
      "hep-ph hep-ex physics.data-an stat.ML\n",
      "cond-mat.str-el cond-mat.mes-hall quant-ph\n",
      "cond-mat.dis-nn cond-mat.mtrl-sci nlin.CD nucl-th\n",
      "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech\n",
      "cs.DS cs.DC cs.LG\n",
      "physics.ao-ph nlin.PS physics.flu-dyn physics.geo-ph\n",
      "cs.CR cs.CY\n",
      "cond-mat.stat-mech cond-mat.quant-gas hep-th quant-ph\n",
      "cond-mat.mes-hall cond-mat.stat-mech physics.optics quant-ph\n",
      "math.HO math.AC\n",
      "physics.atom-ph astro-ph.HE astro-ph.IM astro-ph.SR physics.plasm-ph\n",
      "astro-ph.CO astro-ph.GA physics.comp-ph\n",
      "math.DS nlin.AO\n",
      "cond-mat.stat-mech cs.LG q-bio.QM\n",
      "astro-ph.SR astro-ph.HE hep-ph\n",
      "cs.CV cs.LG cs.SD eess.AS eess.IV\n",
      "cond-mat.stat-mech cond-mat.str-el hep-lat hep-th\n",
      "physics.chem-ph cond-mat.soft physics.app-ph physics.comp-ph\n",
      "hep-ex physics.data-an\n",
      "cs.LG cs.SE\n",
      "astro-ph.GA cs.CV\n",
      "cs.LG cs.PL stat.ML\n",
      "q-bio.MN q-bio.BM\n",
      "cs.LG cs.CR cs.CV\n",
      "hep-th astro-ph.CO gr-qc quant-ph\n",
      "hep-th math-ph math.MP nlin.SI\n",
      "cond-mat.dis-nn cond-mat.mes-hall cond-mat.quant-gas\n",
      "physics.app-ph physics.class-ph\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.other\n",
      "physics.comp-ph cond-mat.mtrl-sci cond-mat.other physics.atom-ph physics.chem-ph\n",
      "cond-mat.soft math.CO\n",
      "cs.CL cs.AI cs.IR\n",
      "astro-ph.CO astro-ph.GA astro-ph.IM\n",
      "math.DS math.CA\n",
      "cond-mat.dis-nn hep-lat quant-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.stat-mech hep-th\n",
      "quant-ph hep-lat\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.str-el quant-ph\n",
      "cs.ET cond-mat.mes-hall physics.app-ph\n",
      "quant-ph nlin.CG\n",
      "stat.AP stat.ME\n",
      "cs.CL cs.CV cs.LG\n",
      "cs.IT cs.CR math.IT\n",
      "hep-th cond-mat.str-el hep-lat\n",
      "cond-mat.mtrl-sci cond-mat.stat-mech physics.comp-ph\n",
      "physics.optics physics.atm-clus physics.atom-ph\n",
      "physics.flu-dyn math.OC\n",
      "cond-mat.soft physics.ao-ph physics.flu-dyn physics.geo-ph\n",
      "stat.AP cs.AI math.OC\n",
      "cond-mat.stat-mech hep-lat\n",
      "gr-qc astro-ph.HE hep-ph hep-th\n",
      "nlin.PS physics.ao-ph physics.flu-dyn\n",
      "gr-qc physics.hist-ph\n",
      "nlin.SI nlin.PS physics.flu-dyn\n",
      "cs.LG cs.SD eess.AS stat.ML\n",
      "cs.CV cs.GR cs.LG eess.IV\n",
      "astro-ph.GA astro-ph.CO gr-qc\n",
      "cs.SI cs.CY physics.soc-ph\n",
      "cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech physics.class-ph\n",
      "cs.CC quant-ph\n",
      "physics.chem-ph cs.LG\n",
      "cond-mat.mtrl-sci astro-ph.IM gr-qc physics.ins-det\n",
      "physics.optics cond-mat.str-el\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph q-bio.OT\n",
      "quant-ph q-fin.RM\n",
      "nucl-th cond-mat.str-el\n",
      "quant-ph physics.acc-ph physics.optics\n",
      "physics.flu-dyn cond-mat.stat-mech math-ph math.MP nlin.CD\n",
      "cs.SC cs.LG\n",
      "cond-mat.dis-nn stat.ML\n",
      "math-ph cond-mat.mes-hall math.MP physics.optics quant-ph\n",
      "astro-ph.EP astro-ph.IM astro-ph.SR\n",
      "astro-ph.EP physics.space-ph\n",
      "gr-qc hep-lat hep-th quant-ph\n",
      "physics.plasm-ph physics.chem-ph\n",
      "physics.flu-dyn cond-mat.soft physics.geo-ph\n",
      "nlin.SI nlin.PS\n",
      "physics.atom-ph physics.chem-ph quant-ph\n",
      "physics.bio-ph astro-ph.SR cond-mat.stat-mech\n",
      "physics.soc-ph cs.GT\n",
      "math-ph math.CA math.MP\n",
      "math.FA quant-ph\n",
      "physics.geo-ph physics.app-ph\n",
      "cs.IR cs.AI cs.CL\n",
      "math.ST math.FA stat.ML stat.TH\n",
      "cs.MA nlin.AO\n",
      "cond-mat.supr-con cond-mat.other\n",
      "cs.NI cs.IT eess.SP math.IT\n",
      "cond-mat.mes-hall cond-mat.quant-gas hep-th\n",
      "astro-ph.EP astro-ph.HE hep-ph nucl-th\n",
      "cs.CL cs.SD eess.AS\n",
      "hep-ph astro-ph.CO astro-ph.HE gr-qc hep-th\n",
      "astro-ph.EP physics.ao-ph\n",
      "astro-ph.CO hep-ph nucl-ex\n",
      "cs.LG cs.CV cs.NE stat.ML\n",
      "cs.HC cs.AI cs.LG\n",
      "physics.med-ph physics.app-ph physics.bio-ph\n",
      "physics.bio-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
      "cond-mat.mes-hall math-ph math.MP\n",
      "hep-ph hep-ex physics.optics\n",
      "hep-th cond-mat.soft math-ph math.MP\n",
      "physics.flu-dyn eess.IV\n",
      "cond-mat.supr-con cond-mat.mtrl-sci quant-ph\n",
      "cs.CV cs.CY\n",
      "math.ST cond-mat.dis-nn cs.LG eess.SP stat.ML stat.TH\n",
      "q-bio.CB cond-mat.stat-mech physics.bio-ph\n",
      "cs.DB cs.AI\n",
      "stat.ML cond-mat.dis-nn cs.LG math.PR\n",
      "q-bio.QM eess.SP\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.str-el\n",
      "cs.AI cs.MA\n",
      "cs.DS math.OC\n",
      "physics.atom-ph cond-mat.mes-hall\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.comp-ph\n",
      "physics.app-ph physics.class-ph physics.optics\n",
      "physics.pop-ph astro-ph.EP astro-ph.IM astro-ph.SR\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.flu-dyn\n",
      "cond-mat.stat-mech cond-mat.mes-hall nlin.PS quant-ph\n",
      "math.CT math.PR\n",
      "hep-ph astro-ph.HE hep-th\n",
      "physics.chem-ph cond-mat.mes-hall quant-ph\n",
      "physics.comp-ph cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "physics.ins-det nucl-ex nucl-th\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.other physics.atm-clus physics.atom-ph\n",
      "quant-ph cs.AI cs.LG\n",
      "astro-ph.EP astro-ph.GA astro-ph.SR\n",
      "physics.optics nlin.AO quant-ph\n",
      "eess.IV physics.ins-det\n",
      "quant-ph hep-lat hep-ph hep-th nucl-th\n",
      "physics.atom-ph hep-ph nucl-th\n",
      "physics.data-an physics.ao-ph stat.ML\n",
      "cond-mat.stat-mech q-bio.PE\n",
      "nlin.SI nlin.PS physics.optics\n",
      "cond-mat.dis-nn cs.LG\n",
      "hep-ph hep-ex physics.comp-ph\n",
      "cond-mat.soft cond-mat.other physics.class-ph\n",
      "physics.ins-det astro-ph.IM\n",
      "quant-ph cond-mat.dis-nn physics.comp-ph\n",
      "q-bio.SC\n",
      "cond-mat.stat-mech physics.chem-ph\n",
      "nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.bio-ph physics.soc-ph\n",
      "hep-ph hep-ex nucl-th physics.comp-ph\n",
      "nlin.PS cond-mat.quant-gas physics.optics\n",
      "physics.atom-ph hep-ph\n",
      "stat.CO cs.CG cs.DC cs.DS cs.SE\n",
      "physics.atom-ph physics.ins-det quant-ph\n",
      "physics.acc-ph physics.ins-det\n",
      "nucl-th cond-mat.stat-mech hep-lat nucl-ex\n",
      "hep-th math-ph math.MP math.QA math.RT\n",
      "physics.med-ph eess.IV physics.app-ph q-bio.QM\n",
      "astro-ph.HE hep-ex\n",
      "physics.comp-ph physics.optics quant-ph\n",
      "eess.IV cond-mat.dis-nn physics.app-ph physics.optics\n",
      "cond-mat.mtrl-sci cond-mat.other nucl-ex\n",
      "cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
      "nlin.SI cond-mat.supr-con\n",
      "math.PR math.LO\n",
      "cs.MA cs.AI cs.RO\n",
      "hep-th cond-mat.mtrl-sci\n",
      "cond-mat.quant-gas nlin.PS physics.optics\n",
      "nucl-ex hep-ex hep-ph nucl-th\n",
      "cond-mat.other gr-qc hep-ph\n",
      "hep-th cond-mat.mes-hall hep-ph\n",
      "nucl-th quant-ph\n",
      "cond-mat.stat-mech physics.comp-ph physics.data-an\n",
      "stat.ML astro-ph.CO cs.LG stat.CO stat.ME\n",
      "quant-ph cond-mat.quant-gas cond-mat.str-el physics.comp-ph\n",
      "cs.LG stat.CO stat.ML\n",
      "cond-mat.soft cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
      "cond-mat.quant-gas physics.atm-clus\n",
      "cond-mat.stat-mech cond-mat.mtrl-sci cond-mat.soft\n",
      "nlin.AO cs.LG cs.NE stat.ML\n",
      "cond-mat.stat-mech nlin.PS physics.optics\n",
      "cond-mat.quant-gas cond-mat.stat-mech nlin.CD\n",
      "hep-th math-ph math.MP quant-ph\n",
      "q-bio.NC nlin.AO\n",
      "cond-mat.soft hep-th\n",
      "physics.atm-clus physics.atom-ph\n",
      "cs.LG cs.NI stat.ML\n",
      "physics.ed-ph cs.SI physics.soc-ph\n",
      "astro-ph.HE astro-ph.IM nucl-ex\n",
      "cond-mat.mes-hall cond-mat.quant-gas math-ph math.MP\n",
      "nucl-th hep-lat\n",
      "physics.optics cond-mat.quant-gas cs.LG nlin.PS\n",
      "eess.SP cs.LG stat.AP stat.ML\n",
      "quant-ph cs.LG physics.comp-ph\n",
      "quant-ph math-ph math.MP math.OA\n",
      "q-bio.QM cs.LG eess.IV\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.other nlin.CD quant-ph\n",
      "astro-ph.CO astro-ph.GA physics.flu-dyn\n",
      "hep-ph cond-mat.quant-gas\n",
      "cond-mat.mes-hall cond-mat.str-el cond-mat.supr-con hep-th\n",
      "astro-ph.IM astro-ph.HE gr-qc\n",
      "cs.IT cs.ET math.IT\n",
      "nucl-th hep-ex hep-lat hep-ph\n",
      "cs.NI eess.SP\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el math-ph math.MP quant-ph\n",
      "hep-th cond-mat.mes-hall gr-qc hep-ph\n",
      "physics.ins-det physics.acc-ph\n",
      "nlin.CD nlin.SI\n",
      "physics.optics cond-mat.mtrl-sci physics.app-ph physics.ins-det\n",
      "cs.ET cs.NE physics.optics\n",
      "physics.ao-ph cond-mat.soft physics.chem-ph\n",
      "physics.optics physics.comp-ph physics.plasm-ph\n",
      "cs.CY cs.CL cs.DL cs.LG\n",
      "cond-mat.stat-mech cs.LG cs.NE\n",
      "astro-ph.GA physics.chem-ph\n",
      "astro-ph.SR astro-ph.HE nucl-ex nucl-th\n",
      "astro-ph.SR astro-ph.HE physics.atom-ph\n",
      "cond-mat.mes-hall physics.ins-det\n",
      "cond-mat.supr-con cond-mat.mes-hall physics.app-ph\n",
      "nlin.CD physics.comp-ph quant-ph\n",
      "math.NA cond-mat.mtrl-sci cs.NA\n",
      "hep-th hep-lat\n",
      "physics.class-ph cs.CR\n",
      "physics.atom-ph physics.comp-ph quant-ph\n",
      "q-fin.TR cs.LG stat.ML\n",
      "hep-ph astro-ph.CO astro-ph.HE hep-ex\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
      "hep-th cond-mat.stat-mech cond-mat.str-el hep-lat quant-ph\n",
      "hep-th cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con hep-ph\n",
      "physics.comp-ph cond-mat.dis-nn cond-mat.mtrl-sci\n",
      "cond-mat.quant-gas hep-th\n",
      "cond-mat.mes-hall cond-mat.soft cond-mat.stat-mech physics.flu-dyn physics.optics\n",
      "physics.atom-ph physics.comp-ph physics.optics\n",
      "q-fin.RM stat.AP\n",
      "physics.chem-ph physics.bio-ph\n",
      "hep-ph physics.comp-ph\n",
      "cond-mat.mes-hall nlin.PS\n",
      "physics.ins-det physics.acc-ph physics.atom-ph physics.geo-ph\n",
      "cond-mat.supr-con cond-mat.quant-gas quant-ph\n",
      "cond-mat.stat-mech nlin.CG\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.supr-con\n",
      "hep-th cond-mat.dis-nn cond-mat.str-el\n",
      "cs.NE cs.NA math.NA\n",
      "cond-mat.dis-nn cond-mat.str-el physics.comp-ph\n",
      "quant-ph cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con physics.ins-det\n",
      "cs.RO cs.LG cs.SY eess.SY\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph physics.comp-ph\n",
      "nlin.PS cond-mat.soft physics.flu-dyn\n",
      "q-bio.QM cond-mat.stat-mech physics.data-an\n",
      "astro-ph.EP astro-ph.CO astro-ph.GA astro-ph.SR\n",
      "physics.atom-ph hep-ph nucl-th physics.chem-ph\n",
      "cond-mat.mtrl-sci physics.ins-det physics.optics\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.app-ph physics.flu-dyn\n",
      "hep-th hep-lat physics.comp-ph\n",
      "physics.chem-ph cond-mat.mes-hall\n",
      "cs.HC cs.LG eess.SP\n",
      "cond-mat.stat-mech cond-mat.dis-nn hep-th nlin.CD\n",
      "physics.comp-ph astro-ph.IM physics.flu-dyn\n",
      "cs.DL cs.CY cs.LG\n",
      "cond-mat.mes-hall cond-mat.dis-nn physics.optics\n",
      "cond-mat.stat-mech math-ph math.MP physics.optics\n",
      "astro-ph.GA physics.comp-ph physics.space-ph\n",
      "physics.flu-dyn cond-mat.soft cond-mat.stat-mech\n",
      "quant-ph cond-mat.quant-gas cond-mat.str-el hep-lat\n",
      "physics.class-ph physics.app-ph\n",
      "cond-mat.stat-mech physics.comp-ph physics.plasm-ph\n",
      "cond-mat.mes-hall hep-ph physics.atom-ph quant-ph\n",
      "cs.CR cs.CY cs.NI\n",
      "nucl-ex astro-ph.SR\n",
      "math-ph math.DG math.MP\n",
      "quant-ph cond-mat.stat-mech hep-th nlin.CD\n",
      "physics.comp-ph physics.chem-ph\n",
      "quant-ph cond-mat.quant-gas cond-mat.stat-mech math-ph math.MP physics.atom-ph\n",
      "quant-ph physics.med-ph q-bio.QM\n",
      "cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph\n",
      "cond-mat.supr-con hep-th math-ph math.MP\n",
      "hep-ph cs.LG physics.comp-ph\n",
      "hep-th cond-mat.stat-mech hep-lat\n",
      "cond-mat.other cond-mat.quant-gas\n",
      "physics.plasm-ph astro-ph.HE gr-qc\n",
      "physics.comp-ph physics.atm-clus physics.chem-ph\n",
      "hep-ph cond-mat.mes-hall cond-mat.str-el\n",
      "cond-mat.str-el cond-mat.stat-mech cond-mat.supr-con\n",
      "q-bio.SC q-bio.NC\n",
      "cs.GR q-bio.NC\n",
      "math.NA cs.NA math.OC stat.ML\n",
      "econ.TH math.GN\n",
      "gr-qc astro-ph.CO hep-th math-ph math.MP\n",
      "cond-mat.soft math.DG\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.supr-con nlin.PS\n",
      "astro-ph.EP physics.comp-ph\n",
      "eess.SP cs.DC cs.LG\n",
      "hep-th astro-ph.GA gr-qc hep-ph\n",
      "hep-th cond-mat.stat-mech hep-ph nucl-th\n",
      "cond-mat.mes-hall hep-ph nucl-th\n",
      "hep-th cond-mat.supr-con hep-ph math.AT nucl-th\n",
      "math.HO cs.MM\n",
      "cond-mat.quant-gas cond-mat.stat-mech hep-lat hep-ph quant-ph\n",
      "quant-ph cond-mat.dis-nn cond-mat.str-el cs.CV\n",
      "hep-th nucl-th\n",
      "cond-mat.dis-nn cond-mat.mes-hall physics.optics quant-ph\n",
      "eess.SY cs.MA cs.SY\n",
      "cs.CV cs.MM\n",
      "quant-ph physics.hist-ph physics.pop-ph\n",
      "nlin.PS q-bio.PE\n",
      "math.NA cs.NA q-bio.QM\n",
      "physics.comp-ph cond-mat.dis-nn\n",
      "math.CO cs.SC\n",
      "cond-mat.supr-con cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft\n",
      "hep-ph hep-lat quant-ph\n",
      "cs.CL cs.LG stat.ML\n",
      "eess.AS cs.SD\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.other\n",
      "cond-mat.dis-nn cond-mat.mes-hall nlin.CD physics.optics\n",
      "hep-ex physics.acc-ph physics.ins-det\n",
      "math-ph cond-mat.quant-gas math.CA math.MP physics.class-ph\n",
      "gr-qc math-ph math.DG math.MP\n",
      "cond-mat.soft nlin.AO\n",
      "math-ph math.DS math.MP physics.flu-dyn\n",
      "cond-mat.mtrl-sci cond-mat.str-el physics.app-ph\n",
      "cs.NI cs.DC\n",
      "cond-mat.stat-mech nlin.AO physics.bio-ph\n",
      "cond-mat.mes-hall cs.ET cs.SY eess.SY physics.comp-ph quant-ph\n",
      "eess.SP cs.CV\n",
      "hep-th hep-lat hep-ph nucl-th\n",
      "math.DG gr-qc math-ph math.MP physics.hist-ph\n",
      "stat.ML cs.LG physics.ao-ph\n",
      "cs.MS cond-mat.soft physics.chem-ph physics.comp-ph\n",
      "eess.SP eess.AS\n",
      "physics.data-an nucl-ex physics.ins-det\n",
      "cond-mat.mes-hall cond-mat.str-el physics.comp-ph\n",
      "cs.IR cs.LG stat.ML\n",
      "cs.SI cond-mat.soft cs.CG math.AT physics.soc-ph\n",
      "physics.atom-ph nucl-th\n",
      "cs.AI cs.LG cs.RO\n",
      "hep-th gr-qc hep-lat\n",
      "q-bio.PE q-bio.QM\n",
      "quant-ph cond-mat.stat-mech cs.IT math.IT nlin.CD\n",
      "physics.app-ph astro-ph.IM\n",
      "quant-ph cond-mat.mtrl-sci physics.atom-ph\n",
      "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el physics.atom-ph quant-ph\n",
      "cond-mat.mes-hall cond-mat.supr-con physics.ins-det quant-ph\n",
      "cond-mat.mtrl-sci physics.app-ph physics.ins-det physics.optics\n",
      "physics.bio-ph physics.ins-det q-bio.BM\n",
      "physics.plasm-ph astro-ph.HE\n",
      "cond-mat.dis-nn cond-mat.mes-hall physics.data-an\n",
      "physics.bio-ph cond-mat.mes-hall q-bio.CB quant-ph\n",
      "astro-ph.SR astro-ph.GA astro-ph.IM\n",
      "cond-mat.dis-nn cond-mat.mes-hall cond-mat.soft physics.comp-ph physics.optics\n",
      "cond-mat.mes-hall nlin.AO physics.optics\n",
      "q-bio.QM cond-mat.mes-hall physics.bio-ph quant-ph\n",
      "q-bio.NC nlin.AO physics.bio-ph\n",
      "physics.optics cond-mat.mes-hall cond-mat.mtrl-sci physics.atom-ph\n",
      "physics.space-ph astro-ph.SR\n",
      "cs.IR cs.DL\n",
      "hep-th cond-mat.dis-nn cond-mat.stat-mech hep-lat hep-ph\n",
      "gr-qc astro-ph.IM physics.data-an\n",
      "quant-ph physics.app-ph physics.atom-ph\n",
      "cond-mat.quant-gas gr-qc hep-th physics.flu-dyn\n",
      "cs.LG cond-mat.dis-nn cond-mat.stat-mech q-bio.NC stat.ML\n",
      "quant-ph cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
      "astro-ph.HE astro-ph.GA astro-ph.SR\n",
      "quant-ph physics.atom-ph stat.ML\n",
      "physics.med-ph physics.atom-ph physics.ins-det\n",
      "physics.bio-ph cond-mat.soft physics.flu-dyn\n",
      "astro-ph.GA astro-ph.HE astro-ph.SR\n",
      "cond-mat.mes-hall physics.chem-ph physics.optics\n",
      "hep-ph astro-ph.HE nucl-ex nucl-th\n",
      "quant-ph cond-mat.other physics.chem-ph\n",
      "nlin.AO physics.soc-ph\n",
      "q-bio.CB physics.bio-ph\n",
      "cond-mat.str-el cond-mat.stat-mech physics.comp-ph\n",
      "cond-mat.stat-mech cond-mat.mes-hall math-ph math.MP math.PR\n",
      "cond-mat.soft q-bio.TO\n",
      "physics.bio-ph q-bio.BM\n",
      "cs.LG physics.flu-dyn\n",
      "physics.optics cond-mat.mes-hall hep-th\n",
      "physics.ins-det cond-mat.soft physics.optics\n",
      "physics.app-ph cond-mat.other physics.flu-dyn\n",
      "cs.DS cs.DM cs.IT math.IT math.PR stat.CO\n",
      "astro-ph.SR physics.space-ph\n",
      "physics.atom-ph cond-mat.mes-hall physics.optics\n",
      "physics.soc-ph cs.LG\n",
      "cs.LG cs.AI cs.GT cs.MA stat.ML\n",
      "eess.SY cs.NI cs.SI cs.SY\n",
      "cs.LO cs.AI cs.CL math.GN math.LO\n",
      "math.CT math.AG math.RT\n",
      "physics.atom-ph nlin.PS\n",
      "hep-th cond-mat.other math-ph math.MP\n",
      "astro-ph.GA astro-ph.CO astro-ph.HE physics.plasm-ph\n",
      "astro-ph.SR physics.plasm-ph physics.space-ph\n",
      "cs.CR cs.DC\n",
      "physics.flu-dyn physics.optics\n",
      "physics.acc-ph physics.ins-det physics.optics\n",
      "physics.data-an hep-ph\n",
      "cs.SI cs.CY nlin.AO\n",
      "hep-lat cond-mat.stat-mech quant-ph\n",
      "hep-th gr-qc nucl-th\n",
      "q-bio.QM stat.AP stat.ML\n",
      "q-bio.QM cs.CV eess.IV\n",
      "cond-mat.mes-hall physics.atm-clus physics.flu-dyn quant-ph\n",
      "physics.optics physics.atom-ph physics.ins-det\n",
      "cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph\n",
      "cond-mat.mes-hall cond-mat.quant-gas physics.optics quant-ph\n",
      "cond-mat.mes-hall cond-mat.str-el physics.chem-ph\n",
      "math.HO math-ph math.MP math.PR\n",
      "cs.PL cs.GT cs.IT cs.LO math.IT\n",
      "cond-mat.quant-gas nlin.CD physics.atom-ph\n",
      "physics.optics physics.class-ph\n",
      "nucl-th gr-qc\n",
      "quant-ph cs.DM math-ph math.CA math.MP\n",
      "hep-ph physics.ins-det\n",
      "physics.soc-ph stat.CO\n",
      "physics.plasm-ph physics.comp-ph physics.data-an\n",
      "physics.med-ph physics.optics\n",
      "math.OC math.CO q-bio.QM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other\n",
      "physics.plasm-ph hep-th\n",
      "physics.comp-ph physics.ed-ph\n",
      "physics.soc-ph cond-mat.dis-nn nlin.AO q-bio.MN\n",
      "quant-ph physics.class-ph\n",
      "hep-ex astro-ph.HE\n",
      "eess.IV cs.CV cs.LG physics.med-ph q-bio.QM\n",
      "math.GT gr-qc math-ph math.MP math.QA\n",
      "physics.gen-ph quant-ph\n",
      "physics.atom-ph physics.chem-ph physics.comp-ph\n",
      "nlin.AO physics.comp-ph\n",
      "cs.CY cs.HC\n",
      "hep-ph astro-ph.CO hep-ex hep-th\n",
      "eess.SP physics.data-an physics.optics\n",
      "cs.RO cs.CV math.OC\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.quant-gas hep-th\n",
      "nlin.PS physics.plasm-ph\n",
      "cs.GT cs.IR\n",
      "physics.optics nlin.CD nlin.PS\n",
      "physics.flu-dyn astro-ph.SR\n",
      "physics.acc-ph cond-mat.other hep-th\n",
      "nlin.PS cond-mat.soft\n",
      "cs.LG cs.RO cs.SY eess.SY stat.ML\n",
      "physics.soc-ph cond-mat.stat-mech cs.CY cs.SI\n",
      "gr-qc physics.class-ph\n",
      "nucl-th physics.optics\n",
      "physics.optics nlin.PS physics.comp-ph\n",
      "physics.chem-ph physics.atom-ph quant-ph\n",
      "quant-ph math.NT\n",
      "math.CA math.DS\n",
      "cs.CL cs.AI cs.IR cs.LG\n",
      "eess.SP cs.GT cs.SY eess.SY\n",
      "cond-mat.other cond-mat.mes-hall\n",
      "cs.MS cs.CE\n",
      "physics.comp-ph cond-mat.mtrl-sci cond-mat.soft\n",
      "astro-ph.SR astro-ph.EP astro-ph.GA astro-ph.IM\n",
      "cs.LG cs.CL stat.ML\n",
      "hep-ex quant-ph\n",
      "cond-mat.str-el cond-mat.stat-mech nlin.PS\n",
      "nlin.AO cs.LG physics.comp-ph stat.ML\n",
      "physics.data-an cond-mat.mtrl-sci cond-mat.str-el\n",
      "math.GT quant-ph\n",
      "cond-mat.soft math.DS nlin.PS\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el\n",
      "quant-ph nucl-ex physics.ins-det\n",
      "nlin.AO cond-mat.soft cond-mat.stat-mech\n",
      "cond-mat.mes-hall cond-mat.other nlin.AO\n",
      "physics.comp-ph physics.class-ph\n",
      "physics.flu-dyn physics.ao-ph physics.geo-ph\n",
      "eess.SY cs.SY physics.app-ph\n",
      "cs.CY cs.CR\n",
      "quant-ph cs.SY eess.SY stat.ML\n",
      "physics.plasm-ph physics.acc-ph physics.optics\n",
      "gr-qc physics.geo-ph\n",
      "quant-ph physics.atom-ph physics.chem-ph\n",
      "math.DG math-ph math.MP math.SG\n",
      "quant-ph gr-qc physics.atom-ph\n",
      "astro-ph.SR astro-ph.GA physics.space-ph\n",
      "physics.atom-ph cond-mat.supr-con quant-ph\n",
      "math.PR math.ST stat.TH\n",
      "physics.med-ph cs.LG cs.NE stat.ML\n",
      "cs.LG q-bio.QM stat.AP stat.ML\n",
      "physics.soc-ph physics.ao-ph\n",
      "physics.app-ph physics.acc-ph physics.plasm-ph\n",
      "math-ph cond-mat.str-el math.MP quant-ph\n",
      "physics.atom-ph physics.med-ph\n",
      "cs.CV cs.HC\n",
      "cs.CY cs.SI q-bio.QM\n",
      "astro-ph.IM astro-ph.EP astro-ph.SR\n",
      "cs.MA stat.AP\n",
      "eess.SP cs.IT math.IT\n",
      "cs.GR cs.CV\n",
      "eess.SY cs.LG cs.SY\n",
      "eess.AS cs.LG cs.NE cs.SD\n",
      "math.HO math.CV\n",
      "quant-ph cs.DC physics.comp-ph\n",
      "physics.comp-ph cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "cs.CV cs.HC cs.LG eess.IV\n",
      "physics.ins-det cs.CV physics.app-ph\n",
      "physics.chem-ph cond-mat.soft cond-mat.stat-mech\n",
      "cond-mat.quant-gas cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "cs.IT cs.NI cs.PF eess.SP math.IT math.OC\n",
      "cond-mat.quant-gas cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "physics.app-ph physics.ins-det\n",
      "eess.SP cs.IT cs.LG math.IT\n",
      "math-ph math.CA math.MP math.SG nlin.SI physics.class-ph\n",
      "astro-ph.HE gr-qc hep-ph hep-th\n",
      "cs.SD cs.LG eess.AS\n",
      "cs.LG cs.AI cs.HC cs.IR\n",
      "cs.SE cs.CL cs.SI\n",
      "quant-ph cond-mat.str-el math-ph math.MP\n",
      "cond-mat.stat-mech cond-mat.quant-gas math-ph math.MP\n",
      "eess.IV cs.CV cs.LG physics.med-ph stat.ML\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.chem-ph\n",
      "physics.soc-ph cs.LG cs.SI stat.ML\n",
      "physics.atom-ph astro-ph.HE\n",
      "cs.SC cs.CC cs.DS\n",
      "hep-ex nucl-ex physics.ins-det\n",
      "quant-ph cond-mat.str-el cond-mat.supr-con\n",
      "eess.IV physics.med-ph\n",
      "cs.HC stat.OT\n",
      "cond-mat.stat-mech physics.bio-ph physics.data-an\n",
      "cond-mat.other cond-mat.stat-mech\n",
      "math.DG math-ph math.MP nlin.SI\n",
      "math.QA hep-th math.RT\n",
      "physics.chem-ph nlin.CD\n",
      "eess.AS cs.CL cs.LG cs.NE cs.SD\n",
      "math.DS math-ph math.MP physics.class-ph\n",
      "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.plasm-ph\n",
      "cs.LG cs.DB cs.DC stat.ML\n",
      "cond-mat.soft cond-mat.mes-hall cond-mat.stat-mech\n",
      "physics.app-ph cond-mat.soft\n",
      "astro-ph.SR astro-ph.HE physics.space-ph\n",
      "hep-ph astro-ph.SR gr-qc\n",
      "physics.soc-ph math.AP nlin.PS q-bio.PE\n",
      "quant-ph cond-mat.other cs.LG physics.comp-ph\n",
      "cs.NI stat.AP\n",
      "math.DG math.RA\n",
      "cs.DS math.PR\n",
      "cond-mat.mes-hall math-ph math.MP quant-ph\n",
      "cond-mat.stat-mech cond-mat.mtrl-sci physics.comp-ph\n",
      "cond-mat.stat-mech cond-mat.soft physics.chem-ph\n",
      "cond-mat.stat-mech cond-mat.str-el nlin.SI\n",
      "physics.flu-dyn cond-mat.stat-mech\n",
      "astro-ph.IM physics.optics\n",
      "econ.GN cs.CY q-fin.EC stat.AP\n",
      "physics.hist-ph quant-ph\n",
      "cs.CL cs.LG eess.AS\n",
      "math-ph hep-th math.MP math.SG nlin.SI physics.flu-dyn\n",
      "cs.LG cs.CV cs.IR stat.ML\n",
      "cond-mat.soft cond-mat.dis-nn physics.flu-dyn\n",
      "cond-mat.mes-hall cond-mat.str-el physics.atom-ph physics.chem-ph\n",
      "q-fin.ST cs.LG q-fin.PM stat.ML\n",
      "cond-mat.soft cond-mat.mes-hall cond-mat.mtrl-sci physics.flu-dyn\n",
      "physics.atm-clus cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "gr-qc physics.optics\n",
      "physics.atom-ph nucl-th quant-ph\n",
      "physics.comp-ph cond-mat.dis-nn physics.data-an quant-ph\n",
      "cs.HC cs.LG cs.RO\n",
      "cond-mat.other nlin.CD quant-ph\n",
      "cs.CR cs.IT cs.SY eess.SP eess.SY math.IT\n",
      "physics.comp-ph astro-ph.IM cs.LG hep-ex\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.optics\n",
      "nucl-th hep-ex\n",
      "nlin.CG math.DS\n",
      "cs.LG cond-mat.stat-mech physics.comp-ph stat.ML\n",
      "math.NA astro-ph.CO cs.NA physics.comp-ph\n",
      "eess.AS cs.CL cs.LG cs.SD\n",
      "astro-ph.CO astro-ph.GA astro-ph.IM gr-qc hep-th\n",
      "gr-qc physics.atom-ph quant-ph\n",
      "cond-mat.dis-nn cond-mat.str-el physics.comp-ph quant-ph\n",
      "cs.HC cs.CY cs.RO\n",
      "cond-mat.stat-mech cond-mat.soft nlin.CD physics.bio-ph\n",
      "astro-ph.IM astro-ph.EP astro-ph.HE physics.space-ph\n",
      "astro-ph.EP astro-ph.SR q-bio.PE\n",
      "cs.SC q-bio.MN\n",
      "physics.data-an cs.CV eess.IV hep-ex\n",
      "quant-ph cond-mat.quant-gas nlin.SI\n",
      "astro-ph.GA astro-ph.IM astro-ph.SR\n",
      "hep-ph cond-mat.mes-hall hep-th nucl-th\n",
      "cond-mat.dis-nn cs.LG eess.SP\n",
      "physics.app-ph cond-mat.supr-con quant-ph\n",
      "cs.LG cs.SI stat.ML\n",
      "physics.flu-dyn cond-mat.mtrl-sci cond-mat.soft\n",
      "cond-mat.soft physics.bio-ph q-bio.TO\n",
      "cond-mat.quant-gas math-ph math.MP nlin.PS\n",
      "cs.LG cs.DM stat.ML\n",
      "astro-ph.GA astro-ph.HE gr-qc\n",
      "cs.NI cs.SI\n",
      "hep-lat cond-mat.quant-gas hep-ph\n",
      "stat.CO astro-ph.IM\n",
      "physics.comp-ph cond-mat.mtrl-sci physics.app-ph physics.chem-ph\n",
      "physics.comp-ph cond-mat.dis-nn cond-mat.stat-mech\n",
      "math-ph math.CA math.FA math.MP\n",
      "math.NT physics.class-ph\n",
      "physics.ao-ph physics.data-an physics.geo-ph\n",
      "math-ph math.MP physics.hist-ph\n",
      "cond-mat.mes-hall cond-mat.soft cond-mat.stat-mech\n",
      "cond-mat.soft cond-mat.stat-mech physics.comp-ph\n",
      "cond-mat.str-el cond-mat.dis-nn cs.LG physics.comp-ph quant-ph\n",
      "cs.LG hep-ex physics.data-an stat.ML\n",
      "physics.optics nlin.CD physics.atm-clus quant-ph\n",
      "cs.SI cs.DM\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.other quant-ph\n",
      "hep-th hep-ph nucl-th physics.flu-dyn\n",
      "physics.plasm-ph math-ph math.MP nlin.CD\n",
      "math.OC cs.RO\n",
      "cond-mat.dis-nn q-bio.TO\n",
      "physics.ins-det astro-ph.IM physics.geo-ph\n",
      "cond-mat.quant-gas physics.flu-dyn\n",
      "eess.SP eess.IV\n",
      "quant-ph cs.CC cs.LG\n",
      "physics.soc-ph cond-mat.stat-mech q-bio.PE\n",
      "physics.soc-ph cs.SI q-bio.PE\n",
      "cs.ET physics.optics\n",
      "astro-ph.IM astro-ph.GA astro-ph.SR\n",
      "hep-ph hep-lat nucl-ex nucl-th physics.atom-ph\n",
      "q-bio.GN cs.DC\n",
      "cond-mat.mtrl-sci physics.comp-ph physics.optics\n",
      "cond-mat.mtrl-sci physics.data-an\n",
      "quant-ph cs.IT hep-th math.IT\n",
      "nucl-th astro-ph.HE cond-mat.supr-con\n",
      "physics.plasm-ph physics.atom-ph physics.comp-ph\n",
      "cond-mat.stat-mech cond-mat.mtrl-sci hep-ph\n",
      "eess.IV cs.LG q-bio.NC\n",
      "cond-mat.quant-gas cond-mat.dis-nn cond-mat.str-el quant-ph\n",
      "cond-mat.mtrl-sci physics.plasm-ph\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.supr-con\n",
      "physics.flu-dyn physics.class-ph\n",
      "astro-ph.SR physics.atom-ph\n",
      "physics.med-ph q-bio.QM q-bio.TO\n",
      "nlin.PS math-ph math.MP nlin.SI\n",
      "cond-mat.mes-hall physics.data-an\n",
      "physics.atom-ph physics.ins-det physics.med-ph\n",
      "nucl-th physics.atom-ph\n",
      "cs.LG nlin.CD physics.data-an q-bio.QM stat.ML\n",
      "cond-mat.str-el cond-mat.quant-gas hep-lat\n",
      "q-bio.NC stat.AP stat.ME\n",
      "hep-lat cond-mat.stat-mech nucl-th quant-ph\n",
      "quant-ph cond-mat.mes-hall cond-mat.stat-mech physics.atom-ph\n",
      "physics.soc-ph cs.DL cs.SI\n",
      "astro-ph.HE nucl-ex nucl-th\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.other\n",
      "astro-ph.IM astro-ph.EP gr-qc physics.ins-det physics.optics\n",
      "physics.app-ph nlin.CD physics.optics\n",
      "cs.OH\n",
      "physics.bio-ph cond-mat.soft cond-mat.stat-mech q-bio.CB q-bio.QM\n",
      "hep-th cond-mat.mes-hall\n",
      "astro-ph.HE astro-ph.EP physics.plasm-ph physics.space-ph\n",
      "hep-th gr-qc math-ph math.MP physics.flu-dyn\n",
      "hep-ph astro-ph.CO cond-mat.mtrl-sci hep-ex physics.ins-det\n",
      "eess.IV cs.CV cs.LG cs.MM\n",
      "q-fin.TR physics.soc-ph\n",
      "cs.SE cs.CR\n",
      "stat.ML cs.AI cs.LG\n",
      "math.OC stat.ML\n",
      "physics.comp-ph cond-mat.mtrl-sci cond-mat.str-el physics.chem-ph\n",
      "quant-ph nlin.AO\n",
      "hep-ph cond-mat.dis-nn cond-mat.mes-hall cond-mat.supr-con\n",
      "cs.GT cs.LG cs.MA stat.ML\n",
      "physics.geo-ph cond-mat.soft physics.ao-ph physics.flu-dyn\n",
      "physics.ins-det physics.atom-ph physics.optics\n",
      "physics.soc-ph cs.LG cs.SI physics.data-an stat.ML\n",
      "astro-ph.GA stat.ME stat.ML\n",
      "nlin.AO cond-mat.other math.DS\n",
      "math.AP cond-mat.soft\n",
      "math.RT math.DS math.NT\n",
      "cs.NI cs.PF\n",
      "quant-ph nlin.AO nlin.CD physics.class-ph\n",
      "cs.SI cs.DC\n",
      "astro-ph.SR nlin.PS physics.flu-dyn stat.AP\n",
      "cond-mat.mtrl-sci cond-mat.other cond-mat.str-el\n",
      "physics.atom-ph nucl-ex physics.plasm-ph\n",
      "cs.CR cs.CV cs.LG stat.ML\n",
      "gr-qc astro-ph.HE astro-ph.SR\n",
      "cs.RO cs.CL cs.CV cs.HC\n",
      "physics.optics nlin.PS quant-ph\n",
      "astro-ph.CO physics.plasm-ph\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.stat-mech\n",
      "quant-ph cs.IT cs.LG math.IT\n",
      "nlin.AO cond-mat.dis-nn cond-mat.stat-mech q-bio.NC\n",
      "cond-mat.quant-gas hep-th nlin.PS\n",
      "eess.IV cs.LG cs.NE eess.SP\n",
      "eess.SP cs.SY eess.SY\n",
      "physics.soc-ph cond-mat.stat-mech cs.MA cs.SI\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.soft cond-mat.stat-mech\n",
      "cond-mat.str-el physics.atom-ph quant-ph\n",
      "cond-mat.mes-hall cond-mat.dis-nn hep-th\n",
      "physics.plasm-ph astro-ph.CO astro-ph.HE cond-mat.soft hep-ph\n",
      "physics.data-an physics.ins-det\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.soft\n",
      "nlin.CD physics.bio-ph physics.flu-dyn\n",
      "physics.atom-ph astro-ph.EP astro-ph.IM physics.ao-ph physics.chem-ph physics.ins-det\n",
      "cs.DS cs.AI cs.DM cs.SI math.OC\n",
      "cs.FL cs.GT cs.LO\n",
      "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.chem-ph physics.flu-dyn\n",
      "math.PR cond-mat.stat-mech\n",
      "quant-ph cs.LG cs.NE\n",
      "hep-th cond-mat.mtrl-sci math-ph math.MP nlin.PS\n",
      "math-ph math.MP physics.flu-dyn\n",
      "cond-mat.quant-gas eess.IV physics.optics quant-ph\n",
      "physics.comp-ph cond-mat.other cs.NA math.NA physics.app-ph\n",
      "gr-qc astro-ph.CO physics.hist-ph\n",
      "cond-mat.mtrl-sci cond-mat.str-el physics.ins-det\n",
      "cs.PL cs.SE\n",
      "cs.CE math.OC\n",
      "cs.SC cs.CC\n",
      "q-bio.PE cond-mat.stat-mech nlin.AO physics.bio-ph\n",
      "stat.ME cs.LG stat.ML\n",
      "physics.flu-dyn cond-mat.other physics.comp-ph\n",
      "q-bio.CB q-bio.SC q-bio.TO\n",
      "astro-ph.GA astro-ph.HE astro-ph.SR gr-qc\n",
      "cond-mat.supr-con cond-mat.soft cond-mat.stat-mech\n",
      "astro-ph.GA cond-mat.mtrl-sci\n",
      "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
      "physics.comp-ph cs.NA math.NA physics.plasm-ph\n",
      "cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
      "eess.IV cs.MM cs.SY eess.SY\n",
      "q-bio.PE cond-mat.dis-nn\n",
      "quant-ph cs.AI cs.ET\n",
      "gr-qc physics.space-ph\n",
      "astro-ph.SR cs.LG\n",
      "cs.SD cs.CV cs.LG eess.AS\n",
      "quant-ph nlin.CD physics.class-ph\n",
      "quant-ph hep-lat nucl-th\n",
      "physics.ao-ph nlin.CD physics.comp-ph physics.flu-dyn physics.geo-ph stat.ML\n",
      "physics.acc-ph cond-mat.mtrl-sci physics.ins-det\n",
      "cond-mat.str-el cond-mat.stat-mech hep-th math-ph math.MP quant-ph\n",
      "math.GT math.GR math.MG math.NT\n",
      "astro-ph.GA gr-qc physics.atom-ph\n",
      "cond-mat.mtrl-sci physics.comp-ph physics.plasm-ph\n",
      "cond-mat.soft physics.comp-ph physics.flu-dyn\n",
      "physics.ed-ph cond-mat.mtrl-sci\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.supr-con hep-th\n",
      "physics.soc-ph cond-mat.stat-mech cs.SI stat.AP\n",
      "cs.LO cs.AI\n",
      "physics.flu-dyn physics.comp-ph physics.data-an stat.CO\n",
      "cond-mat.mes-hall cs.AI cs.LG cs.RO\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph\n",
      "cond-mat.str-el cond-mat.dis-nn physics.comp-ph quant-ph\n",
      "math.DG math.MG\n",
      "cs.SI cs.CL cs.IR\n",
      "cs.CR eess.SP\n",
      "gr-qc astro-ph.GA astro-ph.IM\n",
      "astro-ph.IM physics.ao-ph\n",
      "eess.AS cs.LG cs.SD stat.ML\n",
      "physics.med-ph eess.IV physics.optics\n",
      "quant-ph cond-mat.stat-mech cond-mat.str-el hep-th nlin.CD\n",
      "stat.AP stat.ME stat.ML\n",
      "eess.SP cs.NA math.NA math.OC\n",
      "cond-mat.stat-mech cs.IT math.DS math.IT nlin.CD\n",
      "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech physics.flu-dyn\n",
      "quant-ph hep-th math-ph math.MP nlin.SI\n",
      "cond-mat.quant-gas nlin.PS physics.atom-ph quant-ph\n",
      "physics.atm-clus cond-mat.quant-gas\n",
      "math-ph math.GR math.MP math.RT\n",
      "physics.optics cs.CE physics.app-ph physics.comp-ph\n",
      "gr-qc math-ph math.MP quant-ph\n",
      "cs.LG math.OC\n",
      "cs.CR cs.AI\n",
      "quant-ph cs.IT math.CO math.IT\n",
      "math.DG math.AG math.CV math.MG\n",
      "physics.comp-ph physics.med-ph q-bio.TO\n",
      "math.AG hep-th\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci physics.optics quant-ph\n",
      "gr-qc cond-mat.other hep-ph\n",
      "cond-mat.mtrl-sci cond-mat.str-el physics.chem-ph physics.comp-ph\n",
      "stat.ME physics.chem-ph physics.data-an\n",
      "astro-ph.GA astro-ph.IM physics.ins-det\n",
      "eess.IV physics.data-an\n",
      "eess.IV cs.LG\n",
      "astro-ph.EP astro-ph.GA astro-ph.IM astro-ph.SR\n",
      "quant-ph cond-mat.stat-mech nlin.CD nlin.SI\n",
      "astro-ph.HE gr-qc hep-ph\n",
      "cs.CG cs.CV q-bio.QM\n",
      "eess.SY cs.LG cs.RO cs.SY\n",
      "hep-lat cs.LG physics.comp-ph\n",
      "physics.bio-ph cond-mat.stat-mech q-bio.CB\n",
      "physics.optics gr-qc\n",
      "physics.chem-ph physics.data-an\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.comp-ph physics.optics\n",
      "cond-mat.quant-gas cs.LG eess.IV physics.atom-ph\n",
      "cond-mat.quant-gas cond-mat.dis-nn quant-ph\n",
      "astro-ph.IM hep-ex nucl-ex physics.ins-det\n",
      "hep-ph physics.optics quant-ph\n",
      "stat.ME cs.LG\n",
      "cond-mat.soft cond-mat.mes-hall physics.flu-dyn\n",
      "physics.soc-ph cs.DL physics.hist-ph\n",
      "cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
      "cond-mat.mtrl-sci cond-mat.soft physics.atm-clus physics.chem-ph\n",
      "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el physics.optics\n",
      "physics.flu-dyn cond-mat.soft math.DS\n",
      "quant-ph cond-mat.mtrl-sci cond-mat.other\n",
      "eess.SY cs.CR cs.SY eess.SP\n",
      "astro-ph.CO astro-ph.HE astro-ph.IM hep-ph\n",
      "stat.ME math.DS\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.mes-hall physics.app-ph\n",
      "cs.SI cond-mat.stat-mech physics.soc-ph\n",
      "math.AC math.RA\n",
      "cond-mat.mtrl-sci physics.chem-ph physics.comp-ph\n",
      "cond-mat.soft cond-mat.stat-mech physics.chem-ph physics.plasm-ph\n",
      "cs.NE cs.CV cs.RO\n",
      "physics.ins-det hep-ex hep-ph\n",
      "cond-mat.quant-gas physics.atom-ph physics.chem-ph quant-ph\n",
      "cs.IT cond-mat.stat-mech cs.LG math.IT stat.ML\n",
      "physics.soc-ph cond-mat.soft nlin.AO\n",
      "cond-mat.stat-mech cond-mat.soft physics.optics\n",
      "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
      "cs.RO cs.NE\n",
      "physics.atom-ph cond-mat.mtrl-sci cond-mat.quant-gas\n",
      "cs.CY physics.soc-ph\n",
      "q-bio.TO q-bio.SC\n",
      "q-bio.BM q-bio.SC\n",
      "math.RA math.DG\n",
      "physics.comp-ph cs.NA math.NA physics.flu-dyn stat.ML\n",
      "cs.NE cs.LG math.OC\n",
      "physics.ins-det astro-ph.IM hep-ex nucl-ex\n",
      "stat.CO cs.DM cs.DS cs.IT math.IT math.PR\n",
      "cs.AI cs.NI\n",
      "physics.soc-ph econ.GN nlin.AO q-fin.EC\n",
      "physics.chem-ph cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph\n",
      "physics.hist-ph astro-ph.EP physics.ao-ph physics.geo-ph\n",
      "cond-mat.quant-gas math.CA\n",
      "physics.comp-ph cond-mat.dis-nn cs.LG\n",
      "physics.ins-det physics.app-ph quant-ph\n",
      "astro-ph.HE gr-qc physics.plasm-ph\n",
      "hep-th cond-mat.stat-mech math-ph math.MP nlin.SI\n",
      "quant-ph cond-mat.mes-hall physics.data-an\n",
      "cs.NE cs.RO\n",
      "physics.chem-ph cond-mat.str-el physics.comp-ph quant-ph\n",
      "cs.DB cs.DC cs.LG cs.PF\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.atom-ph physics.optics\n",
      "math.FA math-ph math.MP\n",
      "cs.CE cond-mat.mtrl-sci cs.NA math.NA\n",
      "cs.CR cs.HC\n",
      "nucl-ex astro-ph.SR nucl-th\n",
      "cs.CR cs.AI cs.PF\n",
      "cs.SI nlin.AO physics.soc-ph\n",
      "cs.DB cs.CY cs.LG\n",
      "cs.CV cs.CL cs.LG\n",
      "cs.MM cs.SI\n",
      "cs.ET cond-mat.dis-nn\n",
      "math.CO cs.DM cs.DS math.OC\n",
      "physics.ins-det nucl-ex nucl-th quant-ph\n",
      "cs.SI stat.ML\n",
      "math.OC cs.LG cs.RO\n",
      "nlin.AO math.DS\n",
      "cond-mat.stat-mech cond-mat.quant-gas physics.flu-dyn\n",
      "cs.CR cs.RO\n",
      "physics.plasm-ph cond-mat.stat-mech physics.comp-ph\n",
      "q-bio.PE physics.bio-ph stat.AP\n",
      "hep-lat cond-mat.str-el hep-ph hep-th\n",
      "physics.hist-ph astro-ph.EP astro-ph.SR\n",
      "physics.chem-ph cond-mat.stat-mech quant-ph\n",
      "physics.app-ph physics.ins-det physics.optics quant-ph\n",
      "cond-mat.stat-mech gr-qc\n",
      "physics.acc-ph hep-ex hep-ph\n",
      "math.GR cs.DS\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.mes-hall\n",
      "hep-lat cond-mat.stat-mech cs.LG\n",
      "cond-mat.soft cond-mat.other physics.comp-ph\n",
      "physics.data-an math.DS nlin.CD\n",
      "cond-mat.mtrl-sci physics.comp-ph quant-ph\n",
      "eess.AS cs.SD eess.IV\n",
      "math-ph hep-th math.DG math.MP\n",
      "math-ph cond-mat.stat-mech hep-th math.MP\n",
      "cond-mat.stat-mech hep-th math-ph math.MP nlin.SI\n",
      "cs.CL cs.AI cs.SI\n",
      "cond-mat.other cond-mat.soft physics.atm-clus\n",
      "cond-mat.quant-gas nlin.SI\n",
      "physics.optics eess.IV physics.ins-det\n",
      "physics.class-ph nlin.CD\n",
      "cs.SE cs.LG cs.PL\n",
      "cond-mat.supr-con cond-mat.str-el nucl-th\n",
      "physics.hist-ph astro-ph.SR\n",
      "math.NA cs.NA math.CO\n",
      "cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con quant-ph\n",
      "physics.flu-dyn cond-mat.other nlin.CD\n",
      "cs.SD cs.CR cs.LG eess.AS\n",
      "gr-qc astro-ph.HE physics.space-ph\n",
      "physics.flu-dyn cond-mat.mes-hall cond-mat.soft cond-mat.stat-mech physics.chem-ph\n",
      "physics.comp-ph astro-ph.IM cs.LG gr-qc\n",
      "cs.LG cs.IT math.IT stat.ML\n",
      "cond-mat.soft physics.bio-ph physics.flu-dyn q-bio.CB\n",
      "physics.bio-ph cond-mat.soft nlin.AO\n",
      "astro-ph.IM hep-ex physics.data-an\n",
      "hep-ph astro-ph.CO hep-ex nucl-ex\n",
      "hep-th cond-mat.stat-mech gr-qc hep-lat\n",
      "math.AG math.AT math.CV math.NT\n",
      "gr-qc astro-ph.HE astro-ph.IM\n",
      "physics.class-ph cond-mat.other nlin.AO\n",
      "math.NA cs.NA physics.comp-ph physics.flu-dyn\n",
      "cs.LG cs.MA stat.ML\n",
      "math-ph cond-mat.str-el hep-th math.CT math.MP math.QA\n",
      "hep-ph hep-th nucl-th physics.plasm-ph quant-ph\n",
      "quant-ph gr-qc hep-ph hep-th physics.hist-ph\n",
      "physics.optics physics.app-ph physics.chem-ph\n",
      "astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.SR\n",
      "cs.AI cs.LO\n",
      "cond-mat.dis-nn cond-mat.stat-mech gr-qc hep-lat physics.soc-ph\n",
      "cs.CV cs.LG eess.SP\n",
      "math.NA cs.NA math.OC math.ST physics.data-an stat.TH\n",
      "hep-th math-ph math.DG math.MP math.SG\n",
      "q-bio.GN cs.LG physics.bio-ph q-bio.CB\n",
      "cs.DB cs.CR cs.DS\n",
      "cs.RO cs.GT cs.LG cs.MA\n",
      "math.AP math.CA math.DS\n",
      "physics.comp-ph cond-mat.dis-nn cond-mat.other\n",
      "q-bio.PE cs.SY eess.SY math.DS\n",
      "nlin.PS physics.app-ph physics.optics\n",
      "quant-ph math-ph math.MP physics.class-ph physics.comp-ph\n",
      "q-bio.PE cs.MA q-bio.QM\n",
      "nlin.CD cond-mat.stat-mech physics.hist-ph\n",
      "cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
      "cond-mat.str-el cond-mat.other cond-mat.quant-gas cond-mat.stat-mech\n",
      "physics.data-an eess.IV\n",
      "physics.soc-ph math.DS q-bio.PE\n",
      "gr-qc astro-ph.CO astro-ph.HE hep-ph hep-th\n",
      "cond-mat.mes-hall cond-mat.other physics.atom-ph physics.optics quant-ph\n",
      "physics.med-ph physics.bio-ph q-bio.PE\n",
      "physics.app-ph cond-mat.mtrl-sci cond-mat.soft\n",
      "cs.NI cs.LG eess.SP\n",
      "stat.OT econ.GN q-fin.EC\n",
      "physics.comp-ph cond-mat.str-el quant-ph\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci quant-ph\n",
      "stat.ME q-bio.QM\n",
      "physics.app-ph cond-mat.mtrl-sci cond-mat.supr-con\n",
      "nucl-th nucl-ex physics.chem-ph\n",
      "physics.optics physics.plasm-ph\n",
      "physics.acc-ph hep-ex physics.atom-ph\n",
      "quant-ph cond-mat.dis-nn hep-th\n",
      "q-fin.GN q-fin.MF\n",
      "hep-ph hep-ex nucl-ex nucl-th physics.ins-det\n",
      "cs.CL cs.LO\n",
      "cond-mat.soft physics.app-ph physics.optics\n",
      "cs.NE cs.AI cs.MA\n",
      "hep-ph hep-lat nucl-ex nucl-th\n",
      "cond-mat.quant-gas cs.LG quant-ph\n",
      "eess.SY cs.FL cs.SY\n",
      "physics.optics cond-mat.mes-hall physics.app-ph physics.chem-ph\n",
      "cond-mat.str-el cond-mat.mtrl-sci cond-mat.other cond-mat.stat-mech\n",
      "hep-lat hep-ex hep-ph physics.atom-ph\n",
      "cs.LG cs.SD eess.AS\n",
      "stat.ML cs.LG econ.GN q-fin.EC\n",
      "cs.GT cs.NI eess.SP\n",
      "physics.soc-ph cond-mat.dis-nn q-bio.PE\n",
      "nlin.PS nlin.AO physics.soc-ph\n",
      "hep-lat cond-mat.stat-mech hep-th\n",
      "physics.ins-det astro-ph.IM hep-ex\n",
      "physics.chem-ph physics.comp-ph stat.ML\n",
      "cond-mat.str-el physics.atm-clus\n",
      "cs.DC cond-mat.mtrl-sci\n",
      "physics.ins-det astro-ph.CO astro-ph.IM hep-ex\n",
      "physics.atom-ph physics.app-ph physics.ins-det quant-ph\n",
      "physics.chem-ph physics.ins-det physics.optics\n",
      "cs.LG cs.DM cs.SI stat.ML\n",
      "cond-mat.quant-gas cond-mat.dis-nn cond-mat.stat-mech\n",
      "cs.CV stat.AP\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph physics.data-an physics.ins-det\n",
      "physics.data-an cs.LG hep-ex\n",
      "astro-ph.CO astro-ph.SR gr-qc\n",
      "stat.ME q-bio.MN q-bio.QM\n",
      "physics.geo-ph physics.flu-dyn\n",
      "astro-ph.CO astro-ph.IM hep-ex hep-ph physics.ins-det\n",
      "cond-mat.mtrl-sci physics.atom-ph\n",
      "cond-mat.mtrl-sci cs.LG physics.chem-ph\n",
      "cond-mat.mtrl-sci cs.CE cs.LG cs.NE physics.comp-ph\n",
      "cond-mat.mtrl-sci cs.LG physics.comp-ph stat.ML\n",
      "cond-mat.mtrl-sci astro-ph.HE cond-mat.other nucl-th\n",
      "gr-qc astro-ph.CO hep-ex hep-ph\n",
      "physics.class-ph math-ph math.MP\n",
      "q-bio.PE cond-mat.stat-mech physics.soc-ph\n",
      "hep-ph hep-ex nucl-th physics.atom-ph\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
      "cond-mat.mes-hall physics.ins-det physics.optics quant-ph\n",
      "quant-ph physics.atm-clus physics.optics\n",
      "physics.bio-ph quant-ph\n",
      "cs.CV cs.GR cs.LG\n",
      "physics.comp-ph astro-ph.IM\n",
      "quant-ph astro-ph.IM physics.optics\n",
      "cond-mat.mes-hall hep-ph hep-th\n",
      "physics.ins-det quant-ph\n",
      "physics.optics physics.comp-ph quant-ph\n",
      "physics.soc-ph cs.MA econ.GN q-fin.EC\n",
      "physics.pop-ph\n",
      "astro-ph.IM physics.ed-ph\n",
      "astro-ph.SR stat.AP\n",
      "quant-ph cond-mat.dis-nn physics.chem-ph\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el hep-th quant-ph\n",
      "physics.flu-dyn cs.LG cs.SY eess.SY\n",
      "q-fin.ST q-fin.RM\n",
      "hep-lat cond-mat.str-el hep-th\n",
      "physics.pop-ph astro-ph.CO\n",
      "cs.CL cs.CV cs.HC cs.LG\n",
      "astro-ph.CO astro-ph.HE gr-qc hep-ph\n",
      "math.PR cond-mat.stat-mech nlin.CD\n",
      "cond-mat.mes-hall cond-mat.other cond-mat.quant-gas\n",
      "q-bio.PE nlin.AO\n",
      "q-bio.PE cs.LG\n",
      "quant-ph cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con\n",
      "quant-ph cs.FL nlin.CG\n",
      "eess.AS cs.LG cs.SD q-bio.QM stat.ML\n",
      "cs.SI cs.LG physics.soc-ph stat.ML\n",
      "physics.ins-det astro-ph.IM physics.optics\n",
      "astro-ph.CO astro-ph.GA astro-ph.HE gr-qc\n",
      "physics.soc-ph cs.NE\n",
      "physics.atom-ph physics.chem-ph physics.optics\n",
      "math-ph cond-mat.stat-mech math.MP nlin.SI\n",
      "cs.PF hep-ex physics.ins-det\n",
      "physics.comp-ph cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
      "cond-mat.stat-mech nlin.CG nlin.SI\n",
      "physics.comp-ph cond-mat.soft eess.IV\n",
      "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el quant-ph\n",
      "physics.atom-ph cond-mat.str-el physics.chem-ph\n",
      "cs.LG cs.CV eess.IV stat.ML\n",
      "stat.ME cs.CL cs.LG\n",
      "cond-mat.stat-mech cond-mat.soft math-ph math.MP math.PR\n",
      "q-bio.QM q-bio.BM\n",
      "cs.LG cs.CG stat.ML\n",
      "cond-mat.stat-mech math-ph math.MP physics.comp-ph\n",
      "cs.DS cs.DM math.HO math.OC\n",
      "cs.NE eess.IV\n",
      "cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el hep-th\n",
      "cs.CR cs.SE\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft nlin.CG physics.comp-ph\n",
      "physics.comp-ph physics.chem-ph physics.flu-dyn\n",
      "cs.CV cs.CR\n",
      "cond-mat.str-el physics.geo-ph\n",
      "math.CT math.RT\n",
      "cs.CR cs.IR\n",
      "cs.CL cs.AI cs.CV\n",
      "physics.optics cond-mat.mes-hall physics.plasm-ph\n",
      "physics.bio-ph physics.comp-ph\n",
      "q-bio.PE cond-mat.dis-nn cond-mat.stat-mech physics.soc-ph\n",
      "math-ph hep-th math.MP math.SG\n",
      "hep-ph hep-th math-ph math.MP quant-ph\n",
      "eess.SP cs.NI\n",
      "cs.HC eess.SP\n",
      "physics.class-ph physics.optics\n",
      "cs.ET cs.NE nlin.AO\n",
      "physics.pop-ph physics.soc-ph\n",
      "astro-ph.IM physics.soc-ph\n",
      "astro-ph.HE astro-ph.SR hep-ph nucl-th\n",
      "quant-ph hep-ph physics.optics\n",
      "math.OC math.FA\n",
      "quant-ph nlin.CD physics.optics\n",
      "cond-mat.mes-hall cond-mat.other cond-mat.str-el cond-mat.supr-con physics.app-ph\n",
      "quant-ph cond-mat.dis-nn cs.LG\n",
      "nlin.AO cs.SY eess.SY math.DS\n",
      "gr-qc astro-ph.GA hep-ph hep-th\n",
      "physics.med-ph eess.IV\n",
      "stat.ME math.PR math.ST stat.TH\n",
      "cs.FL cs.DC cs.LO cs.PL\n",
      "cond-mat.dis-nn physics.comp-ph physics.data-an quant-ph\n",
      "cs.LG cs.AI cs.CL cs.CV\n",
      "astro-ph.HE astro-ph.CO astro-ph.GA\n",
      "q-bio.CB q-bio.SC\n",
      "astro-ph.HE astro-ph.IM gr-qc\n",
      "astro-ph.CO astro-ph.GA hep-ph physics.optics\n",
      "cond-mat.soft math-ph math.MP\n",
      "quant-ph gr-qc hep-th math-ph math.MP\n",
      "cs.RO cs.LG cs.MA cs.SY eess.SY math.OC\n",
      "hep-ph cond-mat.stat-mech nucl-th\n",
      "math.PR physics.bio-ph q-bio.QM\n",
      "physics.soc-ph math.DS\n",
      "cs.CV cs.GR eess.IV\n",
      "cond-mat.other hep-ph\n",
      "cond-mat.soft physics.bio-ph physics.flu-dyn physics.med-ph\n",
      "physics.app-ph cond-mat.mes-hall physics.ins-det physics.optics quant-ph\n",
      "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.flu-dyn\n",
      "cond-mat.str-el cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "cs.SI physics.soc-ph stat.AP\n",
      "gr-qc astro-ph.CO astro-ph.HE hep-th\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph physics.optics\n",
      "cond-mat.supr-con nlin.AO physics.optics\n",
      "physics.chem-ph cond-mat.mtrl-sci cond-mat.soft\n",
      "astro-ph.GA physics.comp-ph\n",
      "hep-ph cond-mat.stat-mech hep-th nucl-th\n",
      "hep-th cond-mat.other gr-qc\n",
      "cs.CL cs.SI\n",
      "physics.plasm-ph cond-mat.mes-hall\n",
      "cs.CY physics.ins-det\n",
      "astro-ph.GA astro-ph.IM gr-qc\n",
      "nucl-th astro-ph.HE hep-ph nucl-ex\n",
      "hep-th cond-mat.other hep-ph quant-ph\n",
      "physics.soc-ph nlin.CD\n",
      "astro-ph.HE astro-ph.SR physics.plasm-ph\n",
      "cs.CR cs.CY cs.NI cs.SI\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.quant-gas cond-mat.str-el\n",
      "cs.ET quant-ph\n",
      "cs.SI cs.DB eess.SP\n",
      "physics.flu-dyn astro-ph.GA astro-ph.SR physics.geo-ph physics.plasm-ph\n",
      "astro-ph.EP physics.flu-dyn\n",
      "physics.pop-ph physics.ed-ph\n",
      "astro-ph.IM hep-ex physics.ins-det\n",
      "cs.CL cs.IR cs.LG cs.SI stat.ML\n",
      "cond-mat.stat-mech nlin.PS q-bio.PE q-bio.SC\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall nlin.AO nlin.PS\n",
      "eess.AS cs.CL cs.SD\n",
      "cond-mat.stat-mech math-ph math.MP nlin.CD\n",
      "physics.soc-ph physics.pop-ph\n",
      "math.CT math.AC\n",
      "cs.ET eess.SP\n",
      "cond-mat.soft cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "math-ph hep-th math.MP physics.class-ph\n",
      "quant-ph cond-mat.str-el cs.DS\n",
      "physics.space-ph cs.SY eess.SY\n",
      "cs.AI cs.CV cs.LG\n",
      "physics.chem-ph physics.app-ph\n",
      "physics.optics nlin.AO\n",
      "cs.SI physics.data-an physics.soc-ph\n",
      "q-bio.PE math.DS math.OC physics.soc-ph\n",
      "astro-ph.CO astro-ph.GA astro-ph.IM hep-ph hep-th\n",
      "quant-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
      "math-ph gr-qc math.MP physics.hist-ph\n",
      "physics.app-ph cond-mat.other\n",
      "cond-mat.stat-mech physics.bio-ph q-bio.QM\n",
      "cond-mat.mtrl-sci cond-mat.other physics.comp-ph\n",
      "cond-mat.mtrl-sci nucl-ex quant-ph\n",
      "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
      "cs.SE cs.NI\n",
      "nlin.PS cs.NA math.DS math.NA physics.comp-ph\n",
      "gr-qc physics.ins-det\n",
      "physics.ins-det cond-mat.mes-hall physics.app-ph physics.atom-ph physics.chem-ph quant-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft\n",
      "cond-mat.stat-mech cond-mat.soft physics.data-an\n",
      "astro-ph.IM astro-ph.EP astro-ph.SR stat.CO\n",
      "gr-qc astro-ph.CO hep-th math-ph math.MP quant-ph\n",
      "astro-ph.IM hep-ex physics.data-an physics.ins-det\n",
      "hep-th cond-mat.str-el physics.flu-dyn\n",
      "math-ph hep-ph math.MP quant-ph\n",
      "cs.LG cs.AI cs.CL cs.NE eess.SP\n",
      "astro-ph.CO hep-ex hep-ph physics.ins-det\n",
      "quant-ph cond-mat.mes-hall math-ph math.MP physics.hist-ph\n",
      "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.soft\n",
      "cond-mat.soft cond-mat.mes-hall cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
      "cs.CE eess.IV\n",
      "cs.CE physics.ins-det\n",
      "physics.flu-dyn math.DS physics.ao-ph physics.class-ph physics.plasm-ph\n",
      "physics.optics physics.acc-ph physics.atom-ph\n",
      "quant-ph cond-mat.supr-con physics.atom-ph\n",
      "q-fin.TR q-fin.MF q-fin.ST stat.AP\n",
      "physics.chem-ph cond-mat.str-el quant-ph\n",
      "physics.geo-ph cond-mat.mtrl-sci\n",
      "math.NT math.CO math.PR\n",
      "quant-ph cond-mat.mes-hall cs.LG\n",
      "cond-mat.quant-gas cond-mat.stat-mech nucl-th\n",
      "cs.LG physics.data-an stat.ML\n",
      "physics.data-an cond-mat.stat-mech physics.flu-dyn\n",
      "physics.comp-ph cond-mat.soft stat.ML\n",
      "hep-ph astro-ph.CO astro-ph.GA gr-qc\n",
      "physics.space-ph astro-ph.IM physics.atom-ph physics.chem-ph\n",
      "cs.LO cs.PL cs.SE\n",
      "astro-ph.SR astro-ph.IM physics.comp-ph\n",
      "physics.comp-ph cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
      "cs.SE cs.HC\n",
      "cs.SE cs.HC cs.SI\n",
      "cond-mat.dis-nn cond-mat.mtrl-sci physics.comp-ph\n",
      "physics.flu-dyn cond-mat.stat-mech nlin.PS\n",
      "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP\n",
      "cond-mat.stat-mech hep-th math-ph math.MP math.PR\n",
      "cs.NE cs.LG stat.ML\n",
      "hep-th math-ph math.MP nucl-th\n",
      "physics.hist-ph astro-ph.EP\n",
      "physics.app-ph physics.class-ph physics.flu-dyn\n",
      "hep-th math.DS nlin.CD nlin.PS\n",
      "cs.LG q-bio.QM stat.ML\n",
      "nlin.PS nlin.CD physics.optics\n",
      "cs.SE cs.GR\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.other\n",
      "cs.CR cs.PF\n",
      "physics.acc-ph hep-ex\n",
      "hep-th cond-mat.dis-nn cond-mat.soft cond-mat.stat-mech\n",
      "hep-th cond-mat.supr-con hep-lat hep-ph\n",
      "physics.plasm-ph astro-ph.SR physics.comp-ph\n",
      "physics.flu-dyn cond-mat.mtrl-sci physics.app-ph\n",
      "cs.DB cs.AI cs.CL cs.LG stat.ML\n",
      "cs.CL cs.CV\n",
      "cond-mat.stat-mech cond-mat.dis-nn hep-lat physics.comp-ph\n",
      "physics.flu-dyn cond-mat.quant-gas\n",
      "physics.plasm-ph cond-mat.soft physics.flu-dyn\n",
      "nlin.AO cond-mat.stat-mech nlin.PS physics.bio-ph\n",
      "cond-mat.quant-gas physics.atom-ph physics.optics\n",
      "cs.LO cs.FL cs.IT cs.PL math.IT\n",
      "cs.SI cond-mat.stat-mech cs.LG\n",
      "physics.ins-det physics.app-ph physics.optics\n",
      "math.PR physics.bio-ph\n",
      "physics.ao-ph astro-ph.EP physics.flu-dyn\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech quant-ph\n",
      "physics.comp-ph physics.chem-ph quant-ph\n",
      "astro-ph.IM astro-ph.SR cs.LG stat.ML\n",
      "cond-mat.mes-hall math-ph math.MP nlin.CD\n",
      "astro-ph.EP physics.geo-ph\n",
      "q-bio.QM stat.AP\n",
      "physics.atom-ph hep-ex hep-ph quant-ph\n",
      "quant-ph cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.str-el cs.LG\n",
      "stat.ME math.LO stat.CO stat.ML\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.other cond-mat.soft\n",
      "math.OC cs.MA\n",
      "cs.SE cs.LG\n",
      "cond-mat.stat-mech math.PR physics.data-an\n",
      "cs.SE cs.CY\n",
      "stat.AP q-bio.QM\n",
      "cs.LG cs.DM math.CO stat.ML\n",
      "gr-qc physics.plasm-ph\n",
      "math-ph math.MP physics.optics\n",
      "physics.pop-ph quant-ph\n",
      "cs.GL cs.SE\n",
      "physics.plasm-ph cs.CV\n",
      "cond-mat.quant-gas cond-mat.mtrl-sci cond-mat.other quant-ph\n",
      "physics.soc-ph cs.LG cs.SI\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.stat-mech cond-mat.supr-con quant-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph physics.optics\n",
      "hep-th cond-mat.mes-hall cond-mat.str-el\n",
      "cs.SI cs.CY\n",
      "eess.SP cs.LG cs.RO\n",
      "cs.GR cs.LG\n",
      "astro-ph.GA astro-ph.CO astro-ph.HE\n",
      "q-bio.BM q-bio.QM\n",
      "cs.LG cs.ET stat.ML\n",
      "cond-mat.str-el hep-ph\n",
      "hep-ph math.GR math.RT quant-ph\n",
      "cond-mat.other nlin.CD physics.flu-dyn\n",
      "astro-ph.GA physics.atm-clus\n",
      "stat.ME math.DS physics.soc-ph q-bio.PE\n",
      "cs.CV cs.CL\n",
      "econ.GN nlin.AO physics.soc-ph q-fin.EC\n",
      "q-fin.GN physics.soc-ph\n",
      "q-fin.ST q-fin.GN\n",
      "cs.DC cs.LG math.OC stat.ML\n",
      "hep-ph physics.atom-ph physics.chem-ph\n",
      "hep-th astro-ph.HE hep-ph\n",
      "astro-ph.IM physics.ins-det physics.optics\n",
      "physics.app-ph cond-mat.dis-nn\n",
      "physics.app-ph cs.LG physics.flu-dyn\n",
      "hep-th cond-mat.dis-nn hep-ph\n",
      "cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP quant-ph\n",
      "physics.app-ph eess.SP physics.optics\n",
      "cond-mat.mtrl-sci physics.app-ph physics.comp-ph physics.flu-dyn\n",
      "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
      "astro-ph.CO physics.comp-ph\n",
      "cond-mat.quant-gas cond-mat.dis-nn cond-mat.other nlin.AO quant-ph\n",
      "cond-mat.soft gr-qc physics.class-ph\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.quant-gas\n",
      "stat.ME econ.EM\n",
      "math-ph gr-qc hep-th math.DG math.MP math.QA\n",
      "cs.HC cs.IR\n",
      "q-bio.PE nlin.AO nlin.CD physics.soc-ph\n",
      "math-ph cs.IT math.IT math.MP\n",
      "gr-qc astro-ph.CO hep-th quant-ph\n",
      "q-bio.QM q-bio.PE\n",
      "cond-mat.mes-hall physics.optics physics.plasm-ph\n",
      "quant-ph cond-mat.other hep-th\n",
      "cs.RO cs.CG cs.SI\n",
      "cs.CC cs.ET cs.NA math.NA\n",
      "eess.SP cs.MM cs.NI\n",
      "math.NA cs.NA physics.bio-ph physics.comp-ph q-bio.MN\n",
      "stat.AP cs.LG q-bio.NC\n",
      "cond-mat.supr-con cond-mat.quant-gas cond-mat.stat-mech\n",
      "cs.CY cs.AI cs.LG\n",
      "cs.SD eess.AS stat.ML\n",
      "physics.data-an physics.acc-ph\n",
      "astro-ph.HE astro-ph.GA gr-qc hep-ph\n",
      "cond-mat.str-el cond-mat.stat-mech hep-lat quant-ph\n",
      "gr-qc cond-mat.supr-con\n",
      "cs.CL cs.IR cs.LG cs.NE\n",
      "physics.optics cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "astro-ph.CO astro-ph.HE astro-ph.SR hep-ph\n",
      "cs.CL cs.IR cs.LG cs.SI\n",
      "math.PR cs.SY eess.SY\n",
      "q-bio.GN q-bio.PE\n",
      "hep-th cond-mat.dis-nn cond-mat.str-el quant-ph\n",
      "astro-ph.IM gr-qc physics.data-an physics.ins-det\n",
      "cs.CR cs.CY cs.LG\n",
      "cs.CY cs.CL cs.LG\n",
      "cs.AI math.OC\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph physics.data-an\n",
      "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.chem-ph\n",
      "math.DS math.AG math.CV math.DG\n",
      "quant-ph cond-mat.mes-hall cond-mat.stat-mech nlin.CD\n",
      "cond-mat.stat-mech cond-mat.soft physics.chem-ph physics.comp-ph\n",
      "math.DG gr-qc math-ph math.MP\n",
      "cs.DL cs.DB\n",
      "q-fin.GN cs.SI\n",
      "physics.data-an physics.flu-dyn\n",
      "astro-ph.CO cs.LG\n",
      "stat.AP cs.LG\n",
      "hep-th hep-lat hep-ph math-ph math.MP\n",
      "astro-ph.HE astro-ph.SR physics.space-ph\n",
      "cs.SI cs.CL\n",
      "hep-ph astro-ph.CO cond-mat.other cond-mat.supr-con\n",
      "astro-ph.IM cs.RO\n",
      "q-bio.QM cs.SD eess.AS\n",
      "eess.SP cs.LG q-bio.QM\n",
      "eess.SP cs.RO\n",
      "cond-mat.mes-hall nlin.CD physics.app-ph\n",
      "physics.hist-ph gr-qc hep-ph\n",
      "q-bio.TO physics.optics\n",
      "cond-mat.mes-hall physics.flu-dyn\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech\n",
      "astro-ph.HE astro-ph.SR nucl-th\n",
      "cs.LG cond-mat.dis-nn cond-mat.stat-mech\n",
      "physics.optics nlin.PS physics.atom-ph quant-ph\n",
      "hep-ph astro-ph.CO cond-mat.mtrl-sci\n",
      "quant-ph cond-mat.stat-mech cond-mat.str-el hep-lat\n",
      "eess.IV cs.LG physics.ao-ph stat.ML\n",
      "cond-mat.soft cond-mat.mes-hall cond-mat.stat-mech physics.chem-ph\n",
      "astro-ph.SR astro-ph.EP physics.space-ph\n",
      "gr-qc cond-mat.other\n",
      "astro-ph.CO astro-ph.HE gr-qc\n",
      "nlin.CD astro-ph.EP\n",
      "physics.chem-ph math-ph math.MP quant-ph\n",
      "cs.RO cs.AI cs.LG cs.SY eess.SY\n",
      "physics.ins-det hep-ex hep-ph nucl-ex physics.soc-ph\n",
      "math.DS physics.flu-dyn\n",
      "math.ST math.CO stat.TH\n",
      "cs.DL cs.CL cs.IR cs.LG\n",
      "q-bio.QM cs.LG\n",
      "quant-ph physics.atm-clus physics.atom-ph\n",
      "physics.optics cond-mat.mes-hall nlin.AO\n",
      "cs.HC cs.AI eess.SP\n",
      "physics.atom-ph physics.ins-det\n",
      "eess.SP stat.AP stat.ML\n",
      "cond-mat.dis-nn cs.DM math.PR\n",
      "cs.DC cs.CR cs.DB cs.IR\n",
      "q-bio.QM physics.app-ph physics.ins-det physics.med-ph\n",
      "eess.IV cs.NE physics.comp-ph physics.optics\n",
      "math.PR q-bio.PE\n",
      "physics.ins-det physics.atm-clus physics.atom-ph physics.chem-ph\n",
      "cs.AI cs.RO\n",
      "cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con nucl-th\n",
      "nucl-th cond-mat.quant-gas\n",
      "cs.LG cs.SE stat.ML\n",
      "cond-mat.quant-gas cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
      "hep-th hep-ph math.NT\n",
      "cs.CY cs.AI cs.HC cs.SE\n",
      "hep-ex cs.LG\n",
      "quant-ph q-bio.BM q-bio.GN q-bio.QM stat.ML\n",
      "cond-mat.mes-hall cond-mat.soft cond-mat.str-el\n",
      "quant-ph cond-mat.mes-hall physics.atm-clus\n",
      "quant-ph physics.data-an\n",
      "astro-ph.EP astro-ph.SR physics.space-ph\n",
      "nucl-th cond-mat.quant-gas hep-ph\n",
      "cond-mat.mes-hall cond-mat.dis-nn cond-mat.quant-gas quant-ph\n",
      "nucl-ex nucl-th physics.ins-det\n",
      "cond-mat.str-el cond-mat.stat-mech nlin.CD quant-ph\n",
      "cs.CR cs.LG cs.SI\n",
      "hep-th cond-mat.str-el hep-lat hep-ph nucl-th\n",
      "math-ph math.MP nucl-th physics.atom-ph\n",
      "math.CO cs.DM cs.DS\n",
      "cs.SE cs.CR cs.LG\n",
      "econ.EM stat.AP\n",
      "physics.chem-ph astro-ph.EP astro-ph.SR physics.atom-ph\n",
      "cs.AI cs.HC\n",
      "nlin.CD cond-mat.quant-gas cond-mat.soft\n",
      "physics.comp-ph cond-mat.dis-nn cond-mat.quant-gas\n",
      "gr-qc math-ph math.MP physics.class-ph\n",
      "physics.app-ph eess.SP\n",
      "cond-mat.stat-mech cond-mat.str-el hep-th math-ph math.MP\n",
      "astro-ph.CO astro-ph.GA hep-ex hep-ph\n",
      "hep-ex hep-ph nucl-ex nucl-th\n",
      "cs.GT cs.DM\n",
      "cond-mat.soft cond-mat.other cond-mat.stat-mech physics.chem-ph physics.plasm-ph\n",
      "cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
      "cond-mat.mes-hall physics.app-ph physics.ins-det physics.optics\n",
      "cs.SE cs.RO\n",
      "cond-mat.stat-mech cond-mat.other\n",
      "cs.SE cs.LO\n",
      "nlin.CD cs.LG math.DS\n",
      "cs.AI cs.GT cs.LG cs.MA cs.RO\n",
      "cs.DL cs.CL\n",
      "hep-th math-ph math.GT math.MP\n",
      "physics.app-ph cond-mat.mes-hall physics.ins-det quant-ph\n",
      "physics.comp-ph cs.CE cs.LG physics.chem-ph stat.ML\n",
      "quant-ph cond-mat.quant-gas hep-lat hep-th\n",
      "cs.HC cs.CV cs.DL cs.LG\n",
      "hep-th physics.class-ph quant-ph\n",
      "astro-ph.HE astro-ph.IM cs.LG gr-qc\n",
      "math.OC cs.LG math.AG stat.ML\n",
      "physics.ins-det astro-ph.IM physics.ao-ph physics.optics\n",
      "physics.chem-ph cs.LG physics.comp-ph stat.ML\n",
      "cond-mat.soft math.AP\n",
      "cs.HC cs.IR cs.LG stat.ML\n",
      "cs.SI cs.CY cs.LG\n",
      "cond-mat.stat-mech physics.data-an physics.space-ph\n",
      "cond-mat.stat-mech cond-mat.soft physics.flu-dyn\n",
      "cs.DC cs.OS cs.PF\n",
      "astro-ph.EP cond-mat.stat-mech physics.ao-ph physics.geo-ph\n",
      "q-bio.PE cs.CY\n",
      "math.PR math.AP\n",
      "cs.LG cs.NE math.DG math.GN stat.ML\n",
      "q-bio.NC cs.AI stat.ML\n",
      "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph quant-ph\n",
      "astro-ph.CO astro-ph.GA astro-ph.SR nucl-th\n",
      "cs.CR cs.CY cs.DC cs.GT cs.LG\n",
      "eess.IV cs.NA math.NA physics.optics\n",
      "nucl-th cond-mat.supr-con physics.chem-ph\n",
      "cond-mat.quant-gas cond-mat.dis-nn physics.atom-ph quant-ph\n",
      "quant-ph cond-mat.other math-ph math.MP physics.optics\n",
      "astro-ph.HE physics.flu-dyn\n",
      "quant-ph physics.chem-ph physics.optics\n",
      "physics.ins-det cond-mat.mtrl-sci cond-mat.str-el\n",
      "gr-qc astro-ph.CO hep-ph physics.ins-det\n",
      "physics.optics cond-mat.supr-con physics.chem-ph physics.ins-det\n",
      "physics.app-ph cond-mat.mes-hall physics.comp-ph\n",
      "physics.ins-det nucl-ex quant-ph\n",
      "q-bio.NC stat.ML\n",
      "physics.med-ph physics.ins-det\n",
      "cond-mat.mes-hall physics.comp-ph physics.optics\n",
      "stat.ML cs.LG nlin.CD physics.ao-ph physics.data-an\n",
      "physics.flu-dyn nlin.PS nlin.SI\n",
      "cs.CL cs.LG q-bio.NC\n",
      "hep-th cond-mat.quant-gas hep-ph\n",
      "quant-ph cond-mat.dis-nn physics.optics\n",
      "nucl-ex hep-ex physics.ins-det\n",
      "hep-lat cond-mat.str-el\n",
      "physics.hist-ph astro-ph.EP physics.pop-ph\n",
      "astro-ph.IM astro-ph.GA gr-qc\n",
      "gr-qc astro-ph.CO astro-ph.GA astro-ph.HE\n",
      "math.FA cs.NA math.CV math.NA\n",
      "astro-ph.HE hep-ex physics.app-ph\n",
      "cs.CE physics.bio-ph\n",
      "gr-qc astro-ph.EP hep-th\n",
      "stat.ML cs.AI cs.LG math.OC\n",
      "physics.hist-ph cond-mat.stat-mech\n",
      "hep-th cond-mat.other hep-ph\n",
      "cs.LG cond-mat.dis-nn cs.CV q-bio.NC stat.ML\n",
      "cond-mat.soft hep-th math-ph math.MP nlin.SI\n",
      "physics.ins-det cs.LG physics.data-an\n",
      "q-bio.PE stat.AP\n",
      "cond-mat.other nucl-th quant-ph\n",
      "quant-ph cs.ET physics.atom-ph physics.comp-ph\n",
      "stat.ME astro-ph.GA\n",
      "cs.FL\n",
      "q-bio.PE astro-ph.EP\n",
      "cond-mat.soft cond-mat.mtrl-sci physics.chem-ph\n",
      "math.OC cs.CV cs.RO\n",
      "eess.SP cs.CV eess.IV\n",
      "cs.LG eess.SP\n",
      "cond-mat.mes-hall hep-th math-ph math.MP quant-ph\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "nucl-th astro-ph.SR nucl-ex\n",
      "physics.pop-ph astro-ph.EP astro-ph.HE\n",
      "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
      "eess.IV cs.CV physics.optics\n",
      "astro-ph.SR astro-ph.CO astro-ph.GA\n",
      "cs.AI cs.SE\n",
      "cs.RO cs.AI cs.MA\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci nlin.CD quant-ph\n",
      "physics.comp-ph cs.LG stat.AP\n",
      "astro-ph.EP astro-ph.IM physics.ao-ph\n",
      "hep-lat cs.LG hep-th\n",
      "physics.flu-dyn nlin.PS\n",
      "cond-mat.quant-gas cond-mat.other quant-ph\n",
      "cs.CV cs.CY cs.HC\n",
      "cs.CY stat.ML\n",
      "hep-ph astro-ph.HE gr-qc nucl-th\n",
      "hep-ex astro-ph.CO hep-ph\n",
      "cond-mat.str-el nlin.CD quant-ph\n",
      "cond-mat.quant-gas astro-ph.CO hep-th\n",
      "eess.AS cs.LG cs.MM cs.SD\n",
      "nucl-ex astro-ph.EP astro-ph.HE\n",
      "hep-th hep-lat quant-ph\n",
      "gr-qc astro-ph.CO astro-ph.HE hep-ph\n",
      "cond-mat.soft cs.LG physics.chem-ph physics.comp-ph\n",
      "physics.comp-ph cs.LG physics.flu-dyn\n",
      "physics.comp-ph physics.bio-ph physics.flu-dyn\n",
      "cs.MA cs.SE\n",
      "cs.MS stat.CO\n",
      "cs.AI physics.soc-ph\n",
      "astro-ph.GA astro-ph.SR physics.hist-ph\n",
      "hep-ex physics.acc-ph\n",
      "cond-mat.supr-con cond-mat.str-el quant-ph\n",
      "hep-th math.AG math.CO\n",
      "cs.CY cs.CR cs.NI\n",
      "hep-th cond-mat.quant-gas cond-mat.str-el hep-ph\n",
      "q-bio.BM physics.bio-ph physics.chem-ph\n",
      "cs.CV cs.AI cs.LG cs.MM eess.IV\n",
      "nlin.AO cond-mat.dis-nn q-bio.NC\n",
      "eess.SP cs.CV cs.MM eess.IV stat.ME\n",
      "physics.chem-ph physics.optics quant-ph\n",
      "math-ph math.FA math.MP\n",
      "math.NA cs.CE cs.NA physics.comp-ph\n",
      "astro-ph.SR astro-ph.HE astro-ph.IM\n",
      "cond-mat.stat-mech cond-mat.mes-hall nlin.CD\n",
      "nlin.CD nlin.AO\n",
      "astro-ph.HE astro-ph.GA hep-th\n",
      "q-bio.GN q-bio.PE q-bio.QM\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph physics.optics\n",
      "physics.app-ph physics.atom-ph physics.bio-ph physics.optics\n",
      "hep-ph astro-ph.CO hep-th nucl-th\n",
      "quant-ph cond-mat.stat-mech gr-qc\n",
      "eess.SP physics.ins-det physics.optics\n",
      "quant-ph cond-mat.mes-hall cond-mat.other cond-mat.quant-gas\n",
      "cs.RO cs.MA\n",
      "hep-ph astro-ph.GA astro-ph.SR hep-ex\n",
      "cond-mat.stat-mech hep-ph nucl-th\n",
      "cs.DB cs.SE\n",
      "physics.acc-ph quant-ph\n",
      "physics.ao-ph cs.CE\n",
      "q-bio.OT q-bio.PE\n",
      "math.DS nlin.AO stat.AP\n",
      "math.NA cs.NA q-fin.PR\n",
      "hep-th gr-qc math-ph math.MP math.QA\n",
      "quant-ph physics.bio-ph physics.med-ph physics.optics\n",
      "cond-mat.mes-hall cond-mat.other nlin.AO nlin.CD physics.comp-ph\n",
      "cs.NE cs.CL cs.LG\n",
      "cs.DC hep-ex physics.ins-det\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "physics.flu-dyn physics.ao-ph physics.comp-ph\n",
      "cs.LG eess.SP math.OC stat.ML\n",
      "cs.CV cs.LG cs.MM cs.SD eess.AS eess.IV\n",
      "cs.MM eess.SP\n",
      "quant-ph q-fin.ST\n",
      "cs.HC cs.SI\n",
      "eess.SP cs.AI\n",
      "cs.LG cs.AI stat.AP stat.ML\n",
      "eess.IV cs.LG physics.optics\n",
      "nucl-th hep-th nucl-ex\n",
      "q-bio.PE physics.soc-ph q-bio.QM\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci physics.chem-ph\n",
      "q-bio.NC cs.CV cs.LG eess.IV stat.ML\n",
      "physics.ao-ph nlin.CD physics.app-ph physics.data-an\n",
      "cs.LG cs.AI cs.CL stat.ML\n",
      "physics.ed-ph quant-ph\n",
      "physics.ins-det physics.chem-ph\n",
      "cond-mat.mtrl-sci cond-mat.other physics.chem-ph physics.comp-ph\n",
      "physics.hist-ph astro-ph.IM\n",
      "cond-mat.stat-mech cond-mat.mes-hall cond-mat.soft physics.chem-ph\n",
      "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
      "cs.LO math.LO\n",
      "cs.GR math.DG\n",
      "cs.IT cs.NI math.IT\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other\n",
      "hep-ph nucl-ex nucl-th physics.ins-det\n",
      "physics.chem-ph cond-mat.soft physics.comp-ph\n",
      "quant-ph astro-ph.CO hep-th\n",
      "physics.comp-ph physics.chem-ph stat.AP\n",
      "astro-ph.CO astro-ph.EP astro-ph.SR\n",
      "physics.optics cs.CV eess.IV physics.app-ph\n",
      "cs.CV cond-mat.stat-mech\n",
      "physics.flu-dyn cs.LG physics.comp-ph\n",
      "cs.NI cs.MM\n",
      "cs.CV cs.LG cs.NE eess.IV\n",
      "physics.gen-ph astro-ph.HE quant-ph\n",
      "nlin.AO cs.SI math.DS\n",
      "eess.SP cs.CE cs.IT math.IT\n",
      "cond-mat.stat-mech math-ph math.MP nlin.CD physics.ao-ph physics.flu-dyn\n",
      "astro-ph.HE astro-ph.SR physics.geo-ph\n",
      "cond-mat.mes-hall cond-mat.other physics.optics\n",
      "cs.CV cs.LG math.DS stat.AP stat.ML\n",
      "cs.LG cs.GR stat.ML\n",
      "math.CO math.ST stat.CO stat.TH\n",
      "astro-ph.GA astro-ph.CO astro-ph.HE hep-ph\n",
      "astro-ph.GA nucl-th\n",
      "cond-mat.supr-con cond-mat.dis-nn\n",
      "q-bio.QM physics.bio-ph q-bio.MN\n",
      "physics.ao-ph astro-ph.IM\n",
      "cs.AI cs.NE\n",
      "math.OC cs.LG cs.NE\n",
      "astro-ph.HE astro-ph.GA cond-mat.mtrl-sci\n",
      "q-fin.ST q-fin.TR\n",
      "physics.pop-ph gr-qc physics.class-ph physics.space-ph\n",
      "cs.NE physics.acc-ph physics.comp-ph\n",
      "cs.LG cs.IR cs.SI stat.ML\n",
      "astro-ph.SR astro-ph.GA hep-ph\n",
      "cond-mat.stat-mech math-ph math.MP math.SP quant-ph\n",
      "astro-ph.IM nucl-th\n",
      "astro-ph.HE astro-ph.IM physics.atom-ph\n",
      "cond-mat.stat-mech cond-mat.dis-nn nlin.CD quant-ph\n",
      "hep-th hep-lat nucl-th quant-ph\n",
      "astro-ph.GA gr-qc hep-th\n",
      "cs.DC cs.PF\n",
      "eess.AS cs.LG\n",
      "astro-ph.CO physics.data-an stat.ML\n",
      "physics.comp-ph cond-mat.mtrl-sci cond-mat.str-el\n",
      "physics.comp-ph astro-ph.HE gr-qc\n",
      "q-bio.QM q-bio.BM q-bio.MN q-bio.SC\n",
      "cs.CR cs.NI cs.SI\n",
      "quant-ph cond-mat.quant-gas hep-th physics.optics\n",
      "cond-mat.str-el cond-mat.mes-hall hep-th math-ph math.MP\n",
      "physics.app-ph math.DS\n",
      "astro-ph.HE cs.LG gr-qc\n",
      "physics.app-ph physics.ins-det quant-ph\n",
      "physics.bio-ph cond-mat.soft cond-mat.stat-mech nlin.CG physics.chem-ph\n",
      "physics.soc-ph cs.SY eess.SY math.PR\n",
      "cond-mat.mtrl-sci cs.CE cs.NA math.NA physics.app-ph\n",
      "astro-ph.EP physics.ed-ph\n",
      "math.OC eess.SP\n",
      "cond-mat.mtrl-sci cs.LG physics.chem-ph physics.comp-ph\n",
      "cond-mat.mes-hall physics.chem-ph physics.comp-ph physics.optics quant-ph\n",
      "cs.FL cs.LO\n",
      "physics.space-ph astro-ph.IM\n",
      "physics.optics gr-qc physics.flu-dyn\n",
      "gr-qc astro-ph.HE math-ph math.MP\n",
      "astro-ph.HE astro-ph.GA astro-ph.SR physics.space-ph\n",
      "cs.CV cs.CL cs.IR cs.LG\n",
      "nlin.AO cs.LG nlin.CD\n",
      "astro-ph.SR physics.flu-dyn physics.plasm-ph\n",
      "astro-ph.HE physics.plasm-ph physics.space-ph\n",
      "gr-qc astro-ph.CO hep-ph hep-th quant-ph\n",
      "gr-qc cond-mat.supr-con quant-ph\n",
      "physics.ed-ph physics.hist-ph\n",
      "physics.geo-ph cs.LG\n",
      "cs.NI cs.MS\n",
      "gr-qc cond-mat.quant-gas\n",
      "cs.CL cs.AI cs.CV cs.LG\n",
      "cs.AI cs.CL cs.LG cs.MA\n",
      "cs.CL cs.HC\n",
      "eess.SP cs.CR cs.LG\n",
      "cs.AI stat.OT\n",
      "math-ph math.MP nlin.CD\n",
      "eess.IV cs.CV physics.app-ph\n",
      "astro-ph.GA astro-ph.EP astro-ph.SR\n",
      "quant-ph math-ph math.MP physics.class-ph\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.str-el physics.app-ph\n",
      "cond-mat.quant-gas cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el\n",
      "cs.CY cs.AI cs.HC cs.RO cs.SY eess.SY\n",
      "cs.MA cs.RO cs.SE\n",
      "q-bio.NC stat.AP\n",
      "cond-mat.mes-hall cond-mat.supr-con physics.app-ph\n",
      "cond-mat.stat-mech math.DS\n",
      "cond-mat.dis-nn cond-mat.soft physics.comp-ph\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph\n",
      "cond-mat.stat-mech nlin.CD physics.flu-dyn\n",
      "hep-ex astro-ph.CO hep-ph physics.ins-det\n",
      "astro-ph.SR astro-ph.EP physics.ao-ph\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph\n",
      "cond-mat.stat-mech cs.SI physics.soc-ph\n",
      "physics.ed-ph astro-ph.IM\n",
      "astro-ph.IM physics.space-ph\n",
      "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.str-el cond-mat.supr-con\n",
      "physics.comp-ph cond-mat.stat-mech physics.med-ph\n",
      "physics.hist-ph math-ph math.MP\n",
      "astro-ph.SR astro-ph.EP physics.ao-ph physics.chem-ph\n",
      "q-bio.PE math.CA\n",
      "math.NA cs.NA math.AP math.DS math.FA\n",
      "eess.SP cs.LG cs.SY eess.IV eess.SY\n",
      "cond-mat.soft nlin.CD nlin.PS\n",
      "cs.CV cs.GR cs.LG cs.RO eess.IV\n",
      "physics.ao-ph cond-mat.stat-mech\n",
      "cond-mat.mes-hall hep-th quant-ph\n",
      "cs.IT cs.LG math.IT\n",
      "math-ph cond-mat.mes-hall math.MP quant-ph\n",
      "cs.CY cs.AI\n",
      "quant-ph math.ST stat.TH\n",
      "math.NT math.CV\n",
      "cond-mat.str-el cond-mat.mes-hall cond-mat.other cond-mat.stat-mech\n",
      "q-bio.TO\n",
      "cs.NI cs.SY eess.SP eess.SY\n",
      "q-bio.QM cs.CV cs.LG\n",
      "cond-mat.stat-mech stat.ML\n",
      "cond-mat.quant-gas nucl-th physics.atm-clus quant-ph\n",
      "nlin.AO physics.bio-ph\n",
      "math.PR math.DS\n",
      "cond-mat.dis-nn cond-mat.quant-gas physics.atom-ph quant-ph\n",
      "math.NA cs.NA math-ph math.MP\n",
      "hep-ph hep-th math-ph math.MP\n",
      "physics.plasm-ph physics.app-ph\n",
      "quant-ph hep-lat hep-th\n",
      "cs.AR q-bio.GN stat.CO\n",
      "nlin.CD cond-mat.other math.DS math.NT\n",
      "physics.plasm-ph cond-mat.other physics.chem-ph physics.comp-ph\n",
      "cs.DC cs.CV cs.GR\n",
      "physics.optics cond-mat.quant-gas physics.atm-clus\n",
      "physics.ins-det astro-ph.HE astro-ph.IM\n",
      "gr-qc cond-mat.quant-gas hep-th\n",
      "astro-ph.CO astro-ph.HE hep-ph hep-th\n",
      "physics.atom-ph nucl-th physics.chem-ph\n",
      "cs.CE cs.NA math.NA\n",
      "cs.CV cs.MM eess.IV\n",
      "physics.class-ph quant-ph\n",
      "cond-mat.quant-gas physics.comp-ph\n",
      "physics.soc-ph physics.data-an q-bio.PE q-bio.QM\n",
      "physics.ins-det cond-mat.mtrl-sci physics.acc-ph\n",
      "cs.LG cs.CV cs.RO\n",
      "hep-ph astro-ph.CO astro-ph.IM gr-qc physics.optics\n",
      "cs.LO cs.PL cs.SC\n",
      "astro-ph.GA astro-ph.CO cond-mat.stat-mech\n",
      "cs.CL cs.AI cs.IR cs.LG cs.NE\n",
      "cs.CV cs.GR cs.HC eess.IV\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.stat-mech physics.chem-ph\n",
      "cs.HC cs.CY\n",
      "stat.OT cs.CY\n",
      "astro-ph.SR astro-ph.HE hep-ph nucl-th\n",
      "physics.atom-ph hep-ph physics.chem-ph\n",
      "eess.SP physics.data-an\n",
      "cs.SD cs.IT eess.AS math.IT\n",
      "cs.SI cs.LG physics.data-an\n",
      "eess.AS cs.CL cs.LG\n",
      "astro-ph.CO physics.hist-ph\n",
      "cs.NE cs.LG nlin.CD physics.comp-ph\n",
      "math.SP math.AP math.CV\n",
      "cs.IT math.IT math.PR\n",
      "cs.DL astro-ph.IM physics.soc-ph\n",
      "cs.SE cs.SI\n",
      "econ.GN physics.soc-ph q-fin.EC\n",
      "quant-ph math-ph math.MP physics.data-an stat.AP\n",
      "cs.GR math.GN\n",
      "physics.hist-ph physics.ed-ph\n",
      "cs.SI physics.soc-ph stat.ME\n",
      "hep-th cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech\n",
      "cond-mat.mtrl-sci nucl-ex\n",
      "cs.CY physics.soc-ph stat.AP\n",
      "physics.pop-ph astro-ph.CO physics.bio-ph\n",
      "physics.pop-ph cond-mat.str-el\n",
      "physics.optics cond-mat.dis-nn quant-ph\n",
      "cs.DB cs.LG\n",
      "cs.CE cs.CV\n",
      "stat.AP cs.SI\n",
      "cs.LG q-bio.NC stat.ML\n",
      "cs.SD cs.CL cs.LG eess.AS\n",
      "cs.SD cs.IR cs.LG eess.AS\n",
      "eess.SP cs.DC\n",
      "cond-mat.mes-hall math-ph math.MP physics.optics quant-ph\n",
      "cs.CY physics.ed-ph physics.hist-ph\n",
      "eess.AS cs.LG cs.SD eess.SP stat.ML\n",
      "quant-ph physics.atom-ph physics.plasm-ph\n",
      "cs.CV cs.GR cs.LG stat.ML\n",
      "cond-mat.mtrl-sci cond-mat.stat-mech physics.comp-ph physics.plasm-ph\n",
      "physics.space-ph astro-ph.SR physics.data-an physics.flu-dyn physics.plasm-ph\n",
      "physics.atom-ph astro-ph.IM hep-ph physics.optics\n",
      "cs.PL cs.CY\n",
      "physics.bio-ph cs.SY eess.SY q-bio.QM\n",
      "cs.CY cs.AI cs.LG cs.SI\n",
      "astro-ph.IM astro-ph.GA astro-ph.HE\n",
      "quant-ph cond-mat.mes-hall math-ph math.MP\n",
      "hep-th cond-mat.str-el hep-ph\n",
      "cond-mat.soft cond-mat.stat-mech nlin.PS\n",
      "q-fin.ST cs.CE cs.CV\n",
      "cs.SI cs.CR\n",
      "nlin.AO cond-mat.dis-nn physics.chem-ph q-bio.QM\n",
      "eess.SP cs.AR stat.CO stat.ME\n",
      "cs.CV cs.GR cs.HC\n",
      "cs.LG cs.DB stat.ML\n",
      "q-bio.BM cond-mat.stat-mech\n",
      "cond-mat.supr-con cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci\n",
      "physics.optics nlin.PS physics.bio-ph q-bio.BM\n",
      "nlin.PS nlin.CD\n",
      "physics.comp-ph cs.PF\n",
      "cs.DS cs.DC cs.PF\n",
      "nucl-th astro-ph.HE astro-ph.SR nucl-ex\n",
      "cs.IR cs.DC cs.LG cs.SI\n",
      "cs.CV cs.DC cs.PL\n",
      "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el cond-mat.supr-con\n",
      "cs.DC cs.DB\n",
      "physics.soc-ph physics.bio-ph physics.med-ph\n",
      "eess.SP cs.SY eess.SY math.DS\n",
      "physics.geo-ph nlin.PS physics.flu-dyn\n",
      "physics.soc-ph cs.SI econ.GN q-fin.EC\n",
      "hep-ph cond-mat.supr-con gr-qc\n",
      "cs.NI cs.CY cs.DS\n",
      "physics.flu-dyn cond-mat.mtrl-sci\n",
      "astro-ph.SR physics.geo-ph physics.space-ph\n",
      "cs.IR cs.AI cs.HC cs.NE\n",
      "cs.SI cs.CR cs.CY\n",
      "hep-th astro-ph.CO gr-qc hep-ph quant-ph\n",
      "cs.CL cond-mat.stat-mech\n",
      "cond-mat.mtrl-sci physics.atom-ph physics.chem-ph physics.comp-ph\n",
      "cs.IR cs.CL cs.DL cs.LG\n",
      "cs.LG stat.ME stat.ML\n",
      "cs.AR cs.LG cs.PF\n",
      "physics.space-ph astro-ph.SR physics.geo-ph physics.plasm-ph\n",
      "astro-ph.GA nlin.CD\n",
      "physics.med-ph eess.SP physics.ins-det\n",
      "eess.SP cs.HC\n",
      "cond-mat.soft cond-mat.dis-nn cond-mat.stat-mech physics.class-ph\n",
      "physics.flu-dyn math.DS nlin.CD physics.comp-ph stat.ML\n",
      "physics.bio-ph cs.AI\n",
      "physics.app-ph astro-ph.IM physics.optics\n",
      "physics.flu-dyn physics.app-ph physics.geo-ph\n",
      "physics.optics physics.flu-dyn\n",
      "cs.SE cs.PL\n",
      "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech\n",
      "math-ph cond-mat.stat-mech math.DS math.MP\n",
      "cond-mat.str-el physics.chem-ph physics.comp-ph quant-ph\n",
      "physics.plasm-ph cs.LG\n",
      "physics.app-ph physics.med-ph\n",
      "cs.IT math.IT math.ST stat.TH\n",
      "astro-ph.SR astro-ph.EP cs.LG\n",
      "cs.CR cs.SI\n",
      "cs.LG cs.AI cs.DC stat.ML\n",
      "cs.LG cs.CV cs.RO stat.ML\n",
      "q-bio.QM cs.LG physics.bio-ph\n",
      "cond-mat.quant-gas cond-mat.mes-hall nlin.PS physics.optics\n",
      "physics.flu-dyn cs.CE cs.NA math.NA\n",
      "quant-ph physics.comp-ph physics.optics\n",
      "stat.AP stat.OT\n",
      "physics.class-ph physics.atom-ph\n",
      "cs.CV cs.LG cs.SY eess.SY\n",
      "nlin.AO cond-mat.dis-nn math.DS\n",
      "physics.space-ph astro-ph.EP astro-ph.HE physics.plasm-ph\n",
      "gr-qc physics.ed-ph\n",
      "cs.CY cs.DL\n",
      "cond-mat.mes-hall physics.bio-ph\n",
      "cs.CV cs.AI cs.GR\n",
      "cs.LG cs.HC\n",
      "math.ST econ.EM stat.ME stat.TH\n",
      "math.NA cs.NA math.PR physics.comp-ph\n",
      "eess.AS cs.CR cs.SD\n",
      "cond-mat.soft cond-mat.dis-nn physics.comp-ph physics.data-an\n",
      "cs.CV cs.LG cs.SD eess.AS\n",
      "astro-ph.EP astro-ph.HE\n",
      "astro-ph.SR astro-ph.EP astro-ph.HE\n",
      "physics.med-ph cs.CV cs.CY\n",
      "eess.SP cs.CY\n",
      "nlin.AO cond-mat.dis-nn math.DS physics.bio-ph\n",
      "cond-mat.mes-hall cond-mat.stat-mech physics.comp-ph\n",
      "cond-mat.soft cond-mat.other physics.flu-dyn\n",
      "astro-ph.GA astro-ph.CO physics.data-an\n",
      "cond-mat.stat-mech cond-mat.supr-con quant-ph\n",
      "cs.LO cs.AI cs.PL\n",
      "cs.LO cs.FL cs.GT\n",
      "cs.LG cs.GT cs.NI\n",
      "astro-ph.SR astro-ph.IM gr-qc\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con quant-ph\n",
      "astro-ph.SR astro-ph.HE astro-ph.IM physics.space-ph\n",
      "astro-ph.CO astro-ph.GA cond-mat.stat-mech\n",
      "cs.IR cs.CY\n",
      "astro-ph.IM astro-ph.EP physics.space-ph\n",
      "cs.CL cs.HC cs.LG\n",
      "cs.LG cs.AI cs.CR stat.ML\n",
      "q-bio.PE stat.AP stat.ML\n",
      "cs.CY cs.AR cs.CV\n",
      "astro-ph.GA astro-ph.EP\n",
      "cs.CV cs.AI cs.MM\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph\n",
      "cs.LG cs.AI cs.SY eess.SY stat.ML\n",
      "physics.chem-ph cond-mat.mtrl-sci cond-mat.other physics.plasm-ph\n",
      "cs.MS physics.data-an physics.ins-det\n",
      "physics.med-ph q-bio.TO\n",
      "quant-ph cond-mat.mes-hall hep-th\n",
      "cond-mat.stat-mech astro-ph.HE physics.comp-ph\n",
      "cs.CL cs.AI cs.CY cs.LG\n",
      "cs.AI cs.HC cs.LG\n",
      "cs.CL cs.AI cs.MA\n",
      "cond-mat.mtrl-sci physics.app-ph physics.comp-ph\n",
      "cs.AR\n",
      "cs.HC cs.CR\n",
      "physics.app-ph cs.LG\n",
      "physics.flu-dyn cond-mat.dis-nn cond-mat.soft physics.geo-ph\n",
      "cs.RO cs.AI cs.HC\n",
      "cs.LG cs.CL cs.IR\n",
      "eess.AS cs.CL cs.LG cs.SD stat.ML\n",
      "cs.CV cs.CR eess.IV\n",
      "cs.LG cs.CR\n",
      "physics.geo-ph cs.LG stat.AP stat.ML\n",
      "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.soft physics.app-ph physics.chem-ph\n",
      "cs.LG cs.AR cs.PL\n",
      "astro-ph.EP physics.comp-ph physics.geo-ph physics.optics\n",
      "cs.AI cs.CY cs.HC\n",
      "cs.CL cs.FL cs.LG\n",
      "physics.atom-ph physics.app-ph physics.ins-det\n",
      "eess.IV cs.LG eess.SP\n",
      "cs.MM cs.GR\n",
      "astro-ph.IM cs.DC\n",
      "cs.SD cs.LG eess.AS quant-ph\n",
      "cs.LG cs.NE\n",
      "cs.NI cs.GT cs.LG cs.MA\n",
      "cs.SD cs.HC cs.LG eess.AS\n",
      "cond-mat.soft physics.bio-ph q-bio.SC\n",
      "cs.AI cs.CL\n",
      "cs.LG q-bio.MN stat.AP stat.ML\n",
      "cs.AI cs.LG physics.flu-dyn physics.soc-ph\n",
      "cs.CY cs.MA cs.SE\n",
      "cs.SD eess.AS q-bio.NC\n",
      "cs.CY cs.CL\n",
      "math.RT math.GR\n",
      "physics.app-ph cond-mat.mtrl-sci cs.NA math.NA\n",
      "physics.pop-ph gr-qc\n",
      "eess.SY cs.RO cs.SY\n",
      "physics.bio-ph physics.med-ph\n",
      "gr-qc astro-ph.CO astro-ph.HE astro-ph.SR\n",
      "cs.NI cs.CY\n",
      "cs.LG cs.AI cs.SY eess.SY\n",
      "q-bio.QM cs.LG q-bio.GN q-bio.PE\n",
      "physics.chem-ph physics.acc-ph physics.app-ph\n",
      "cs.CR cs.AI cs.CV cs.LG\n",
      "cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph physics.data-an\n",
      "q-bio.NC cs.AI cs.LG cs.RO\n",
      "cond-mat.dis-nn physics.geo-ph\n",
      "cs.LG cs.DC\n",
      "cs.CV cs.CG cs.LG\n",
      "cs.SI cs.HC\n",
      "cond-mat.mtrl-sci physics.flu-dyn\n",
      "q-fin.TR cs.MA\n",
      "eess.SY cs.CR cs.SY math.OC\n",
      "eess.SY cs.DC cs.SY math.OC\n",
      "quant-ph cs.LO math.PR\n",
      "cs.SD eess.AS physics.app-ph\n",
      "cs.LG cs.DS\n",
      "q-bio.NC cs.SY eess.SY\n",
      "math.CO math.FA math.NT\n",
      "cs.GR astro-ph.IM math.MG physics.data-an\n",
      "cs.LG cs.CV cs.IT cs.MM math.IT\n",
      "physics.atom-ph hep-ph physics.ins-det physics.plasm-ph quant-ph\n",
      "cs.CR cs.CV\n",
      "cs.LG cs.AI cs.CL\n",
      "physics.space-ph physics.ao-ph\n",
      "physics.app-ph cond-mat.mes-hall cond-mat.str-el\n",
      "cs.AR cs.LG eess.IV\n",
      "cs.SI cs.GT\n",
      "cs.LG cs.CV q-bio.QM\n",
      "physics.comp-ph cond-mat.stat-mech physics.plasm-ph\n",
      "physics.flu-dyn physics.data-an\n",
      "eess.SP cs.LG cs.SD eess.AS\n",
      "cs.LG nlin.CD\n",
      "cs.LG physics.flu-dyn stat.AP\n",
      "astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.HE astro-ph.SR\n",
      "physics.bio-ph cond-mat.soft physics.flu-dyn physics.med-ph\n",
      "physics.med-ph cs.CV cs.LG\n",
      "physics.gen-ph physics.optics\n",
      "physics.pop-ph astro-ph.IM cs.CY\n",
      "econ.GN physics.soc-ph q-fin.EC stat.AP\n",
      "math.CO math.CA math.FA math.NT\n",
      "astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.SR physics.chem-ph\n",
      "cond-mat.mes-hall physics.app-ph physics.comp-ph\n",
      "cs.ET cond-mat.stat-mech cs.CC cs.NE\n",
      "cs.LO math.CT\n",
      "eess.IV cs.CV physics.ins-det\n",
      "astro-ph.EP nlin.CD\n",
      "cs.LG q-bio.BM\n",
      "astro-ph.EP physics.ao-ph physics.flu-dyn physics.geo-ph\n",
      "cs.CY cs.DB\n",
      "physics.chem-ph cond-mat.mtrl-sci cs.LG\n",
      "cs.LG cond-mat.mtrl-sci physics.chem-ph\n",
      "physics.atm-clus cond-mat.quant-gas physics.chem-ph\n",
      "cs.LG cs.CL cs.CV\n",
      "physics.flu-dyn math.DS nlin.CD\n",
      "gr-qc astro-ph hep-th\n",
      "math.GT math.AT\n",
      "nlin.SI hep-th quant-ph\n"
     ]
    }
   ],
   "source": [
    "cat_list= df['categories'].unique()\n",
    "print(*cat_list, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ml_df = df[df['categories'].str.contains(\"cs.LG\")]\n",
    "\n",
    "sentencesList= ml_df['abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077\n"
     ]
    }
   ],
   "source": [
    "print(len(ml_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The extraction and understanding of temporal events and their relations are\n",
      "major challenges in natural language processing. Processing text on a\n",
      "sentence-by-sentence or expression-by-expression basis often fails, in part due\n",
      "to the challenge of capturing the global consistency of the text. We present an\n",
      "ensemble method, which reconciles the outputs of multiple classifiers of\n",
      "temporal expressions across the text using integer programming. Computational\n",
      "experiments show that the ensemble improves upon the best individual results\n",
      "from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and\n",
      "SemEval-2016 Task 12 (Clinical TempEval).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sentencesList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1611.10351'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df.iloc[1]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computing embeddings:283.8583347797394\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "embeddings = model.encode(sentencesList, convert_to_tensor=True)\n",
    "end_time = time.time()\n",
    "print(\"Time for computing embeddings:\"+ str(end_time-start_time) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1077, 768])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1412.1866 \n",
      "Title :Integer-Programming Ensemble of Temporal-Relations Classifiers\n",
      "  The extraction and understanding of temporal events and their relations are\n",
      "major challenges in natural language processing. Processing text on a\n",
      "sentence-by-sentence or expression-by-expression basis often fails, in part due\n",
      "to the challenge of capturing the global consistency of the text. We present an\n",
      "ensemble method, which reconciles the outputs of multiple classifiers of\n",
      "temporal expressions across the text using integer programming. Computational\n",
      "experiments show that the ensemble improves upon the best individual results\n",
      "from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and\n",
      "SemEval-2016 Task 12 (Clinical TempEval).\n",
      "\n",
      "**Paper Id :2004.01546 \n",
      "Title :Temporarily-Aware Context Modelling using Generative Adversarial\n",
      "  Networks for Speech Activity Detection\n",
      "  This paper presents a novel framework for Speech Activity Detection (SAD).\n",
      "Inspired by the recent success of multi-task learning approaches in the speech\n",
      "processing domain, we propose a novel joint learning framework for SAD. We\n",
      "utilise generative adversarial networks to automatically learn a loss function\n",
      "for joint prediction of the frame-wise speech/ non-speech classifications\n",
      "together with the next audio segment. In order to exploit the temporal\n",
      "relationships within the input signal, we propose a temporal discriminator\n",
      "which aims to ensure that the predicted signal is temporally consistent. We\n",
      "evaluate the proposed framework on multiple public benchmarks, including NIST\n",
      "OpenSAT' 17, AMI Meeting and HAVIC, where we demonstrate its capability to\n",
      "outperform state-of-the-art SAD approaches. Furthermore, our cross-database\n",
      "evaluations demonstrate the robustness of the proposed approach across\n",
      "different languages, accents, and acoustic environments.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1611.10351 \n",
      "Title :Joint Causal Inference from Multiple Contexts\n",
      "  The gold standard for discovering causal relations is by means of\n",
      "experimentation. Over the last decades, alternative methods have been proposed\n",
      "that can infer causal relations between variables from certain statistical\n",
      "patterns in purely observational data. We introduce Joint Causal Inference\n",
      "(JCI), a novel approach to causal discovery from multiple data sets from\n",
      "different contexts that elegantly unifies both approaches. JCI is a causal\n",
      "modeling framework rather than a specific algorithm, and it can be implemented\n",
      "using any causal discovery algorithm that can take into account certain\n",
      "background knowledge. JCI can deal with different types of interventions (e.g.,\n",
      "perfect, imperfect, stochastic, etc.) in a unified fashion, and does not\n",
      "require knowledge of intervention targets or types in case of interventional\n",
      "data. We explain how several well-known causal discovery algorithms can be seen\n",
      "as addressing special cases of the JCI framework, and we also propose novel\n",
      "implementations that extend existing causal discovery methods for purely\n",
      "observational data to the JCI setting. We evaluate different JCI\n",
      "implementations on synthetic data and on flow cytometry protein expression data\n",
      "and conclude that JCI implementations can considerably outperform\n",
      "state-of-the-art causal discovery algorithms.\n",
      "\n",
      "**Paper Id :2010.12455 \n",
      "Title :Primal-Dual Mesh Convolutional Neural Networks\n",
      "  Recent works in geometric deep learning have introduced neural networks that\n",
      "allow performing inference tasks on three-dimensional geometric data by\n",
      "defining convolution, and sometimes pooling, operations on triangle meshes.\n",
      "These methods, however, either consider the input mesh as a graph, and do not\n",
      "exploit specific geometric properties of meshes for feature aggregation and\n",
      "downsampling, or are specialized for meshes, but rely on a rigid definition of\n",
      "convolution that does not properly capture the local topology of the mesh. We\n",
      "propose a method that combines the advantages of both types of approaches,\n",
      "while addressing their limitations: we extend a primal-dual framework drawn\n",
      "from the graph-neural-network literature to triangle meshes, and define\n",
      "convolutions on two types of graphs constructed from an input mesh. Our method\n",
      "takes features for both edges and faces of a 3D mesh as input and dynamically\n",
      "aggregates them using an attention mechanism. At the same time, we introduce a\n",
      "pooling operation with a precise geometric interpretation, that allows handling\n",
      "variations in the mesh connectivity by clustering mesh faces in a task-driven\n",
      "fashion. We provide theoretical insights of our approach using tools from the\n",
      "mesh-simplification literature. In addition, we validate experimentally our\n",
      "method in the tasks of shape classification and shape segmentation, where we\n",
      "obtain comparable or superior performance to the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "**Paper Id :1912.13163 \n",
      "Title :Federated Learning with Cooperating Devices: A Consensus Approach for\n",
      "  Massive IoT Networks\n",
      "  Federated learning (FL) is emerging as a new paradigm to train machine\n",
      "learning models in distributed systems. Rather than sharing, and disclosing,\n",
      "the training dataset with the server, the model parameters (e.g. neural\n",
      "networks weights and biases) are optimized collectively by large populations of\n",
      "interconnected devices, acting as local learners. FL can be applied to\n",
      "power-constrained IoT devices with slow and sporadic connections. In addition,\n",
      "it does not need data to be exported to third parties, preserving privacy.\n",
      "Despite these benefits, a main limit of existing approaches is the centralized\n",
      "optimization which relies on a server for aggregation and fusion of local\n",
      "parameters; this has the drawback of a single point of failure and scaling\n",
      "issues for increasing network size. The paper proposes a fully distributed (or\n",
      "server-less) learning approach: the proposed FL algorithms leverage the\n",
      "cooperation of devices that perform data operations inside the network by\n",
      "iterating local computations and mutual interactions via consensus-based\n",
      "methods. The approach lays the groundwork for integration of FL within 5G and\n",
      "beyond networks characterized by decentralized connectivity and computing, with\n",
      "intelligence distributed over the end-devices. The proposed methodology is\n",
      "verified by experimental datasets collected inside an industrial IoT\n",
      "environment.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1704.00196 \n",
      "Title :Faster Subgradient Methods for Functions with H\\\"olderian Growth\n",
      "  The purpose of this manuscript is to derive new convergence results for\n",
      "several subgradient methods applied to minimizing nonsmooth convex functions\n",
      "with H\\\"olderian growth. The growth condition is satisfied in many applications\n",
      "and includes functions with quadratic growth and weakly sharp minima as special\n",
      "cases. To this end there are three main contributions. First, for a constant\n",
      "and sufficiently small stepsize, we show that the subgradient method achieves\n",
      "linear convergence up to a certain region including the optimal set, with error\n",
      "of the order of the stepsize. Second, if appropriate problem parameters are\n",
      "known, we derive a decaying stepsize which obtains a much faster convergence\n",
      "rate than is suggested by the classical $O(1/\\sqrt{k})$ result for the\n",
      "subgradient method. Thirdly we develop a novel \"descending stairs\" stepsize\n",
      "which obtains this faster convergence rate and also obtains linear convergence\n",
      "for the special case of weakly sharp functions. We also develop an adaptive\n",
      "variant of the \"descending stairs\" stepsize which achieves the same convergence\n",
      "rate without requiring an error bound constant which is difficult to estimate\n",
      "in practice.\n",
      "\n",
      "**Paper Id :1910.11561 \n",
      "Title :Convergence Analysis of Block Coordinate Algorithms with Determinantal\n",
      "  Sampling\n",
      "  We analyze the convergence rate of the randomized Newton-like method\n",
      "introduced by Qu et. al. (2016) for smooth and convex objectives, which uses\n",
      "random coordinate blocks of a Hessian-over-approximation matrix $\\bM$ instead\n",
      "of the true Hessian. The convergence analysis of the algorithm is challenging\n",
      "because of its complex dependence on the structure of $\\bM$. However, we show\n",
      "that when the coordinate blocks are sampled with probability proportional to\n",
      "their determinant, the convergence rate depends solely on the eigenvalue\n",
      "distribution of matrix $\\bM$, and has an analytically tractable form. To do so,\n",
      "we derive a fundamental new expectation formula for determinantal point\n",
      "processes. We show that determinantal sampling allows us to reason about the\n",
      "optimal subset size of blocks in terms of the spectrum of $\\bM$. Additionally,\n",
      "we provide a numerical evaluation of our analysis, demonstrating cases where\n",
      "determinantal sampling is superior or on par with uniform sampling.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1708.06633 \n",
      "Title :Nonparametric regression using deep neural networks with ReLU activation\n",
      "  function\n",
      "  Consider the multivariate nonparametric regression model. It is shown that\n",
      "estimators based on sparsely connected deep neural networks with ReLU\n",
      "activation function and properly chosen network architecture achieve the\n",
      "minimax rates of convergence (up to $\\log n$-factors) under a general\n",
      "composition assumption on the regression function. The framework includes many\n",
      "well-studied structural constraints such as (generalized) additive models.\n",
      "While there is a lot of flexibility in the network architecture, the tuning\n",
      "parameter is the sparsity of the network. Specifically, we consider large\n",
      "networks with number of potential network parameters exceeding the sample size.\n",
      "The analysis gives some insights into why multilayer feedforward neural\n",
      "networks perform well in practice. Interestingly, for ReLU activation function\n",
      "the depth (number of layers) of the neural network architectures plays an\n",
      "important role and our theory suggests that for nonparametric regression,\n",
      "scaling the network depth with the sample size is natural. It is also shown\n",
      "that under the composition assumption wavelet estimators can only achieve\n",
      "suboptimal rates.\n",
      "\n",
      "**Paper Id :1910.02333 \n",
      "Title :The Role of Neural Network Activation Functions\n",
      "  A wide variety of activation functions have been proposed for neural\n",
      "networks. The Rectified Linear Unit (ReLU) is especially popular today. There\n",
      "are many practical reasons that motivate the use of the ReLU. This paper\n",
      "provides new theoretical characterizations that support the use of the ReLU,\n",
      "its variants such as the leaky ReLU, as well as other activation functions in\n",
      "the case of univariate, single-hidden layer feedforward neural networks. Our\n",
      "results also explain the importance of commonly used strategies in the design\n",
      "and training of neural networks such as \"weight decay\" and \"path-norm\"\n",
      "regularization, and provide a new justification for the use of \"skip\n",
      "connections\" in network architectures. These new insights are obtained through\n",
      "the lens of spline theory. In particular, we show how neural network training\n",
      "problems are related to infinite-dimensional optimizations posed over Banach\n",
      "spaces of functions whose solutions are well-known to be fractional and\n",
      "polynomial splines, where the particular Banach space (which controls the order\n",
      "of the spline) depends on the choice of activation function.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1709.04212 \n",
      "Title :Asymptotic Bayesian Generalization Error in Latent Dirichlet Allocation\n",
      "  and Stochastic Matrix Factorization\n",
      "  Latent Dirichlet allocation (LDA) is useful in document analysis, image\n",
      "processing, and many information systems; however, its generalization\n",
      "performance has been left unknown because it is a singular learning machine to\n",
      "which regular statistical theory can not be applied.\n",
      "  Stochastic matrix factorization (SMF) is a restricted matrix factorization in\n",
      "which matrix factors are stochastic; the column of the matrix is in a simplex.\n",
      "SMF is being applied to image recognition and text mining. We can understand\n",
      "SMF as a statistical model by which a stochastic matrix of given data is\n",
      "represented by a product of two stochastic matrices, whose generalization\n",
      "performance has also been left unknown because of non-regularity.\n",
      "  In this paper, by using an algebraic and geometric method, we show the\n",
      "analytic equivalence of LDA and SMF, both of which have the same real log\n",
      "canonical threshold (RLCT), resulting in that they asymptotically have the same\n",
      "Bayesian generalization error and the same log marginal likelihood. Moreover,\n",
      "we derive the upper bound of the RLCT and prove that it is smaller than the\n",
      "dimension of the parameter divided by two, hence the Bayesian generalization\n",
      "errors of them are smaller than those of regular statistical models.\n",
      "\n",
      "**Paper Id :2001.02568 \n",
      "Title :A Group Norm Regularized Factorization Model for Subspace Segmentation\n",
      "  Subspace segmentation assumes that data comes from the union of different\n",
      "subspaces and the purpose of segmentation is to partition the data into the\n",
      "corresponding subspace. Low-rank representation (LRR) is a classic\n",
      "spectral-type method for solving subspace segmentation problems, that is, one\n",
      "first obtains an affinity matrix by solving a LRR model and then performs\n",
      "spectral clustering for segmentation. This paper proposes a group norm\n",
      "regularized factorization model (GNRFM) inspired by the LRR model for subspace\n",
      "segmentation and then designs an Accelerated Augmented Lagrangian Method (AALM)\n",
      "algorithm to solve this model. Specifically, we adopt group norm regularization\n",
      "to make the columns of the factor matrix sparse, thereby achieving a purpose of\n",
      "low rank, which means no Singular Value Decompositions (SVD) are required and\n",
      "the computational complexity of each step is greatly reduced. We obtain\n",
      "affinity matrices by using different LRR models and then performing cluster\n",
      "testing on different sets of synthetic noisy data and real data, respectively.\n",
      "Compared with traditional models and algorithms, the proposed method is faster\n",
      "and more robust to noise, so the final clustering results are better. Moreover,\n",
      "the numerical results show that our algorithm converges fast and only requires\n",
      "approximately ten iterations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1710.09859 \n",
      "Title :Kernel k-Groups via Hartigan's Method\n",
      "  Energy statistics was proposed by Sz\\' ekely in the 80's inspired by Newton's\n",
      "gravitational potential in classical mechanics and it provides a model-free\n",
      "hypothesis test for equality of distributions. In its original form, energy\n",
      "statistics was formulated in Euclidean spaces. More recently, it was\n",
      "generalized to metric spaces of negative type. In this paper, we consider a\n",
      "formulation for the clustering problem using a weighted version of energy\n",
      "statistics in spaces of negative type. We show that this approach leads to a\n",
      "quadratically constrained quadratic program in the associated kernel space,\n",
      "establishing connections with graph partitioning problems and kernel methods in\n",
      "machine learning. To find local solutions of such an optimization problem, we\n",
      "propose kernel k-groups, which is an extension of Hartigan's method to kernel\n",
      "spaces. Kernel k-groups is cheaper than spectral clustering and has the same\n",
      "computational cost as kernel k-means (which is based on Lloyd's heuristic) but\n",
      "our numerical results show an improved performance, especially in higher\n",
      "dimensions. Moreover, we verify the efficiency of kernel k-groups in community\n",
      "detection in sparse stochastic block models which has fascinating applications\n",
      "in several areas of science.\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1711.01464 \n",
      "Title :Gaussian Kernel in Quantum Learning\n",
      "  The Gaussian kernel is a very popular kernel function used in many machine\n",
      "learning algorithms, especially in support vector machines (SVMs). It is more\n",
      "often used than polynomial kernels when learning from nonlinear datasets, and\n",
      "is usually employed in formulating the classical SVM for nonlinear problems. In\n",
      "[3], Rebentrost et al. discussed an elegant quantum version of a least square\n",
      "support vector machine using quantum polynomial kernels, which is exponentially\n",
      "faster than the classical counterpart. This paper demonstrates a quantum\n",
      "version of the Gaussian kernel and analyzes its runtime complexity using the\n",
      "quantum random access memory (QRAM) in the context of quantum SVM. Our analysis\n",
      "shows that the runtime computational complexity of the quantum Gaussian kernel\n",
      "seems to be significantly faster as compared to its classical version.\n",
      "\n",
      "**Paper Id :1804.10905 \n",
      "Title :An Investigation on Support Vector Clustering for Big Data in Quantum\n",
      "  Paradigm\n",
      "  The support vector clustering algorithm is a well-known clustering algorithm\n",
      "based on support vector machines using Gaussian or polynomial kernels. The\n",
      "classical support vector clustering algorithm works well in general, but its\n",
      "performance degrades when applied on big data. In this paper, we have\n",
      "investigated the performance of support vector clustering algorithm implemented\n",
      "in a quantum paradigm for possible run-time improvements. We have developed and\n",
      "analyzed a quantum version of the support vector clustering algorithm. The\n",
      "proposed approach is based on the quantum support vector machine and quantum\n",
      "kernels (i.e., Gaussian and polynomial). The proposed quantum version of the\n",
      "SVM clustering method demonstrates a significant speed-up gain on the overall\n",
      "run-time complexity as compared to the classical counterpart.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1711.10467 \n",
      "Title :Implicit Regularization in Nonconvex Statistical Estimation: Gradient\n",
      "  Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind\n",
      "  Deconvolution\n",
      "  Recent years have seen a flurry of activities in designing provably efficient\n",
      "nonconvex procedures for solving statistical estimation problems. Due to the\n",
      "highly nonconvex nature of the empirical loss, state-of-the-art procedures\n",
      "often require proper regularization (e.g. trimming, regularized cost,\n",
      "projection) in order to guarantee fast convergence. For vanilla procedures such\n",
      "as gradient descent, however, prior theory either recommends highly\n",
      "conservative learning rates to avoid overshooting, or completely lacks\n",
      "performance guarantees.\n",
      "  This paper uncovers a striking phenomenon in nonconvex optimization: even in\n",
      "the absence of explicit regularization, gradient descent enforces proper\n",
      "regularization implicitly under various statistical models. In fact, gradient\n",
      "descent follows a trajectory staying within a basin that enjoys nice geometry,\n",
      "consisting of points incoherent with the sampling mechanism. This \"implicit\n",
      "regularization\" feature allows gradient descent to proceed in a far more\n",
      "aggressive fashion without overshooting, which in turn results in substantial\n",
      "computational savings. Focusing on three fundamental statistical estimation\n",
      "problems, i.e. phase retrieval, low-rank matrix completion, and blind\n",
      "deconvolution, we establish that gradient descent achieves near-optimal\n",
      "statistical and computational guarantees without explicit regularization. In\n",
      "particular, by marrying statistical modeling with generic optimization theory,\n",
      "we develop a general recipe for analyzing the trajectories of iterative\n",
      "algorithms via a leave-one-out perturbation argument. As a byproduct, for noisy\n",
      "matrix completion, we demonstrate that gradient descent achieves near-optimal\n",
      "error control --- measured entrywise and by the spectral norm --- which might\n",
      "be of independent interest.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1802.02558 \n",
      "Title :Intentional Control of Type I Error over Unconscious Data Distortion: a\n",
      "  Neyman-Pearson Approach to Text Classification\n",
      "  This paper addresses the challenges in classifying textual data obtained from\n",
      "open online platforms, which are vulnerable to distortion. Most existing\n",
      "classification methods minimize the overall classification error and may yield\n",
      "an undesirably large type I error (relevant textual messages are classified as\n",
      "irrelevant), particularly when available data exhibit an asymmetry between\n",
      "relevant and irrelevant information. Data distortion exacerbates this situation\n",
      "and often leads to fallacious prediction. To deal with inestimable data\n",
      "distortion, we propose the use of the Neyman-Pearson (NP) classification\n",
      "paradigm, which minimizes type II error under a user-specified type I error\n",
      "constraint. Theoretically, we show that the NP oracle is unaffected by data\n",
      "distortion when the class conditional distributions remain the same.\n",
      "Empirically, we study a case of classifying posts about worker strikes obtained\n",
      "from a leading Chinese microblogging platform, which are frequently prone to\n",
      "extensive, unpredictable and inestimable censorship. We demonstrate that, even\n",
      "though the training and test data are susceptible to different distortion and\n",
      "therefore potentially follow different distributions, our proposed NP methods\n",
      "control the type I error on test data at the targeted level. The methods and\n",
      "implementation pipeline proposed in our case study are applicable to many other\n",
      "problems involving data distortion.\n",
      "\n",
      "**Paper Id :2008.11752 \n",
      "Title :Appropriateness of Performance Indices for Imbalanced Data\n",
      "  Classification: An Analysis\n",
      "  Indices quantifying the performance of classifiers under class-imbalance,\n",
      "often suffer from distortions depending on the constitution of the test set or\n",
      "the class-specific classification accuracy, creating difficulties in assessing\n",
      "the merit of the classifier. We identify two fundamental conditions that a\n",
      "performance index must satisfy to be respectively resilient to altering number\n",
      "of testing instances from each class and the number of classes in the test set.\n",
      "In light of these conditions, under the effect of class imbalance, we\n",
      "theoretically analyze four indices commonly used for evaluating binary\n",
      "classifiers and five popular indices for multi-class classifiers. For indices\n",
      "violating any of the conditions, we also suggest remedial modification and\n",
      "normalization. We further investigate the capability of the indices to retain\n",
      "information about the classification performance over all the classes, even\n",
      "when the classifier exhibits extreme performance on some classes. Simulation\n",
      "studies are performed on high dimensional deep representations of subset of the\n",
      "ImageNet dataset using four state-of-the-art classifiers tailored for handling\n",
      "class imbalance. Finally, based on our theoretical findings and empirical\n",
      "evidence, we recommend the appropriate indices that should be used to evaluate\n",
      "the performance of classifiers in presence of class-imbalance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1802.03774 \n",
      "Title :On Kernel Method-Based Connectionist Models and Supervised Deep Learning\n",
      "  Without Backpropagation\n",
      "  We propose a novel family of connectionist models based on kernel machines\n",
      "and consider the problem of learning layer-by-layer a compositional hypothesis\n",
      "class, i.e., a feedforward, multilayer architecture, in a supervised setting.\n",
      "In terms of the models, we present a principled method to \"kernelize\" (partly\n",
      "or completely) any neural network (NN). With this method, we obtain a\n",
      "counterpart of any given NN that is powered by kernel machines instead of\n",
      "neurons. In terms of learning, when learning a feedforward deep architecture in\n",
      "a supervised setting, one needs to train all the components simultaneously\n",
      "using backpropagation (BP) since there are no explicit targets for the hidden\n",
      "layers (Rumelhart86). We consider without loss of generality the two-layer case\n",
      "and present a general framework that explicitly characterizes a target for the\n",
      "hidden layer that is optimal for minimizing the objective function of the\n",
      "network. This characterization then makes possible a purely greedy training\n",
      "scheme that learns one layer at a time, starting from the input layer. We\n",
      "provide realizations of the abstract framework under certain architectures and\n",
      "objective functions. Based on these realizations, we present a layer-wise\n",
      "training algorithm for an l-layer feedforward network for classification, where\n",
      "l>=2 can be arbitrary. This algorithm can be given an intuitive geometric\n",
      "interpretation that makes the learning dynamics transparent. Empirical results\n",
      "are provided to complement our theory. We show that the kernelized networks,\n",
      "trained layer-wise, compare favorably with classical kernel machines as well as\n",
      "other connectionist models trained by BP. We also visualize the inner workings\n",
      "of the greedy kernelized models to validate our claim on the transparency of\n",
      "the layer-wise algorithm.\n",
      "\n",
      "**Paper Id :2006.02341 \n",
      "Title :Non-Euclidean Universal Approximation\n",
      "  Modifications to a neural network's input and output layers are often\n",
      "required to accommodate the specificities of most practical learning tasks.\n",
      "However, the impact of such changes on architecture's approximation\n",
      "capabilities is largely not understood. We present general conditions\n",
      "describing feature and readout maps that preserve an architecture's ability to\n",
      "approximate any continuous functions uniformly on compacts. As an application,\n",
      "we show that if an architecture is capable of universal approximation, then\n",
      "modifying its final layer to produce binary values creates a new architecture\n",
      "capable of deterministically approximating any classifier. In particular, we\n",
      "obtain guarantees for deep CNNs and deep feed-forward networks. Our results\n",
      "also have consequences within the scope of geometric deep learning.\n",
      "Specifically, when the input and output spaces are Cartan-Hadamard manifolds,\n",
      "we obtain geometrically meaningful feature and readout maps satisfying our\n",
      "criteria. Consequently, commonly used non-Euclidean regression models between\n",
      "spaces of symmetric positive definite matrices are extended to universal DNNs.\n",
      "The same result allows us to show that the hyperbolic feed-forward networks,\n",
      "used for hierarchical learning, are universal. Our result is also used to show\n",
      "that the common practice of randomizing all but the last two layers of a DNN\n",
      "produces a universal family of functions with probability one. We also provide\n",
      "conditions on a DNN's first (resp. last) few layer's connections and activation\n",
      "function which guarantee that these layers can have a width equal to the input\n",
      "(resp. output) space's dimension while not negatively affecting the\n",
      "architecture's approximation capabilities.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1803.02965 \n",
      "Title :A Multi-Objective Deep Reinforcement Learning Framework\n",
      "  This paper introduces a new scalable multi-objective deep reinforcement\n",
      "learning (MODRL) framework based on deep Q-networks. We develop a\n",
      "high-performance MODRL framework that supports both single-policy and\n",
      "multi-policy strategies, as well as both linear and non-linear approaches to\n",
      "action selection. The experimental results on two benchmark problems\n",
      "(two-objective deep sea treasure environment and three-objective Mountain Car\n",
      "problem) indicate that the proposed framework is able to find the\n",
      "Pareto-optimal solutions effectively. The proposed framework is generic and\n",
      "highly modularized, which allows the integration of different deep\n",
      "reinforcement learning algorithms in different complex problem domains. This\n",
      "therefore overcomes many disadvantages involved with standard multi-objective\n",
      "reinforcement learning methods in the current literature. The proposed\n",
      "framework acts as a testbed platform that accelerates the development of MODRL\n",
      "for solving increasingly complicated multi-objective problems.\n",
      "\n",
      "**Paper Id :1812.11794 \n",
      "Title :Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n",
      "  Challenges, Solutions and Applications\n",
      "  Reinforcement learning (RL) algorithms have been around for decades and\n",
      "employed to solve various sequential decision-making problems. These algorithms\n",
      "however have faced great challenges when dealing with high-dimensional\n",
      "environments. The recent development of deep learning has enabled RL methods to\n",
      "drive optimal policies for sophisticated and capable agents, which can perform\n",
      "efficiently in these challenging environments. This paper addresses an\n",
      "important aspect of deep RL related to situations that require multiple agents\n",
      "to communicate and cooperate to solve complex tasks. A survey of different\n",
      "approaches to problems related to multi-agent deep RL (MADRL) is presented,\n",
      "including non-stationarity, partial observability, continuous state and action\n",
      "spaces, multi-agent training schemes, multi-agent transfer learning. The merits\n",
      "and demerits of the reviewed methods will be analyzed and discussed, with their\n",
      "corresponding applications explored. It is envisaged that this review provides\n",
      "insights about various MADRL methods and can lead to future development of more\n",
      "robust and highly useful multi-agent learning methods for solving real-world\n",
      "problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1803.05985 \n",
      "Title :EEG machine learning with Higuchi fractal dimension and Sample Entropy\n",
      "  as features for successful detection of depression\n",
      "  Reliable diagnosis of depressive disorder is essential for both optimal\n",
      "treatment and prevention of fatal outcomes. In this study, we aimed to\n",
      "elucidate the effectiveness of two non-linear measures, Higuchi Fractal\n",
      "Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders\n",
      "when applied on EEG. HFD and SampEn of EEG signals were used as features for\n",
      "seven machine learning algorithms including Multilayer Perceptron, Logistic\n",
      "Regression, Support Vector Machines with the linear and polynomial kernel,\n",
      "Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG\n",
      "between healthy control subjects and patients diagnosed with depression. We\n",
      "confirmed earlier observations that both non-linear measures can discriminate\n",
      "EEG signals of patients from healthy control subjects. The results suggest that\n",
      "good classification is possible even with a small number of principal\n",
      "components. Average accuracy among classifiers ranged from 90.24% to 97.56%.\n",
      "Among the two measures, SampEn had better performance. Using HFD and SampEn and\n",
      "a variety of machine learning techniques we can accurately discriminate\n",
      "patients diagnosed with depression vs controls which can serve as a highly\n",
      "sensitive, clinically relevant marker for the diagnosis of depressive\n",
      "disorders.\n",
      "\n",
      "**Paper Id :2011.09801 \n",
      "Title :Novel Classification of Ischemic Heart Disease Using Artificial Neural\n",
      "  Network\n",
      "  Ischemic heart disease (IHD), particularly in its chronic stable form, is a\n",
      "subtle pathology due to its silent behavior before developing in unstable\n",
      "angina, myocardial infarction or sudden cardiac death. Machine learning\n",
      "techniques applied to parameters extracted form heart rate variability (HRV)\n",
      "signal seem to be a valuable support in the early diagnosis of some cardiac\n",
      "diseases. However, so far, IHD patients were identified using Artificial Neural\n",
      "Networks (ANNs) applied to a limited number of HRV parameters and only to very\n",
      "few subjects. In this study, we used several linear and non-linear HRV\n",
      "parameters applied to ANNs, in order to confirm these results on a large cohort\n",
      "of 965 sample of subjects and to identify which features could discriminate IHD\n",
      "patients with high accuracy. By using principal component analysis and stepwise\n",
      "regression, we reduced the original 17 parameters to five, used as inputs, for\n",
      "a series of ANNs. The highest accuracy of 82% was achieved using meanRR, LFn,\n",
      "SD1, gender and age parameters and two hidden neurons.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1804.02484 \n",
      "Title :Approximating Hamiltonian dynamics with the Nystr\\\"om method\n",
      "  Simulating the time-evolution of quantum mechanical systems is BQP-hard and\n",
      "expected to be one of the foremost applications of quantum computers. We\n",
      "consider classical algorithms for the approximation of Hamiltonian dynamics\n",
      "using subsampling methods from randomized numerical linear algebra. We derive a\n",
      "simulation technique whose runtime scales polynomially in the number of qubits\n",
      "and the Frobenius norm of the Hamiltonian. As an immediate application, we show\n",
      "that sample based quantum simulation, a type of evolution where the Hamiltonian\n",
      "is a density matrix, can be efficiently classically simulated under specific\n",
      "structural conditions. Our main technical contribution is a randomized\n",
      "algorithm for approximating Hermitian matrix exponentials. The proof leverages\n",
      "a low-rank, symmetric approximation via the Nystr\\\"om method. Our results\n",
      "suggest that under strong sampling assumptions there exist classical\n",
      "poly-logarithmic time simulations of quantum computations.\n",
      "\n",
      "**Paper Id :1910.11561 \n",
      "Title :Convergence Analysis of Block Coordinate Algorithms with Determinantal\n",
      "  Sampling\n",
      "  We analyze the convergence rate of the randomized Newton-like method\n",
      "introduced by Qu et. al. (2016) for smooth and convex objectives, which uses\n",
      "random coordinate blocks of a Hessian-over-approximation matrix $\\bM$ instead\n",
      "of the true Hessian. The convergence analysis of the algorithm is challenging\n",
      "because of its complex dependence on the structure of $\\bM$. However, we show\n",
      "that when the coordinate blocks are sampled with probability proportional to\n",
      "their determinant, the convergence rate depends solely on the eigenvalue\n",
      "distribution of matrix $\\bM$, and has an analytically tractable form. To do so,\n",
      "we derive a fundamental new expectation formula for determinantal point\n",
      "processes. We show that determinantal sampling allows us to reason about the\n",
      "optimal subset size of blocks in terms of the spectrum of $\\bM$. Additionally,\n",
      "we provide a numerical evaluation of our analysis, demonstrating cases where\n",
      "determinantal sampling is superior or on par with uniform sampling.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1804.10168 \n",
      "Title :BEST : A decision tree algorithm that handles missing values\n",
      "  The main contribution of this paper is the development of a new decision tree\n",
      "algorithm. The proposed approach allows users to guide the algorithm through\n",
      "the data partitioning process. We believe this feature has many applications\n",
      "but in this paper we demonstrate how to utilize this algorithm to analyse data\n",
      "sets containing missing values. We tested our algorithm against simulated data\n",
      "sets with various missing data structures and a real data set. The results\n",
      "demonstrate that this new classification procedure efficiently handles missing\n",
      "values and produces results that are slightly more accurate and more\n",
      "interpretable than most common procedures without any imputations or\n",
      "pre-processing.\n",
      "\n",
      "**Paper Id :1911.08871 \n",
      "Title :CNAK : Cluster Number Assisted K-means\n",
      "  Determining the number of clusters present in a dataset is an important\n",
      "problem in cluster analysis. Conventional clustering techniques generally\n",
      "assume this parameter to be provided up front. %user supplied. %Recently,\n",
      "robustness of any given clustering algorithm is analyzed to measure cluster\n",
      "stability/instability which in turn determines the cluster number. In this\n",
      "paper, we propose a method which analyzes cluster stability for predicting the\n",
      "cluster number. Under the same computational framework, the technique also\n",
      "finds representatives of the clusters. The method is apt for handling big data,\n",
      "as we design the algorithm using \\emph{Monte-Carlo} simulation. Also, we\n",
      "explore a few pertinent issues found to be of also clustering. Experiments\n",
      "reveal that the proposed method is capable of identifying a single cluster. It\n",
      "is robust in handling high dimensional dataset and performs reasonably well\n",
      "over datasets having cluster imbalance. Moreover, it can indicate cluster\n",
      "hierarchy, if present. Overall we have observed significant improvement in\n",
      "speed and quality for predicting cluster numbers as well as the composition of\n",
      "clusters in a large dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1804.10905 \n",
      "Title :An Investigation on Support Vector Clustering for Big Data in Quantum\n",
      "  Paradigm\n",
      "  The support vector clustering algorithm is a well-known clustering algorithm\n",
      "based on support vector machines using Gaussian or polynomial kernels. The\n",
      "classical support vector clustering algorithm works well in general, but its\n",
      "performance degrades when applied on big data. In this paper, we have\n",
      "investigated the performance of support vector clustering algorithm implemented\n",
      "in a quantum paradigm for possible run-time improvements. We have developed and\n",
      "analyzed a quantum version of the support vector clustering algorithm. The\n",
      "proposed approach is based on the quantum support vector machine and quantum\n",
      "kernels (i.e., Gaussian and polynomial). The proposed quantum version of the\n",
      "SVM clustering method demonstrates a significant speed-up gain on the overall\n",
      "run-time complexity as compared to the classical counterpart.\n",
      "\n",
      "**Paper Id :1711.01464 \n",
      "Title :Gaussian Kernel in Quantum Learning\n",
      "  The Gaussian kernel is a very popular kernel function used in many machine\n",
      "learning algorithms, especially in support vector machines (SVMs). It is more\n",
      "often used than polynomial kernels when learning from nonlinear datasets, and\n",
      "is usually employed in formulating the classical SVM for nonlinear problems. In\n",
      "[3], Rebentrost et al. discussed an elegant quantum version of a least square\n",
      "support vector machine using quantum polynomial kernels, which is exponentially\n",
      "faster than the classical counterpart. This paper demonstrates a quantum\n",
      "version of the Gaussian kernel and analyzes its runtime complexity using the\n",
      "quantum random access memory (QRAM) in the context of quantum SVM. Our analysis\n",
      "shows that the runtime computational complexity of the quantum Gaussian kernel\n",
      "seems to be significantly faster as compared to its classical version.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1805.07852 \n",
      "Title :Accelerated Bayesian Optimization throughWeight-Prior Tuning\n",
      "  Bayesian optimization (BO) is a widely-used method for optimizing expensive\n",
      "(to evaluate) problems. At the core of most BO methods is the modeling of the\n",
      "objective function using a Gaussian Process (GP) whose covariance is selected\n",
      "from a set of standard covariance functions. From a weight-space view, this\n",
      "models the objective as a linear function in a feature space implied by the\n",
      "given covariance K, with an arbitrary Gaussian weight prior ${\\bf w} \\sim\n",
      "\\mathcal{N} ({\\bf 0}, {\\bf I})$. In many practical applications there is data\n",
      "available that has a similar (covariance) structure to the objective, but\n",
      "which, having different form, cannot be used directly in standard transfer\n",
      "learning. In this paper we show how such auxiliary data may be used to\n",
      "construct a GP covariance corresponding to a more appropriate weight prior for\n",
      "the objective function. Building on this, we show that we may accelerate BO by\n",
      "modeling the objective function using this (learned) weight prior, which we\n",
      "demonstrate on both test functions and a practical application to short-polymer\n",
      "fibre manufacture.\n",
      "\n",
      "**Paper Id :1901.07114 \n",
      "Title :Training Neural Networks as Learning Data-adaptive Kernels: Provable\n",
      "  Representation and Approximation Benefits\n",
      "  Consider the problem: given the data pair $(\\mathbf{x}, \\mathbf{y})$ drawn\n",
      "from a population with $f_*(x) = \\mathbf{E}[\\mathbf{y} | \\mathbf{x} = x]$,\n",
      "specify a neural network model and run gradient flow on the weights over time\n",
      "until reaching any stationarity. How does $f_t$, the function computed by the\n",
      "neural network at time $t$, relate to $f_*$, in terms of approximation and\n",
      "representation? What are the provable benefits of the adaptive representation\n",
      "by neural networks compared to the pre-specified fixed basis representation in\n",
      "the classical nonparametric literature? We answer the above questions via a\n",
      "dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the\n",
      "training process of neural networks. Firstly, we show that when reaching any\n",
      "local stationarity, gradient flow learns an adaptive RKHS representation and\n",
      "performs the global least-squares projection onto the adaptive RKHS,\n",
      "simultaneously. Secondly, we prove that as the RKHS is data-adaptive and\n",
      "task-specific, the residual for $f_*$ lies in a subspace that is potentially\n",
      "much smaller than the orthogonal complement of the RKHS. The result formalizes\n",
      "the representation and approximation benefits of neural networks. Lastly, we\n",
      "show that the neural network function computed by gradient flow converges to\n",
      "the kernel ridgeless regression with an adaptive kernel, in the limit of\n",
      "vanishing regularization. The adaptive kernel viewpoint provides new angles of\n",
      "studying the approximation, representation, generalization, and optimization\n",
      "advantages of neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1805.09235 \n",
      "Title :Cramer-Wold AutoEncoder\n",
      "  We propose a new generative model, Cramer-Wold Autoencoder (CWAE). Following\n",
      "WAE, we directly encourage normality of the latent space. Our paper uses also\n",
      "the recent idea from Sliced WAE (SWAE) model, which uses one-dimensional\n",
      "projections as a method of verifying closeness of two distributions. The\n",
      "crucial new ingredient is the introduction of a new (Cramer-Wold) metric in the\n",
      "space of densities, which replaces the Wasserstein metric used in SWAE. We show\n",
      "that the Cramer-Wold metric between Gaussian mixtures is given by a simple\n",
      "analytic formula, which results in the removal of sampling necessary to\n",
      "estimate the cost function in WAE and SWAE models. As a consequence, while\n",
      "drastically simplifying the optimization procedure, CWAE produces samples of a\n",
      "matching perceptual quality to other SOTA models.\n",
      "\n",
      "**Paper Id :1907.00865 \n",
      "Title :Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale\n",
      "  Bayesian Deep Learning\n",
      "  We propose Radial Bayesian Neural Networks (BNNs): a variational approximate\n",
      "posterior for BNNs which scales well to large models while maintaining a\n",
      "distribution over weight-space with full support. Other scalable Bayesian deep\n",
      "learning methods, like MC dropout or deep ensembles, have discrete support-they\n",
      "assign zero probability to almost all of the weight-space. Unlike these\n",
      "discrete support methods, Radial BNNs' full support makes them suitable for use\n",
      "as a prior for sequential inference. In addition, they solve the conceptual\n",
      "challenges with the a priori implausibility of weight distributions with\n",
      "discrete support. The Radial BNN is motivated by avoiding a sampling problem in\n",
      "'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble'\n",
      "pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are\n",
      "robust to hyperparameters and can be efficiently applied to a challenging\n",
      "real-world medical application without needing ad-hoc tweaks and intensive\n",
      "tuning. In fact, in this setting Radial BNNs out-perform discrete-support\n",
      "methods like MC dropout. Lastly, by using Radial BNNs as a theoretically\n",
      "principled, robust alternative to MFVI we make significant strides in a\n",
      "Bayesian continual learning evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1806.08781 \n",
      "Title :Quantum Codes from Neural Networks\n",
      "  We examine the usefulness of applying neural networks as a variational state\n",
      "ansatz for many-body quantum systems in the context of quantum\n",
      "information-processing tasks. In the neural network state ansatz, the complex\n",
      "amplitude function of a quantum state is computed by a neural network. The\n",
      "resulting multipartite entanglement structure captured by this ansatz has\n",
      "proven rich enough to describe the ground states and unitary dynamics of\n",
      "various physical systems of interest. In the present paper, we initiate the\n",
      "study of neural network states in quantum information-processing tasks. We\n",
      "demonstrate that neural network states are capable of efficiently representing\n",
      "quantum codes for quantum information transmission and quantum error\n",
      "correction, supplying further evidence for the usefulness of neural network\n",
      "states to describe multipartite entanglement. In particular, we show the\n",
      "following main results: a) Neural network states yield quantum codes with a\n",
      "high coherent information for two important quantum channels, the generalized\n",
      "amplitude damping channel and the dephrasure channel. These codes outperform\n",
      "all other known codes for these channels, and cannot be found using a direct\n",
      "parametrization of the quantum state. b) For the depolarizing channel, the\n",
      "neural network state ansatz reliably finds the best known codes given by\n",
      "repetition codes. c) Neural network states can be used to represent absolutely\n",
      "maximally entangled states, a special type of quantum error-correcting codes.\n",
      "In all three cases, the neural network state ansatz provides an efficient and\n",
      "versatile means as a variational parametrization of these highly entangled\n",
      "states.\n",
      "\n",
      "**Paper Id :1906.10155 \n",
      "Title :Machine Learning Phase Transitions with a Quantum Processor\n",
      "  Machine learning has emerged as a promising approach to study the properties\n",
      "of many-body systems. Recently proposed as a tool to classify phases of matter,\n",
      "the approach relies on classical simulation methods$-$such as Monte\n",
      "Carlo$-$which are known to experience an exponential slowdown when simulating\n",
      "certain quantum systems. To overcome this slowdown while still leveraging\n",
      "machine learning, we propose a variational quantum algorithm which merges\n",
      "quantum simulation and quantum machine learning to classify phases of matter.\n",
      "Our classifier is directly fed labeled states recovered by the variational\n",
      "quantum eigensolver algorithm, thereby avoiding the data reading slowdown\n",
      "experienced in many applications of quantum enhanced machine learning. We\n",
      "propose families of variational ansatz states that are inspired directly by\n",
      "tensor networks. This allows us to use tools from tensor network theory to\n",
      "explain properties of the phase diagrams the presented method recovers.\n",
      "Finally, we propose a nearest-neighbour (checkerboard) quantum neural network.\n",
      "This majority vote quantum classifier is successfully trained to recognize\n",
      "phases of matter with $99\\%$ accuracy for the transverse field Ising model and\n",
      "$94\\%$ accuracy for the XXZ model. These findings suggest that our merger\n",
      "between quantum simulation and quantum enhanced machine learning offers a\n",
      "fertile ground to develop computational insights into quantum systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1807.04255 \n",
      "Title :On the Fundamental Limits of Coded Data Shuffling for Distributed\n",
      "  Machine Learning\n",
      "  We consider the data shuffling problem in a distributed learning system, in\n",
      "which a master node is connected to a set of worker nodes, via a shared link,\n",
      "in order to communicate a set of files to the worker nodes. The master node has\n",
      "access to a database of files. In every shuffling iteration, each worker node\n",
      "processes a new subset of files, and has excess storage to partially cache the\n",
      "remaining files, assuming the cached files are uncoded. The caches of the\n",
      "worker nodes are updated every iteration, and they should be designed to\n",
      "satisfy any possible unknown permutation of the files in subsequent iterations.\n",
      "For this problem, we characterize the exact load-memory trade-off for\n",
      "worst-case shuffling by deriving the minimum communication load for a given\n",
      "storage capacity per worker node. As a byproduct, the exact load-memory\n",
      "trade-off for any shuffling is characterized when the number of files is equal\n",
      "to the number of worker nodes. We propose a novel deterministic coded shuffling\n",
      "scheme, which improves the state of the art, by exploiting the cache memories\n",
      "to create coded functions that can be decoded by several worker nodes. Then, we\n",
      "prove the optimality of our proposed scheme by deriving a matching lower bound\n",
      "and showing that the placement phase of the proposed coded shuffling scheme is\n",
      "optimal over all shuffles.\n",
      "\n",
      "**Paper Id :1905.07210 \n",
      "Title :Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using\n",
      "  Non-IID Data\n",
      "  This paper proposes a cooperative mechanism for mitigating the performance\n",
      "degradation due to non-independent-and-identically-distributed (non-IID) data\n",
      "in collaborative machine learning (ML), namely federated learning (FL), which\n",
      "trains an ML model using the rich data and computational resources of mobile\n",
      "clients without gathering their data to central systems. The data of mobile\n",
      "clients is typically non-IID owing to diversity among mobile clients' interests\n",
      "and usage, and FL with non-IID data could degrade the model performance.\n",
      "Therefore, to mitigate the degradation induced by non-IID data, we assume that\n",
      "a limited number (e.g., less than 1%) of clients allow their data to be\n",
      "uploaded to a server, and we propose a hybrid learning mechanism referred to as\n",
      "Hybrid-FL, wherein the server updates the model using the data gathered from\n",
      "the clients and aggregates the model with the models trained by clients. The\n",
      "Hybrid-FL solves both client- and data-selection problems via heuristic\n",
      "algorithms, which try to select the optimal sets of clients who train models\n",
      "with their own data, clients who upload their data to the server, and data\n",
      "uploaded to the server. The algorithms increase the number of clients\n",
      "participating in FL and make more data gather in the server IID, thereby\n",
      "improving the prediction accuracy of the aggregated model. Evaluations, which\n",
      "consist of network simulations and ML experiments, demonstrate that the\n",
      "proposed scheme achieves a 13.5% higher classification accuracy than those of\n",
      "the previously proposed schemes for the non-IID case.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1807.10261 \n",
      "Title :Novelty Detection Meets Collider Physics\n",
      "  Novelty detection is the machine learning task to recognize data, which\n",
      "belong to an unknown pattern. Complementary to supervised learning, it allows\n",
      "to analyze data model-independently. We demonstrate the potential role of\n",
      "novelty detection in collider physics, using autoencoder-based deep neural\n",
      "network. Explicitly, we develop a set of density-based novelty evaluators,\n",
      "which are sensitive to the clustering of unknown-pattern testing data or\n",
      "new-physics signal events, for the design of detection algorithms. We also\n",
      "explore the influence of the known-pattern data fluctuations, arising from\n",
      "non-signal regions, on detection sensitivity. Strategies to address it are\n",
      "proposed. The algorithms are applied to detecting fermionic di-top partner and\n",
      "resonant di-top productions at LHC, and exotic Higgs decays of two specific\n",
      "modes at a $e^+e^-$ future collider. With parton-level analysis, we conclude\n",
      "that potentially the new-physics benchmarks can be recognized with high\n",
      "efficiency.\n",
      "\n",
      "**Paper Id :1807.11916 \n",
      "Title :End-to-End Physics Event Classification with CMS Open Data: Applying\n",
      "  Image-Based Deep Learning to Detector Data for the Direct Classification of\n",
      "  Collision Events at the LHC\n",
      "  This paper describes the construction of novel end-to-end image-based\n",
      "classifiers that directly leverage low-level simulated detector data to\n",
      "discriminate signal and background processes in pp collision events at the\n",
      "Large Hadron Collider at CERN. To better understand what end-to-end classifiers\n",
      "are capable of learning from the data and to address a number of associated\n",
      "challenges, we distinguish the decay of the standard model Higgs boson into two\n",
      "photons from its leading background sources using high-fidelity simulated CMS\n",
      "Open Data. We demonstrate the ability of end-to-end classifiers to learn from\n",
      "the angular distribution of the photons recorded as electromagnetic showers,\n",
      "their intrinsic shapes, and the energy of their constituent hits, even when the\n",
      "underlying particles are not fully resolved, delivering a clear advantage in\n",
      "such cases over purely kinematics-based classifiers.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1807.10300 \n",
      "Title :Discovering physical concepts with neural networks\n",
      "  Despite the success of neural networks at solving concrete physics problems,\n",
      "their use as a general-purpose tool for scientific discovery is still in its\n",
      "infancy. Here, we approach this problem by modelling a neural network\n",
      "architecture after the human physical reasoning process, which has similarities\n",
      "to representation learning. This allows us to make progress towards the\n",
      "long-term goal of machine-assisted scientific discovery from experimental data\n",
      "without making prior assumptions about the system. We apply this method to toy\n",
      "examples and show that the network finds the physically relevant parameters,\n",
      "exploits conservation laws to make predictions, and can help to gain conceptual\n",
      "insights, e.g. Copernicus' conclusion that the solar system is heliocentric.\n",
      "\n",
      "**Paper Id :2007.05500 \n",
      "Title :Scientific Discovery by Generating Counterfactuals using Image\n",
      "  Translation\n",
      "  Model explanation techniques play a critical role in understanding the source\n",
      "of a model's performance and making its decisions transparent. Here we\n",
      "investigate if explanation techniques can also be used as a mechanism for\n",
      "scientific discovery. We make three contributions: first, we propose a\n",
      "framework to convert predictions from explanation techniques to a mechanism of\n",
      "discovery. Second, we show how generative models in combination with black-box\n",
      "predictors can be used to generate hypotheses (without human priors) that can\n",
      "be critically examined. Third, with these techniques we study classification\n",
      "models for retinal images predicting Diabetic Macular Edema (DME), where recent\n",
      "work showed that a CNN trained on these images is likely learning novel\n",
      "features in the image. We demonstrate that the proposed framework is able to\n",
      "explain the underlying scientific mechanism, thus bridging the gap between the\n",
      "model's performance and human understanding.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1807.10997 \n",
      "Title :Optimal Tap Setting of Voltage Regulation Transformers Using Batch\n",
      "  Reinforcement Learning\n",
      "  In this paper, we address the problem of setting the tap positions of load\n",
      "tap changers (LTCs) for voltage regulation in radial power distribution systems\n",
      "under uncertain load dynamics. The objective is to find a policy to determine\n",
      "the tap positions that only uses measurements of voltage magnitudes and\n",
      "topology information so as to minimize the voltage deviation across the system.\n",
      "We formulate this problem as a Markov decision process (MDP), and propose a\n",
      "batch reinforcement learning (RL) algorithm to solve it. By taking advantage of\n",
      "a linearized power flow model, we propose an effective algorithm to estimate\n",
      "the voltage magnitudes under different tap settings, which allows the RL\n",
      "algorithm to explore the state and action spaces freely offline without\n",
      "impacting the system operation. To circumvent the \"curse of dimensionality\"\n",
      "resulted from the large state and action spaces, we propose a sequential\n",
      "learning algorithm to learn an action-value function for each LTC, based on\n",
      "which the optimal tap positions can be directly determined. The effectiveness\n",
      "of the proposed algorithm is validated via numerical simulations on the IEEE\n",
      "13-bus and 123-bus distribution test feeders.\n",
      "\n",
      "**Paper Id :2007.10284 \n",
      "Title :Learning High-Level Policies for Model Predictive Control\n",
      "  The combination of policy search and deep neural networks holds the promise\n",
      "of automating a variety of decision-making tasks. Model Predictive\n",
      "Control~(MPC) provides robust solutions to robot control tasks by making use of\n",
      "a dynamical model of the system and solving an optimization problem online over\n",
      "a short planning horizon. In this work, we leverage probabilistic\n",
      "decision-making approaches and the generalization capability of artificial\n",
      "neural networks to the powerful online optimization by learning a deep\n",
      "high-level policy for the MPC~(High-MPC). Conditioning on robot's local\n",
      "observations, the trained neural network policy is capable of adaptively\n",
      "selecting high-level decision variables for the low-level MPC controller, which\n",
      "then generates optimal control commands for the robot. First, we formulate the\n",
      "search of high-level decision variables for MPC as a policy search problem,\n",
      "specifically, a probabilistic inference problem. The problem can be solved in a\n",
      "closed-form solution. Second, we propose a self-supervised learning algorithm\n",
      "for learning a neural network high-level policy, which is useful for online\n",
      "hyperparameter adaptations in highly dynamic environments. We demonstrate the\n",
      "importance of incorporating the online adaption into autonomous robots by using\n",
      "the proposed method to solve a challenging control problem, where the task is\n",
      "to control a simulated quadrotor to fly through a swinging gate. We show that\n",
      "our approach can handle situations that are difficult for standard MPC.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1807.11916 \n",
      "Title :End-to-End Physics Event Classification with CMS Open Data: Applying\n",
      "  Image-Based Deep Learning to Detector Data for the Direct Classification of\n",
      "  Collision Events at the LHC\n",
      "  This paper describes the construction of novel end-to-end image-based\n",
      "classifiers that directly leverage low-level simulated detector data to\n",
      "discriminate signal and background processes in pp collision events at the\n",
      "Large Hadron Collider at CERN. To better understand what end-to-end classifiers\n",
      "are capable of learning from the data and to address a number of associated\n",
      "challenges, we distinguish the decay of the standard model Higgs boson into two\n",
      "photons from its leading background sources using high-fidelity simulated CMS\n",
      "Open Data. We demonstrate the ability of end-to-end classifiers to learn from\n",
      "the angular distribution of the photons recorded as electromagnetic showers,\n",
      "their intrinsic shapes, and the energy of their constituent hits, even when the\n",
      "underlying particles are not fully resolved, delivering a clear advantage in\n",
      "such cases over purely kinematics-based classifiers.\n",
      "\n",
      "**Paper Id :1902.08276 \n",
      "Title :End-to-End Jet Classification of Quarks and Gluons with the CMS Open\n",
      "  Data\n",
      "  We describe the construction of end-to-end jet image classifiers based on\n",
      "simulated low-level detector data to discriminate quark- vs. gluon-initiated\n",
      "jets with high-fidelity simulated CMS Open Data. We highlight the importance of\n",
      "precise spatial information and demonstrate competitive performance to existing\n",
      "state-of-the-art jet classifiers. We further generalize the end-to-end approach\n",
      "to event-level classification of quark vs. gluon di-jet QCD events. We compare\n",
      "the fully end-to-end approach to using hand-engineered features and demonstrate\n",
      "that the end-to-end algorithm is robust against the effects of underlying event\n",
      "and pile-up.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1808.00131 \n",
      "Title :A Theory of Dichotomous Valuation with Applications to Variable\n",
      "  Selection\n",
      "  An econometric or statistical model may undergo a marginal gain if we admit a\n",
      "new variable to the model, and a marginal loss if we remove an existing\n",
      "variable from the model. Assuming equality of opportunity among all candidate\n",
      "variables, we derive a valuation framework by the expected marginal gain and\n",
      "marginal loss in all potential modeling scenarios. However, marginal gain and\n",
      "loss are not symmetric; thus, we introduce three unbiased solutions. When used\n",
      "in variable selection, our new approaches significantly outperform several\n",
      "popular methods used in practice. The results also explore some novel traits of\n",
      "the Shapley value.\n",
      "\n",
      "**Paper Id :2011.01821 \n",
      "Title :Minimax Pareto Fairness: A Multi Objective Perspective\n",
      "  In this work we formulate and formally characterize group fairness as a\n",
      "multi-objective optimization problem, where each sensitive group risk is a\n",
      "separate objective. We propose a fairness criterion where a classifier achieves\n",
      "minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary\n",
      "harm, and can lead to the best zero-gap model if policy dictates so. We provide\n",
      "a simple optimization algorithm compatible with deep neural networks to satisfy\n",
      "these constraints. Since our method does not require test-time access to\n",
      "sensitive attributes, it can be applied to reduce worst-case classification\n",
      "errors between outcomes in unbalanced classification problems. We test the\n",
      "proposed methodology on real case-studies of predicting income, ICU patient\n",
      "mortality, skin lesions classification, and assessing credit risk,\n",
      "demonstrating how our framework compares favorably to other approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1808.00408 \n",
      "Title :Geometry of energy landscapes and the optimizability of deep neural\n",
      "  networks\n",
      "  Deep neural networks are workhorse models in machine learning with multiple\n",
      "layers of non-linear functions composed in series. Their loss function is\n",
      "highly non-convex, yet empirically even gradient descent minimisation is\n",
      "sufficient to arrive at accurate and predictive models. It is hitherto unknown\n",
      "why are deep neural networks easily optimizable. We analyze the energy\n",
      "landscape of a spin glass model of deep neural networks using random matrix\n",
      "theory and algebraic geometry. We analytically show that the multilayered\n",
      "structure holds the key to optimizability: Fixing the number of parameters and\n",
      "increasing network depth, the number of stationary points in the loss function\n",
      "decreases, minima become more clustered in parameter space, and the tradeoff\n",
      "between the depth and width of minima becomes less severe. Our analytical\n",
      "results are numerically verified through comparison with neural networks\n",
      "trained on a set of classical benchmark datasets. Our model uncovers generic\n",
      "design principles of machine learning models.\n",
      "\n",
      "**Paper Id :2003.03633 \n",
      "Title :AL2: Progressive Activation Loss for Learning General Representations in\n",
      "  Classification Neural Networks\n",
      "  The large capacity of neural networks enables them to learn complex\n",
      "functions. To avoid overfitting, networks however require a lot of training\n",
      "data that can be expensive and time-consuming to collect. A common practical\n",
      "approach to attenuate overfitting is the use of network regularization\n",
      "techniques. We propose a novel regularization method that progressively\n",
      "penalizes the magnitude of activations during training. The combined activation\n",
      "signals produced by all neurons in a given layer form the representation of the\n",
      "input image in that feature space. We propose to regularize this representation\n",
      "in the last feature layer before classification layers. Our method's effect on\n",
      "generalization is analyzed with label randomization tests and cumulative\n",
      "ablations. Experimental results show the advantages of our approach in\n",
      "comparison with commonly-used regularizers on standard benchmark datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1808.06079 \n",
      "Title :Community detection in networks without observing edges\n",
      "  We develop a Bayesian hierarchical model to identify communities in networks\n",
      "for which we do not observe the edges directly, but instead observe a series of\n",
      "interdependent signals for each of the nodes. Fitting the model provides an\n",
      "end-to-end community detection algorithm that does not extract information as a\n",
      "sequence of point estimates but propagates uncertainties from the raw data to\n",
      "the community labels. Our approach naturally supports multiscale community\n",
      "detection as well as the selection of an optimal scale using model comparison.\n",
      "We study the properties of the algorithm using synthetic data and apply it to\n",
      "daily returns of constituents of the S&P100 index as well as climate data from\n",
      "US cities.\n",
      "\n",
      "**Paper Id :1903.07677 \n",
      "Title :Deep Fundamental Factor Models\n",
      "  Deep fundamental factor models are developed to automatically capture\n",
      "non-linearity and interaction effects in factor modeling. Uncertainty\n",
      "quantification provides interpretability with interval estimation, ranking of\n",
      "factor importances and estimation of interaction effects. With no hidden layers\n",
      "we recover a linear factor model and for one or more hidden layers, uncertainty\n",
      "bands for the sensitivity to each input naturally arise from the network\n",
      "weights. Using 3290 assets in the Russell 1000 index over a period of December\n",
      "1989 to January 2018, we assess a 49 factor model and generate information\n",
      "ratios that are approximately 1.5x greater than the OLS factor model.\n",
      "Furthermore, we compare our deep fundamental factor model with a quadratic\n",
      "LASSO model and demonstrate the superior performance and robustness to\n",
      "outliers. The Python source code and the data used for this study are provided.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1808.06935 \n",
      "Title :Smart energy models for atomistic simulations using a DFT-driven\n",
      "  multifidelity approach\n",
      "  The reliability of atomistic simulations depends on the quality of the\n",
      "underlying energy models providing the source of physical information, for\n",
      "instance for the calculation of migration barriers in atomistic Kinetic Monte\n",
      "Carlo simulations. Accurate (high-fidelity) methods are often available, but\n",
      "since they are usually computationally expensive, they must be replaced by less\n",
      "accurate (low-fidelity) models that introduce some degrees of approximation.\n",
      "Machine-learning techniques such as artificial neural networks are usually\n",
      "employed to work around this limitation and extract the needed parameters from\n",
      "large databases of high-fidelity data, but the latter are often computationally\n",
      "expensive to produce. This work introduces an alternative method based on the\n",
      "multifidelity approach, where correlations between high-fidelity and\n",
      "low-fidelity outputs are exploited to make an educated guess of the\n",
      "high-fidelity outcome based only on quick low-fidelity estimations, hence\n",
      "without the need of running full expensive high-fidelity calculations. With\n",
      "respect to neural networks, this approach is expected to require less training\n",
      "data because of the lower amount of fitting parameters involved. The method is\n",
      "tested on the prediction of ab initio formation and migration energies of\n",
      "vacancy diffusion in iron-copper alloys, and compared with the neural networks\n",
      "trained on the same database.\n",
      "\n",
      "**Paper Id :2006.13222 \n",
      "Title :Certified variational quantum algorithms for eigenstate preparation\n",
      "  Solutions to many-body problem instances often involve an intractable number\n",
      "of degrees of freedom and admit no known approximations in general form. In\n",
      "practice, representing quantum-mechanical states of a given Hamiltonian using\n",
      "available numerical methods, in particular those based on variational Monte\n",
      "Carlo simulations, become exponentially more challenging with increasing system\n",
      "size. Recently quantum algorithms implemented as variational models have been\n",
      "proposed to accelerate such simulations. The variational ansatz states are\n",
      "characterized by a polynomial number of parameters devised in a way to minimize\n",
      "the expectation value of a given Hamiltonian, which is emulated by local\n",
      "measurements. In this study, we develop a means to certify the termination of\n",
      "variational algorithms. We demonstrate our approach by applying it to three\n",
      "models: the transverse field Ising model, the model of one-dimensional spinless\n",
      "fermions with competing interactions, and the Schwinger model of quantum\n",
      "electrodynamics. By means of comparison, we observe that our approach shows\n",
      "better performance near critical points in these models. We hence take a\n",
      "further step to improve the applicability and to certify the results of\n",
      "variational quantum simulators.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1808.07452 \n",
      "Title :Generalized Canonical Polyadic Tensor Decomposition\n",
      "  Tensor decomposition is a fundamental unsupervised machine learning method in\n",
      "data science, with applications including network analysis and sensor data\n",
      "processing. This work develops a generalized canonical polyadic (GCP) low-rank\n",
      "tensor decomposition that allows other loss functions besides squared error.\n",
      "For instance, we can use logistic loss or Kullback-Leibler divergence, enabling\n",
      "tensor decomposition for binary or count data. We present a variety\n",
      "statistically-motivated loss functions for various scenarios. We provide a\n",
      "generalized framework for computing gradients and handling missing data that\n",
      "enables the use of standard optimization methods for fitting the model. We\n",
      "demonstrate the flexibility of GCP on several real-world examples including\n",
      "interactions in a social network, neural activity in a mouse, and monthly\n",
      "rainfall measurements in India.\n",
      "\n",
      "**Paper Id :2001.05559 \n",
      "Title :Mode-Assisted Unsupervised Learning of Restricted Boltzmann Machines\n",
      "  Restricted Boltzmann machines (RBMs) are a powerful class of generative\n",
      "models, but their training requires computing a gradient that, unlike\n",
      "supervised backpropagation on typical loss functions, is notoriously difficult\n",
      "even to approximate. Here, we show that properly combining standard gradient\n",
      "updates with an off-gradient direction, constructed from samples of the RBM\n",
      "ground state (mode), improves their training dramatically over traditional\n",
      "gradient methods. This approach, which we call mode training, promotes faster\n",
      "training and stability, in addition to lower converged relative entropy (KL\n",
      "divergence). Along with the proofs of stability and convergence of this method,\n",
      "we also demonstrate its efficacy on synthetic datasets where we can compute KL\n",
      "divergences exactly, as well as on a larger machine learning standard, MNIST.\n",
      "The mode training we suggest is quite versatile, as it can be applied in\n",
      "conjunction with any given gradient method, and is easily extended to more\n",
      "general energy-based neural network structures such as deep, convolutional and\n",
      "unrestricted Boltzmann machines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1808.10101 \n",
      "Title :DP-ADMM: ADMM-based Distributed Learning with Differential Privacy\n",
      "  Alternating Direction Method of Multipliers (ADMM) is a widely used tool for\n",
      "machine learning in distributed settings, where a machine learning model is\n",
      "trained over distributed data sources through an interactive process of local\n",
      "computation and message passing. Such an iterative process could cause privacy\n",
      "concerns of data owners. The goal of this paper is to provide differential\n",
      "privacy for ADMM-based distributed machine learning. Prior approaches on\n",
      "differentially private ADMM exhibit low utility under high privacy guarantee\n",
      "and often assume the objective functions of the learning problems to be smooth\n",
      "and strongly convex. To address these concerns, we propose a novel\n",
      "differentially private ADMM-based distributed learning algorithm called\n",
      "DP-ADMM, which combines an approximate augmented Lagrangian function with\n",
      "time-varying Gaussian noise addition in the iterative process to achieve higher\n",
      "utility for general objective functions under the same differential privacy\n",
      "guarantee. We also apply the moments accountant method to bound the end-to-end\n",
      "privacy loss. The theoretical analysis shows that DP-ADMM can be applied to a\n",
      "wider class of distributed learning problems, is provably convergent, and\n",
      "offers an explicit utility-privacy tradeoff. To our knowledge, this is the\n",
      "first paper to provide explicit convergence and utility properties for\n",
      "differentially private ADMM-based distributed learning algorithms. The\n",
      "evaluation results demonstrate that our approach can achieve good convergence\n",
      "and model accuracy under high end-to-end differential privacy guarantee.\n",
      "\n",
      "**Paper Id :2005.02426 \n",
      "Title :Communication-Efficient Distributed Stochastic AUC Maximization with\n",
      "  Deep Neural Networks\n",
      "  In this paper, we study distributed algorithms for large-scale AUC\n",
      "maximization with a deep neural network as a predictive model. Although\n",
      "distributed learning techniques have been investigated extensively in deep\n",
      "learning, they are not directly applicable to stochastic AUC maximization with\n",
      "deep neural networks due to its striking differences from standard loss\n",
      "minimization problems (e.g., cross-entropy). Towards addressing this challenge,\n",
      "we propose and analyze a communication-efficient distributed optimization\n",
      "algorithm based on a {\\it non-convex concave} reformulation of the AUC\n",
      "maximization, in which the communication of both the primal variable and the\n",
      "dual variable between each worker and the parameter server only occurs after\n",
      "multiple steps of gradient-based updates in each worker. Compared with the\n",
      "naive parallel version of an existing algorithm that computes stochastic\n",
      "gradients at individual machines and averages them for updating the model\n",
      "parameters, our algorithm requires a much less number of communication rounds\n",
      "and still achieves a linear speedup in theory. To the best of our knowledge,\n",
      "this is the \\textbf{first} work that solves the {\\it non-convex concave\n",
      "min-max} problem for AUC maximization with deep neural networks in a\n",
      "communication-efficient distributed manner while still maintaining the linear\n",
      "speedup property in theory. Our experiments on several benchmark datasets show\n",
      "the effectiveness of our algorithm and also confirm our theory.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1809.05250 \n",
      "Title :Real-Time Nonparametric Anomaly Detection in High-Dimensional Settings\n",
      "  Timely detection of abrupt anomalies is crucial for real-time monitoring and\n",
      "security of modern systems producing high-dimensional data. With this goal, we\n",
      "propose effective and scalable algorithms. Proposed algorithms are\n",
      "nonparametric as both the nominal and anomalous multivariate data distributions\n",
      "are assumed unknown. We extract useful univariate summary statistics and\n",
      "perform anomaly detection in a single-dimensional space. We model anomalies as\n",
      "persistent outliers and propose to detect them via a cumulative sum-like\n",
      "algorithm. In case the observed data have a low intrinsic dimensionality, we\n",
      "learn a submanifold in which the nominal data are embedded and evaluate whether\n",
      "the sequentially acquired data persistently deviate from the nominal\n",
      "submanifold. Further, in the general case, we learn an acceptance region for\n",
      "nominal data via Geometric Entropy Minimization and evaluate whether the\n",
      "sequentially observed data persistently fall outside the acceptance region. We\n",
      "provide an asymptotic lower bound and an asymptotic approximation for the\n",
      "average false alarm period of the proposed algorithm. Moreover, we provide a\n",
      "sufficient condition to asymptotically guarantee that the decision statistic of\n",
      "the proposed algorithm does not diverge in the absence of anomalies.\n",
      "Experiments illustrate the effectiveness of the proposed schemes in quick and\n",
      "accurate anomaly detection in high-dimensional settings.\n",
      "\n",
      "**Paper Id :1811.05076 \n",
      "Title :Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n",
      "  and its Statistical Optimality\n",
      "  We consider the problem of decomposing a higher-order tensor with binary\n",
      "entries. Such data problems arise frequently in applications such as\n",
      "neuroimaging, recommendation system, topic modeling, and sensor network\n",
      "localization. We propose a multilinear Bernoulli model, develop a\n",
      "rank-constrained likelihood-based estimation method, and obtain the theoretical\n",
      "accuracy guarantees. In contrast to continuous-valued problems, the binary\n",
      "tensor problem exhibits an interesting phase transition phenomenon according to\n",
      "the signal-to-noise ratio. The error bound for the parameter tensor estimation\n",
      "is established, and we show that the obtained rate is minimax optimal under the\n",
      "considered model. Furthermore, we develop an alternating optimization algorithm\n",
      "with convergence guarantees. The efficacy of our approach is demonstrated\n",
      "through both simulations and analyses of multiple data sets on the tasks of\n",
      "tensor completion and clustering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1809.06640 \n",
      "Title :Neural Network Decoders for Large-Distance 2D Toric Codes\n",
      "  We still do not have perfect decoders for topological codes that can satisfy\n",
      "all needs of different experimental setups. Recently, a few neural network\n",
      "based decoders have been studied, with the motivation that they can adapt to a\n",
      "wide range of noise models, and can easily run on dedicated chips without a\n",
      "full-fledged computer. The later feature might lead to fast speed and the\n",
      "ability to operate at low temperatures. However, a question which has not been\n",
      "addressed in previous works is whether neural network decoders can handle 2D\n",
      "topological codes with large distances. In this work, we provide a positive\n",
      "answer for the toric code. The structure of our neural network decoder is\n",
      "inspired by the renormalization group decoder. With a fairly strict policy on\n",
      "training time, when the bit-flip error rate is lower than $9\\%$ and syndrome\n",
      "extraction is perfect, the neural network decoder performs better when code\n",
      "distance increases. With a less strict policy, we find it is not hard for the\n",
      "neural decoder to achieve a performance close to the minimum-weight perfect\n",
      "matching algorithm. The numerical simulation is done up to code distance\n",
      "$d=64$. Last but not least, we describe and analyze a few failed approaches.\n",
      "They guide us to the final design of our neural decoder, but also serve as a\n",
      "caution when we gauge the versatility of stock deep neural networks. The source\n",
      "code of our neural decoder can be found at\n",
      "https://github.com/XiaotongNi/toric-code-neural-decoder .\n",
      "\n",
      "**Paper Id :1912.09132 \n",
      "Title :Mean field theory for deep dropout networks: digging up gradient\n",
      "  backpropagation deeply\n",
      "  In recent years, the mean field theory has been applied to the study of\n",
      "neural networks and has achieved a great deal of success. The theory has been\n",
      "applied to various neural network structures, including CNNs, RNNs, Residual\n",
      "networks, and Batch normalization. Inevitably, recent work has also covered the\n",
      "use of dropout. The mean field theory shows that the existence of depth scales\n",
      "that limit the maximum depth of signal propagation and gradient\n",
      "backpropagation. However, the gradient backpropagation is derived under the\n",
      "gradient independence assumption that weights used during feed forward are\n",
      "drawn independently from the ones used in backpropagation. This is not how\n",
      "neural networks are trained in a real setting. Instead, the same weights used\n",
      "in a feed-forward step needs to be carried over to its corresponding\n",
      "backpropagation. Using this realistic condition, we perform theoretical\n",
      "computation on linear dropout networks and a series of experiments on dropout\n",
      "networks. Our empirical results show an interesting phenomenon that the length\n",
      "gradients can backpropagate for a single input and a pair of inputs are\n",
      "governed by the same depth scale. Besides, we study the relationship between\n",
      "variance and mean of statistical metrics of the gradient and shown an emergence\n",
      "of universality. Finally, we investigate the maximum trainable length for deep\n",
      "dropout networks through a series of experiments using MNIST and CIFAR10 and\n",
      "provide a more precise empirical formula that describes the trainable length\n",
      "than original work.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1809.07180 \n",
      "Title :Projective Splitting with Forward Steps only Requires Continuity\n",
      "  A recent innovation in projective splitting algorithms for monotone operator\n",
      "inclusions has been the development of a procedure using two forward steps\n",
      "instead of the customary proximal steps for operators that are Lipschitz\n",
      "continuous. This paper shows that the Lipschitz assumption is unnecessary when\n",
      "the forward steps are performed in finite-dimensional spaces: a backtracking\n",
      "linesearch yields a convergent algorithm for operators that are merely\n",
      "continuous with full domain.\n",
      "\n",
      "**Paper Id :1910.09529 \n",
      "Title :Adaptive Gradient Descent without Descent\n",
      "  We present a strikingly simple proof that two rules are sufficient to\n",
      "automate gradient descent: 1) don't increase the stepsize too fast and 2) don't\n",
      "overstep the local curvature. No need for functional values, no line search, no\n",
      "information about the function except for the gradients. By following these\n",
      "rules, you get a method adaptive to the local geometry, with convergence\n",
      "guarantees depending only on the smoothness in a neighborhood of a solution.\n",
      "Given that the problem is convex, our method converges even if the global\n",
      "smoothness constant is infinity. As an illustration, it can minimize arbitrary\n",
      "continuously twice-differentiable convex function. We examine its performance\n",
      "on a range of convex and nonconvex problems, including logistic regression and\n",
      "matrix factorization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1809.08516 \n",
      "Title :Adversarial Defense via Data Dependent Activation Function and Total\n",
      "  Variation Minimization\n",
      "  We improve the robustness of Deep Neural Net (DNN) to adversarial attacks by\n",
      "using an interpolating function as the output activation. This data-dependent\n",
      "activation remarkably improves both the generalization and robustness of DNN.\n",
      "In the CIFAR10 benchmark, we raise the robust accuracy of the adversarially\n",
      "trained ResNet20 from $\\sim 46\\%$ to $\\sim 69\\%$ under the state-of-the-art\n",
      "Iterative Fast Gradient Sign Method (IFGSM) based adversarial attack. When we\n",
      "combine this data-dependent activation with total variation minimization on\n",
      "adversarial images and training data augmentation, we achieve an improvement in\n",
      "robust accuracy by 38.9$\\%$ for ResNet56 under the strongest IFGSM attack.\n",
      "Furthermore, We provide an intuitive explanation of our defense by analyzing\n",
      "the geometry of the feature space.\n",
      "\n",
      "**Paper Id :2005.02552 \n",
      "Title :Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder\n",
      "  Whereas adversarial training is employed as the main defence strategy against\n",
      "specific adversarial samples, it has limited generalization capability and\n",
      "incurs excessive time complexity. In this paper, we propose an attack-agnostic\n",
      "defence framework to enhance the intrinsic robustness of neural networks,\n",
      "without jeopardizing the ability of generalizing clean samples. Our Feature\n",
      "Pyramid Decoder (FPD) framework applies to all block-based convolutional neural\n",
      "networks (CNNs). It implants denoising and image restoration modules into a\n",
      "targeted CNN, and it also constraints the Lipschitz constant of the\n",
      "classification layer. Moreover, we propose a two-phase strategy to train the\n",
      "FPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\n",
      "multi-task and self-supervised learning. Evaluated against a variety of\n",
      "white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\n",
      "sufficient robustness against general adversarial samples on MNIST, SVHN and\n",
      "CALTECH. In addition, if we further conduct adversarial training, the\n",
      "FPD-enhanced CNNs perform better than their non-enhanced versions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1809.10611 \n",
      "Title :A Successive-Elimination Approach to Adaptive Robotic Sensing\n",
      "  We study an adaptive source seeking problem, in which a mobile robot must\n",
      "identify the strongest emitter(s) of a signal in an environment with background\n",
      "emissions. Background signals may be highly heterogeneous and can mislead\n",
      "algorithms that are based on receding horizon control. We propose AdaSearch, a\n",
      "general algorithm for adaptive source seeking in the face of heterogeneous\n",
      "background noise. AdaSearch combines global trajectory planning with principled\n",
      "confidence intervals in order to concentrate measurements in promising regions\n",
      "while guaranteeing sufficient coverage of the entire area. Theoretical analysis\n",
      "shows that AdaSearch confers gains over a uniform sampling strategy when the\n",
      "distribution of background signals is highly variable. Simulation experiments\n",
      "demonstrate that when applied to the problem of radioactive source seeking,\n",
      "AdaSearch outperforms both uniform sampling and a receding time horizon\n",
      "information-maximization approach based on the current literature. We also\n",
      "demonstrate AdaSearch in hardware, providing further evidence of its potential\n",
      "for real-time implementation.\n",
      "\n",
      "**Paper Id :2004.05273 \n",
      "Title :Safe Multi-Agent Interaction through Robust Control Barrier Functions\n",
      "  with Learned Uncertainties\n",
      "  Robots operating in real world settings must navigate and maintain safety\n",
      "while interacting with many heterogeneous agents and obstacles. Multi-Agent\n",
      "Control Barrier Functions (CBF) have emerged as a computationally efficient\n",
      "tool to guarantee safety in multi-agent environments, but they assume perfect\n",
      "knowledge of both the robot dynamics and other agents' dynamics. While\n",
      "knowledge of the robot's dynamics might be reasonably well known, the\n",
      "heterogeneity of agents in real-world environments means there will always be\n",
      "considerable uncertainty in our prediction of other agents' dynamics. This work\n",
      "aims to learn high-confidence bounds for these dynamic uncertainties using\n",
      "Matrix-Variate Gaussian Process models, and incorporates them into a robust\n",
      "multi-agent CBF framework. We transform the resulting min-max robust CBF into a\n",
      "quadratic program, which can be efficiently solved in real time. We verify via\n",
      "simulation results that the nominal multi-agent CBF is often violated during\n",
      "agent interactions, whereas our robust formulation maintains safety with a much\n",
      "higher probability and adapts to learned uncertainties\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.00386 \n",
      "Title :Harmonic Alignment\n",
      "  We propose a novel framework for combining datasets via alignment of their\n",
      "intrinsic geometry. This alignment can be used to fuse data originating from\n",
      "disparate modalities, or to correct batch effects while preserving intrinsic\n",
      "data structure. Importantly, we do not assume any pointwise correspondence\n",
      "between datasets, but instead rely on correspondence between a (possibly\n",
      "unknown) subset of data features. We leverage this assumption to construct an\n",
      "isometric alignment between the data. This alignment is obtained by relating\n",
      "the expansion of data features in harmonics derived from diffusion operators\n",
      "defined over each dataset. These expansions encode each feature as a function\n",
      "of the data geometry. We use this to relate the diffusion coordinates of each\n",
      "dataset through our assumption of partial feature correspondence. Then, a\n",
      "unified diffusion geometry is constructed over the aligned data, which can also\n",
      "be used to correct the original data measurements. We demonstrate our method on\n",
      "several datasets, showing in particular its effectiveness in biological\n",
      "applications including fusion of single-cell RNA sequencing (scRNA-seq) and\n",
      "single-cell ATAC sequencing (scATAC-seq) data measured on the same population\n",
      "of cells, and removal of batch effect between biological samples.\n",
      "\n",
      "**Paper Id :2006.14293 \n",
      "Title :Neural Decomposition: Functional ANOVA with Variational Autoencoders\n",
      "  Variational Autoencoders (VAEs) have become a popular approach for\n",
      "dimensionality reduction. However, despite their ability to identify latent\n",
      "low-dimensional structures embedded within high-dimensional data, these latent\n",
      "representations are typically hard to interpret on their own. Due to the\n",
      "black-box nature of VAEs, their utility for healthcare and genomics\n",
      "applications has been limited. In this paper, we focus on characterising the\n",
      "sources of variation in Conditional VAEs. Our goal is to provide a\n",
      "feature-level variance decomposition, i.e. to decompose variation in the data\n",
      "by separating out the marginal additive effects of latent variables z and fixed\n",
      "inputs c from their non-linear interactions. We propose to achieve this through\n",
      "what we call Neural Decomposition - an adaptation of the well-known concept of\n",
      "functional ANOVA variance decomposition from classical statistics to deep\n",
      "learning models. We show how identifiability can be achieved by training models\n",
      "subject to constraints on the marginal properties of the decoder networks. We\n",
      "demonstrate the utility of our Neural Decomposition on a series of synthetic\n",
      "examples as well as high-dimensional genomics data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.05546 \n",
      "Title :Uncertainty in Neural Networks: Approximately Bayesian Ensembling\n",
      "  Understanding the uncertainty of a neural network's (NN) predictions is\n",
      "essential for many purposes. The Bayesian framework provides a principled\n",
      "approach to this, however applying it to NNs is challenging due to large\n",
      "numbers of parameters and data. Ensembling NNs provides an easily\n",
      "implementable, scalable method for uncertainty quantification, however, it has\n",
      "been criticised for not being Bayesian. This work proposes one modification to\n",
      "the usual process that we argue does result in approximate Bayesian inference;\n",
      "regularising parameters about values drawn from a distribution which can be set\n",
      "equal to the prior. A theoretical analysis of the procedure in a simplified\n",
      "setting suggests the recovered posterior is centred correctly but tends to have\n",
      "an underestimated marginal variance, and overestimated correlation. However,\n",
      "two conditions can lead to exact recovery. We argue that these conditions are\n",
      "partially present in NNs. Empirical evaluations demonstrate it has an advantage\n",
      "over standard ensembling, and is competitive with variational methods.\n",
      "\n",
      "**Paper Id :1905.04305 \n",
      "Title :Spectral Reconstruction with Deep Neural Networks\n",
      "  We explore artificial neural networks as a tool for the reconstruction of\n",
      "spectral functions from imaginary time Green's functions, a classic\n",
      "ill-conditioned inverse problem. Our ansatz is based on a supervised learning\n",
      "framework in which prior knowledge is encoded in the training data and the\n",
      "inverse transformation manifold is explicitly parametrised through a neural\n",
      "network. We systematically investigate this novel reconstruction approach,\n",
      "providing a detailed analysis of its performance on physically motivated mock\n",
      "data, and compare it to established methods of Bayesian inference. The\n",
      "reconstruction accuracy is found to be at least comparable, and potentially\n",
      "superior in particular at larger noise levels. We argue that the use of\n",
      "labelled training data in a supervised setting and the freedom in defining an\n",
      "optimisation objective are inherent advantages of the present approach and may\n",
      "lead to significant improvements over state-of-the-art methods in the future.\n",
      "Potential directions for further research are discussed in detail.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.05547 \n",
      "Title :Physics-Driven Regularization of Deep Neural Networks for Enhanced\n",
      "  Engineering Design and Analysis\n",
      "  In this paper, we introduce a physics-driven regularization method for\n",
      "training of deep neural networks (DNNs) for use in engineering design and\n",
      "analysis problems. In particular, we focus on prediction of a physical system,\n",
      "for which in addition to training data, partial or complete information on a\n",
      "set of governing laws is also available. These laws often appear in the form of\n",
      "differential equations, derived from first principles, empirically-validated\n",
      "laws, or domain expertise, and are usually neglected in data-driven prediction\n",
      "of engineering systems. We propose a training approach that utilizes the known\n",
      "governing laws and regularizes data-driven DNN models by penalizing divergence\n",
      "from those laws. The first two numerical examples are synthetic examples, where\n",
      "we show that in constructing a DNN model that best fits the measurements from a\n",
      "physical system, the use of our proposed regularization results in DNNs that\n",
      "are more interpretable with smaller generalization errors, compared to other\n",
      "common regularization methods. The last two examples concern metamodeling for a\n",
      "random Burgers' system and for aerodynamic analysis of passenger vehicles,\n",
      "where we demonstrate that the proposed regularization provides superior\n",
      "generalization accuracy compared to other common alternatives.\n",
      "\n",
      "**Paper Id :2011.02280 \n",
      "Title :Physics-Informed Echo State Networks\n",
      "  We propose a physics-informed Echo State Network (ESN) to predict the\n",
      "evolution of chaotic systems. Compared to conventional ESNs, the\n",
      "physics-informed ESNs are trained to solve supervised learning tasks while\n",
      "ensuring that their predictions do not violate physical laws. This is achieved\n",
      "by introducing an additional loss function during the training, which is based\n",
      "on the system's governing equations. The additional loss function penalizes\n",
      "non-physical predictions without the need of any additional training data. This\n",
      "approach is demonstrated on a chaotic Lorenz system and a truncation of the\n",
      "Charney-DeVore system. Compared to the conventional ESNs, the physics-informed\n",
      "ESNs improve the predictability horizon by about two Lyapunov times. This\n",
      "approach is also shown to be robust with regard to noise. The proposed\n",
      "framework shows the potential of using machine learning combined with prior\n",
      "physical knowledge to improve the time-accurate prediction of chaotic dynamical\n",
      "systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.05934 \n",
      "Title :A System for Massively Parallel Hyperparameter Tuning\n",
      "  Modern learning models are characterized by large hyperparameter spaces and\n",
      "long training times. These properties, coupled with the rise of parallel\n",
      "computing and the growing demand to productionize machine learning workloads,\n",
      "motivate the need to develop mature hyperparameter optimization functionality\n",
      "in distributed computing settings. We address this challenge by first\n",
      "introducing a simple and robust hyperparameter optimization algorithm called\n",
      "ASHA, which exploits parallelism and aggressive early-stopping to tackle\n",
      "large-scale hyperparameter optimization problems. Our extensive empirical\n",
      "results show that ASHA outperforms existing state-of-the-art hyperparameter\n",
      "optimization methods; scales linearly with the number of workers in distributed\n",
      "settings; and is suitable for massive parallelism, as demonstrated on a task\n",
      "with 500 workers. We then describe several design decisions we encountered,\n",
      "along with our associated solutions, when integrating ASHA in Determined AI's\n",
      "end-to-end production-quality machine learning system that offers\n",
      "hyperparameter tuning as a service.\n",
      "\n",
      "**Paper Id :1909.02119 \n",
      "Title :Inductive-bias-driven Reinforcement Learning For Efficient Schedules in\n",
      "  Heterogeneous Clusters\n",
      "  The problem of scheduling of workloads onto heterogeneous processors (e.g.,\n",
      "CPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\n",
      "system schedulers rely on application/system-specific heuristics that have to\n",
      "be built on a case-by-case basis. Recent work has demonstrated ML techniques\n",
      "for automating the heuristic search by using black-box approaches which require\n",
      "significant training data and time, which make them challenging to use in\n",
      "practice. This paper presents Symphony, a scheduling framework that addresses\n",
      "the challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n",
      "(RL) model for scheduling, which inherently models the resource dependencies\n",
      "identified from the system architecture; and (ii) a sampling-based technique to\n",
      "compute the gradients of a Bayesian model without performing full probabilistic\n",
      "inference. Together, these techniques reduce both the amount of training data\n",
      "and the time required to produce scheduling policies that significantly\n",
      "outperform black-box approaches by up to 2.2x.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.08179 \n",
      "Title :Thermodynamics and Feature Extraction by Machine Learning\n",
      "  Machine learning methods are powerful in distinguishing different phases of\n",
      "matter in an automated way and provide a new perspective on the study of\n",
      "physical phenomena. We train a Restricted Boltzmann Machine (RBM) on data\n",
      "constructed with spin configurations sampled from the Ising Hamiltonian at\n",
      "different values of temperature and external magnetic field using Monte Carlo\n",
      "methods. From the trained machine we obtain the flow of iterative\n",
      "reconstruction of spin state configurations to faithfully reproduce the\n",
      "observables of the physical system. We find that the flow of the trained RBM\n",
      "approaches the spin configurations of the maximal possible specific heat which\n",
      "resemble the near criticality region of the Ising model. In the special case of\n",
      "the vanishing magnetic field the trained RBM converges to the critical point of\n",
      "the Renormalization Group (RG) flow of the lattice model. Our results suggest\n",
      "an alternative explanation of how the machine identifies the physical phase\n",
      "transitions, by recognizing certain properties of the configuration like the\n",
      "maximization of the specific heat, instead of associating directly the\n",
      "recognition procedure with the RG flow and its fixed points. Then from the\n",
      "reconstructed data we deduce the critical exponent associated to the\n",
      "magnetization to find satisfactory agreement with the actual physical value. We\n",
      "assume no prior knowledge about the criticality of the system and its\n",
      "Hamiltonian.\n",
      "\n",
      "**Paper Id :2006.09113 \n",
      "Title :Topological defects and confinement with machine learning: the case of\n",
      "  monopoles in compact electrodynamics\n",
      "  We investigate the advantages of machine learning techniques to recognize the\n",
      "dynamics of topological objects in quantum field theories. We consider the\n",
      "compact U(1) gauge theory in three spacetime dimensions as the simplest example\n",
      "of a theory that exhibits confinement and mass gap phenomena generated by\n",
      "monopoles. We train a neural network with a generated set of monopole\n",
      "configurations to distinguish between confinement and deconfinement phases,\n",
      "from which it is possible to determine the deconfinement transition point and\n",
      "to predict several observables. The model uses a supervised learning approach\n",
      "and treats the monopole configurations as three-dimensional images (holograms).\n",
      "We show that the model can determine the transition temperature with accuracy,\n",
      "which depends on the criteria implemented in the algorithm. More importantly,\n",
      "we train the neural network with configurations from a single lattice size\n",
      "before making predictions for configurations from other lattice sizes, from\n",
      "which a reliable estimation of the critical temperatures are obtained.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.09425 \n",
      "Title :Using Deep Learning to Extend the Range of Air-Pollution Monitoring and\n",
      "  Forecasting\n",
      "  Across numerous applications, forecasting relies on numerical solvers for\n",
      "partial differential equations (PDEs). Although the use of deep-learning\n",
      "techniques has been proposed, actual applications have been restricted by the\n",
      "fact the training data are obtained using traditional PDE solvers. Thereby, the\n",
      "uses of deep-learning techniques were limited to domains, where the PDE solver\n",
      "was applicable.\n",
      "  We demonstrate a deep-learning framework for air-pollution monitoring and\n",
      "forecasting that provides the ability to train across different model domains,\n",
      "as well as a reduction in the run-time by two orders of magnitude. It presents\n",
      "a first-of-a-kind implementation that combines deep-learning and\n",
      "domain-decomposition techniques to allow model deployments extend beyond the\n",
      "domain(s) on which the it has been trained.\n",
      "\n",
      "**Paper Id :1909.02119 \n",
      "Title :Inductive-bias-driven Reinforcement Learning For Efficient Schedules in\n",
      "  Heterogeneous Clusters\n",
      "  The problem of scheduling of workloads onto heterogeneous processors (e.g.,\n",
      "CPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\n",
      "system schedulers rely on application/system-specific heuristics that have to\n",
      "be built on a case-by-case basis. Recent work has demonstrated ML techniques\n",
      "for automating the heuristic search by using black-box approaches which require\n",
      "significant training data and time, which make them challenging to use in\n",
      "practice. This paper presents Symphony, a scheduling framework that addresses\n",
      "the challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n",
      "(RL) model for scheduling, which inherently models the resource dependencies\n",
      "identified from the system architecture; and (ii) a sampling-based technique to\n",
      "compute the gradients of a Bayesian model without performing full probabilistic\n",
      "inference. Together, these techniques reduce both the amount of training data\n",
      "and the time required to produce scheduling policies that significantly\n",
      "outperform black-box approaches by up to 2.2x.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.10342 \n",
      "Title :Predicting optical coherence tomography-derived diabetic macular edema\n",
      "  grades from fundus photographs using deep learning\n",
      "  Diabetic eye disease is one of the fastest growing causes of preventable\n",
      "blindness. With the advent of anti-VEGF (vascular endothelial growth factor)\n",
      "therapies, it has become increasingly important to detect center-involved\n",
      "diabetic macular edema (ci-DME). However, center-involved diabetic macular\n",
      "edema is diagnosed using optical coherence tomography (OCT), which is not\n",
      "generally available at screening sites because of cost and workflow\n",
      "constraints. Instead, screening programs rely on the detection of hard exudates\n",
      "in color fundus photographs as a proxy for DME, often resulting in high false\n",
      "positive or false negative calls. To improve the accuracy of DME screening, we\n",
      "trained a deep learning model to use color fundus photographs to predict\n",
      "ci-DME. Our model had an ROC-AUC of 0.89 (95% CI: 0.87-0.91), which corresponds\n",
      "to a sensitivity of 85% at a specificity of 80%. In comparison, three retinal\n",
      "specialists had similar sensitivities (82-85%), but only half the specificity\n",
      "(45-50%, p<0.001 for each comparison with model). The positive predictive value\n",
      "(PPV) of the model was 61% (95% CI: 56-66%), approximately double the 36-38% by\n",
      "the retinal specialists. In addition to predicting ci-DME, our model was able\n",
      "to detect the presence of intraretinal fluid with an AUC of 0.81 (95% CI:\n",
      "0.81-0.86) and subretinal fluid with an AUC of 0.88 (95% CI: 0.85-0.91). The\n",
      "ability of deep learning algorithms to make clinically relevant predictions\n",
      "that generally require sophisticated 3D-imaging equipment from simple 2D images\n",
      "has broad relevance to many other applications in medical imaging.\n",
      "\n",
      "**Paper Id :2003.13145 \n",
      "Title :Can AI help in screening Viral and COVID-19 pneumonia?\n",
      "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\n",
      "caused thousands of causalities and infected several millions of people\n",
      "worldwide. Any technological tool enabling rapid screening of the COVID-19\n",
      "infection with high accuracy can be crucially helpful to healthcare\n",
      "professionals. The main clinical tool currently in use for the diagnosis of\n",
      "COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\n",
      "is expensive, less-sensitive and requires specialized medical personnel. X-ray\n",
      "imaging is an easily accessible tool that can be an excellent alternative in\n",
      "the COVID-19 diagnosis. This research was taken to investigate the utility of\n",
      "artificial intelligence (AI) in the rapid and accurate detection of COVID-19\n",
      "from chest X-ray images. The aim of this paper is to propose a robust technique\n",
      "for automatic detection of COVID-19 pneumonia from digital chest X-ray images\n",
      "applying pre-trained deep-learning algorithms while maximizing the detection\n",
      "accuracy. A public database was created by the authors combining several public\n",
      "databases and also by collecting images from recently published articles. The\n",
      "database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\n",
      "normal chest X-ray images. Transfer learning technique was used with the help\n",
      "of image augmentation to train and validate several pre-trained deep\n",
      "Convolutional Neural Networks (CNNs). The networks were trained to classify two\n",
      "different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\n",
      "COVID-19 pneumonia with and without image augmentation. The classification\n",
      "accuracy, precision, sensitivity, and specificity for both the schemes were\n",
      "99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\n",
      "respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1810.11922 \n",
      "Title :The Expressive Power of Parameterized Quantum Circuits\n",
      "  Parameterized quantum circuits (PQCs) have been broadly used as a hybrid\n",
      "quantum-classical machine learning scheme to accomplish generative tasks.\n",
      "However, whether PQCs have better expressive power than classical generative\n",
      "neural networks, such as restricted or deep Boltzmann machines, remains an open\n",
      "issue. In this paper, we prove that PQCs with a simple structure already\n",
      "outperform any classical neural network for generative tasks, unless the\n",
      "polynomial hierarchy collapses. Our proof builds on known results from tensor\n",
      "networks and quantum circuits (in particular, instantaneous quantum polynomial\n",
      "circuits). In addition, PQCs equipped with ancillary qubits for post-selection\n",
      "have even stronger expressive power than those without post-selection. We\n",
      "employ them as an application for Bayesian learning, since it is possible to\n",
      "learn prior probabilities rather than assuming they are known. We expect that\n",
      "it will find many more applications in semi-supervised learning where prior\n",
      "distributions are normally assumed to be unknown. Lastly, we conduct several\n",
      "numerical experiments using the Rigetti Forest platform to demonstrate the\n",
      "performance of the proposed Bayesian quantum circuit.\n",
      "\n",
      "**Paper Id :1904.06194 \n",
      "Title :Compressing deep neural networks by matrix product operators\n",
      "  A deep neural network is a parametrization of a multilayer mapping of signals\n",
      "in terms of many alternatively arranged linear and nonlinear transformations.\n",
      "The linear transformations, which are generally used in the fully connected as\n",
      "well as convolutional layers, contain most of the variational parameters that\n",
      "are trained and stored. Compressing a deep neural network to reduce its number\n",
      "of variational parameters but not its prediction power is an important but\n",
      "challenging problem toward the establishment of an optimized scheme in training\n",
      "efficiently these parameters and in lowering the risk of overfitting. Here we\n",
      "show that this problem can be effectively solved by representing linear\n",
      "transformations with matrix product operators (MPOs), which is a tensor network\n",
      "originally proposed in physics to characterize the short-range entanglement in\n",
      "one-dimensional quantum states. We have tested this approach in five typical\n",
      "neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\n",
      "widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\n",
      "representation indeed sets up a faithful and efficient mapping between input\n",
      "and output signals, which can keep or even improve the prediction accuracy with\n",
      "a dramatically reduced number of parameters. Our method greatly simplifies the\n",
      "representations in deep learning, and opens a possible route toward\n",
      "establishing a framework of modern neural networks which might be simpler and\n",
      "cheaper, but more efficient.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.00147 \n",
      "Title :DOLORES: Deep Contextualized Knowledge Graph Embeddings\n",
      "  We introduce a new method DOLORES for learning knowledge graph embeddings\n",
      "that effectively captures contextual cues and dependencies among entities and\n",
      "relations. First, we note that short paths on knowledge graphs comprising of\n",
      "chains of entities and relations can encode valuable information regarding\n",
      "their contextual usage. We operationalize this notion by representing knowledge\n",
      "graphs not as a collection of triples but as a collection of entity-relation\n",
      "chains, and learn embeddings for entities and relations using deep neural\n",
      "models that capture such contextual usage. In particular, our model is based on\n",
      "Bi-Directional LSTMs and learn deep representations of entities and relations\n",
      "from constructed entity-relation chains. We show that these representations can\n",
      "very easily be incorporated into existing models to significantly advance the\n",
      "state of the art on several knowledge graph prediction tasks like link\n",
      "prediction, triple classification, and missing relation type prediction (in\n",
      "some cases by at least 9.5%).\n",
      "\n",
      "**Paper Id :1905.10702 \n",
      "Title :MDE: Multiple Distance Embeddings for Link Prediction in Knowledge\n",
      "  Graphs\n",
      "  Over the past decade, knowledge graphs became popular for capturing\n",
      "structured domain knowledge. Relational learning models enable the prediction\n",
      "of missing links inside knowledge graphs. More specifically, latent distance\n",
      "approaches model the relationships among entities via a distance between latent\n",
      "representations. Translating embedding models (e.g., TransE) are among the most\n",
      "popular latent distance approaches which use one distance function to learn\n",
      "multiple relation patterns. However, they are mostly inefficient in capturing\n",
      "symmetric relations since the representation vector norm for all the symmetric\n",
      "relations becomes equal to zero. They also lose information when learning\n",
      "relations with reflexive patterns since they become symmetric and transitive.\n",
      "We propose the Multiple Distance Embedding model (MDE) that addresses these\n",
      "limitations and a framework to collaboratively combine variant latent\n",
      "distance-based terms. Our solution is based on two principles: 1) we use a\n",
      "limit-based loss instead of a margin ranking loss and, 2) by learning\n",
      "independent embedding vectors for each of the terms we can collectively train\n",
      "and predict using contradicting distance terms. We further demonstrate that MDE\n",
      "allows modeling relations with (anti)symmetry, inversion, and composition\n",
      "patterns. We propose MDE as a neural network model that allows us to map\n",
      "non-linear relations between the embedding vectors and the expected output of\n",
      "the score function. Our empirical results show that MDE performs competitively\n",
      "to state-of-the-art embedding models on several benchmark datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.00821 \n",
      "Title :OrthoNet: Multilayer Network Data Clustering\n",
      "  Network data appears in very diverse applications, like biological, social,\n",
      "or sensor networks. Clustering of network nodes into categories or communities\n",
      "has thus become a very common task in machine learning and data mining. Network\n",
      "data comes with some information about the network edges. In some cases, this\n",
      "network information can even be given with multiple views or multiple layers,\n",
      "each one representing a different type of relationship between the network\n",
      "nodes. Increasingly often, network nodes also carry a feature vector. We\n",
      "propose in this paper to extend the node clustering problem, that commonly\n",
      "considers only the network information, to a problem where both the network\n",
      "information and the node features are considered together for learning a\n",
      "clustering-friendly representation of the feature space. Specifically, we\n",
      "design a generic two-step algorithm for multilayer network data clustering. The\n",
      "first step aggregates the different layers of network information into a graph\n",
      "representation given by the geometric mean of the network Laplacian matrices.\n",
      "The second step uses a neural net to learn a feature embedding that is\n",
      "consistent with the structure given by the network layers. We propose a novel\n",
      "algorithm for efficiently training the neural net via stochastic gradient\n",
      "descent, which encourages the neural net outputs to span the leading\n",
      "eigenvectors of the aggregated Laplacian matrix, in order to capture the\n",
      "pairwise interactions on the network, and provide a clustering-friendly\n",
      "representation of the feature space. We demonstrate with an extensive set of\n",
      "experiments on synthetic and real datasets that our method leads to a\n",
      "significant improvement w.r.t. state-of-the-art multilayer graph clustering\n",
      "algorithms, as it judiciously combines nodes features and network information\n",
      "in the node embedding algorithms.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.01165 \n",
      "Title :Convergence of the Deep BSDE Method for Coupled FBSDEs\n",
      "  The recently proposed numerical algorithm, deep BSDE method, has shown\n",
      "remarkable performance in solving high-dimensional forward-backward stochastic\n",
      "differential equations (FBSDEs) and parabolic partial differential equations\n",
      "(PDEs). This article lays a theoretical foundation for the deep BSDE method in\n",
      "the general case of coupled FBSDEs. In particular, a posteriori error\n",
      "estimation of the solution is provided and it is proved that the error\n",
      "converges to zero given the universal approximation capability of neural\n",
      "networks. Numerical results are presented to demonstrate the accuracy of the\n",
      "analyzed algorithm in solving high-dimensional coupled FBSDEs.\n",
      "\n",
      "**Paper Id :1910.06948 \n",
      "Title :Data-Driven Deep Learning of Partial Differential Equations in Modal\n",
      "  Space\n",
      "  We present a framework for recovering/approximating unknown time-dependent\n",
      "partial differential equation (PDE) using its solution data. Instead of\n",
      "identifying the terms in the underlying PDE, we seek to approximate the\n",
      "evolution operator of the underlying PDE numerically. The evolution operator of\n",
      "the PDE, defined in infinite-dimensional space, maps the solution from a\n",
      "current time to a future time and completely characterizes the solution\n",
      "evolution of the underlying unknown PDE. Our recovery strategy relies on\n",
      "approximation of the evolution operator in a properly defined modal space,\n",
      "i.e., generalized Fourier space, in order to reduce the problem to finite\n",
      "dimensions. The finite dimensional approximation is then accomplished by\n",
      "training a deep neural network structure, which is based on residual network\n",
      "(ResNet), using the given data. Error analysis is provided to illustrate the\n",
      "predictive accuracy of the proposed method. A set of examples of different\n",
      "types of PDEs, including inviscid Burgers' equation that develops discontinuity\n",
      "in its solution, are presented to demonstrate the effectiveness of the proposed\n",
      "method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.01443 \n",
      "Title :SSCNets: Robustifying DNNs using Secure Selective Convolutional Filters\n",
      "  In this paper, we introduce a novel technique based on the Secure Selective\n",
      "Convolutional (SSC) techniques in the training loop that increases the\n",
      "robustness of a given DNN by allowing it to learn the data distribution based\n",
      "on the important edges in the input image. We validate our technique on\n",
      "Convolutional DNNs against the state-of-the-art attacks from the open-source\n",
      "Cleverhans library using the MNIST, the CIFAR-10, and the CIFAR-100 datasets.\n",
      "Our experimental results show that the attack success rate, as well as the\n",
      "imperceptibility of the adversarial images, can be significantly reduced by\n",
      "adding effective pre-processing functions, i.e., Sobel filtering.\n",
      "\n",
      "**Paper Id :2005.02552 \n",
      "Title :Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder\n",
      "  Whereas adversarial training is employed as the main defence strategy against\n",
      "specific adversarial samples, it has limited generalization capability and\n",
      "incurs excessive time complexity. In this paper, we propose an attack-agnostic\n",
      "defence framework to enhance the intrinsic robustness of neural networks,\n",
      "without jeopardizing the ability of generalizing clean samples. Our Feature\n",
      "Pyramid Decoder (FPD) framework applies to all block-based convolutional neural\n",
      "networks (CNNs). It implants denoising and image restoration modules into a\n",
      "targeted CNN, and it also constraints the Lipschitz constant of the\n",
      "classification layer. Moreover, we propose a two-phase strategy to train the\n",
      "FPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\n",
      "multi-task and self-supervised learning. Evaluated against a variety of\n",
      "white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\n",
      "sufficient robustness against general adversarial samples on MNIST, SVHN and\n",
      "CALTECH. In addition, if we further conduct adversarial training, the\n",
      "FPD-enhanced CNNs perform better than their non-enhanced versions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.03862 \n",
      "Title :Targeting Solutions in Bayesian Multi-Objective Optimization: Sequential\n",
      "  and Batch Versions\n",
      "  Multi-objective optimization aims at finding trade-off solutions to\n",
      "conflicting objectives. These constitute the Pareto optimal set. In the context\n",
      "of expensive-to-evaluate functions, it is impossible and often non-informative\n",
      "to look for the entire set. As an end-user would typically prefer a certain\n",
      "part of the objective space, we modify the Bayesian multi-objective\n",
      "optimization algorithm which uses Gaussian Processes to maximize the Expected\n",
      "Hypervolume Improvement, to focus the search in the preferred region. The\n",
      "cumulated effects of the Gaussian Processes and the targeting strategy lead to\n",
      "a particularly efficient convergence to the desired part of the Pareto set. To\n",
      "take advantage of parallel computing, a multi-point extension of the targeting\n",
      "criterion is proposed and analyzed.\n",
      "\n",
      "**Paper Id :2008.07029 \n",
      "Title :Uncertainty aware Search Framework for Multi-Objective Bayesian\n",
      "  Optimization with Constraints\n",
      "  We consider the problem of constrained multi-objective (MO) blackbox\n",
      "optimization using expensive function evaluations, where the goal is to\n",
      "approximate the true Pareto set of solutions satisfying a set of constraints\n",
      "while minimizing the number of function evaluations. We propose a novel\n",
      "framework named Uncertainty-aware Search framework for Multi-Objective\n",
      "Optimization with Constraints (USeMOC) to efficiently select the sequence of\n",
      "inputs for evaluation to solve this problem. The selection method of USeMOC\n",
      "consists of solving a cheap constrained MO optimization problem via surrogate\n",
      "models of the true functions to identify the most promising candidates and\n",
      "picking the best candidate based on a measure of uncertainty. We applied this\n",
      "framework to optimize the design of a multi-output switched-capacitor voltage\n",
      "regulator via expensive simulations. Our experimental results show that USeMOC\n",
      "is able to achieve more than 90 % reduction in the number of simulations needed\n",
      "to uncover optimized circuits.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.04047 \n",
      "Title :A Microprocessor implemented in 65nm CMOS with Configurable and\n",
      "  Bit-scalable Accelerator for Programmable In-memory Computing\n",
      "  This paper presents a programmable in-memory-computing processor,\n",
      "demonstrated in a 65nm CMOS technology. For data-centric workloads, such as\n",
      "deep neural networks, data movement often dominates when implemented with\n",
      "today's computing architectures. This has motivated spatial architectures,\n",
      "where the arrangement of data-storage and compute hardware is distributed and\n",
      "explicitly aligned to the computation dataflow, most notably for matrix-vector\n",
      "multiplication. In-memory computing is a spatial architecture where processing\n",
      "elements correspond to dense bit cells, providing local storage and compute,\n",
      "typically employing analog operation. Though this raises the potential for high\n",
      "energy efficiency and throughput, analog operation has significantly limited\n",
      "robustness, scale, and programmability. This paper describes a 590kb\n",
      "in-memory-computing accelerator integrated in a programmable processor\n",
      "architecture, by exploiting recent approaches to charge-domain in-memory\n",
      "computing. The architecture takes the approach of tight coupling with an\n",
      "embedded CPU, through accelerator interfaces enabling integration in the\n",
      "standard processor memory space. Additionally, a near-memory-computing datapath\n",
      "both enables diverse computations locally, to address operations required\n",
      "across applications, and enables bit-precision scalability for\n",
      "matrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme.\n",
      "Chip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput\n",
      "of 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element\n",
      "precisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b\n",
      "weights and activations for CIFAR-10 classification consume 5.3/105.2\n",
      "$\\mu$J/image at 176/23 fps, with accuracy at the level of digital/software\n",
      "implementation (89.3/92.4 $\\%$ accuracy).\n",
      "\n",
      "**Paper Id :2002.03780 \n",
      "Title :Photonic tensor cores for machine learning\n",
      "  With an ongoing trend in computing hardware towards increased heterogeneity,\n",
      "domain-specific co-processors are emerging as alternatives to centralized\n",
      "paradigms. The tensor core unit (TPU) has shown to outperform graphic process\n",
      "units by almost 3-orders of magnitude enabled by higher signal throughout and\n",
      "energy efficiency. In this context, photons bear a number of synergistic\n",
      "physical properties while phase-change materials allow for local nonvolatile\n",
      "mnemonic functionality in these emerging distributed non van-Neumann\n",
      "architectures. While several photonic neural network designs have been\n",
      "explored, a photonic TPU to perform matrix vector multiplication and summation\n",
      "is yet outstanding. Here we introduced an integrated photonics-based TPU by\n",
      "strategically utilizing a) photonic parallelism via wavelength division\n",
      "multiplexing, b) high 2 Peta-operations-per second throughputs enabled by 10s\n",
      "of picosecond-short delays from optoelectronics and compact photonic integrated\n",
      "circuitry, and c) zero power-consuming novel photonic multi-state memories\n",
      "based on phase-change materials featuring vanishing losses in the amorphous\n",
      "state. Combining these physical synergies of material, function, and system, we\n",
      "show that the performance of this 8-bit photonic TPU can be 2-3 orders higher\n",
      "compared to an electrical TPU whilst featuring similar chip areas. This work\n",
      "shows that photonic specialized processors have the potential to augment\n",
      "electronic systems and may perform exceptionally well in network-edge devices\n",
      "in the looming 5G networks and beyond.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.04817 \n",
      "Title :A test case for application of convolutional neural networks to\n",
      "  spatio-temporal climate data: Re-identifying clustered weather patterns\n",
      "  Convolutional neural networks (CNNs) can potentially provide powerful tools\n",
      "for classifying and identifying patterns in climate and environmental data.\n",
      "However, because of the inherent complexities of such data, which are often\n",
      "spatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\n",
      "designed/evaluated for each specific dataset and application. Yet to start,\n",
      "CNN, a supervised technique, requires a large labeled dataset. Labeling demands\n",
      "(human) expert time, which combined with the limited number of relevant\n",
      "examples in this area, can discourage using CNNs for new problems. To address\n",
      "these challenges, here we (1) Propose an effective auto-labeling strategy based\n",
      "on using an unsupervised clustering algorithm and evaluating the performance of\n",
      "CNNs in re-identifying these clusters; (2) Use this approach to label thousands\n",
      "of daily large-scale weather patterns over North America in the outputs of a\n",
      "fully-coupled climate model and show the capabilities of CNNs in re-identifying\n",
      "the 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\n",
      "cluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\n",
      "nonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\n",
      "training samples per cluster. Effects of architecture and hyperparameters on\n",
      "the performance of CNNs are examined and discussed.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.05076 \n",
      "Title :Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n",
      "  and its Statistical Optimality\n",
      "  We consider the problem of decomposing a higher-order tensor with binary\n",
      "entries. Such data problems arise frequently in applications such as\n",
      "neuroimaging, recommendation system, topic modeling, and sensor network\n",
      "localization. We propose a multilinear Bernoulli model, develop a\n",
      "rank-constrained likelihood-based estimation method, and obtain the theoretical\n",
      "accuracy guarantees. In contrast to continuous-valued problems, the binary\n",
      "tensor problem exhibits an interesting phase transition phenomenon according to\n",
      "the signal-to-noise ratio. The error bound for the parameter tensor estimation\n",
      "is established, and we show that the obtained rate is minimax optimal under the\n",
      "considered model. Furthermore, we develop an alternating optimization algorithm\n",
      "with convergence guarantees. The efficacy of our approach is demonstrated\n",
      "through both simulations and analyses of multiple data sets on the tasks of\n",
      "tensor completion and clustering.\n",
      "\n",
      "**Paper Id :2002.06524 \n",
      "Title :Tensor denoising and completion based on ordinal observations\n",
      "  Higher-order tensors arise frequently in applications such as neuroimaging,\n",
      "recommendation system, social network analysis, and psychological studies. We\n",
      "consider the problem of low-rank tensor estimation from possibly incomplete,\n",
      "ordinal-valued observations. Two related problems are studied, one on tensor\n",
      "denoising and another on tensor completion. We propose a multi-linear\n",
      "cumulative link model, develop a rank-constrained M-estimator, and obtain\n",
      "theoretical accuracy guarantees. Our mean squared error bound enjoys a faster\n",
      "convergence rate than previous results, and we show that the proposed estimator\n",
      "is minimax optimal under the class of low-rank models. Furthermore, the\n",
      "procedure developed serves as an efficient completion method which guarantees\n",
      "consistent recovery of an order-$K$ $(d,\\ldots,d)$-dimensional low-rank tensor\n",
      "using only $\\tilde{\\mathcal{O}}(Kd)$ noisy, quantized observations. We\n",
      "demonstrate the outperformance of our approach over previous methods on the\n",
      "tasks of clustering and collaborative filtering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.07594 \n",
      "Title :Measurement-based adaptation protocol with quantum reinforcement\n",
      "  learning in a Rigetti quantum computer\n",
      "  We present an experimental realization of a measurement-based adaptation\n",
      "protocol with quantum reinforcement learning in a Rigetti cloud quantum\n",
      "computer. The experiment in this few-qubit superconducting chip faithfully\n",
      "reproduces the theoretical proposal, setting the first steps towards a\n",
      "semiautonomous quantum agent. This experiment paves the way towards quantum\n",
      "reinforcement learning with superconducting circuits.\n",
      "\n",
      "**Paper Id :1906.03388 \n",
      "Title :Protocol for implementing quantum nonparametric learning with trapped\n",
      "  ions\n",
      "  Nonparametric learning is able to make reliable predictions by extracting\n",
      "information from similarities between a new set of input data and all samples.\n",
      "Here we point out a quantum paradigm of nonparametric learning which offers an\n",
      "exponential speedup over the sample size. By encoding data into quantum feature\n",
      "space, similarity between the data is defined as an inner product of quantum\n",
      "states. A quantum training state is introduced to superpose all data of\n",
      "samples, encoding relevant information for learning in its bipartite\n",
      "entanglement spectrum. We demonstrate that a trained state for prediction can\n",
      "be obtained by entanglement spectrum transformation, using quantum matrix\n",
      "toolbox. We further work out a feasible protocol to implement the quantum\n",
      "nonparametric learning with trapped ions, and demonstrate the power of quantum\n",
      "superposition for machine learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.08790 \n",
      "Title :Learning Quadratic Games on Networks\n",
      "  Individuals, or organizations, cooperate with or compete against one another\n",
      "in a wide range of practical situations. Such strategic interactions are often\n",
      "modeled as games played on networks, where an individual's payoff depends not\n",
      "only on her action but also on that of her neighbors. The current literature\n",
      "has largely focused on analyzing the characteristics of network games in the\n",
      "scenario where the structure of the network, which is represented by a graph,\n",
      "is known beforehand. It is often the case, however, that the actions of the\n",
      "players are readily observable while the underlying interaction network remains\n",
      "hidden. In this paper, we propose two novel frameworks for learning, from the\n",
      "observations on individual actions, network games with linear-quadratic\n",
      "payoffs, and in particular, the structure of the interaction network. Our\n",
      "frameworks are based on the Nash equilibrium of such games and involve solving\n",
      "a joint optimization problem for the graph structure and the individual\n",
      "marginal benefits. Both synthetic and real-world experiments demonstrate the\n",
      "effectiveness of the proposed frameworks, which have theoretical as well as\n",
      "practical implications for understanding strategic interactions in a network\n",
      "environment.\n",
      "\n",
      "**Paper Id :1910.04817 \n",
      "Title :Estimation of Bounds on Potential Outcomes For Decision Making\n",
      "  Estimation of individual treatment effects is commonly used as the basis for\n",
      "contextual decision making in fields such as healthcare, education, and\n",
      "economics. However, it is often sufficient for the decision maker to have\n",
      "estimates of upper and lower bounds on the potential outcomes of decision\n",
      "alternatives to assess risks and benefits. We show that, in such cases, we can\n",
      "improve sample efficiency by estimating simple functions that bound these\n",
      "outcomes instead of estimating their conditional expectations, which may be\n",
      "complex and hard to estimate. Our analysis highlights a trade-off between the\n",
      "complexity of the learning task and the confidence with which the learned\n",
      "bounds hold. Guided by these findings, we develop an algorithm for learning\n",
      "upper and lower bounds on potential outcomes which optimize an objective\n",
      "function defined by the decision maker, subject to the probability that bounds\n",
      "are violated being small. Using a clinical dataset and a well-known causality\n",
      "benchmark, we demonstrate that our algorithm outperforms baselines, providing\n",
      "tighter, more reliable bounds.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.08936 \n",
      "Title :A Neural Network Study of Blasius Equation\n",
      "  In this work we applied a feed forward neural network to solve Blasius\n",
      "equation which is a third-order nonlinear differential equation. Blasius\n",
      "equation is a kind of boundary layer flow. We solved Blasius equation without\n",
      "reducing it into a system of first order equation. Numerical results are\n",
      "presented and a comparison according to some studies is made in the form of\n",
      "their results. Obtained results are found to be in good agreement with the\n",
      "given studies.\n",
      "\n",
      "**Paper Id :1811.01165 \n",
      "Title :Convergence of the Deep BSDE Method for Coupled FBSDEs\n",
      "  The recently proposed numerical algorithm, deep BSDE method, has shown\n",
      "remarkable performance in solving high-dimensional forward-backward stochastic\n",
      "differential equations (FBSDEs) and parabolic partial differential equations\n",
      "(PDEs). This article lays a theoretical foundation for the deep BSDE method in\n",
      "the general case of coupled FBSDEs. In particular, a posteriori error\n",
      "estimation of the solution is provided and it is proved that the error\n",
      "converges to zero given the universal approximation capability of neural\n",
      "networks. Numerical results are presented to demonstrate the accuracy of the\n",
      "analyzed algorithm in solving high-dimensional coupled FBSDEs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1811.12166 \n",
      "Title :Prediction of ESG Compliance using a Heterogeneous Information Network\n",
      "  Negative screening is one method to avoid interactions with inappropriate\n",
      "entities. For example, financial institutions keep investment exclusion lists\n",
      "of inappropriate firms that have environmental, social, and government (ESG)\n",
      "problems. They create their investment exclusion lists by gathering information\n",
      "from various news sources to keep their portfolios profitable as well as green.\n",
      "International organizations also maintain smart sanctions lists that are used\n",
      "to prohibit trade with entities that are involved in illegal activities. In the\n",
      "present paper, we focus on the prediction of investment exclusion lists in the\n",
      "finance domain. We construct a vast heterogeneous information network that\n",
      "covers the necessary information surrounding each firm, which is assembled\n",
      "using seven professionally curated datasets and two open datasets, which\n",
      "results in approximately 50 million nodes and 400 million edges in total.\n",
      "Exploiting these vast datasets and motivated by how professional investigators\n",
      "and journalists undertake their daily investigations, we propose a model that\n",
      "can learn to predict firms that are more likely to be added to an investment\n",
      "exclusion list in the near future. Our approach is tested using the negative\n",
      "news investment exclusion list data of more than 35,000 firms worldwide from\n",
      "January 2012 to May 2018. Comparing with the state-of-the-art methods with and\n",
      "without using the network, we show that the predictive accuracy is\n",
      "substantially improved when using the vast information stored in the\n",
      "heterogeneous information network. This work suggests new ways to consolidate\n",
      "the diffuse information contained in big data to monitor dominant firms on a\n",
      "global scale for better risk management and more socially responsible\n",
      "investment.\n",
      "\n",
      "**Paper Id :1906.06843 \n",
      "Title :Predicting Research Trends with Semantic and Neural Networks with an\n",
      "  application in Quantum Physics\n",
      "  The vast and growing number of publications in all disciplines of science\n",
      "cannot be comprehended by a single human researcher. As a consequence,\n",
      "researchers have to specialize in narrow sub-disciplines, which makes it\n",
      "challenging to uncover scientific connections beyond the own field of research.\n",
      "Thus access to structured knowledge from a large corpus of publications could\n",
      "help pushing the frontiers of science. Here we demonstrate a method to build a\n",
      "semantic network from published scientific literature, which we call SemNet. We\n",
      "use SemNet to predict future trends in research and to inspire new,\n",
      "personalized and surprising seeds of ideas in science. We apply it in the\n",
      "discipline of quantum physics, which has seen an unprecedented growth of\n",
      "activity in recent years. In SemNet, scientific knowledge is represented as an\n",
      "evolving network using the content of 750,000 scientific papers published since\n",
      "1919. The nodes of the network correspond to physical concepts, and links\n",
      "between two nodes are drawn when two physical concepts are concurrently studied\n",
      "in research articles. We identify influential and prize-winning research topics\n",
      "from the past inside SemNet thus confirm that it stores useful semantic\n",
      "knowledge. We train a deep neural network using states of SemNet of the past,\n",
      "to predict future developments in quantum physics research, and confirm high\n",
      "quality predictions using historic data. With the neural network and\n",
      "theoretical network tools we are able to suggest new, personalized,\n",
      "out-of-the-box ideas, by identifying pairs of concepts which have unique and\n",
      "extremal semantic network properties. Finally, we consider possible future\n",
      "developments and implications of our findings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1812.04369 \n",
      "Title :Variational Bayesian Weighted Complex Network Reconstruction\n",
      "  Complex network reconstruction is a hot topic in many fields. Currently, the\n",
      "most popular data-driven reconstruction framework is based on lasso. However,\n",
      "it is found that, in the presence of noise, lasso loses efficiency for weighted\n",
      "networks. This paper builds a new framework to cope with this problem. The key\n",
      "idea is to employ a series of linear regression problems to model the\n",
      "relationship between network nodes, and then to use an efficient variational\n",
      "Bayesian algorithm to infer the unknown coefficients. The numerical experiments\n",
      "conducted on both synthetic and real data demonstrate that the new method\n",
      "outperforms lasso with regard to both reconstruction accuracy and running\n",
      "speed.\n",
      "\n",
      "**Paper Id :1909.03681 \n",
      "Title :Outlier Detection in High Dimensional Data\n",
      "  High-dimensional data poses unique challenges in outlier detection process.\n",
      "Most of the existing algorithms fail to properly address the issues stemming\n",
      "from a large number of features. In particular, outlier detection algorithms\n",
      "perform poorly on data set of small size with a large number of features. In\n",
      "this paper, we propose a novel outlier detection algorithm based on principal\n",
      "component analysis and kernel density estimation. The proposed method is\n",
      "designed to address the challenges of dealing with high-dimensional data by\n",
      "projecting the original data onto a smaller space and using the innate\n",
      "structure of the data to calculate anomaly scores for each data point.\n",
      "Numerical experiments on synthetic and real-life data show that our method\n",
      "performs well on high-dimensional data. In particular, the proposed method\n",
      "outperforms the benchmark methods as measured by the $F_1$-score. Our method\n",
      "also produces better-than-average execution times compared to the benchmark\n",
      "methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1812.05988 \n",
      "Title :Class Mean Vector Component and Discriminant Analysis\n",
      "  The kernel matrix used in kernel methods encodes all the information required\n",
      "for solving complex nonlinear problems defined on data representations in the\n",
      "input space using simple, but implicitly defined, solutions. Spectral analysis\n",
      "on the kernel matrix defines an explicit nonlinear mapping of the input data\n",
      "representations to a subspace of the kernel space, which can be used for\n",
      "directly applying linear methods. However, the selection of the kernel subspace\n",
      "is crucial for the performance of the proceeding processing steps. In this\n",
      "paper, we propose a component analysis method for kernel-based dimensionality\n",
      "reduction that optimally preserves the pair-wise distances of the class means\n",
      "in the feature space. We provide extensive analysis on the connection of the\n",
      "proposed criterion to those used in kernel principal component analysis and\n",
      "kernel discriminant analysis, leading to a discriminant analysis version of the\n",
      "proposed method. Our analysis also provides more insights on the properties of\n",
      "the feature spaces obtained by applying these methods.\n",
      "\n",
      "**Paper Id :2003.09504 \n",
      "Title :Ellipsoidal Subspace Support Vector Data Description\n",
      "  In this paper, we propose a novel method for transforming data into a\n",
      "low-dimensional space optimized for one-class classification. The proposed\n",
      "method iteratively transforms data into a new subspace optimized for\n",
      "ellipsoidal encapsulation of target class data. We provide both linear and\n",
      "non-linear formulations for the proposed method. The method takes into account\n",
      "the covariance of the data in the subspace; hence, it yields a more generalized\n",
      "solution as compared to Subspace Support Vector Data Description for a\n",
      "hypersphere. We propose different regularization terms expressing the class\n",
      "variance in the projected space. We compare the results with classic and\n",
      "recently proposed one-class classification methods and achieve better results\n",
      "in the majority of cases. The proposed method is also noticed to converge much\n",
      "faster than recently proposed Subspace Support Vector Data Description.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1812.06319 \n",
      "Title :Likelihood Quantile Networks for Coordinating Multi-Agent Reinforcement\n",
      "  Learning\n",
      "  When multiple agents learn in a decentralized manner, the environment appears\n",
      "non-stationary from the perspective of an individual agent due to the\n",
      "exploration and learning of the other agents. Recently proposed deep\n",
      "multi-agent reinforcement learning methods have tried to mitigate this\n",
      "non-stationarity by attempting to determine which samples are from other agent\n",
      "exploration or suboptimality and take them less into account during learning.\n",
      "Based on the same philosophy, this paper introduces a decentralized quantile\n",
      "estimator, which aims to improve performance by distinguishing non-stationary\n",
      "samples based on the likelihood of returns. In particular, each agent considers\n",
      "the likelihood that other agent exploration and policy changes are occurring,\n",
      "essentially utilizing the agent's own estimations to weigh the learning rate\n",
      "that should be applied towards the given samples. We introduce a formal method\n",
      "of calculating differences of our return distribution representations and\n",
      "methods for utilizing it to guide updates. We also explore the effect of\n",
      "risk-seeking strategies for adjusting learning over time and propose adaptive\n",
      "risk distortion functions which guides risk sensitivity. Our experiments, on\n",
      "traditional benchmarks and new domains, show our methods are more stable,\n",
      "sample efficient and more likely to converge to a joint optimal policy than\n",
      "previous methods.\n",
      "\n",
      "**Paper Id :1911.01546 \n",
      "Title :Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy\n",
      "  While maximizing expected return is the goal in most reinforcement learning\n",
      "approaches, risk-sensitive objectives such as conditional value at risk (CVaR)\n",
      "are more suitable for many high-stakes applications. However, relatively little\n",
      "is known about how to explore to quickly learn policies with good CVaR. In this\n",
      "paper, we present the first algorithm for sample-efficient learning of\n",
      "CVaR-optimal policies in Markov decision processes based on the optimism in the\n",
      "face of uncertainty principle. This method relies on a novel optimistic version\n",
      "of the distributional Bellman operator that moves probability mass from the\n",
      "lower to the upper tail of the return distribution. We prove asymptotic\n",
      "convergence and optimism of this operator for the tabular policy evaluation\n",
      "case. We further demonstrate that our algorithm finds CVaR-optimal policies\n",
      "substantially faster than existing baselines in several simulated environments\n",
      "with discrete and continuous state spaces.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1812.08491 \n",
      "Title :cuPC: CUDA-based Parallel PC Algorithm for Causal Structure Learning on\n",
      "  GPU\n",
      "  The main goal in many fields in the empirical sciences is to discover causal\n",
      "relationships among a set of variables from observational data. PC algorithm is\n",
      "one of the promising solutions to learn underlying causal structure by\n",
      "performing a number of conditional independence tests. In this paper, we\n",
      "propose a novel GPU-based parallel algorithm, called cuPC, to execute an\n",
      "order-independent version of PC. The proposed solution has two variants, cuPC-E\n",
      "and cuPC-S, which parallelize PC in two different ways for multivariate normal\n",
      "distribution. Experimental results show the scalability of the proposed\n",
      "algorithms with respect to the number of variables, the number of samples, and\n",
      "different graph densities. For instance, in one of the most challenging\n",
      "datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On\n",
      "average, cuPC-E and cuPC-S achieve 500 X and 1300 X speedup, respectively,\n",
      "compared to serial implementation on CPU. The source code of cuPC is available\n",
      "online [1].\n",
      "\n",
      "**Paper Id :1904.10900 \n",
      "Title :Learning big Gaussian Bayesian networks: partition, estimation, and\n",
      "  fusion\n",
      "  Structure learning of Bayesian networks has always been a challenging\n",
      "problem. Nowadays, massive-size networks with thousands or more of nodes but\n",
      "fewer samples frequently appear in many areas. We develop a divide-and-conquer\n",
      "framework, called partition-estimation-fusion (PEF), for structure learning of\n",
      "such big networks. The proposed method first partitions nodes into clusters,\n",
      "then learns a subgraph on each cluster of nodes, and finally fuses all learned\n",
      "subgraphs into one Bayesian network. The PEF method is designed in a flexible\n",
      "way so that any structure learning method may be used in the second step to\n",
      "learn a subgraph structure as either a DAG or a CPDAG. In the clustering step,\n",
      "we adapt the hierarchical clustering method to automatically choose a proper\n",
      "number of clusters. In the fusion step, we propose a novel hybrid method that\n",
      "sequentially add edges between subgraphs. Extensive numerical experiments\n",
      "demonstrate the competitive performance of our PEF method, in terms of both\n",
      "speed and accuracy compared to existing methods. Our method can improve the\n",
      "accuracy of structure learning by 20% or more, while reducing running time up\n",
      "to two orders-of-magnitude.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1812.09066 \n",
      "Title :Marvels and Pitfalls of the Langevin Algorithm in Noisy High-dimensional\n",
      "  Inference\n",
      "  Gradient-descent-based algorithms and their stochastic versions have\n",
      "widespread applications in machine learning and statistical inference. In this\n",
      "work we perform an analytic study of the performances of one of them, the\n",
      "Langevin algorithm, in the context of noisy high-dimensional inference. We\n",
      "employ the Langevin algorithm to sample the posterior probability measure for\n",
      "the spiked matrix-tensor model. The typical behaviour of this algorithm is\n",
      "described by a system of integro-differential equations that we call the\n",
      "Langevin state evolution, whose solution is compared with the one of the state\n",
      "evolution of approximate message passing (AMP). Our results show that,\n",
      "remarkably, the algorithmic threshold of the Langevin algorithm is sub-optimal\n",
      "with respect to the one given by AMP. We conjecture this phenomenon to be due\n",
      "to the residual glassiness present in that region of parameters. Finally we\n",
      "show how a landscape-annealing protocol, that uses the Langevin algorithm but\n",
      "violate the Bayes-optimality condition, can approach the performance of AMP.\n",
      "\n",
      "**Paper Id :2006.13222 \n",
      "Title :Certified variational quantum algorithms for eigenstate preparation\n",
      "  Solutions to many-body problem instances often involve an intractable number\n",
      "of degrees of freedom and admit no known approximations in general form. In\n",
      "practice, representing quantum-mechanical states of a given Hamiltonian using\n",
      "available numerical methods, in particular those based on variational Monte\n",
      "Carlo simulations, become exponentially more challenging with increasing system\n",
      "size. Recently quantum algorithms implemented as variational models have been\n",
      "proposed to accelerate such simulations. The variational ansatz states are\n",
      "characterized by a polynomial number of parameters devised in a way to minimize\n",
      "the expectation value of a given Hamiltonian, which is emulated by local\n",
      "measurements. In this study, we develop a means to certify the termination of\n",
      "variational algorithms. We demonstrate our approach by applying it to three\n",
      "models: the transverse field Ising model, the model of one-dimensional spinless\n",
      "fermions with competing interactions, and the Schwinger model of quantum\n",
      "electrodynamics. By means of comparison, we observe that our approach shows\n",
      "better performance near critical points in these models. We hence take a\n",
      "further step to improve the applicability and to certify the results of\n",
      "variational quantum simulators.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1812.11794 \n",
      "Title :Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n",
      "  Challenges, Solutions and Applications\n",
      "  Reinforcement learning (RL) algorithms have been around for decades and\n",
      "employed to solve various sequential decision-making problems. These algorithms\n",
      "however have faced great challenges when dealing with high-dimensional\n",
      "environments. The recent development of deep learning has enabled RL methods to\n",
      "drive optimal policies for sophisticated and capable agents, which can perform\n",
      "efficiently in these challenging environments. This paper addresses an\n",
      "important aspect of deep RL related to situations that require multiple agents\n",
      "to communicate and cooperate to solve complex tasks. A survey of different\n",
      "approaches to problems related to multi-agent deep RL (MADRL) is presented,\n",
      "including non-stationarity, partial observability, continuous state and action\n",
      "spaces, multi-agent training schemes, multi-agent transfer learning. The merits\n",
      "and demerits of the reviewed methods will be analyzed and discussed, with their\n",
      "corresponding applications explored. It is envisaged that this review provides\n",
      "insights about various MADRL methods and can lead to future development of more\n",
      "robust and highly useful multi-agent learning methods for solving real-world\n",
      "problems.\n",
      "\n",
      "**Paper Id :1803.02965 \n",
      "Title :A Multi-Objective Deep Reinforcement Learning Framework\n",
      "  This paper introduces a new scalable multi-objective deep reinforcement\n",
      "learning (MODRL) framework based on deep Q-networks. We develop a\n",
      "high-performance MODRL framework that supports both single-policy and\n",
      "multi-policy strategies, as well as both linear and non-linear approaches to\n",
      "action selection. The experimental results on two benchmark problems\n",
      "(two-objective deep sea treasure environment and three-objective Mountain Car\n",
      "problem) indicate that the proposed framework is able to find the\n",
      "Pareto-optimal solutions effectively. The proposed framework is generic and\n",
      "highly modularized, which allows the integration of different deep\n",
      "reinforcement learning algorithms in different complex problem domains. This\n",
      "therefore overcomes many disadvantages involved with standard multi-objective\n",
      "reinforcement learning methods in the current literature. The proposed\n",
      "framework acts as a testbed platform that accelerates the development of MODRL\n",
      "for solving increasingly complicated multi-objective problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.00409 \n",
      "Title :Neural Clustering Processes\n",
      "  Probabilistic clustering models (or equivalently, mixture models) are basic\n",
      "building blocks in countless statistical models and involve latent random\n",
      "variables over discrete spaces. For these models, posterior inference methods\n",
      "can be inaccurate and/or very slow. In this work we introduce deep network\n",
      "architectures trained with labeled samples from any generative model of\n",
      "clustered datasets. At test time, the networks generate approximate posterior\n",
      "samples of cluster labels for any new dataset of arbitrary size. We develop two\n",
      "complementary approaches to this task, requiring either O(N) or O(K) network\n",
      "forward passes per dataset, where N is the dataset size and K the number of\n",
      "clusters. Unlike previous approaches, our methods sample the labels of all the\n",
      "data points from a well-defined posterior, and can learn nonparametric Bayesian\n",
      "posteriors since they do not limit the number of mixture components. As a\n",
      "scientific application, we present a novel approach to neural spike sorting for\n",
      "high-density multielectrode arrays.\n",
      "\n",
      "**Paper Id :2006.14084 \n",
      "Title :Multilabel Classification by Hierarchical Partitioning and\n",
      "  Data-dependent Grouping\n",
      "  In modern multilabel classification problems, each data instance belongs to a\n",
      "small number of classes from a large set of classes. In other words, these\n",
      "problems involve learning very sparse binary label vectors. Moreover, in\n",
      "large-scale problems, the labels typically have certain (unknown) hierarchy. In\n",
      "this paper we exploit the sparsity of label vectors and the hierarchical\n",
      "structure to embed them in low-dimensional space using label groupings.\n",
      "Consequently, we solve the classification problem in a much lower dimensional\n",
      "space and then obtain labels in the original space using an appropriately\n",
      "defined lifting. Our method builds on the work of (Ubaru & Mazumdar, 2017),\n",
      "where the idea of group testing was also explored for multilabel\n",
      "classification. We first present a novel data-dependent grouping approach,\n",
      "where we use a group construction based on a low-rank Nonnegative Matrix\n",
      "Factorization (NMF) of the label matrix of training instances. The construction\n",
      "also allows us, using recent results, to develop a fast prediction algorithm\n",
      "that has a logarithmic runtime in the number of labels. We then present a\n",
      "hierarchical partitioning approach that exploits the label hierarchy in large\n",
      "scale problems to divide up the large label space and create smaller\n",
      "sub-problems, which can then be solved independently via the grouping approach.\n",
      "Numerical results on many benchmark datasets illustrate that, compared to other\n",
      "popular methods, our proposed methods achieve competitive accuracy with\n",
      "significantly lower computational costs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.01718 \n",
      "Title :Deep Network Embedding for Graph Representation Learning in Signed\n",
      "  Networks\n",
      "  Network embedding has attracted an increasing attention over the past few\n",
      "years. As an effective approach to solve graph mining problems, network\n",
      "embedding aims to learn a low-dimensional feature vector representation for\n",
      "each node of a given network. The vast majority of existing network embedding\n",
      "algorithms, however, are only designed for unsigned networks, and the signed\n",
      "networks containing both positive and negative links, have pretty distinct\n",
      "properties from the unsigned counterpart. In this paper, we propose a deep\n",
      "network embedding model to learn the low-dimensional node vector\n",
      "representations with structural balance preservation for the signed networks.\n",
      "The model employs a semi-supervised stacked auto-encoder to reconstruct the\n",
      "adjacency connections of a given signed network. As the adjacency connections\n",
      "are overwhelmingly positive in the real-world signed networks, we impose a\n",
      "larger penalty to make the auto-encoder focus more on reconstructing the scarce\n",
      "negative links than the abundant positive links. In addition, to preserve the\n",
      "structural balance property of signed networks, we design the pairwise\n",
      "constraints to make the positively connected nodes much closer than the\n",
      "negatively connected nodes in the embedding space. Based on the network\n",
      "representations learned by the proposed model, we conduct link sign prediction\n",
      "and community detection in signed networks. Extensive experimental results in\n",
      "real-world datasets demonstrate the superiority of the proposed model over the\n",
      "state-of-the-art network embedding algorithms for graph representation learning\n",
      "in signed networks.\n",
      "\n",
      "**Paper Id :2002.07366 \n",
      "Title :Adversarial Deep Network Embedding for Cross-network Node Classification\n",
      "  In this paper, the task of cross-network node classification, which leverages\n",
      "the abundant labeled nodes from a source network to help classify unlabeled\n",
      "nodes in a target network, is studied. The existing domain adaptation\n",
      "algorithms generally fail to model the network structural information, and the\n",
      "current network embedding models mainly focus on single-network applications.\n",
      "Thus, both of them cannot be directly applied to solve the cross-network node\n",
      "classification problem. This motivates us to propose an adversarial\n",
      "cross-network deep network embedding (ACDNE) model to integrate adversarial\n",
      "domain adaptation with deep network embedding so as to learn network-invariant\n",
      "node representations that can also well preserve the network structural\n",
      "information. In ACDNE, the deep network embedding module utilizes two feature\n",
      "extractors to jointly preserve attributed affinity and topological proximities\n",
      "between nodes. In addition, a node classifier is incorporated to make node\n",
      "representations label-discriminative. Moreover, an adversarial domain\n",
      "adaptation technique is employed to make node representations\n",
      "network-invariant. Extensive experimental results demonstrate that the proposed\n",
      "ACDNE model achieves the state-of-the-art performance in cross-network node\n",
      "classification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.01963 \n",
      "Title :Machine learning topological phases in real space\n",
      "  We develop a supervised machine learning algorithm that is able to learn\n",
      "topological phases of finite condensed matter systems from bulk data in real\n",
      "lattice space. The algorithm employs diagonalization in real space together\n",
      "with any supervised learning algorithm to learn topological phases through an\n",
      "eigenvector ensembling procedure. We combine our algorithm with decision trees\n",
      "and random forests to successfully recover topological phase diagrams of\n",
      "Su-Schrieffer-Heeger (SSH) models from bulk lattice data in real space and show\n",
      "how the Shannon information entropy of ensembles of lattice eigenvectors can be\n",
      "used to retrieve a signal detailing how topological information is distributed\n",
      "in the bulk. We further use insights obtained from these information entropy\n",
      "signatures to engineer global topological features from real space lattice data\n",
      "that still carry most of the topological information in the lattice, while\n",
      "greatly diminishing the size of feature space, thus effectively amounting to a\n",
      "topological lattice compression. Finally, we explore the theoretical\n",
      "possibility of interpreting the information entropy topological signatures in\n",
      "terms of emergent information entropy wave functions, which lead us to\n",
      "Heisenberg and Hirschman uncertainty relations for topological phase\n",
      "transitions. The discovery of Shannon information entropy signals associated\n",
      "with topological phase transitions from the analysis of data from several\n",
      "thousand SSH systems illustrates how model explainability in machine learning\n",
      "can advance the research of exotic quantum materials with properties that may\n",
      "power future technological applications such as qubit engineering for quantum\n",
      "computing.\n",
      "\n",
      "**Paper Id :2003.01504 \n",
      "Title :Towards Novel Insights in Lattice Field Theory with Explainable Machine\n",
      "  Learning\n",
      "  Machine learning has the potential to aid our understanding of phase\n",
      "structures in lattice quantum field theories through the statistical analysis\n",
      "of Monte Carlo samples. Available algorithms, in particular those based on deep\n",
      "learning, often demonstrate remarkable performance in the search for previously\n",
      "unidentified features, but tend to lack transparency if applied naively. To\n",
      "address these shortcomings, we propose representation learning in combination\n",
      "with interpretability methods as a framework for the identification of\n",
      "observables. More specifically, we investigate action parameter regression as a\n",
      "pretext task while using layer-wise relevance propagation (LRP) to identify the\n",
      "most important observables depending on the location in the phase diagram. The\n",
      "approach is put to work in the context of a scalar Yukawa model in (2+1)d.\n",
      "First, we investigate a multilayer perceptron to determine an importance\n",
      "hierarchy of several predefined, standard observables. The method is then\n",
      "applied directly to the raw field configurations using a convolutional network,\n",
      "demonstrating the ability to reconstruct all order parameters from the learned\n",
      "filter weights. Based on our results, we argue that due to its broad\n",
      "applicability, attribution methods such as LRP could prove a useful and\n",
      "versatile tool in our search for new physical insights. In the case of the\n",
      "Yukawa model, it facilitates the construction of an observable that\n",
      "characterises the symmetric phase.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.03775 \n",
      "Title :Creative AI Through Evolutionary Computation\n",
      "  The main power of artificial intelligence is not in modeling what we already\n",
      "know, but in creating solutions that are new. Such solutions exist in extremely\n",
      "large, high-dimensional, and complex search spaces. Population-based search\n",
      "techniques, i.e. variants of evolutionary computation, are well suited to\n",
      "finding them. These techniques are also well positioned to take advantage of\n",
      "large-scale parallel computing resources, making creative AI through\n",
      "evolutionary computation the likely \"next deep learning\".\n",
      "\n",
      "**Paper Id :1909.11655 \n",
      "Title :Augmenting Genetic Algorithms with Deep Neural Networks for Exploring\n",
      "  the Chemical Space\n",
      "  Challenges in natural sciences can often be phrased as optimization problems.\n",
      "Machine learning techniques have recently been applied to solve such problems.\n",
      "One example in chemistry is the design of tailor-made organic materials and\n",
      "molecules, which requires efficient methods to explore the chemical space. We\n",
      "present a genetic algorithm (GA) that is enhanced with a neural network (DNN)\n",
      "based discriminator model to improve the diversity of generated molecules and\n",
      "at the same time steer the GA. We show that our algorithm outperforms other\n",
      "generative models in optimization tasks. We furthermore present a way to\n",
      "increase interpretability of genetic algorithms, which helped us to derive\n",
      "design principles.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.03887 \n",
      "Title :Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement\n",
      "  Learning through Memory-driven Communication\n",
      "  Deep reinforcement learning algorithms have recently been used to train\n",
      "multiple interacting agents in a centralised manner whilst keeping their\n",
      "execution decentralised. When the agents can only acquire partial observations\n",
      "and are faced with tasks requiring coordination and synchronisation skills,\n",
      "inter-agent communication plays an essential role. In this work, we propose a\n",
      "framework for multi-agent training using deep deterministic policy gradients\n",
      "that enables concurrent, end-to-end learning of an explicit communication\n",
      "protocol through a memory device. During training, the agents learn to perform\n",
      "read and write operations enabling them to infer a shared representation of the\n",
      "world. We empirically demonstrate that concurrent learning of the communication\n",
      "device and individual policies can improve inter-agent coordination and\n",
      "performance in small-scale systems. Our experimental results show that the\n",
      "proposed method achieves superior performance in scenarios with up to six\n",
      "agents. We illustrate how different communication patterns can emerge on six\n",
      "different tasks of increasing complexity. Furthermore, we study the effects of\n",
      "corrupting the communication channel, provide a visualisation of the\n",
      "time-varying memory content as the underlying task is being solved and validate\n",
      "the building blocks of the proposed memory device through ablation studies.\n",
      "\n",
      "**Paper Id :2003.08839 \n",
      "Title :Monotonic Value Function Factorisation for Deep Multi-Agent\n",
      "  Reinforcement Learning\n",
      "  In many real-world settings, a team of agents must coordinate its behaviour\n",
      "while acting in a decentralised fashion. At the same time, it is often possible\n",
      "to train the agents in a centralised fashion where global state information is\n",
      "available and communication constraints are lifted. Learning joint\n",
      "action-values conditioned on extra state information is an attractive way to\n",
      "exploit centralised learning, but the best strategy for then extracting\n",
      "decentralised policies is unclear. Our solution is QMIX, a novel value-based\n",
      "method that can train decentralised policies in a centralised end-to-end\n",
      "fashion. QMIX employs a mixing network that estimates joint action-values as a\n",
      "monotonic combination of per-agent values. We structurally enforce that the\n",
      "joint-action value is monotonic in the per-agent values, through the use of\n",
      "non-negative weights in the mixing network, which guarantees consistency\n",
      "between the centralised and decentralised policies. To evaluate the performance\n",
      "of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new\n",
      "benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a\n",
      "challenging set of SMAC scenarios and show that it significantly outperforms\n",
      "existing multi-agent reinforcement learning methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.04345 \n",
      "Title :High-dimensional structure learning of binary pairwise Markov networks:\n",
      "  A comparative numerical study\n",
      "  Learning the undirected graph structure of a Markov network from data is a\n",
      "problem that has received a lot of attention during the last few decades. As a\n",
      "result of the general applicability of the model class, a myriad of methods\n",
      "have been developed in parallel in several research fields. Recently, as the\n",
      "size of the considered systems has increased, the focus of new methods has been\n",
      "shifted towards the high-dimensional domain. In particular, introduction of the\n",
      "pseudo-likelihood function has pushed the limits of score-based methods which\n",
      "were originally based on the likelihood function. At the same time, methods\n",
      "based on simple pairwise tests have been developed to meet the challenges\n",
      "arising from increasingly large data sets in computational biology. Apart from\n",
      "being applicable to high-dimensional problems, methods based on the\n",
      "pseudo-likelihood and pairwise tests are fundamentally very different. To\n",
      "compare the accuracy of the different types of methods, an extensive numerical\n",
      "study is performed on data generated by binary pairwise Markov networks. A\n",
      "parallelizable Gibbs sampler, based on restricted Boltzmann machines, is\n",
      "proposed as a tool to efficiently sample from sparse high-dimensional networks.\n",
      "The results of the study show that pairwise methods can be more accurate than\n",
      "pseudo-likelihood methods in settings often encountered in high-dimensional\n",
      "structure learning applications.\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.05331 \n",
      "Title :Optimization Models for Machine Learning: A Survey\n",
      "  This paper surveys the machine learning literature and presents in an\n",
      "optimization framework several commonly used machine learning approaches.\n",
      "Particularly, mathematical optimization models are presented for regression,\n",
      "classification, clustering, deep learning, and adversarial learning, as well as\n",
      "new emerging applications in machine teaching, empirical model learning, and\n",
      "Bayesian network structure learning. Such models can benefit from the\n",
      "advancement of numerical optimization techniques which have already played a\n",
      "distinctive role in several machine learning settings. The strengths and the\n",
      "shortcomings of these models are discussed and potential research directions\n",
      "and open problems are highlighted.\n",
      "\n",
      "**Paper Id :1812.11794 \n",
      "Title :Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n",
      "  Challenges, Solutions and Applications\n",
      "  Reinforcement learning (RL) algorithms have been around for decades and\n",
      "employed to solve various sequential decision-making problems. These algorithms\n",
      "however have faced great challenges when dealing with high-dimensional\n",
      "environments. The recent development of deep learning has enabled RL methods to\n",
      "drive optimal policies for sophisticated and capable agents, which can perform\n",
      "efficiently in these challenging environments. This paper addresses an\n",
      "important aspect of deep RL related to situations that require multiple agents\n",
      "to communicate and cooperate to solve complex tasks. A survey of different\n",
      "approaches to problems related to multi-agent deep RL (MADRL) is presented,\n",
      "including non-stationarity, partial observability, continuous state and action\n",
      "spaces, multi-agent training schemes, multi-agent transfer learning. The merits\n",
      "and demerits of the reviewed methods will be analyzed and discussed, with their\n",
      "corresponding applications explored. It is envisaged that this review provides\n",
      "insights about various MADRL methods and can lead to future development of more\n",
      "robust and highly useful multi-agent learning methods for solving real-world\n",
      "problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.06082 \n",
      "Title :Probabilistic symmetries and invariant neural networks\n",
      "  Treating neural network inputs and outputs as random variables, we\n",
      "characterize the structure of neural networks that can be used to model data\n",
      "that are invariant or equivariant under the action of a compact group. Much\n",
      "recent research has been devoted to encoding invariance under symmetry\n",
      "transformations into neural network architectures, in an effort to improve the\n",
      "performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised\n",
      "settings. By considering group invariance from the perspective of probabilistic\n",
      "symmetry, we establish a link between functional and probabilistic symmetry,\n",
      "and obtain generative functional representations of probability distributions\n",
      "that are invariant or equivariant under the action of a compact group. Our\n",
      "representations completely characterize the structure of neural networks that\n",
      "can be used to model such distributions and yield a general program for\n",
      "constructing invariant stochastic or deterministic neural networks. We\n",
      "demonstrate that examples from the recent literature are special cases, and\n",
      "develop the details of the general program for exchangeable sequences and\n",
      "arrays.\n",
      "\n",
      "**Paper Id :2006.02341 \n",
      "Title :Non-Euclidean Universal Approximation\n",
      "  Modifications to a neural network's input and output layers are often\n",
      "required to accommodate the specificities of most practical learning tasks.\n",
      "However, the impact of such changes on architecture's approximation\n",
      "capabilities is largely not understood. We present general conditions\n",
      "describing feature and readout maps that preserve an architecture's ability to\n",
      "approximate any continuous functions uniformly on compacts. As an application,\n",
      "we show that if an architecture is capable of universal approximation, then\n",
      "modifying its final layer to produce binary values creates a new architecture\n",
      "capable of deterministically approximating any classifier. In particular, we\n",
      "obtain guarantees for deep CNNs and deep feed-forward networks. Our results\n",
      "also have consequences within the scope of geometric deep learning.\n",
      "Specifically, when the input and output spaces are Cartan-Hadamard manifolds,\n",
      "we obtain geometrically meaningful feature and readout maps satisfying our\n",
      "criteria. Consequently, commonly used non-Euclidean regression models between\n",
      "spaces of symmetric positive definite matrices are extended to universal DNNs.\n",
      "The same result allows us to show that the hyperbolic feed-forward networks,\n",
      "used for hierarchical learning, are universal. Our result is also used to show\n",
      "that the common practice of randomizing all but the last two layers of a DNN\n",
      "produces a universal family of functions with probability one. We also provide\n",
      "conditions on a DNN's first (resp. last) few layer's connections and activation\n",
      "function which guarantee that these layers can have a width equal to the input\n",
      "(resp. output) space's dimension while not negatively affecting the\n",
      "architecture's approximation capabilities.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.07114 \n",
      "Title :Training Neural Networks as Learning Data-adaptive Kernels: Provable\n",
      "  Representation and Approximation Benefits\n",
      "  Consider the problem: given the data pair $(\\mathbf{x}, \\mathbf{y})$ drawn\n",
      "from a population with $f_*(x) = \\mathbf{E}[\\mathbf{y} | \\mathbf{x} = x]$,\n",
      "specify a neural network model and run gradient flow on the weights over time\n",
      "until reaching any stationarity. How does $f_t$, the function computed by the\n",
      "neural network at time $t$, relate to $f_*$, in terms of approximation and\n",
      "representation? What are the provable benefits of the adaptive representation\n",
      "by neural networks compared to the pre-specified fixed basis representation in\n",
      "the classical nonparametric literature? We answer the above questions via a\n",
      "dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the\n",
      "training process of neural networks. Firstly, we show that when reaching any\n",
      "local stationarity, gradient flow learns an adaptive RKHS representation and\n",
      "performs the global least-squares projection onto the adaptive RKHS,\n",
      "simultaneously. Secondly, we prove that as the RKHS is data-adaptive and\n",
      "task-specific, the residual for $f_*$ lies in a subspace that is potentially\n",
      "much smaller than the orthogonal complement of the RKHS. The result formalizes\n",
      "the representation and approximation benefits of neural networks. Lastly, we\n",
      "show that the neural network function computed by gradient flow converges to\n",
      "the kernel ridgeless regression with an adaptive kernel, in the limit of\n",
      "vanishing regularization. The adaptive kernel viewpoint provides new angles of\n",
      "studying the approximation, representation, generalization, and optimization\n",
      "advantages of neural networks.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.08096 \n",
      "Title :Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps\n",
      "  for Time Series Prediction\n",
      "  Despite the recent popularity of deep generative state space models, few\n",
      "comparisons have been made between network architectures and the inference\n",
      "steps of the Bayesian filtering framework -- with most models simultaneously\n",
      "approximating both state transition and update steps with a single recurrent\n",
      "neural network (RNN). In this paper, we introduce the Recurrent Neural Filter\n",
      "(RNF), a novel recurrent autoencoder architecture that learns distinct\n",
      "representations for each Bayesian filtering step, captured by a series of\n",
      "encoders and decoders. Testing this on three real-world time series datasets,\n",
      "we demonstrate that the decoupled representations learnt not only improve the\n",
      "accuracy of one-step-ahead forecasts while providing realistic uncertainty\n",
      "estimates, but also facilitate multistep prediction through the separation of\n",
      "encoder stages.\n",
      "\n",
      "**Paper Id :2004.13408 \n",
      "Title :Time Series Forecasting With Deep Learning: A Survey\n",
      "  Numerous deep learning architectures have been developed to accommodate the\n",
      "diversity of time series datasets across different domains. In this article, we\n",
      "survey common encoder and decoder designs used in both one-step-ahead and\n",
      "multi-horizon time series forecasting -- describing how temporal information is\n",
      "incorporated into predictions by each model. Next, we highlight recent\n",
      "developments in hybrid deep learning models, which combine well-studied\n",
      "statistical models with neural network components to improve pure methods in\n",
      "either category. Lastly, we outline some ways in which deep learning can also\n",
      "facilitate decision support with time series data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.08361 \n",
      "Title :Learning Global Pairwise Interactions with Bayesian Neural Networks\n",
      "  Estimating global pairwise interaction effects, i.e., the difference between\n",
      "the joint effect and the sum of marginal effects of two input features, with\n",
      "uncertainty properly quantified, is centrally important in science\n",
      "applications. We propose a non-parametric probabilistic method for detecting\n",
      "interaction effects of unknown form. First, the relationship between the\n",
      "features and the output is modelled using a Bayesian neural network, capable of\n",
      "representing complex interactions and principled uncertainty. Second,\n",
      "interaction effects and their uncertainty are estimated from the trained model.\n",
      "For the second step, we propose an intuitive global interaction measure:\n",
      "Bayesian Group Expected Hessian (GEH), which aggregates information of local\n",
      "interactions as captured by the Hessian. GEH provides a natural trade-off\n",
      "between type I and type II error and, moreover, comes with theoretical\n",
      "guarantees ensuring that the estimated interaction effects and their\n",
      "uncertainty can be improved by training a more accurate BNN. The method\n",
      "empirically outperforms available non-probabilistic alternatives on simulated\n",
      "and real-world data. Finally, we demonstrate its ability to detect\n",
      "interpretable interactions between higher-level features (at deeper layers of\n",
      "the neural network).\n",
      "\n",
      "**Paper Id :2006.14293 \n",
      "Title :Neural Decomposition: Functional ANOVA with Variational Autoencoders\n",
      "  Variational Autoencoders (VAEs) have become a popular approach for\n",
      "dimensionality reduction. However, despite their ability to identify latent\n",
      "low-dimensional structures embedded within high-dimensional data, these latent\n",
      "representations are typically hard to interpret on their own. Due to the\n",
      "black-box nature of VAEs, their utility for healthcare and genomics\n",
      "applications has been limited. In this paper, we focus on characterising the\n",
      "sources of variation in Conditional VAEs. Our goal is to provide a\n",
      "feature-level variance decomposition, i.e. to decompose variation in the data\n",
      "by separating out the marginal additive effects of latent variables z and fixed\n",
      "inputs c from their non-linear interactions. We propose to achieve this through\n",
      "what we call Neural Decomposition - an adaptation of the well-known concept of\n",
      "functional ANOVA variance decomposition from classical statistics to deep\n",
      "learning models. We show how identifiability can be achieved by training models\n",
      "subject to constraints on the marginal properties of the decoder networks. We\n",
      "demonstrate the utility of our Neural Decomposition on a series of synthetic\n",
      "examples as well as high-dimensional genomics data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.09054 \n",
      "Title :Deep Learning on Small Datasets without Pre-Training using Cosine Loss\n",
      "  Two things seem to be indisputable in the contemporary deep learning\n",
      "discourse: 1. The categorical cross-entropy loss after softmax activation is\n",
      "the method of choice for classification. 2. Training a CNN classifier from\n",
      "scratch on small datasets does not work well. In contrast to this, we show that\n",
      "the cosine loss function provides significantly better performance than\n",
      "cross-entropy on datasets with only a handful of samples per class. For\n",
      "example, the accuracy achieved on the CUB-200-2011 dataset without pre-training\n",
      "is by 30% higher than with the cross-entropy loss. Further experiments on other\n",
      "popular datasets confirm our findings. Moreover, we demonstrate that\n",
      "integrating prior knowledge in the form of class hierarchies is straightforward\n",
      "with the cosine loss and improves classification performance further.\n",
      "\n",
      "**Paper Id :1909.02729 \n",
      "Title :A Baseline for Few-Shot Image Classification\n",
      "  Fine-tuning a deep network trained with the standard cross-entropy loss is a\n",
      "strong baseline for few-shot learning. When fine-tuned transductively, this\n",
      "outperforms the current state-of-the-art on standard datasets such as\n",
      "Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same\n",
      "hyper-parameters. The simplicity of this approach enables us to demonstrate the\n",
      "first few-shot learning results on the ImageNet-21k dataset. We find that using\n",
      "a large number of meta-training classes results in high few-shot accuracies\n",
      "even for a large number of few-shot classes. We do not advocate our approach as\n",
      "the solution for few-shot learning, but simply use the results to highlight\n",
      "limitations of current benchmarks and few-shot protocols. We perform extensive\n",
      "studies on benchmark datasets to propose a metric that quantifies the\n",
      "\"hardness\" of a few-shot episode. This metric can be used to report the\n",
      "performance of few-shot algorithms in a more systematic way.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.09178 \n",
      "Title :A general model for plane-based clustering with loss function\n",
      "  In this paper, we propose a general model for plane-based clustering. The\n",
      "general model contains many existing plane-based clustering methods, e.g.,\n",
      "k-plane clustering (kPC), proximal plane clustering (PPC), twin support vector\n",
      "clustering (TWSVC) and its extensions. Under this general model, one may obtain\n",
      "an appropriate clustering method for specific purpose. The general model is a\n",
      "procedure corresponding to an optimization problem, where the optimization\n",
      "problem minimizes the total loss of the samples. Thereinto, the loss of a\n",
      "sample derives from both within-cluster and between-cluster. In theory, the\n",
      "termination conditions are discussed, and we prove that the general model\n",
      "terminates in a finite number of steps at a local or weak local optimal point.\n",
      "Furthermore, based on this general model, we propose a plane-based clustering\n",
      "method by introducing a new loss function to capture the data distribution\n",
      "precisely. Experimental results on artificial and public available datasets\n",
      "verify the effectiveness of the proposed method.\n",
      "\n",
      "**Paper Id :2008.09994 \n",
      "Title :Discriminative Residual Analysis for Image Set Classification with\n",
      "  Posture and Age Variations\n",
      "  Image set recognition has been widely applied in many practical problems like\n",
      "real-time video retrieval and image caption tasks. Due to its superior\n",
      "performance, it has grown into a significant topic in recent years. However,\n",
      "images with complicated variations, e.g., postures and human ages, are\n",
      "difficult to address, as these variations are continuous and gradual with\n",
      "respect to image appearance. Consequently, the crucial point of image set\n",
      "recognition is to mine the intrinsic connection or structural information from\n",
      "the image batches with variations. In this work, a Discriminant Residual\n",
      "Analysis (DRA) method is proposed to improve the classification performance by\n",
      "discovering discriminant features in related and unrelated groups.\n",
      "Specifically, DRA attempts to obtain a powerful projection which casts the\n",
      "residual representations into a discriminant subspace. Such a projection\n",
      "subspace is expected to magnify the useful information of the input space as\n",
      "much as possible, then the relation between the training set and the test set\n",
      "described by the given metric or distance will be more precise in the\n",
      "discriminant subspace. We also propose a nonfeasance strategy by defining\n",
      "another approach to construct the unrelated groups, which help to reduce\n",
      "furthermore the cost of sampling errors. Two regularization approaches are used\n",
      "to deal with the probable small sample size problem. Extensive experiments are\n",
      "conducted on benchmark databases, and the results show superiority and\n",
      "efficiency of the new methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.10435 \n",
      "Title :A Deep Learning Framework for Assessing Physical Rehabilitation\n",
      "  Exercises\n",
      "  Computer-aided assessment of physical rehabilitation entails evaluation of\n",
      "patient performance in completing prescribed rehabilitation exercises, based on\n",
      "processing movement data captured with a sensory system. Despite the essential\n",
      "role of rehabilitation assessment toward improved patient outcomes and reduced\n",
      "healthcare costs, existing approaches lack versatility, robustness, and\n",
      "practical relevance. In this paper, we propose a deep learning-based framework\n",
      "for automated assessment of the quality of physical rehabilitation exercises.\n",
      "The main components of the framework are metrics for quantifying movement\n",
      "performance, scoring functions for mapping the performance metrics into\n",
      "numerical scores of movement quality, and deep neural network models for\n",
      "generating quality scores of input movements via supervised learning. The\n",
      "proposed performance metric is defined based on the log-likelihood of a\n",
      "Gaussian mixture model, and encodes low-dimensional data representation\n",
      "obtained with a deep autoencoder network. The proposed deep spatio-temporal\n",
      "neural network arranges data into temporal pyramids, and exploits the spatial\n",
      "characteristics of human movements by using sub-networks to process joint\n",
      "displacements of individual body parts. The presented framework is validated\n",
      "using a dataset of ten rehabilitation exercises. The significance of this work\n",
      "is that it is the first that implements deep neural networks for assessment of\n",
      "rehabilitation performance.\n",
      "\n",
      "**Paper Id :1910.08978 \n",
      "Title :Attention Enriched Deep Learning Model for Breast Tumor Segmentation in\n",
      "  Ultrasound Images\n",
      "  Incorporating human domain knowledge for breast tumor diagnosis is\n",
      "challenging, since shape, boundary, curvature, intensity, or other common\n",
      "medical priors vary significantly across patients and cannot be employed. This\n",
      "work proposes a new approach for integrating visual saliency into a deep\n",
      "learning model for breast tumor segmentation in ultrasound images. Visual\n",
      "saliency refers to image maps containing regions that are more likely to\n",
      "attract radiologists visual attention. The proposed approach introduces\n",
      "attention blocks into a U-Net architecture, and learns feature representations\n",
      "that prioritize spatial regions with high saliency levels. The validation\n",
      "results demonstrate increased accuracy for tumor segmentation relative to\n",
      "models without salient attention layers. The approach achieved a Dice\n",
      "similarity coefficient of 90.5 percent on a dataset of 510 images. The salient\n",
      "attention model has potential to enhance accuracy and robustness in processing\n",
      "medical images of other organs, by providing a means to incorporate\n",
      "task-specific knowledge into deep learning architectures.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1901.10583 \n",
      "Title :Automatic end-to-end De-identification: Is high accuracy the only\n",
      "  metric?\n",
      "  De-identification of electronic health records (EHR) is a vital step towards\n",
      "advancing health informatics research and maximising the use of available data.\n",
      "It is a two-step process where step one is the identification of protected\n",
      "health information (PHI), and step two is replacing such PHI with surrogates.\n",
      "Despite the recent advances in automatic de-identification of EHR, significant\n",
      "obstacles remain if the abundant health data available are to be used to the\n",
      "full potential. Accuracy in de-identification could be considered a necessary,\n",
      "but not sufficient condition for the use of EHR without individual patient\n",
      "consent. We present here a comprehensive review of the progress to date, both\n",
      "the impressive successes in achieving high accuracy and the significant risks\n",
      "and challenges that remain. To best of our knowledge, this is the first paper\n",
      "to present a complete picture of end-to-end automatic de-identification. We\n",
      "review 18 recently published automatic de-identification systems -designed to\n",
      "de-identify EHR in the form of free text- to show the advancements made in\n",
      "improving the overall accuracy of the system, and in identifying individual\n",
      "PHI. We argue that despite the improvements in accuracy there remain challenges\n",
      "in surrogate generation and replacements of identified PHIs, and the risks\n",
      "posed to patient protection and privacy.\n",
      "\n",
      "**Paper Id :2009.07608 \n",
      "Title :Deep Learning in Photoacoustic Tomography: Current approaches and future\n",
      "  directions\n",
      "  Biomedical photoacoustic tomography, which can provide high resolution 3D\n",
      "soft tissue images based on the optical absorption, has advanced to the stage\n",
      "at which translation from the laboratory to clinical settings is becoming\n",
      "possible. The need for rapid image formation and the practical restrictions on\n",
      "data acquisition that arise from the constraints of a clinical workflow are\n",
      "presenting new image reconstruction challenges. There are many classical\n",
      "approaches to image reconstruction, but ameliorating the effects of incomplete\n",
      "or imperfect data through the incorporation of accurate priors is challenging\n",
      "and leads to slow algorithms. Recently, the application of Deep Learning, or\n",
      "deep neural networks, to this problem has received a great deal of attention.\n",
      "This paper reviews the literature on learned image reconstruction, summarising\n",
      "the current trends, and explains how these new approaches fit within, and to\n",
      "some extent have arisen from, a framework that encompasses classical\n",
      "reconstruction methods. In particular, it shows how these new techniques can be\n",
      "understood from a Bayesian perspective, providing useful insights. The paper\n",
      "also provides a concise tutorial demonstration of three prototypical approaches\n",
      "to learned image reconstruction. The code and data sets for these\n",
      "demonstrations are available to researchers. It is anticipated that it is in in\n",
      "vivo applications - where data may be sparse, fast imaging critical and priors\n",
      "difficult to construct by hand - that Deep Learning will have the most impact.\n",
      "With this in mind, the paper concludes with some indications of possible future\n",
      "research directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.00089 \n",
      "Title :Safe, Efficient, and Comfortable Velocity Control based on Reinforcement\n",
      "  Learning for Autonomous Driving\n",
      "  A model used for velocity control during car following was proposed based on\n",
      "deep reinforcement learning (RL). To fulfil the multi-objectives of car\n",
      "following, a reward function reflecting driving safety, efficiency, and comfort\n",
      "was constructed. With the reward function, the RL agent learns to control\n",
      "vehicle speed in a fashion that maximizes cumulative rewards, through trials\n",
      "and errors in the simulation environment. A total of 1,341 car-following events\n",
      "extracted from the Next Generation Simulation (NGSIM) dataset were used to\n",
      "train the model. Car-following behavior produced by the model were compared\n",
      "with that observed in the empirical NGSIM data, to demonstrate the model's\n",
      "ability to follow a lead vehicle safely, efficiently, and comfortably. Results\n",
      "show that the model demonstrates the capability of safe, efficient, and\n",
      "comfortable velocity control in that it 1) has small percentages (8\\%) of\n",
      "dangerous minimum time to collision values (\\textless\\ 5s) than human drivers\n",
      "in the NGSIM data (35\\%); 2) can maintain efficient and safe headways in the\n",
      "range of 1s to 2s; and 3) can follow the lead vehicle comfortably with smooth\n",
      "acceleration. The results indicate that reinforcement learning methods could\n",
      "contribute to the development of autonomous driving systems.\n",
      "\n",
      "**Paper Id :2005.04755 \n",
      "Title :BayesRace: Learning to race autonomously using prior experience\n",
      "  Autonomous race cars require perception, estimation, planning, and control\n",
      "modules which work together asynchronously while driving at the limit of a\n",
      "vehicle's handling capability. A fundamental challenge encountered in designing\n",
      "these software components lies in predicting the vehicle's future state (e.g.\n",
      "position, orientation, and speed) with high accuracy. The root cause is the\n",
      "difficulty in identifying vehicle model parameters that capture the effects of\n",
      "lateral tire slip. We present a model-based planning and control framework for\n",
      "autonomous racing that significantly reduces the effort required in system\n",
      "identification and control design. Our approach alleviates the gap induced by\n",
      "simulation-based controller design by learning from on-board sensor\n",
      "measurements. A major focus of this work is empirical, thus, we demonstrate our\n",
      "contributions by experiments on validated 1:43 and 1:10 scale autonomous racing\n",
      "simulations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.00177 \n",
      "Title :Critical initialisation in continuous approximations of binary neural\n",
      "  networks\n",
      "  The training of stochastic neural network models with binary ($\\pm1$) weights\n",
      "and activations via continuous surrogate networks is investigated. We derive\n",
      "new surrogates using a novel derivation based on writing the stochastic neural\n",
      "network as a Markov chain. This derivation also encompasses existing variants\n",
      "of the surrogates presented in the literature. Following this, we theoretically\n",
      "study the surrogates at initialisation. We derive, using mean field theory, a\n",
      "set of scalar equations describing how input signals propagate through the\n",
      "randomly initialised networks. The equations reveal whether so-called critical\n",
      "initialisations exist for each surrogate network, where the network can be\n",
      "trained to arbitrary depth. Moreover, we predict theoretically and confirm\n",
      "numerically, that common weight initialisation schemes used in standard\n",
      "continuous networks, when applied to the mean values of the stochastic binary\n",
      "weights, yield poor training performance. This study shows that, contrary to\n",
      "common intuition, the means of the stochastic binary weights should be\n",
      "initialised close to $\\pm 1$, for deeper networks to be trainable.\n",
      "\n",
      "**Paper Id :2006.14619 \n",
      "Title :Recurrent Quantum Neural Networks\n",
      "  Recurrent neural networks are the foundation of many sequence-to-sequence\n",
      "models in machine learning, such as machine translation and speech synthesis.\n",
      "In contrast, applied quantum computing is in its infancy. Nevertheless there\n",
      "already exist quantum machine learning models such as variational quantum\n",
      "eigensolvers which have been used successfully e.g. in the context of energy\n",
      "minimization tasks. In this work we construct a quantum recurrent neural\n",
      "network (QRNN) with demonstrable performance on non-trivial tasks such as\n",
      "sequence learning and integer digit classification. The QRNN cell is built from\n",
      "parametrized quantum neurons, which, in conjunction with amplitude\n",
      "amplification, create a nonlinear activation of polynomials of its inputs and\n",
      "cell state, and allow the extraction of a probability distribution over\n",
      "predicted classes at each step. To study the model's performance, we provide an\n",
      "implementation in pytorch, which allows the relatively efficient optimization\n",
      "of parametrized quantum circuits with thousands of parameters. We establish a\n",
      "QRNN training setup by benchmarking optimization hyperparameters, and analyse\n",
      "suitable network topologies for simple memorisation and sequence prediction\n",
      "tasks from Elman's seminal paper (1990) on temporal structure learning. We then\n",
      "proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN\n",
      "each image pixel-by-pixel; and by utilising modern data augmentation as\n",
      "preprocessing step. Finally, we analyse to what extent the unitary nature of\n",
      "the network counteracts the vanishing gradient problem that plagues many\n",
      "existing quantum classifiers and classical RNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.00450 \n",
      "Title :Time Series Deconfounder: Estimating Treatment Effects over Time in the\n",
      "  Presence of Hidden Confounders\n",
      "  The estimation of treatment effects is a pervasive problem in medicine.\n",
      "Existing methods for estimating treatment effects from longitudinal\n",
      "observational data assume that there are no hidden confounders, an assumption\n",
      "that is not testable in practice and, if it does not hold, leads to biased\n",
      "estimates. In this paper, we develop the Time Series Deconfounder, a method\n",
      "that leverages the assignment of multiple treatments over time to enable the\n",
      "estimation of treatment effects in the presence of multi-cause hidden\n",
      "confounders. The Time Series Deconfounder uses a novel recurrent neural network\n",
      "architecture with multitask output to build a factor model over time and infer\n",
      "latent variables that render the assigned treatments conditionally independent;\n",
      "then, it performs causal inference using these latent variables that act as\n",
      "substitutes for the multi-cause unobserved confounders. We provide a\n",
      "theoretical analysis for obtaining unbiased causal effects of time-varying\n",
      "exposures using the Time Series Deconfounder. Using both simulated and real\n",
      "data we show the effectiveness of our method in deconfounding the estimation of\n",
      "treatment responses over time.\n",
      "\n",
      "**Paper Id :2002.04083 \n",
      "Title :Estimating Counterfactual Treatment Outcomes over Time Through\n",
      "  Adversarially Balanced Representations\n",
      "  Identifying when to give treatments to patients and how to select among\n",
      "multiple treatments over time are important medical problems with a few\n",
      "existing solutions. In this paper, we introduce the Counterfactual Recurrent\n",
      "Network (CRN), a novel sequence-to-sequence model that leverages the\n",
      "increasingly available patient observational data to estimate treatment effects\n",
      "over time and answer such medical questions. To handle the bias from\n",
      "time-varying confounders, covariates affecting the treatment assignment policy\n",
      "in the observational data, CRN uses domain adversarial training to build\n",
      "balancing representations of the patient history. At each timestep, CRN\n",
      "constructs a treatment invariant representation which removes the association\n",
      "between patient history and treatment assignments and thus can be reliably used\n",
      "for making counterfactual predictions. On a simulated model of tumour growth,\n",
      "with varying degree of time-dependent confounding, we show how our model\n",
      "achieves lower error in estimating counterfactuals and in choosing the correct\n",
      "treatment and timing of treatment than current state-of-the-art methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.00505 \n",
      "Title :Differentiable Grammars for Videos\n",
      "  This paper proposes a novel algorithm which learns a formal regular grammar\n",
      "from real-world continuous data, such as videos. Learning latent terminals,\n",
      "non-terminals, and production rules directly from continuous data allows the\n",
      "construction of a generative model capturing sequential structures with\n",
      "multiple possibilities. Our model is fully differentiable, and provides easily\n",
      "interpretable results which are important in order to understand the learned\n",
      "structures. It outperforms the state-of-the-art on several challenging datasets\n",
      "and is more accurate for forecasting future activities in videos. We plan to\n",
      "open-source the code. https://sites.google.com/view/differentiable-grammars\n",
      "\n",
      "**Paper Id :2003.05189 \n",
      "Title :Convolutional Kernel Networks for Graph-Structured Data\n",
      "  We introduce a family of multilayer graph kernels and establish new links\n",
      "between graph convolutional neural networks and kernel methods. Our approach\n",
      "generalizes convolutional kernel networks to graph-structured data, by\n",
      "representing graphs as a sequence of kernel feature maps, where each node\n",
      "carries information about local graph substructures. On the one hand, the\n",
      "kernel point of view offers an unsupervised, expressive, and easy-to-regularize\n",
      "data representation, which is useful when limited samples are available. On the\n",
      "other hand, our model can also be trained end-to-end on large-scale data,\n",
      "leading to new types of graph convolutional neural networks. We show that our\n",
      "method achieves competitive performance on several graph classification\n",
      "benchmarks, while offering simple model interpretation. Our code is freely\n",
      "available at https://github.com/claying/GCKN.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.02119 \n",
      "Title :Mol-CycleGAN - a generative model for molecular optimization\n",
      "  Designing a molecule with desired properties is one of the biggest challenges\n",
      "in drug development, as it requires optimization of chemical compound\n",
      "structures with respect to many complex properties. To augment the compound\n",
      "design process we introduce Mol-CycleGAN - a CycleGAN-based model that\n",
      "generates optimized compounds with high structural similarity to the original\n",
      "ones. Namely, given a molecule our model generates a structurally similar one\n",
      "with an optimized value of the considered property. We evaluate the performance\n",
      "of the model on selected optimization objectives related to structural\n",
      "properties (presence of halogen groups, number of aromatic rings) and to a\n",
      "physicochemical property (penalized logP). In the task of optimization of\n",
      "penalized logP of drug-like molecules our model significantly outperforms\n",
      "previous results.\n",
      "\n",
      "**Paper Id :1909.11655 \n",
      "Title :Augmenting Genetic Algorithms with Deep Neural Networks for Exploring\n",
      "  the Chemical Space\n",
      "  Challenges in natural sciences can often be phrased as optimization problems.\n",
      "Machine learning techniques have recently been applied to solve such problems.\n",
      "One example in chemistry is the design of tailor-made organic materials and\n",
      "molecules, which requires efficient methods to explore the chemical space. We\n",
      "present a genetic algorithm (GA) that is enhanced with a neural network (DNN)\n",
      "based discriminator model to improve the diversity of generated molecules and\n",
      "at the same time steer the GA. We show that our algorithm outperforms other\n",
      "generative models in optimization tasks. We furthermore present a way to\n",
      "increase interpretability of genetic algorithms, which helped us to derive\n",
      "design principles.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.02181 \n",
      "Title :Attention in Natural Language Processing\n",
      "  Attention is an increasingly popular mechanism used in a wide range of neural\n",
      "architectures. The mechanism itself has been realized in a variety of formats.\n",
      "However, because of the fast-paced advances in this domain, a systematic\n",
      "overview of attention is still missing. In this article, we define a unified\n",
      "model for attention architectures in natural language processing, with a focus\n",
      "on those designed to work with vector representations of the textual data. We\n",
      "propose a taxonomy of attention models according to four dimensions: the\n",
      "representation of the input, the compatibility function, the distribution\n",
      "function, and the multiplicity of the input and/or output. We present the\n",
      "examples of how prior information can be exploited in attention models and\n",
      "discuss ongoing research efforts and open challenges in the area, providing the\n",
      "first extensive categorization of the vast body of literature in this exciting\n",
      "domain.\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.02495 \n",
      "Title :Cost-Effective Incentive Allocation via Structured Counterfactual\n",
      "  Inference\n",
      "  We address a practical problem ubiquitous in modern marketing campaigns, in\n",
      "which a central agent tries to learn a policy for allocating strategic\n",
      "financial incentives to customers and observes only bandit feedback. In\n",
      "contrast to traditional policy optimization frameworks, we take into account\n",
      "the additional reward structure and budget constraints common in this setting,\n",
      "and develop a new two-step method for solving this constrained counterfactual\n",
      "policy optimization problem. Our method first casts the reward estimation\n",
      "problem as a domain adaptation problem with supplementary structure, and then\n",
      "subsequently uses the estimators for optimizing the policy with constraints. We\n",
      "also establish theoretical error bounds for our estimation procedure and we\n",
      "empirically show that the approach leads to significant improvement on both\n",
      "synthetic and real datasets.\n",
      "\n",
      "**Paper Id :2002.09615 \n",
      "Title :Preference Modeling with Context-Dependent Salient Features\n",
      "  We consider the problem of estimating a ranking on a set of items from noisy\n",
      "pairwise comparisons given item features. We address the fact that pairwise\n",
      "comparison data often reflects irrational choice, e.g. intransitivity. Our key\n",
      "observation is that two items compared in isolation from other items may be\n",
      "compared based on only a salient subset of features. Formalizing this\n",
      "framework, we propose the salient feature preference model and prove a finite\n",
      "sample complexity result for learning the parameters of our model and the\n",
      "underlying ranking with maximum likelihood estimation. We also provide\n",
      "empirical results that support our theoretical bounds and illustrate how our\n",
      "model explains systematic intransitivity. Finally we demonstrate strong\n",
      "performance of maximum likelihood estimation of our model on both synthetic\n",
      "data and two real data sets: the UT Zappos50K data set and comparison data\n",
      "about the compactness of legislative districts in the US.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.03865 \n",
      "Title :Deep learning approach based on dimensionality reduction for designing\n",
      "  electromagnetic nanostructures\n",
      "  In this paper, we demonstrate a computationally efficient new approach based\n",
      "on deep learning (DL) techniques for analysis, design, and optimization of\n",
      "electromagnetic (EM) nanostructures. We use the strong correlation among\n",
      "features of a generic EM problem to considerably reduce the dimensionality of\n",
      "the problem and thus, the computational complexity, without imposing\n",
      "considerable errors. By employing the dimensionality reduction concept using\n",
      "the more recently demonstrated autoencoder technique, we redefine the\n",
      "conventional many-to-one design problem in EM nanostructures into a one-to-one\n",
      "problem plus a much simpler many-to-one problem, which can be simply solved\n",
      "using an analytic formulation. This approach reduces the computational\n",
      "complexity in solving both the forward problem (i.e., analysis) and the inverse\n",
      "problem (i.e., design) by orders of magnitude compared to conventional\n",
      "approaches. In addition, it provides analytic formulations that, despite their\n",
      "complexity, can be used to obtain intuitive understanding of the physics and\n",
      "dynamics of EM wave interaction with nanostructures with minimal computation\n",
      "requirements. As a proof-of-concept, we applied such an efficacious method to\n",
      "design a new class of on-demand reconfigurable optical metasurfaces based on\n",
      "phase-change materials (PCM). We envision that the integration of such a\n",
      "DL-based technique with full-wave commercial software packages offers a\n",
      "powerful toolkit to facilitate the analysis, design, and optimization of the EM\n",
      "nanostructures as well as explaining, understanding, and predicting the\n",
      "observed responses in such structures.\n",
      "\n",
      "**Paper Id :2007.00936 \n",
      "Title :Deep Neural Networks for Nonlinear Model Order Reduction of Unsteady\n",
      "  Flows\n",
      "  Unsteady fluid systems are nonlinear high-dimensional dynamical systems that\n",
      "may exhibit multiple complex phenomena both in time and space. Reduced Order\n",
      "Modeling (ROM) of fluid flows has been an active research topic in the recent\n",
      "decade with the primary goal to decompose complex flows to a set of features\n",
      "most important for future state prediction and control, typically using a\n",
      "dimensionality reduction technique. In this work, a novel data-driven technique\n",
      "based on the power of deep neural networks for reduced order modeling of the\n",
      "unsteady fluid flows is introduced. An autoencoder network is used for\n",
      "nonlinear dimension reduction and feature extraction as an alternative for\n",
      "singular value decomposition (SVD). Then, the extracted features are used as an\n",
      "input for long short-term memory network (LSTM) to predict the velocity field\n",
      "at future time instances. The proposed autoencoder-LSTM method is compared with\n",
      "non-intrusive reduced order models based on dynamic mode decomposition (DMD)\n",
      "and proper orthogonal decomposition (POD). Moreover, an autoencoder-DMD\n",
      "algorithm is introduced for reduced order modeling, which uses the autoencoder\n",
      "network for dimensionality reduction rather than SVD rank truncation. Results\n",
      "show that the autoencoder-LSTM method is considerably capable of predicting\n",
      "fluid flow evolution, where higher values for coefficient of determination\n",
      "$R^{2}$ are obtained using autoencoder-LSTM compared to other models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.04057 \n",
      "Title :Deep autoregressive models for the efficient variational simulation of\n",
      "  many-body quantum systems\n",
      "  Artificial Neural Networks were recently shown to be an efficient\n",
      "representation of highly-entangled many-body quantum states. In practical\n",
      "applications, neural-network states inherit numerical schemes used in\n",
      "Variational Monte Carlo, most notably the use of Markov-Chain Monte-Carlo\n",
      "(MCMC) sampling to estimate quantum expectations. The local stochastic sampling\n",
      "in MCMC caps the potential advantages of neural networks in two ways: (i) Its\n",
      "intrinsic computational cost sets stringent practical limits on the width and\n",
      "depth of the networks, and therefore limits their expressive capacity; (ii) Its\n",
      "difficulty in generating precise and uncorrelated samples can result in\n",
      "estimations of observables that are very far from their true value. Inspired by\n",
      "the state-of-the-art generative models used in machine learning, we propose a\n",
      "specialized Neural Network architecture that supports efficient and exact\n",
      "sampling, completely circumventing the need for Markov Chain sampling. We\n",
      "demonstrate our approach for two-dimensional interacting spin models,\n",
      "showcasing the ability to obtain accurate results on larger system sizes than\n",
      "those currently accessible to neural-network quantum states.\n",
      "\n",
      "**Paper Id :1907.06589 \n",
      "Title :Experimental quantum homodyne tomography via machine learning\n",
      "  Complete characterization of states and processes that occur within quantum\n",
      "devices is crucial for understanding and testing their potential to outperform\n",
      "classical technologies for communications and computing. However, solving this\n",
      "task with current state-of-the-art techniques becomes unwieldy for large and\n",
      "complex quantum systems. Here we realize and experimentally demonstrate a\n",
      "method for complete characterization of a quantum harmonic oscillator based on\n",
      "an artificial neural network known as the restricted Boltzmann machine. We\n",
      "apply the method to optical homodyne tomography and show it to allow full\n",
      "estimation of quantum states based on a smaller amount of experimental data\n",
      "compared to state-of-the-art methods. We link this advantage to reduced\n",
      "overfitting. Although our experiment is in the optical domain, our method\n",
      "provides a way of exploring quantum resources in a broad class of large-scale\n",
      "physical systems, such as superconducting circuits, atomic and molecular\n",
      "ensembles, and optomechanical systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.04376 \n",
      "Title :An adaptive stochastic optimization algorithm for resource allocation\n",
      "  We consider the classical problem of sequential resource allocation where a\n",
      "decision maker must repeatedly divide a budget between several resources, each\n",
      "with diminishing returns. This can be recast as a specific stochastic\n",
      "optimization problem where the objective is to maximize the cumulative reward,\n",
      "or equivalently to minimize the regret. We construct an algorithm that is {\\em\n",
      "adaptive} to the complexity of the problem, expressed in term of the regularity\n",
      "of the returns of the resources, measured by the exponent in the {\\L}ojasiewicz\n",
      "inequality (or by their universal concavity parameter). Our\n",
      "parameter-independent algorithm recovers the optimal rates for strongly-concave\n",
      "functions and the classical fast rates of multi-armed bandit (for linear reward\n",
      "functions). Moreover, the algorithm improves existing results on stochastic\n",
      "optimization in this regret minimization setting for intermediate cases.\n",
      "\n",
      "**Paper Id :2007.01160 \n",
      "Title :Tight Bounds on Minimax Regret under Logarithmic Loss via\n",
      "  Self-Concordance\n",
      "  We consider the classical problem of sequential probability assignment under\n",
      "logarithmic loss while competing against an arbitrary, potentially\n",
      "nonparametric class of experts. We obtain tight bounds on the minimax regret\n",
      "via a new approach that exploits the self-concordance property of the\n",
      "logarithmic loss. We show that for any expert class with (sequential) metric\n",
      "entropy $\\mathcal{O}(\\gamma^{-p})$ at scale $\\gamma$, the minimax regret is\n",
      "$\\mathcal{O}(n^{p/(p+1)})$, and that this rate cannot be improved without\n",
      "additional assumptions on the expert class under consideration. As an\n",
      "application of our techniques, we resolve the minimax regret for nonparametric\n",
      "Lipschitz classes of experts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.06740 \n",
      "Title :Leveraging Communication Topologies Between Learning Agents in Deep\n",
      "  Reinforcement Learning\n",
      "  A common technique to improve learning performance in deep reinforcement\n",
      "learning (DRL) and many other machine learning algorithms is to run multiple\n",
      "learning agents in parallel. A neglected component in the development of these\n",
      "algorithms has been how best to arrange the learning agents involved to improve\n",
      "distributed search. Here we draw upon results from the networked optimization\n",
      "literatures suggesting that arranging learning agents in communication networks\n",
      "other than fully connected topologies (the implicit way agents are commonly\n",
      "arranged in) can improve learning. We explore the relative performance of four\n",
      "popular families of graphs and observe that one such family (Erdos-Renyi random\n",
      "graphs) empirically outperforms the de facto fully-connected communication\n",
      "topology across several DRL benchmark tasks. Additionally, we observe that 1000\n",
      "learning agents arranged in an Erdos-Renyi graph can perform as well as 3000\n",
      "agents arranged in the standard fully-connected topology, showing the large\n",
      "learning improvement possible when carefully designing the topology over which\n",
      "agents communicate. We complement these empirical results with a theoretical\n",
      "investigation of why our alternate topologies perform better. Overall, our work\n",
      "suggests that distributed machine learning algorithms could be made more\n",
      "effective if the communication topology between learning agents was optimized.\n",
      "\n",
      "**Paper Id :2004.04704 \n",
      "Title :Heuristics for Link Prediction in Multiplex Networks\n",
      "  Link prediction, or the inference of future or missing connections between\n",
      "entities, is a well-studied problem in network analysis. A multitude of\n",
      "heuristics exist for link prediction in ordinary networks with a single type of\n",
      "connection. However, link prediction in multiplex networks, or networks with\n",
      "multiple types of connections, is not a well understood problem. We propose a\n",
      "novel general framework and three families of heuristics for multiplex network\n",
      "link prediction that are simple, interpretable, and take advantage of the rich\n",
      "connection type correlation structure that exists in many real world networks.\n",
      "We further derive a theoretical threshold for determining when to use a\n",
      "different connection type based on the number of links that overlap with an\n",
      "Erdos-Renyi random graph. Through experiments with simulated and real world\n",
      "scientific collaboration, transportation and global trade networks, we\n",
      "demonstrate that the proposed heuristics show increased performance with the\n",
      "richness of connection type correlation structure and significantly outperform\n",
      "their baseline heuristics for ordinary networks with a single connection type.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.08234 \n",
      "Title :An Empirical Study of Large-Batch Stochastic Gradient Descent with\n",
      "  Structured Covariance Noise\n",
      "  The choice of batch-size in a stochastic optimization algorithm plays a\n",
      "substantial role for both optimization and generalization. Increasing the\n",
      "batch-size used typically improves optimization but degrades generalization. To\n",
      "address the problem of improving generalization while maintaining optimal\n",
      "convergence in large-batch training, we propose to add covariance noise to the\n",
      "gradients. We demonstrate that the learning performance of our method is more\n",
      "accurately captured by the structure of the covariance matrix of the noise\n",
      "rather than by the variance of gradients. Moreover, over the convex-quadratic,\n",
      "we prove in theory that it can be characterized by the Frobenius norm of the\n",
      "noise matrix. Our empirical studies with standard deep learning\n",
      "model-architectures and datasets shows that our method not only improves\n",
      "generalization performance in large-batch training, but furthermore, does so in\n",
      "a way where the optimization performance remains desirable and the training\n",
      "duration is not elongated.\n",
      "\n",
      "**Paper Id :2007.02040 \n",
      "Title :Discount Factor as a Regularizer in Reinforcement Learning\n",
      "  Specifying a Reinforcement Learning (RL) task involves choosing a suitable\n",
      "planning horizon, which is typically modeled by a discount factor. It is known\n",
      "that applying RL algorithms with a lower discount factor can act as a\n",
      "regularizer, improving performance in the limited data regime. Yet the exact\n",
      "nature of this regularizer has not been investigated. In this work, we fill in\n",
      "this gap. For several Temporal-Difference (TD) learning methods, we show an\n",
      "explicit equivalence between using a reduced discount factor and adding an\n",
      "explicit regularization term to the algorithm's loss. Motivated by the\n",
      "equivalence, we empirically study this technique compared to standard $L_2$\n",
      "regularization by extensive experiments in discrete and continuous domains,\n",
      "using tabular and functional representations. Our experiments suggest the\n",
      "regularization effectiveness is strongly related to properties of the available\n",
      "data, such as size, distribution, and mixing rate.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.08276 \n",
      "Title :End-to-End Jet Classification of Quarks and Gluons with the CMS Open\n",
      "  Data\n",
      "  We describe the construction of end-to-end jet image classifiers based on\n",
      "simulated low-level detector data to discriminate quark- vs. gluon-initiated\n",
      "jets with high-fidelity simulated CMS Open Data. We highlight the importance of\n",
      "precise spatial information and demonstrate competitive performance to existing\n",
      "state-of-the-art jet classifiers. We further generalize the end-to-end approach\n",
      "to event-level classification of quark vs. gluon di-jet QCD events. We compare\n",
      "the fully end-to-end approach to using hand-engineered features and demonstrate\n",
      "that the end-to-end algorithm is robust against the effects of underlying event\n",
      "and pile-up.\n",
      "\n",
      "**Paper Id :1807.11916 \n",
      "Title :End-to-End Physics Event Classification with CMS Open Data: Applying\n",
      "  Image-Based Deep Learning to Detector Data for the Direct Classification of\n",
      "  Collision Events at the LHC\n",
      "  This paper describes the construction of novel end-to-end image-based\n",
      "classifiers that directly leverage low-level simulated detector data to\n",
      "discriminate signal and background processes in pp collision events at the\n",
      "Large Hadron Collider at CERN. To better understand what end-to-end classifiers\n",
      "are capable of learning from the data and to address a number of associated\n",
      "challenges, we distinguish the decay of the standard model Higgs boson into two\n",
      "photons from its leading background sources using high-fidelity simulated CMS\n",
      "Open Data. We demonstrate the ability of end-to-end classifiers to learn from\n",
      "the angular distribution of the photons recorded as electromagnetic showers,\n",
      "their intrinsic shapes, and the energy of their constituent hits, even when the\n",
      "underlying particles are not fully resolved, delivering a clear advantage in\n",
      "such cases over purely kinematics-based classifiers.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.08753 \n",
      "Title :Quantum Learning Boolean Linear Functions w.r.t. Product Distributions\n",
      "  The problem of learning Boolean linear functions from quantum examples w.r.t.\n",
      "the uniform distribution can be solved on a quantum computer using the\n",
      "Bernstein-Vazirani algorithm. A similar strategy can be applied in the case of\n",
      "noisy quantum training data, as was observed in arXiv:1702.08255v2 [quant-ph].\n",
      "However, extensions of these learning algorithms beyond the uniform\n",
      "distribution have not yet been studied. We employ the biased quantum Fourier\n",
      "transform introduced in arXiv:1802.05690v2 [quant-ph] to develop efficient\n",
      "quantum algorithms for learning Boolean linear functions on $n$ bits from\n",
      "quantum examples w.r.t. a biased product distribution. Our first procedure is\n",
      "applicable to any (except full) bias and requires $\\mathcal{O}(\\ln (n))$\n",
      "quantum examples. The number of quantum examples used by our second algorithm\n",
      "is independent of $n$, but the strategy is applicable only for small bias.\n",
      "Moreover, we show that the second procedure is stable w.r.t. noisy training\n",
      "data and w.r.t. faulty quantum gates. This also enables us to solve a version\n",
      "of the learning problem in which the underlying distribution is not known in\n",
      "advance. Finally, we prove lower bounds on the classical and quantum sample\n",
      "complexities of the learning problem. Whereas classically, $\\Omega (n)$\n",
      "examples are necessary independently of the bias, we are able to establish a\n",
      "quantum sample complexity lower bound of $\\Omega (\\ln (n))$ only under an\n",
      "assumption of large bias. Nevertheless, this allows for a discussion of the\n",
      "performance of our suggested learning algorithms w.r.t. sample complexity. With\n",
      "our analysis we contribute to a more quantitative understanding of the power\n",
      "and limitations of quantum training data for learning classical functions.\n",
      "\n",
      "**Paper Id :1907.10552 \n",
      "Title :A neural network oracle for quantum nonlocality problems in networks\n",
      "  Characterizing quantum nonlocality in networks is a challenging, but\n",
      "important problem. Using quantum sources one can achieve distributions which\n",
      "are unattainable classically. A key point in investigations is to decide\n",
      "whether an observed probability distribution can be reproduced using only\n",
      "classical resources. This causal inference task is challenging even for simple\n",
      "networks, both analytically and using standard numerical techniques. We propose\n",
      "to use neural networks as numerical tools to overcome these challenges, by\n",
      "learning the classical strategies required to reproduce a distribution. As\n",
      "such, the neural network acts as an oracle, demonstrating that a behavior is\n",
      "classical if it can be learned. We apply our method to several examples in the\n",
      "triangle configuration. After demonstrating that the method is consistent with\n",
      "previously known results, we give solid evidence that the distribution\n",
      "presented in [N. Gisin, Entropy 21(3), 325 (2019)] is indeed nonlocal as\n",
      "conjectured. Finally we examine the genuinely nonlocal distribution presented\n",
      "in [M.-O. Renou et al., PRL 123, 140401 (2019)], and, guided by the findings of\n",
      "the neural network, conjecture nonlocality in a new range of parameters in\n",
      "these distributions. The method allows us to get an estimate on the noise\n",
      "robustness of all examined distributions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.09216 \n",
      "Title :Revealing quantum chaos with machine learning\n",
      "  Understanding properties of quantum matter is an outstanding challenge in\n",
      "science. In this paper, we demonstrate how machine-learning methods can be\n",
      "successfully applied for the classification of various regimes in\n",
      "single-particle and many-body systems. We realize neural network algorithms\n",
      "that perform a classification between regular and chaotic behavior in quantum\n",
      "billiard models with remarkably high accuracy. We use the variational\n",
      "autoencoder for autosupervised classification of regular/chaotic wave\n",
      "functions, as well as demonstrating that variational autoencoders could be used\n",
      "as a tool for detection of anomalous quantum states, such as quantum scars. By\n",
      "taking this method further, we show that machine learning techniques allow us\n",
      "to pin down the transition from integrability to many-body quantum chaos in\n",
      "Heisenberg XXZ spin chains. For both cases, we confirm the existence of\n",
      "universal W shapes that characterize the transition. Our results pave the way\n",
      "for exploring the power of machine learning tools for revealing exotic\n",
      "phenomena in quantum many-body systems.\n",
      "\n",
      "**Paper Id :2002.04613 \n",
      "Title :Neural network wave functions and the sign problem\n",
      "  Neural quantum states (NQS) are a promising approach to study many-body\n",
      "quantum physics. However, they face a major challenge when applied to lattice\n",
      "models: Convolutional networks struggle to converge to ground states with a\n",
      "nontrivial sign structure. We tackle this problem by proposing a neural network\n",
      "architecture with a simple, explicit, and interpretable phase ansatz, which can\n",
      "robustly represent such states and achieve state-of-the-art variational\n",
      "energies for both conventional and frustrated antiferromagnets. In the latter\n",
      "case, our approach uncovers low-energy states that exhibit the Marshall sign\n",
      "rule and are therefore inconsistent with the expected ground state. Such states\n",
      "are the likely cause of the obstruction for NQS-based variational Monte Carlo\n",
      "to access the true ground states of these systems. We discuss the implications\n",
      "of this observation and suggest potential strategies to overcome the problem.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.09705 \n",
      "Title :Beyond the Self: Using Grounded Affordances to Interpret and Describe\n",
      "  Others' Actions\n",
      "  We propose a developmental approach that allows a robot to interpret and\n",
      "describe the actions of human agents by reusing previous experience. The robot\n",
      "first learns the association between words and object affordances by\n",
      "manipulating the objects in its environment. It then uses this information to\n",
      "learn a mapping between its own actions and those performed by a human in a\n",
      "shared environment. It finally fuses the information from these two models to\n",
      "interpret and describe human actions in light of its own experience. In our\n",
      "experiments, we show that the model can be used flexibly to do inference on\n",
      "different aspects of the scene. We can predict the effects of an action on the\n",
      "basis of object properties. We can revise the belief that a certain action\n",
      "occurred, given the observed effects of the human action. In an early action\n",
      "recognition fashion, we can anticipate the effects when the action has only\n",
      "been partially observed. By estimating the probability of words given the\n",
      "evidence and feeding them into a pre-defined grammar, we can generate relevant\n",
      "descriptions of the scene. We believe that this is a step towards providing\n",
      "robots with the fundamental skills to engage in social collaboration with\n",
      "humans.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.10379 \n",
      "Title :Can learning from natural image denoising be used for seismic data\n",
      "  interpolation?\n",
      "  We propose a convolutional neural network (CNN) denoising based method for\n",
      "seismic data interpolation. It provides a simple and efficient way to break\n",
      "though the lack problem of geophysical training labels that are often required\n",
      "by deep learning methods. The new method consists of two steps: (1) Train a set\n",
      "of CNN denoisers from natural image clean-noisy pairs to learn denoising; (2)\n",
      "Integrate the trained CNN denoisers into project onto convex set (POCS)\n",
      "framework to perform seismic data interpolation. The method alleviates the\n",
      "demanding of seismic big data with similar features as applications of\n",
      "end-to-end deep learning on seismic data interpolation. Additionally, the\n",
      "proposed method is flexible for many cases of traces missing because missing\n",
      "cases are not involved in the training step, and thus it is of plug-and-play\n",
      "nature. These indicate the high generalizability of our approach and the\n",
      "reduction of the need of the problem-specific training. Primary results on\n",
      "synthetic and field data show promising interpolation performances of the\n",
      "presented CNN-POCS method in terms of signal-to-noise ratio, de-aliasing and\n",
      "weak-feature reconstruction, in comparison with traditional $f$-$x$ prediction\n",
      "filtering and curvelet transform based POCS methods.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1902.10445 \n",
      "Title :Efficient Learning for Deep Quantum Neural Networks\n",
      "  Neural networks enjoy widespread success in both research and industry and,\n",
      "with the imminent advent of quantum technology, it is now a crucial challenge\n",
      "to design quantum neural networks for fully quantum learning tasks. Here we\n",
      "propose the use of quantum neurons as a building block for quantum feed-forward\n",
      "neural networks capable of universal quantum computation. We describe the\n",
      "efficient training of these networks using the fidelity as a cost function and\n",
      "provide both classical and efficient quantum implementations. Our method allows\n",
      "for fast optimisation with reduced memory requirements: the number of qudits\n",
      "required scales with only the width, allowing the optimisation of deep\n",
      "networks. We benchmark our proposal for the quantum task of learning an unknown\n",
      "unitary and find remarkable generalisation behaviour and a striking robustness\n",
      "to noisy training data.\n",
      "\n",
      "**Paper Id :2006.14619 \n",
      "Title :Recurrent Quantum Neural Networks\n",
      "  Recurrent neural networks are the foundation of many sequence-to-sequence\n",
      "models in machine learning, such as machine translation and speech synthesis.\n",
      "In contrast, applied quantum computing is in its infancy. Nevertheless there\n",
      "already exist quantum machine learning models such as variational quantum\n",
      "eigensolvers which have been used successfully e.g. in the context of energy\n",
      "minimization tasks. In this work we construct a quantum recurrent neural\n",
      "network (QRNN) with demonstrable performance on non-trivial tasks such as\n",
      "sequence learning and integer digit classification. The QRNN cell is built from\n",
      "parametrized quantum neurons, which, in conjunction with amplitude\n",
      "amplification, create a nonlinear activation of polynomials of its inputs and\n",
      "cell state, and allow the extraction of a probability distribution over\n",
      "predicted classes at each step. To study the model's performance, we provide an\n",
      "implementation in pytorch, which allows the relatively efficient optimization\n",
      "of parametrized quantum circuits with thousands of parameters. We establish a\n",
      "QRNN training setup by benchmarking optimization hyperparameters, and analyse\n",
      "suitable network topologies for simple memorisation and sequence prediction\n",
      "tasks from Elman's seminal paper (1990) on temporal structure learning. We then\n",
      "proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN\n",
      "each image pixel-by-pixel; and by utilising modern data augmentation as\n",
      "preprocessing step. Finally, we analyse to what extent the unitary nature of\n",
      "the network counteracts the vanishing gradient problem that plagues many\n",
      "existing quantum classifiers and classical RNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.02787 \n",
      "Title :GRATIS: GeneRAting TIme Series with diverse and controllable\n",
      "  characteristics\n",
      "  The explosion of time series data in recent years has brought a flourish of\n",
      "new time series analysis methods, for forecasting, clustering, classification\n",
      "and other tasks. The evaluation of these new methods requires either collecting\n",
      "or simulating a diverse set of time series benchmarking data to enable reliable\n",
      "comparisons against alternative approaches. We propose GeneRAting TIme Series\n",
      "with diverse and controllable characteristics, named GRATIS, with the use of\n",
      "mixture autoregressive (MAR) models. We simulate sets of time series using MAR\n",
      "models and investigate the diversity and coverage of the generated time series\n",
      "in a time series feature space. By tuning the parameters of the MAR models,\n",
      "GRATIS is also able to efficiently generate new time series with controllable\n",
      "features. In general, as a costless surrogate to the traditional data\n",
      "collection approach, GRATIS can be used as an evaluation tool for tasks such as\n",
      "time series forecasting and classification. We illustrate the usefulness of our\n",
      "time series generation process through a time series forecasting application.\n",
      "\n",
      "**Paper Id :1904.08064 \n",
      "Title :Forecasting with time series imaging\n",
      "  Feature-based time series representations have attracted substantial\n",
      "attention in a wide range of time series analysis methods. Recently, the use of\n",
      "time series features for forecast model averaging has been an emerging research\n",
      "focus in the forecasting community. Nonetheless, most of the existing\n",
      "approaches depend on the manual choice of an appropriate set of features.\n",
      "Exploiting machine learning methods to extract features from time series\n",
      "automatically becomes crucial in state-of-the-art time series analysis. In this\n",
      "paper, we introduce an automated approach to extract time series features based\n",
      "on time series imaging. We first transform time series into recurrence plots,\n",
      "from which local features can be extracted using computer vision algorithms.\n",
      "The extracted features are used for forecast model averaging. Our experiments\n",
      "show that forecasting based on automatically extracted features, with less\n",
      "human intervention and a more comprehensive view of the raw time series data,\n",
      "yields highly comparable performances with the best methods in the largest\n",
      "forecasting competition dataset (M4) and outperforms the top methods in the\n",
      "Tourism forecasting competition dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.03096 \n",
      "Title :Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few\n",
      "  Examples\n",
      "  Few-shot classification refers to learning a classifier for new classes given\n",
      "only a few examples. While a plethora of models have emerged to tackle it, we\n",
      "find the procedure and datasets that are used to assess their progress lacking.\n",
      "To address this limitation, we propose Meta-Dataset: a new benchmark for\n",
      "training and evaluating models that is large-scale, consists of diverse\n",
      "datasets, and presents more realistic tasks. We experiment with popular\n",
      "baselines and meta-learners on Meta-Dataset, along with a competitive method\n",
      "that we propose. We analyze performance as a function of various\n",
      "characteristics of test tasks and examine the models' ability to leverage\n",
      "diverse training sources for improving their generalization. We also propose a\n",
      "new set of baselines for quantifying the benefit of meta-learning in\n",
      "Meta-Dataset. Our extensive experimentation has uncovered important research\n",
      "challenges and we hope to inspire work in these directions.\n",
      "\n",
      "**Paper Id :2002.12764 \n",
      "Title :Towards Learning a Universal Non-Semantic Representation of Speech\n",
      "  The ultimate goal of transfer learning is to reduce labeled data requirements\n",
      "by exploiting a pre-existing embedding model trained for different datasets or\n",
      "tasks. The visual and language communities have established benchmarks to\n",
      "compare embeddings, but the speech community has yet to do so. This paper\n",
      "proposes a benchmark for comparing speech representations on non-semantic\n",
      "tasks, and proposes a representation based on an unsupervised triplet-loss\n",
      "objective. The proposed representation outperforms other representations on the\n",
      "benchmark, and even exceeds state-of-the-art performance on a number of\n",
      "transfer learning tasks. The embedding is trained on a publicly available\n",
      "dataset, and it is tested on a variety of low-resource downstream tasks,\n",
      "including personalization tasks and medical domain. The benchmark, models, and\n",
      "evaluation code are publicly released.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.03315 \n",
      "Title :Provable Tensor Ring Completion\n",
      "  Tensor completion recovers a multi-dimensional array from a limited number of\n",
      "measurements. Using the recently proposed tensor ring (TR) decomposition, in\n",
      "this paper we show that a d-order tensor of dimensional size n and TR rank r\n",
      "can be exactly recovered with high probability by solving a convex optimization\n",
      "program, given n^{d/2} r^2 ln^7(n^{d/2})samples. The proposed TR incoherence\n",
      "condition under which the result holds is similar to the matrix incoherence\n",
      "condition. The experiments on synthetic data verify the recovery guarantee for\n",
      "TR completion. Moreover, the experiments on real-world data show that our\n",
      "method improves the recovery performance compared with the state-of-the-art\n",
      "methods.\n",
      "\n",
      "**Paper Id :2002.06524 \n",
      "Title :Tensor denoising and completion based on ordinal observations\n",
      "  Higher-order tensors arise frequently in applications such as neuroimaging,\n",
      "recommendation system, social network analysis, and psychological studies. We\n",
      "consider the problem of low-rank tensor estimation from possibly incomplete,\n",
      "ordinal-valued observations. Two related problems are studied, one on tensor\n",
      "denoising and another on tensor completion. We propose a multi-linear\n",
      "cumulative link model, develop a rank-constrained M-estimator, and obtain\n",
      "theoretical accuracy guarantees. Our mean squared error bound enjoys a faster\n",
      "convergence rate than previous results, and we show that the proposed estimator\n",
      "is minimax optimal under the class of low-rank models. Furthermore, the\n",
      "procedure developed serves as an efficient completion method which guarantees\n",
      "consistent recovery of an order-$K$ $(d,\\ldots,d)$-dimensional low-rank tensor\n",
      "using only $\\tilde{\\mathcal{O}}(Kd)$ noisy, quantized observations. We\n",
      "demonstrate the outperformance of our approach over previous methods on the\n",
      "tasks of clustering and collaborative filtering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.03425 \n",
      "Title :The Ethics of AI Ethics -- An Evaluation of Guidelines\n",
      "  Current advances in research, development and application of artificial\n",
      "intelligence (AI) systems have yielded a far-reaching discourse on AI ethics.\n",
      "In consequence, a number of ethics guidelines have been released in recent\n",
      "years. These guidelines comprise normative principles and recommendations aimed\n",
      "to harness the \"disruptive\" potentials of new AI technologies. Designed as a\n",
      "comprehensive evaluation, this paper analyzes and compares these guidelines\n",
      "highlighting overlaps but also omissions. As a result, I give a detailed\n",
      "overview of the field of AI ethics. Finally, I also examine to what extent the\n",
      "respective ethical principles and values are implemented in the practice of\n",
      "research, development and application of AI systems - and how the effectiveness\n",
      "in the demands of AI ethics can be improved.\n",
      "\n",
      "**Paper Id :1905.08883 \n",
      "Title :Explainable Machine Learning for Scientific Insights and Discoveries\n",
      "  Machine learning methods have been remarkably successful for a wide range of\n",
      "application areas in the extraction of essential information from data. An\n",
      "exciting and relatively recent development is the uptake of machine learning in\n",
      "the natural sciences, where the major goal is to obtain novel scientific\n",
      "insights and discoveries from observational or simulated data. A prerequisite\n",
      "for obtaining a scientific outcome is domain knowledge, which is needed to gain\n",
      "explainability, but also to enhance scientific consistency. In this article we\n",
      "review explainable machine learning in view of applications in the natural\n",
      "sciences and discuss three core elements which we identified as relevant in\n",
      "this context: transparency, interpretability, and explainability. With respect\n",
      "to these core elements, we provide a survey of recent scientific works that\n",
      "incorporate machine learning and the way that explainable machine learning is\n",
      "used in combination with domain knowledge from the application areas.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.07677 \n",
      "Title :Deep Fundamental Factor Models\n",
      "  Deep fundamental factor models are developed to automatically capture\n",
      "non-linearity and interaction effects in factor modeling. Uncertainty\n",
      "quantification provides interpretability with interval estimation, ranking of\n",
      "factor importances and estimation of interaction effects. With no hidden layers\n",
      "we recover a linear factor model and for one or more hidden layers, uncertainty\n",
      "bands for the sensitivity to each input naturally arise from the network\n",
      "weights. Using 3290 assets in the Russell 1000 index over a period of December\n",
      "1989 to January 2018, we assess a 49 factor model and generate information\n",
      "ratios that are approximately 1.5x greater than the OLS factor model.\n",
      "Furthermore, we compare our deep fundamental factor model with a quadratic\n",
      "LASSO model and demonstrate the superior performance and robustness to\n",
      "outliers. The Python source code and the data used for this study are provided.\n",
      "\n",
      "**Paper Id :1808.07452 \n",
      "Title :Generalized Canonical Polyadic Tensor Decomposition\n",
      "  Tensor decomposition is a fundamental unsupervised machine learning method in\n",
      "data science, with applications including network analysis and sensor data\n",
      "processing. This work develops a generalized canonical polyadic (GCP) low-rank\n",
      "tensor decomposition that allows other loss functions besides squared error.\n",
      "For instance, we can use logistic loss or Kullback-Leibler divergence, enabling\n",
      "tensor decomposition for binary or count data. We present a variety\n",
      "statistically-motivated loss functions for various scenarios. We provide a\n",
      "generalized framework for computing gradients and handling missing data that\n",
      "enables the use of standard optimization methods for fitting the model. We\n",
      "demonstrate the flexibility of GCP on several real-world examples including\n",
      "interactions in a social network, neural activity in a mouse, and monthly\n",
      "rainfall measurements in India.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.07789 \n",
      "Title :Predicting Citywide Crowd Flows in Irregular Regions Using Multi-View\n",
      "  Graph Convolutional Networks\n",
      "  Being able to predict the crowd flows in each and every part of a city,\n",
      "especially in irregular regions, is strategically important for traffic\n",
      "control, risk assessment, and public safety. However, it is very challenging\n",
      "because of interactions and spatial correlations between different regions. In\n",
      "addition, it is affected by many factors: i) multiple temporal correlations\n",
      "among different time intervals: closeness, period, trend; ii) complex external\n",
      "influential factors: weather, events; iii) meta features: time of the day, day\n",
      "of the week, and so on. In this paper, we formulate crowd flow forecasting in\n",
      "irregular regions as a spatio-temporal graph (STG) prediction problem in which\n",
      "each node represents a region with time-varying flows. By extending graph\n",
      "convolution to handle the spatial information, we propose using spatial graph\n",
      "convolution to build a multi-view graph convolutional network (MVGCN) for the\n",
      "crowd flow forecasting problem, where different views can capture different\n",
      "factors as mentioned above. We evaluate MVGCN using four real-world datasets\n",
      "(taxicabs and bikes) and extensive experimental results show that our approach\n",
      "outperforms the adaptations of state-of-the-art methods. And we have developed\n",
      "a crowd flow forecasting system for irregular regions that can now be used\n",
      "internally.\n",
      "\n",
      "**Paper Id :2005.01690 \n",
      "Title :Learning Geo-Contextual Embeddings for Commuting Flow Prediction\n",
      "  Predicting commuting flows based on infrastructure and land-use information\n",
      "is critical for urban planning and public policy development. However, it is a\n",
      "challenging task given the complex patterns of commuting flows. Conventional\n",
      "models, such as gravity model, are mainly derived from physics principles and\n",
      "limited by their predictive power in real-world scenarios where many factors\n",
      "need to be considered. Meanwhile, most existing machine learning-based methods\n",
      "ignore the spatial correlations and fail to model the influence of nearby\n",
      "regions. To address these issues, we propose Geo-contextual Multitask Embedding\n",
      "Learner (GMEL), a model that captures the spatial correlations from geographic\n",
      "contextual information for commuting flow prediction. Specifically, we first\n",
      "construct a geo-adjacency network containing the geographic contextual\n",
      "information. Then, an attention mechanism is proposed based on the framework of\n",
      "graph attention network (GAT) to capture the spatial correlations and encode\n",
      "geographic contextual information to embedding space. Two separate GATs are\n",
      "used to model supply and demand characteristics. A multitask learning framework\n",
      "is used to introduce stronger restrictions and enhance the effectiveness of the\n",
      "embedding representation. Finally, a gradient boosting machine is trained based\n",
      "on the learned embeddings to predict commuting flows. We evaluate our model\n",
      "using real-world datasets from New York City and the experimental results\n",
      "demonstrate the effectiveness of our proposal against the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.08066 \n",
      "Title :Trained Quantization Thresholds for Accurate and Efficient Fixed-Point\n",
      "  Inference of Deep Neural Networks\n",
      "  We propose a method of training quantization thresholds (TQT) for uniform\n",
      "symmetric quantizers using standard backpropagation and gradient descent.\n",
      "Contrary to prior work, we show that a careful analysis of the straight-through\n",
      "estimator for threshold gradients allows for a natural range-precision\n",
      "trade-off leading to better optima. Our quantizers are constrained to use\n",
      "power-of-2 scale-factors and per-tensor scaling of weights and activations to\n",
      "make it amenable for hardware implementations. We present analytical support\n",
      "for the general robustness of our methods and empirically validate them on\n",
      "various CNNs for ImageNet classification. We are able to achieve\n",
      "near-floating-point accuracy on traditionally difficult networks such as\n",
      "MobileNets with less than 5 epochs of quantized (8-bit) retraining. Finally, we\n",
      "present Graffitist, a framework that enables automatic quantization of\n",
      "TensorFlow graphs for TQT (available at https://github.com/Xilinx/graffitist ).\n",
      "\n",
      "**Paper Id :1911.09070 \n",
      "Title :EfficientDet: Scalable and Efficient Object Detection\n",
      "  Model efficiency has become increasingly important in computer vision. In\n",
      "this paper, we systematically study neural network architecture design choices\n",
      "for object detection and propose several key optimizations to improve\n",
      "efficiency. First, we propose a weighted bi-directional feature pyramid network\n",
      "(BiFPN), which allows easy and fast multiscale feature fusion; Second, we\n",
      "propose a compound scaling method that uniformly scales the resolution, depth,\n",
      "and width for all backbone, feature network, and box/class prediction networks\n",
      "at the same time. Based on these optimizations and better backbones, we have\n",
      "developed a new family of object detectors, called EfficientDet, which\n",
      "consistently achieve much better efficiency than prior art across a wide\n",
      "spectrum of resource constraints. In particular, with single model and\n",
      "single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO\n",
      "test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using\n",
      "13x - 42x fewer FLOPs than previous detectors. Code is available at\n",
      "https://github.com/google/automl/tree/master/efficientdet.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.09284 \n",
      "Title :Learning Mixtures of Separable Dictionaries for Tensor Data: Analysis\n",
      "  and Algorithms\n",
      "  This work addresses the problem of learning sparse representations of tensor\n",
      "data using structured dictionary learning. It proposes learning a mixture of\n",
      "separable dictionaries to better capture the structure of tensor data by\n",
      "generalizing the separable dictionary learning model. Two different approaches\n",
      "for learning mixture of separable dictionaries are explored and sufficient\n",
      "conditions for local identifiability of the underlying dictionary are derived\n",
      "in each case. Moreover, computational algorithms are developed to solve the\n",
      "problem of learning mixture of separable dictionaries in both batch and online\n",
      "settings. Numerical experiments are used to show the usefulness of the proposed\n",
      "model and the efficacy of the developed algorithms.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.10742 \n",
      "Title :Generative Tensor Network Classification Model for Supervised Machine\n",
      "  Learning\n",
      "  Tensor network (TN) has recently triggered extensive interests in developing\n",
      "machine-learning models in quantum many-body Hilbert space. Here we purpose a\n",
      "generative TN classification (GTNC) approach for supervised learning. The\n",
      "strategy is to train the generative TN for each class of the samples to\n",
      "construct the classifiers. The classification is implemented by comparing the\n",
      "distance in the many-body Hilbert space. The numerical experiments by GTNC show\n",
      "impressive performance on the MNIST and Fashion-MNIST dataset. The testing\n",
      "accuracy is competitive to the state-of-the-art convolutional neural network\n",
      "while higher than the naive Bayes classifier (a generative classifier) and\n",
      "support vector machine. Moreover, GTNC is more efficient than the existing TN\n",
      "models that are in general discriminative. By investigating the distances in\n",
      "the many-body Hilbert space, we find that (a) the samples are naturally\n",
      "clustering in such a space; and (b) bounding the bond dimensions of the TN's to\n",
      "finite values corresponds to removing redundant information in the image\n",
      "recognition. These two characters make GTNC an adaptive and universal model of\n",
      "excellent performance.\n",
      "\n",
      "**Paper Id :2005.03355 \n",
      "Title :Quantum correlation alignment for unsupervised domain adaptation\n",
      "  Correlation alignment (CORAL), a representative domain adaptation (DA)\n",
      "algorithm, decorrelates and aligns a labelled source domain dataset to an\n",
      "unlabelled target domain dataset to minimize the domain shift such that a\n",
      "classifier can be applied to predict the target domain labels. In this paper,\n",
      "we implement the CORAL on quantum devices by two different methods. One method\n",
      "utilizes quantum basic linear algebra subroutines (QBLAS) to implement the\n",
      "CORAL with exponential speedup in the number and dimension of the given data\n",
      "samples. The other method is achieved through a variational hybrid\n",
      "quantum-classical procedure. In addition, the numerical experiments of the\n",
      "CORAL with three different types of data sets, namely the synthetic data, the\n",
      "synthetic-Iris data, the handwritten digit data, are presented to evaluate the\n",
      "performance of our work. The simulation results prove that the variational\n",
      "quantum correlation alignment algorithm (VQCORAL) can achieve competitive\n",
      "performance compared with the classical CORAL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.11373 \n",
      "Title :Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning\n",
      "  Approach to Adaptively Targeting Emergent Drug Resistance\n",
      "  The challenge in controlling stochastic systems in which low-probability\n",
      "events can set the system on catastrophic trajectories is to develop a robust\n",
      "ability to respond to such events without significantly compromising the\n",
      "optimality of the baseline control policy. This paper presents CelluDose, a\n",
      "stochastic simulation-trained deep reinforcement learning adaptive feedback\n",
      "control prototype for automated precision drug dosing targeting stochastic and\n",
      "heterogeneous cell proliferation. Drug resistance can emerge from random and\n",
      "variable mutations in targeted cell populations; in the absence of an\n",
      "appropriate dosing policy, emergent resistant subpopulations can proliferate\n",
      "and lead to treatment failure. Dynamic feedback dosage control holds promise in\n",
      "combatting this phenomenon, but the application of traditional control\n",
      "approaches to such systems is fraught with challenges due to the complexity of\n",
      "cell dynamics, uncertainty in model parameters, and the need in medical\n",
      "applications for a robust controller that can be trusted to properly handle\n",
      "unexpected outcomes. Here, training on a sample biological scenario identified\n",
      "single-drug and combination therapy policies that exhibit a 100% success rate\n",
      "at suppressing cell proliferation and responding to diverse system\n",
      "perturbations while establishing low-dose no-event baselines. These policies\n",
      "were found to be highly robust to variations in a key model parameter subject\n",
      "to significant uncertainty and unpredictable dynamical changes.\n",
      "\n",
      "**Paper Id :2004.05273 \n",
      "Title :Safe Multi-Agent Interaction through Robust Control Barrier Functions\n",
      "  with Learned Uncertainties\n",
      "  Robots operating in real world settings must navigate and maintain safety\n",
      "while interacting with many heterogeneous agents and obstacles. Multi-Agent\n",
      "Control Barrier Functions (CBF) have emerged as a computationally efficient\n",
      "tool to guarantee safety in multi-agent environments, but they assume perfect\n",
      "knowledge of both the robot dynamics and other agents' dynamics. While\n",
      "knowledge of the robot's dynamics might be reasonably well known, the\n",
      "heterogeneity of agents in real-world environments means there will always be\n",
      "considerable uncertainty in our prediction of other agents' dynamics. This work\n",
      "aims to learn high-confidence bounds for these dynamic uncertainties using\n",
      "Matrix-Variate Gaussian Process models, and incorporates them into a robust\n",
      "multi-agent CBF framework. We transform the resulting min-max robust CBF into a\n",
      "quadratic program, which can be efficiently solved in real time. We verify via\n",
      "simulation results that the nominal multi-agent CBF is often violated during\n",
      "agent interactions, whereas our robust formulation maintains safety with a much\n",
      "higher probability and adapts to learned uncertainties\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.11835 \n",
      "Title :A Survey on Graph Kernels\n",
      "  Graph kernels have become an established and widely-used technique for\n",
      "solving classification tasks on graphs. This survey gives a comprehensive\n",
      "overview of techniques for kernel-based graph classification developed in the\n",
      "past 15 years. We describe and categorize graph kernels based on properties\n",
      "inherent to their design, such as the nature of their extracted graph features,\n",
      "their method of computation and their applicability to problems in practice. In\n",
      "an extensive experimental evaluation, we study the classification accuracy of a\n",
      "large suite of graph kernels on established benchmarks as well as new datasets.\n",
      "We compare the performance of popular kernels with several baseline methods and\n",
      "study the effect of applying a Gaussian RBF kernel to the metric induced by a\n",
      "graph kernel. In doing so, we find that simple baselines become competitive\n",
      "after this transformation on some datasets. Moreover, we study the extent to\n",
      "which existing graph kernels agree in their predictions (and prediction errors)\n",
      "and obtain a data-driven categorization of kernels as result. Finally, based on\n",
      "our experimental results, we derive a practitioner's guide to kernel-based\n",
      "graph classification.\n",
      "\n",
      "**Paper Id :1903.03096 \n",
      "Title :Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few\n",
      "  Examples\n",
      "  Few-shot classification refers to learning a classifier for new classes given\n",
      "only a few examples. While a plethora of models have emerged to tackle it, we\n",
      "find the procedure and datasets that are used to assess their progress lacking.\n",
      "To address this limitation, we propose Meta-Dataset: a new benchmark for\n",
      "training and evaluating models that is large-scale, consists of diverse\n",
      "datasets, and presents more realistic tasks. We experiment with popular\n",
      "baselines and meta-learners on Meta-Dataset, along with a competitive method\n",
      "that we propose. We analyze performance as a function of various\n",
      "characteristics of test tasks and examine the models' ability to leverage\n",
      "diverse training sources for improving their generalization. We also propose a\n",
      "new set of baselines for quantifying the benefit of meta-learning in\n",
      "Meta-Dataset. Our extensive experimentation has uncovered important research\n",
      "challenges and we hope to inspire work in these directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.12021 \n",
      "Title :Counting the learnable functions of structured data\n",
      "  Cover's function counting theorem is a milestone in the theory of artificial\n",
      "neural networks. It provides an answer to the fundamental question of\n",
      "determining how many binary assignments (dichotomies) of $p$ points in $n$\n",
      "dimensions can be linearly realized. Regrettably, it has proved hard to extend\n",
      "the same approach to more advanced problems than the classification of points.\n",
      "In particular, an emerging necessity is to find methods to deal with structured\n",
      "data, and specifically with non-pointlike patterns. A prominent case is that of\n",
      "invariant recognition, whereby identification of a stimulus is insensitive to\n",
      "irrelevant transformations on the inputs (such as rotations or changes in\n",
      "perspective in an image). An object is therefore represented by an extended\n",
      "perceptual manifold, consisting of inputs that are classified similarly. Here,\n",
      "we develop a function counting theory for structured data of this kind, by\n",
      "extending Cover's combinatorial technique, and we derive analytical expressions\n",
      "for the average number of dichotomies of generically correlated sets of\n",
      "patterns. As an application, we obtain a closed formula for the capacity of a\n",
      "binary classifier trained to distinguish general polytopes of any dimension.\n",
      "These results may help extend our theoretical understanding of generalization,\n",
      "feature extraction, and invariant object recognition by neural networks.\n",
      "\n",
      "**Paper Id :2003.01695 \n",
      "Title :Robust data encodings for quantum classifiers\n",
      "  Data representation is crucial for the success of machine learning models. In\n",
      "the context of quantum machine learning with near-term quantum computers,\n",
      "equally important considerations of how to efficiently input (encode) data and\n",
      "effectively deal with noise arise. In this work, we study data encodings for\n",
      "binary quantum classification and investigate their properties both with and\n",
      "without noise. For the common classifier we consider, we show that encodings\n",
      "determine the classes of learnable decision boundaries as well as the set of\n",
      "points which retain the same classification in the presence of noise. After\n",
      "defining the notion of a robust data encoding, we prove several results on\n",
      "robustness for different channels, discuss the existence of robust encodings,\n",
      "and prove an upper bound on the number of robust points in terms of fidelities\n",
      "between noisy and noiseless states. Numerical results for several example\n",
      "implementations are provided to reinforce our findings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1903.12370 \n",
      "Title :On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based\n",
      "  Models\n",
      "  This study investigates the effects of Markov chain Monte Carlo (MCMC)\n",
      "sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is\n",
      "restricted to the family of unnormalized probability densities for which the\n",
      "negative log density (or energy function) is a ConvNet. We find that many of\n",
      "the techniques used to stabilize training in previous studies are not\n",
      "necessary. ML learning with a ConvNet potential requires only a few\n",
      "hyper-parameters and no regularization. Using this minimal framework, we\n",
      "identify a variety of ML learning outcomes that depend solely on the\n",
      "implementation of MCMC sampling.\n",
      "  On one hand, we show that it is easy to train an energy-based model which can\n",
      "sample realistic images with short-run Langevin. ML can be effective and stable\n",
      "even when MCMC samples have much higher energy than true steady-state samples\n",
      "throughout training. Based on this insight, we introduce an ML method with\n",
      "purely noise-initialized MCMC, high-quality short-run synthesis, and the same\n",
      "budget as ML with informative MCMC initialization such as CD or PCD. Unlike\n",
      "previous models, our energy model can obtain realistic high-diversity samples\n",
      "from a noise signal after training.\n",
      "  On the other hand, ConvNet potentials learned with non-convergent MCMC do not\n",
      "have a valid steady-state and cannot be considered approximate unnormalized\n",
      "densities of the training data because long-run MCMC samples differ greatly\n",
      "from observed images. We show that it is much harder to train a ConvNet\n",
      "potential to learn a steady-state over realistic images. To our knowledge,\n",
      "long-run MCMC samples of all previous models lose the realism of short-run\n",
      "samples. With correct tuning of Langevin noise, we train the first ConvNet\n",
      "potentials for which long-run and steady-state MCMC samples are realistic\n",
      "images.\n",
      "\n",
      "**Paper Id :2002.07217 \n",
      "Title :Decision-Making with Auto-Encoding Variational Bayes\n",
      "  To make decisions based on a model fit with auto-encoding variational Bayes\n",
      "(AEVB), practitioners often let the variational distribution serve as a\n",
      "surrogate for the posterior distribution. This approach yields biased estimates\n",
      "of the expected risk, and therefore leads to poor decisions for two reasons.\n",
      "First, the model fit with AEVB may not equal the underlying data distribution.\n",
      "Second, the variational distribution may not equal the posterior distribution\n",
      "under the fitted model. We explore how fitting the variational distribution\n",
      "based on several objective functions other than the ELBO, while continuing to\n",
      "fit the generative model based on the ELBO, affects the quality of downstream\n",
      "decisions. For the probabilistic principal component analysis model, we\n",
      "investigate how importance sampling error, as well as the bias of the model\n",
      "parameter estimates, varies across several approximate posteriors when used as\n",
      "proposal distributions. Our theoretical results suggest that a posterior\n",
      "approximation distinct from the variational distribution should be used for\n",
      "making decisions. Motivated by these theoretical results, we propose learning\n",
      "several approximate proposals for the best model and combining them using\n",
      "multiple importance sampling for decision-making. In addition to toy examples,\n",
      "we present a full-fledged case study of single-cell RNA sequencing. In this\n",
      "challenging instance of multiple hypothesis testing, our proposed approach\n",
      "surpasses the current state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.00507 \n",
      "Title :Semisupervised Clustering by Queries and Locally Encodable Source Coding\n",
      "  Source coding is the canonical problem of data compression in information\n",
      "theory. In a locally encodable source coding, each compressed bit depends on\n",
      "only few bits of the input. In this paper, we show that a recently popular\n",
      "model of semi-supervised clustering is equivalent to locally encodable source\n",
      "coding. In this model, the task is to perform multiclass labeling of unlabeled\n",
      "elements. At the beginning, we can ask in parallel a set of simple queries to\n",
      "an oracle who provides (possibly erroneous) binary answers to the queries. The\n",
      "queries cannot involve more than two (or a fixed constant number of) elements.\n",
      "Now the labeling of all the elements (or clustering) must be performed based on\n",
      "the noisy query answers. The goal is to recover all the correct labelings while\n",
      "minimizing the number of such queries. The equivalence to locally encodable\n",
      "source codes leads us to find lower bounds on the number of queries required in\n",
      "a variety of scenarios. We provide querying schemes based on pairwise `same\n",
      "cluster' queries - and pairwise AND queries and show provable performance\n",
      "guarantees for each of the schemes.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.01949 \n",
      "Title :Automatic diagnosis of the 12-lead ECG using a deep neural network\n",
      "  The role of automatic electrocardiogram (ECG) analysis in clinical practice\n",
      "is limited by the accuracy of existing models. Deep Neural Networks (DNNs) are\n",
      "models composed of stacked transformations that learn tasks by examples. This\n",
      "technology has recently achieved striking success in a variety of task and\n",
      "there are great expectations on how it might improve clinical practice. Here we\n",
      "present a DNN model trained in a dataset with more than 2 million labeled exams\n",
      "analyzed by the Telehealth Network of Minas Gerais and collected under the\n",
      "scope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The\n",
      "DNN outperform cardiology resident medical doctors in recognizing 6 types of\n",
      "abnormalities in 12-lead ECG recordings, with F1 scores above 80% and\n",
      "specificity over 99%. These results indicate ECG analysis based on DNNs,\n",
      "previously studied in a single-lead setup, generalizes well to 12-lead exams,\n",
      "taking the technology closer to the standard clinical practice.\n",
      "\n",
      "**Paper Id :2010.03204 \n",
      "Title :Cardiac Arrhythmia Detection from ECG with Convolutional Recurrent\n",
      "  Neural Networks\n",
      "  Except for a few specific types, cardiac arrhythmias are not immediately\n",
      "life-threatening. However, if not treated appropriately, they can cause serious\n",
      "complications. In particular, atrial fibrillation, which is characterized by\n",
      "fast and irregular heart beats, increases the risk of stroke. We propose three\n",
      "neural network architectures to detect abnormal rhythms from single-lead ECG\n",
      "signals. These architectures combine convolutional layers to extract high-level\n",
      "features pertinent for arrhythmia detection from sliding windows and recurrent\n",
      "layers to aggregate these features over signals of varying durations. We\n",
      "applied the neural networks to the dataset used for the challenge of Computing\n",
      "in Cardiology 2017 and a dataset built by joining three databases available on\n",
      "PhysioNet. Our architectures achieved an accuracy of 86.23% on the first\n",
      "dataset, similar to the winning entries of the challenge, and an accuracy of\n",
      "92.02% on the second dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.02214 \n",
      "Title :The Born Supremacy: Quantum Advantage and Training of an Ising Born\n",
      "  Machine\n",
      "  The search for an application of near-term quantum devices is widespread.\n",
      "Quantum Machine Learning is touted as a potential utilisation of such devices,\n",
      "particularly those which are out of the reach of the simulation capabilities of\n",
      "classical computers. In this work, we propose a generative Quantum Machine\n",
      "Learning Model, called the Ising Born Machine (IBM), which we show cannot, in\n",
      "the worst case, and up to suitable notions of error, be simulated efficiently\n",
      "by a classical device. We also show this holds for all the circuit families\n",
      "encountered during training. In particular, we explore quantum circuit learning\n",
      "using non-universal circuits derived from Ising Model Hamiltonians, which are\n",
      "implementable on near term quantum devices.\n",
      "  We propose two novel training methods for the IBM by utilising the Stein\n",
      "Discrepancy and the Sinkhorn Divergence cost functions. We show numerically,\n",
      "both using a simulator within Rigetti's Forest platform and on the Aspen-1 16Q\n",
      "chip, that the cost functions we suggest outperform the more commonly used\n",
      "Maximum Mean Discrepancy (MMD) for differentiable training. We also propose an\n",
      "improvement to the MMD by proposing a novel utilisation of quantum kernels\n",
      "which we demonstrate provides improvements over its classical counterpart. We\n",
      "discuss the potential of these methods to learn `hard' quantum distributions, a\n",
      "feat which would demonstrate the advantage of quantum over classical computers,\n",
      "and provide the first formal definitions for what we call `Quantum Learning\n",
      "Supremacy'. Finally, we propose a novel view on the area of quantum circuit\n",
      "compilation by using the IBM to `mimic' target quantum circuits using classical\n",
      "output data only.\n",
      "\n",
      "**Paper Id :2006.14619 \n",
      "Title :Recurrent Quantum Neural Networks\n",
      "  Recurrent neural networks are the foundation of many sequence-to-sequence\n",
      "models in machine learning, such as machine translation and speech synthesis.\n",
      "In contrast, applied quantum computing is in its infancy. Nevertheless there\n",
      "already exist quantum machine learning models such as variational quantum\n",
      "eigensolvers which have been used successfully e.g. in the context of energy\n",
      "minimization tasks. In this work we construct a quantum recurrent neural\n",
      "network (QRNN) with demonstrable performance on non-trivial tasks such as\n",
      "sequence learning and integer digit classification. The QRNN cell is built from\n",
      "parametrized quantum neurons, which, in conjunction with amplitude\n",
      "amplification, create a nonlinear activation of polynomials of its inputs and\n",
      "cell state, and allow the extraction of a probability distribution over\n",
      "predicted classes at each step. To study the model's performance, we provide an\n",
      "implementation in pytorch, which allows the relatively efficient optimization\n",
      "of parametrized quantum circuits with thousands of parameters. We establish a\n",
      "QRNN training setup by benchmarking optimization hyperparameters, and analyse\n",
      "suitable network topologies for simple memorisation and sequence prediction\n",
      "tasks from Elman's seminal paper (1990) on temporal structure learning. We then\n",
      "proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN\n",
      "each image pixel-by-pixel; and by utilising modern data augmentation as\n",
      "preprocessing step. Finally, we analyse to what extent the unitary nature of\n",
      "the network counteracts the vanishing gradient problem that plagues many\n",
      "existing quantum classifiers and classical RNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.02311 \n",
      "Title :Approximation Rates for Neural Networks with General Activation\n",
      "  Functions\n",
      "  We prove some new results concerning the approximation rate of neural\n",
      "networks with general activation functions. Our first result concerns the rate\n",
      "of approximation of a two layer neural network with a polynomially-decaying\n",
      "non-sigmoidal activation function. We extend the dimension independent\n",
      "approximation rates previously obtained to this new class of activation\n",
      "functions. Our second result gives a weaker, but still dimension independent,\n",
      "approximation rate for a larger class of activation functions, removing the\n",
      "polynomial decay assumption. This result applies to any bounded, integrable\n",
      "activation function. Finally, we show that a stratified sampling approach can\n",
      "be used to improve the approximation rate for polynomially decaying activation\n",
      "functions under mild additional assumptions.\n",
      "\n",
      "**Paper Id :1910.02333 \n",
      "Title :The Role of Neural Network Activation Functions\n",
      "  A wide variety of activation functions have been proposed for neural\n",
      "networks. The Rectified Linear Unit (ReLU) is especially popular today. There\n",
      "are many practical reasons that motivate the use of the ReLU. This paper\n",
      "provides new theoretical characterizations that support the use of the ReLU,\n",
      "its variants such as the leaky ReLU, as well as other activation functions in\n",
      "the case of univariate, single-hidden layer feedforward neural networks. Our\n",
      "results also explain the importance of commonly used strategies in the design\n",
      "and training of neural networks such as \"weight decay\" and \"path-norm\"\n",
      "regularization, and provide a new justification for the use of \"skip\n",
      "connections\" in network architectures. These new insights are obtained through\n",
      "the lens of spline theory. In particular, we show how neural network training\n",
      "problems are related to infinite-dimensional optimizations posed over Banach\n",
      "spaces of functions whose solutions are well-known to be fractional and\n",
      "polynomial splines, where the particular Banach space (which controls the order\n",
      "of the spline) depends on the choice of activation function.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.04238 \n",
      "Title :Learning Backtrackless Aligned-Spatial Graph Convolutional Networks for\n",
      "  Graph Classification\n",
      "  In this paper, we develop a novel Backtrackless Aligned-Spatial Graph\n",
      "Convolutional Network (BASGCN) model to learn effective features for graph\n",
      "classification. Our idea is to transform arbitrary-sized graphs into\n",
      "fixed-sized backtrackless aligned grid structures and define a new spatial\n",
      "graph convolution operation associated with the grid structures. We show that\n",
      "the proposed BASGCN model not only reduces the problems of information loss and\n",
      "imprecise information representation arising in existing spatially-based Graph\n",
      "Convolutional Network (GCN) models, but also bridges the theoretical gap\n",
      "between traditional Convolutional Neural Network (CNN) models and\n",
      "spatially-based GCN models. Furthermore, the proposed BASGCN model can both\n",
      "adaptively discriminate the importance between specified vertices during the\n",
      "convolution process and reduce the notorious tottering problem of existing\n",
      "spatially-based GCNs related to the Weisfeiler-Lehman algorithm, explaining the\n",
      "effectiveness of the proposed model. Experiments on standard graph datasets\n",
      "demonstrate the effectiveness of the proposed model.\n",
      "\n",
      "**Paper Id :2001.05313 \n",
      "Title :Tensor Graph Convolutional Networks for Text Classification\n",
      "  Compared to sequential learning models, graph-based neural networks exhibit\n",
      "some excellent properties, such as ability capturing global information. In\n",
      "this paper, we investigate graph-based neural networks for text classification\n",
      "problem. A new framework TensorGCN (tensor graph convolutional networks), is\n",
      "presented for this task. A text graph tensor is firstly constructed to describe\n",
      "semantic, syntactic, and sequential contextual information. Then, two kinds of\n",
      "propagation learning perform on the text graph tensor. The first is intra-graph\n",
      "propagation used for aggregating information from neighborhood nodes in a\n",
      "single graph. The second is inter-graph propagation used for harmonizing\n",
      "heterogeneous information between graphs. Extensive experiments are conducted\n",
      "on benchmark datasets, and the results illustrate the effectiveness of our\n",
      "proposed framework. Our proposed TensorGCN presents an effective way to\n",
      "harmonize and integrate heterogeneous information from different kinds of\n",
      "graphs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.04326 \n",
      "Title :A Comparative Analysis of the Optimization and Generalization Property\n",
      "  of Two-layer Neural Network and Random Feature Models Under Gradient Descent\n",
      "  Dynamics\n",
      "  A fairly comprehensive analysis is presented for the gradient descent\n",
      "dynamics for training two-layer neural network models in the situation when the\n",
      "parameters in both layers are updated. General initialization schemes as well\n",
      "as general regimes for the network width and training data size are considered.\n",
      "In the over-parametrized regime, it is shown that gradient descent dynamics can\n",
      "achieve zero training loss exponentially fast regardless of the quality of the\n",
      "labels. In addition, it is proved that throughout the training process the\n",
      "functions represented by the neural network model are uniformly close to that\n",
      "of a kernel method. For general values of the network width and training data\n",
      "size, sharp estimates of the generalization error is established for target\n",
      "functions in the appropriate reproducing kernel Hilbert space.\n",
      "\n",
      "**Paper Id :1708.06633 \n",
      "Title :Nonparametric regression using deep neural networks with ReLU activation\n",
      "  function\n",
      "  Consider the multivariate nonparametric regression model. It is shown that\n",
      "estimators based on sparsely connected deep neural networks with ReLU\n",
      "activation function and properly chosen network architecture achieve the\n",
      "minimax rates of convergence (up to $\\log n$-factors) under a general\n",
      "composition assumption on the regression function. The framework includes many\n",
      "well-studied structural constraints such as (generalized) additive models.\n",
      "While there is a lot of flexibility in the network architecture, the tuning\n",
      "parameter is the sparsity of the network. Specifically, we consider large\n",
      "networks with number of potential network parameters exceeding the sample size.\n",
      "The analysis gives some insights into why multilayer feedforward neural\n",
      "networks perform well in practice. Interestingly, for ReLU activation function\n",
      "the depth (number of layers) of the neural network architectures plays an\n",
      "important role and our theory suggests that for nonparametric regression,\n",
      "scaling the network depth with the sample size is natural. It is also shown\n",
      "that under the composition assumption wavelet estimators can only achieve\n",
      "suboptimal rates.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.05333 \n",
      "Title :Bayesian estimation of the latent dimension and communities in\n",
      "  stochastic blockmodels\n",
      "  Spectral embedding of adjacency or Laplacian matrices of undirected graphs is\n",
      "a common technique for representing a network in a lower dimensional latent\n",
      "space, with optimal theoretical guarantees. The embedding can be used to\n",
      "estimate the community structure of the network, with strong consistency\n",
      "results in the stochastic blockmodel framework. One of the main practical\n",
      "limitations of standard algorithms for community detection from spectral\n",
      "embeddings is that the number of communities and the latent dimension of the\n",
      "embedding must be specified in advance. In this article, a novel Bayesian model\n",
      "for simultaneous and automatic selection of the appropriate dimension of the\n",
      "latent space and the number of blocks is proposed. Extensions to directed and\n",
      "bipartite graphs are discussed. The model is tested on simulated and real world\n",
      "network data, showing promising performance for recovering latent community\n",
      "structure.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.05902 \n",
      "Title :Experimental neural network enhanced quantum tomography\n",
      "  Quantum tomography is currently ubiquitous for testing any implementation of\n",
      "a quantum information processing device. Various sophisticated procedures for\n",
      "state and process reconstruction from measured data are well developed and\n",
      "benefit from precise knowledge of the model describing state preparation and\n",
      "the measurement apparatus. However, physical models suffer from intrinsic\n",
      "limitations as actual measurement operators and trial states cannot be known\n",
      "precisely. This scenario inevitably leads to state-preparation-and-measurement\n",
      "(SPAM) errors degrading reconstruction performance. Here we develop and\n",
      "experimentally implement a machine learning based protocol reducing SPAM\n",
      "errors. We trained a supervised neural network to filter the experimental data\n",
      "and hence uncovered salient patterns that characterize the measurement\n",
      "probabilities for the original state and the ideal experimental apparatus free\n",
      "from SPAM errors. We compared the neural network state reconstruction protocol\n",
      "with a protocol treating SPAM errors by process tomography, as well as to a\n",
      "SPAM-agnostic protocol with idealized measurements. The average reconstruction\n",
      "fidelity is shown to be enhanced by 10\\% and 27\\%, respectively. The presented\n",
      "methods apply to the vast range of quantum experiments which rely on\n",
      "tomography.\n",
      "\n",
      "**Paper Id :1912.07286 \n",
      "Title :Variational Quantum Circuits for Quantum State Tomography\n",
      "  Quantum state tomography is a key process in most quantum experiments. In\n",
      "this work, we employ quantum machine learning for state tomography. Given an\n",
      "unknown quantum state, it can be learned by maximizing the fidelity between the\n",
      "output of a variational quantum circuit and this state. The number of\n",
      "parameters of the variational quantum circuit grows linearly with the number of\n",
      "qubits and the circuit depth, so that only polynomial measurements are\n",
      "required, even for highly-entangled states. After that, a subsequent classical\n",
      "circuit simulator is used to transform the information of the target quantum\n",
      "state from the variational quantum circuit into a familiar format. We\n",
      "demonstrate our method by performing numerical simulations for the tomography\n",
      "of the ground state of a one-dimensional quantum spin chain, using a\n",
      "variational quantum circuit simulator. Our method is suitable for near-term\n",
      "quantum computing platforms, and could be used for relatively large-scale\n",
      "quantum state tomography for experimentally relevant quantum states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.06194 \n",
      "Title :Compressing deep neural networks by matrix product operators\n",
      "  A deep neural network is a parametrization of a multilayer mapping of signals\n",
      "in terms of many alternatively arranged linear and nonlinear transformations.\n",
      "The linear transformations, which are generally used in the fully connected as\n",
      "well as convolutional layers, contain most of the variational parameters that\n",
      "are trained and stored. Compressing a deep neural network to reduce its number\n",
      "of variational parameters but not its prediction power is an important but\n",
      "challenging problem toward the establishment of an optimized scheme in training\n",
      "efficiently these parameters and in lowering the risk of overfitting. Here we\n",
      "show that this problem can be effectively solved by representing linear\n",
      "transformations with matrix product operators (MPOs), which is a tensor network\n",
      "originally proposed in physics to characterize the short-range entanglement in\n",
      "one-dimensional quantum states. We have tested this approach in five typical\n",
      "neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\n",
      "widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\n",
      "representation indeed sets up a faithful and efficient mapping between input\n",
      "and output signals, which can keep or even improve the prediction accuracy with\n",
      "a dramatically reduced number of parameters. Our method greatly simplifies the\n",
      "representations in deep learning, and opens a possible route toward\n",
      "establishing a framework of modern neural networks which might be simpler and\n",
      "cheaper, but more efficient.\n",
      "\n",
      "**Paper Id :2011.07929 \n",
      "Title :On the equivalence of molecular graph convolution and molecular wave\n",
      "  function with poor basis set\n",
      "  In this study, we demonstrate that the linear combination of atomic orbitals\n",
      "(LCAO), an approximation of quantum physics introduced by Pauling and\n",
      "Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)\n",
      "for molecules. However, GCNs involve unnecessary nonlinearity and deep\n",
      "architecture. We also verify that molecular GCNs are based on a poor basis\n",
      "function set compared with the standard one used in theoretical calculations or\n",
      "quantum chemical simulations. From these observations, we describe the quantum\n",
      "deep field (QDF), a machine learning (ML) model based on an underlying quantum\n",
      "physics, in particular the density functional theory (DFT). We believe that the\n",
      "QDF model can be easily understood because it can be regarded as a single\n",
      "linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to\n",
      "learn an energy functional and a Hohenberg--Kohn map that have nonlinearities\n",
      "inherent in quantum physics and the DFT. For molecular energy prediction tasks,\n",
      "we demonstrated the viability of an ``extrapolation,'' in which we trained a\n",
      "QDF model with small molecules, tested it with large molecules, and achieved\n",
      "high extrapolation performance. This will lead to reliable and practical\n",
      "applications for discovering effective materials. The implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.06292 \n",
      "Title :Adversarial Learning in Statistical Classification: A Comprehensive\n",
      "  Review of Defenses Against Attacks\n",
      "  There is great potential for damage from adversarial learning (AL) attacks on\n",
      "machine-learning based systems. In this paper, we provide a contemporary survey\n",
      "of AL, focused particularly on defenses against attacks on statistical\n",
      "classifiers. After introducing relevant terminology and the goals and range of\n",
      "possible knowledge of both attackers and defenders, we survey recent work on\n",
      "test-time evasion (TTE), data poisoning (DP), and reverse engineering (RE)\n",
      "attacks and particularly defenses against same. In so doing, we distinguish\n",
      "robust classification from anomaly detection (AD), unsupervised from\n",
      "supervised, and statistical hypothesis-based defenses from ones that do not\n",
      "have an explicit null (no attack) hypothesis; we identify the hyperparameters a\n",
      "particular method requires, its computational complexity, as well as the\n",
      "performance measures on which it was evaluated and the obtained quality. We\n",
      "then dig deeper, providing novel insights that challenge conventional AL wisdom\n",
      "and that target unresolved issues, including: 1) robust classification versus\n",
      "AD as a defense strategy; 2) the belief that attack success increases with\n",
      "attack strength, which ignores susceptibility to AD; 3) small perturbations for\n",
      "test-time evasion attacks: a fallacy or a requirement?; 4) validity of the\n",
      "universal assumption that a TTE attacker knows the ground-truth class for the\n",
      "example to be attacked; 5) black, grey, or white box attacks as the standard\n",
      "for defense evaluation; 6) susceptibility of query-based RE to an AD defense.\n",
      "We also discuss attacks on the privacy of training data. We then present\n",
      "benchmark comparisons of several defenses against TTE, RE, and backdoor DP\n",
      "attacks on images. The paper concludes with a discussion of future work.\n",
      "\n",
      "**Paper Id :2005.14124 \n",
      "Title :Active Fuzzing for Testing and Securing Cyber-Physical Systems\n",
      "  Cyber-physical systems (CPSs) in critical infrastructure face a pervasive\n",
      "threat from attackers, motivating research into a variety of countermeasures\n",
      "for securing them. Assessing the effectiveness of these countermeasures is\n",
      "challenging, however, as realistic benchmarks of attacks are difficult to\n",
      "manually construct, blindly testing is ineffective due to the enormous search\n",
      "spaces and resource requirements, and intelligent fuzzing approaches require\n",
      "impractical amounts of data and network access. In this work, we propose active\n",
      "fuzzing, an automatic approach for finding test suites of packet-level CPS\n",
      "network attacks, targeting scenarios in which attackers can observe sensors and\n",
      "manipulate packets, but have no existing knowledge about the payload encodings.\n",
      "Our approach learns regression models for predicting sensor values that will\n",
      "result from sampled network packets, and uses these predictions to guide a\n",
      "search for payload manipulations (i.e. bit flips) most likely to drive the CPS\n",
      "into an unsafe state. Key to our solution is the use of online active learning,\n",
      "which iteratively updates the models by sampling payloads that are estimated to\n",
      "maximally improve them. We evaluate the efficacy of active fuzzing by\n",
      "implementing it for a water purification plant testbed, finding it can\n",
      "automatically discover a test suite of flow, pressure, and over/underflow\n",
      "attacks, all with substantially less time, data, and network access than the\n",
      "most comparable approach. Finally, we demonstrate that our prediction models\n",
      "can also be utilised as countermeasures themselves, implementing them as\n",
      "anomaly detectors and early warning systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.06517 \n",
      "Title :Improving detection of protein-ligand binding sites with 3D segmentation\n",
      "  In recent years machine learning (ML) took bio- and cheminformatics fields by\n",
      "storm, providing new solutions for a vast repertoire of problems related to\n",
      "protein sequence, structure, and interactions analysis. ML techniques, deep\n",
      "neural networks especially, were proven more effective than classical models\n",
      "for tasks like predicting binding affinity for molecular complex. In this work\n",
      "we investigated the earlier stage of drug discovery process - finding druggable\n",
      "pockets on protein surface, that can be later used to design active molecules.\n",
      "For this purpose we developed a 3D fully convolutional neural network capable\n",
      "of binding site segmentation. Our solution has high prediction accuracy and\n",
      "provides intuitive representations of the results, which makes it easy to\n",
      "incorporate into drug discovery projects. The model's source code, together\n",
      "with scripts for most common use-cases is freely available at\n",
      "http://gitlab.com/cheminfIBB/kalasanty\n",
      "\n",
      "**Paper Id :2002.11044 \n",
      "Title :Regression with Deep Learning for Sensor Performance Optimization\n",
      "  Neural networks with at least two hidden layers are called deep networks.\n",
      "Recent developments in AI and computer programming in general has led to\n",
      "development of tools such as Tensorflow, Keras, NumPy etc. making it easier to\n",
      "model and draw conclusions from data. In this work we re-approach non-linear\n",
      "regression with deep learning enabled by Keras and Tensorflow. In particular,\n",
      "we use deep learning to parametrize a non-linear multivariate relationship\n",
      "between inputs and outputs of an industrial sensor with an intent to optimize\n",
      "the sensor performance based on selected key metrics.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.07698 \n",
      "Title :Multimodal Subspace Support Vector Data Description\n",
      "  In this paper, we propose a novel method for projecting data from multiple\n",
      "modalities to a new subspace optimized for one-class classification. The\n",
      "proposed method iteratively transforms the data from the original feature space\n",
      "of each modality to a new common feature space along with finding a joint\n",
      "compact description of data coming from all the modalities. For data in each\n",
      "modality, we define a separate transformation to map the data from the\n",
      "corresponding feature space to the new optimized subspace by exploiting the\n",
      "available information from the class of interest only. We also propose\n",
      "different regularization strategies for the proposed method and provide both\n",
      "linear and non-linear formulations. The proposed Multimodal Subspace Support\n",
      "Vector Data Description outperforms all the competing methods using data from a\n",
      "single modality or fusing data from all modalities in four out of five\n",
      "datasets.\n",
      "\n",
      "**Paper Id :2003.09504 \n",
      "Title :Ellipsoidal Subspace Support Vector Data Description\n",
      "  In this paper, we propose a novel method for transforming data into a\n",
      "low-dimensional space optimized for one-class classification. The proposed\n",
      "method iteratively transforms data into a new subspace optimized for\n",
      "ellipsoidal encapsulation of target class data. We provide both linear and\n",
      "non-linear formulations for the proposed method. The method takes into account\n",
      "the covariance of the data in the subspace; hence, it yields a more generalized\n",
      "solution as compared to Subspace Support Vector Data Description for a\n",
      "hypersphere. We propose different regularization terms expressing the class\n",
      "variance in the projected space. We compare the results with classic and\n",
      "recently proposed one-class classification methods and achieve better results\n",
      "in the majority of cases. The proposed method is also noticed to converge much\n",
      "faster than recently proposed Subspace Support Vector Data Description.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.07773 \n",
      "Title :Convolutional Neural Networks for Classification of Alzheimer's Disease:\n",
      "  Overview and Reproducible Evaluation\n",
      "  Over 30 papers have proposed to use convolutional neural network (CNN) for AD\n",
      "classification from anatomical MRI. However, the classification performance is\n",
      "difficult to compare across studies due to variations in components such as\n",
      "participant selection, image preprocessing or validation procedure. Moreover,\n",
      "these studies are hardly reproducible because their frameworks are not publicly\n",
      "accessible and because implementation details are lacking. Lastly, some of\n",
      "these papers may report a biased performance due to inadequate or unclear\n",
      "validation or model selection procedures. In the present work, we aim to\n",
      "address these limitations through three main contributions. First, we performed\n",
      "a systematic literature review and found that more than half of the surveyed\n",
      "papers may have suffered from data leakage. Our second contribution is the\n",
      "extension of our open-source framework for classification of AD using CNN and\n",
      "T1-weighted MRI. Finally, we used this framework to rigorously compare\n",
      "different CNN architectures. The data was split into training/validation/test\n",
      "sets at the very beginning and only the training/validation sets were used for\n",
      "model selection. To avoid any overfitting, the test sets were left untouched\n",
      "until the end of the peer-review process. Overall, the different 3D approaches\n",
      "(3D-subject, 3D-ROI, 3D-patch) achieved similar performances while that of the\n",
      "2D slice approach was lower. Of note, the different CNN approaches did not\n",
      "perform better than a SVM with voxel-based features. The different approaches\n",
      "generalized well to similar populations but not to datasets with different\n",
      "inclusion criteria or demographical characteristics.\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.08064 \n",
      "Title :Forecasting with time series imaging\n",
      "  Feature-based time series representations have attracted substantial\n",
      "attention in a wide range of time series analysis methods. Recently, the use of\n",
      "time series features for forecast model averaging has been an emerging research\n",
      "focus in the forecasting community. Nonetheless, most of the existing\n",
      "approaches depend on the manual choice of an appropriate set of features.\n",
      "Exploiting machine learning methods to extract features from time series\n",
      "automatically becomes crucial in state-of-the-art time series analysis. In this\n",
      "paper, we introduce an automated approach to extract time series features based\n",
      "on time series imaging. We first transform time series into recurrence plots,\n",
      "from which local features can be extracted using computer vision algorithms.\n",
      "The extracted features are used for forecast model averaging. Our experiments\n",
      "show that forecasting based on automatically extracted features, with less\n",
      "human intervention and a more comprehensive view of the raw time series data,\n",
      "yields highly comparable performances with the best methods in the largest\n",
      "forecasting competition dataset (M4) and outperforms the top methods in the\n",
      "Tourism forecasting competition dataset.\n",
      "\n",
      "**Paper Id :2010.01309 \n",
      "Title :Personality Trait Detection Using Bagged SVM over BERT Word Embedding\n",
      "  Ensembles\n",
      "  Recently, the automatic prediction of personality traits has received\n",
      "increasing attention and has emerged as a hot topic within the field of\n",
      "affective computing. In this work, we present a novel deep learning-based\n",
      "approach for automated personality detection from text. We leverage state of\n",
      "the art advances in natural language understanding, namely the BERT language\n",
      "model to extract contextualized word embeddings from textual data for automated\n",
      "author personality detection. Our primary goal is to develop a computationally\n",
      "efficient, high-performance personality prediction model which can be easily\n",
      "used by a large number of people without access to huge computation resources.\n",
      "Our extensive experiments with this ideology in mind, led us to develop a novel\n",
      "model which feeds contextualized embeddings along with psycholinguistic\n",
      "features toa Bagged-SVM classifier for personality trait prediction. Our model\n",
      "outperforms the previous state of the art by 1.04% and, at the same time is\n",
      "significantly more computationally efficient to train. We report our results on\n",
      "the famous gold standard Essays dataset for personality detection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.08405 \n",
      "Title :Event-based Vision: A Survey\n",
      "  Event cameras are bio-inspired sensors that differ from conventional frame\n",
      "cameras: Instead of capturing images at a fixed rate, they asynchronously\n",
      "measure per-pixel brightness changes, and output a stream of events that encode\n",
      "the time, location and sign of the brightness changes. Event cameras offer\n",
      "attractive properties compared to traditional cameras: high temporal resolution\n",
      "(in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low\n",
      "power consumption, and high pixel bandwidth (on the order of kHz) resulting in\n",
      "reduced motion blur. Hence, event cameras have a large potential for robotics\n",
      "and computer vision in challenging scenarios for traditional cameras, such as\n",
      "low-latency, high speed, and high dynamic range. However, novel methods are\n",
      "required to process the unconventional output of these sensors in order to\n",
      "unlock their potential. This paper provides a comprehensive overview of the\n",
      "emerging field of event-based vision, with a focus on the applications and the\n",
      "algorithms developed to unlock the outstanding properties of event cameras. We\n",
      "present event cameras from their working principle, the actual sensors that are\n",
      "available and the tasks that they have been used for, from low-level vision\n",
      "(feature detection and tracking, optic flow, etc.) to high-level vision\n",
      "(reconstruction, segmentation, recognition). We also discuss the techniques\n",
      "developed to process events, including learning-based techniques, as well as\n",
      "specialized processors for these novel sensors, such as spiking neural\n",
      "networks. Additionally, we highlight the challenges that remain to be tackled\n",
      "and the opportunities that lie ahead in the search for a more efficient,\n",
      "bio-inspired way for machines to perceive and interact with the world.\n",
      "\n",
      "**Paper Id :2003.09148 \n",
      "Title :Event-based Asynchronous Sparse Convolutional Networks\n",
      "  Event cameras are bio-inspired sensors that respond to per-pixel brightness\n",
      "changes in the form of asynchronous and sparse \"events\". Recently, pattern\n",
      "recognition algorithms, such as learning-based methods, have made significant\n",
      "progress with event cameras by converting events into synchronous dense,\n",
      "image-like representations and applying traditional machine learning methods\n",
      "developed for standard cameras. However, these approaches discard the spatial\n",
      "and temporal sparsity inherent in event data at the cost of higher\n",
      "computational complexity and latency. In this work, we present a general\n",
      "framework for converting models trained on synchronous image-like event\n",
      "representations into asynchronous models with identical output, thus directly\n",
      "leveraging the intrinsic asynchronous and sparse nature of the event data. We\n",
      "show both theoretically and experimentally that this drastically reduces the\n",
      "computational complexity and latency of high-capacity, synchronous neural\n",
      "networks without sacrificing accuracy. In addition, our framework has several\n",
      "desirable characteristics: (i) it exploits spatio-temporal sparsity of events\n",
      "explicitly, (ii) it is agnostic to the event representation, network\n",
      "architecture, and task, and (iii) it does not require any train-time change,\n",
      "since it is compatible with the standard neural networks' training process. We\n",
      "thoroughly validate the proposed framework on two computer vision tasks: object\n",
      "detection and object recognition. In these tasks, we reduce the computational\n",
      "complexity up to 20 times with respect to high-latency neural networks. At the\n",
      "same time, we outperform state-of-the-art asynchronous approaches up to 24% in\n",
      "prediction accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.08548 \n",
      "Title :A New Class of Time Dependent Latent Factor Models with Applications\n",
      "  In many applications, observed data are influenced by some combination of\n",
      "latent causes. For example, suppose sensors are placed inside a building to\n",
      "record responses such as temperature, humidity, power consumption and noise\n",
      "levels. These random, observed responses are typically affected by many\n",
      "unobserved, latent factors (or features) within the building such as the number\n",
      "of individuals, the turning on and off of electrical devices, power surges,\n",
      "etc. These latent factors are usually present for a contiguous period of time\n",
      "before disappearing; further, multiple factors could be present at a time. This\n",
      "paper develops new probabilistic methodology and inference methods for random\n",
      "object generation influenced by latent features exhibiting temporal\n",
      "persistence. Every datum is associated with subsets of a potentially infinite\n",
      "number of hidden, persistent features that account for temporal dynamics in an\n",
      "observation. The ensuing class of dynamic models constructed by adapting the\n",
      "Indian Buffet Process --- a probability measure on the space of random,\n",
      "unbounded binary matrices --- finds use in a variety of applications arising in\n",
      "operations, signal processing, biomedicine, marketing, image analysis, etc.\n",
      "Illustrations using synthetic and real data are provided.\n",
      "\n",
      "**Paper Id :2002.05909 \n",
      "Title :Deep reconstruction of strange attractors from time series\n",
      "  Experimental measurements of physical systems often have a limited number of\n",
      "independent channels, causing essential dynamical variables to remain\n",
      "unobserved. However, many popular methods for unsupervised inference of latent\n",
      "dynamics from experimental data implicitly assume that the measurements have\n",
      "higher intrinsic dimensionality than the underlying system---making coordinate\n",
      "identification a dimensionality reduction problem. Here, we study the opposite\n",
      "limit, in which hidden governing coordinates must be inferred from only a\n",
      "low-dimensional time series of measurements. Inspired by classical analysis\n",
      "techniques for partial observations of chaotic attractors, we introduce a\n",
      "general embedding technique for univariate and multivariate time series,\n",
      "consisting of an autoencoder trained with a novel latent-space loss function.\n",
      "We show that our technique reconstructs the strange attractors of synthetic and\n",
      "real-world systems better than existing techniques, and that it creates\n",
      "consistent, predictive representations of even stochastic systems. We conclude\n",
      "by using our technique to discover dynamical attractors in diverse systems such\n",
      "as patient electrocardiograms, household electricity usage, neural spiking, and\n",
      "eruptions of the Old Faithful geyser---demonstrating diverse applications of\n",
      "our technique for exploratory data analysis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.08827 \n",
      "Title :Deep Residual Autoencoders for Expectation Maximization-inspired\n",
      "  Dictionary Learning\n",
      "  We introduce a neural-network architecture, termed the constrained recurrent\n",
      "sparse autoencoder (CRsAE), that solves convolutional dictionary learning\n",
      "problems, thus establishing a link between dictionary learning and neural\n",
      "networks. Specifically, we leverage the interpretation of the\n",
      "alternating-minimization algorithm for dictionary learning as an approximate\n",
      "Expectation-Maximization algorithm to develop autoencoders that enable the\n",
      "simultaneous training of the dictionary and regularization parameter (ReLU\n",
      "bias). The forward pass of the encoder approximates the sufficient statistics\n",
      "of the E-step as the solution to a sparse coding problem, using an iterative\n",
      "proximal gradient algorithm called FISTA. The encoder can be interpreted either\n",
      "as a recurrent neural network or as a deep residual network, with two-sided\n",
      "ReLU non-linearities in both cases. The M-step is implemented via a two-stage\n",
      "back-propagation. The first stage relies on a linear decoder applied to the\n",
      "encoder and a norm-squared loss. It parallels the dictionary update step in\n",
      "dictionary learning. The second stage updates the regularization parameter by\n",
      "applying a loss function to the encoder that includes a prior on the parameter\n",
      "motivated by Bayesian statistics. We demonstrate in an image-denoising task\n",
      "that CRsAE learns Gabor-like filters, and that the EM-inspired approach for\n",
      "learning biases is superior to the conventional approach. In an application to\n",
      "recordings of electrical activity from the brain, we demonstrate that CRsAE\n",
      "learns realistic spike templates and speeds up the process of identifying spike\n",
      "times by 900x compared to algorithms based on convex optimization.\n",
      "\n",
      "**Paper Id :1907.03211 \n",
      "Title :Convolutional dictionary learning based auto-encoders for natural\n",
      "  exponential-family distributions\n",
      "  We introduce a class of auto-encoder neural networks tailored to data from\n",
      "the natural exponential family (e.g., count data). The architectures are\n",
      "inspired by the problem of learning the filters in a convolutional generative\n",
      "model with sparsity constraints, often referred to as convolutional dictionary\n",
      "learning (CDL). Our work is the first to combine ideas from convolutional\n",
      "generative models and deep learning for data that are naturally modeled with a\n",
      "non-Gaussian distribution (e.g., binomial and Poisson). This perspective\n",
      "provides us with a scalable and flexible framework that can be re-purposed for\n",
      "a wide range of tasks and assumptions on the generative model. Specifically,\n",
      "the iterative optimization procedure for solving CDL, an unsupervised task, is\n",
      "mapped to an unfolded and constrained neural network, with iterative\n",
      "adjustments to the inputs to account for the generative distribution. We also\n",
      "show that the framework can easily be extended for discriminative training,\n",
      "appropriate for a supervised task. We demonstrate 1) that fitting the\n",
      "generative model to learn, in an unsupervised fashion, the latent stimulus that\n",
      "underlies neural spiking data leads to better goodness-of-fit compared to other\n",
      "baselines, 2) competitive performance compared to state-of-the-art algorithms\n",
      "for supervised Poisson image denoising, with significantly fewer parameters,\n",
      "and 3) gradient dynamics of shallow binomial auto-encoder.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.09235 \n",
      "Title :Reliable Multi-label Classification: Prediction with Partial Abstention\n",
      "  In contrast to conventional (single-label) classification, the setting of\n",
      "multilabel classification (MLC) allows an instance to belong to several classes\n",
      "simultaneously. Thus, instead of selecting a single class label, predictions\n",
      "take the form of a subset of all labels. In this paper, we study an extension\n",
      "of the setting of MLC, in which the learner is allowed to partially abstain\n",
      "from a prediction, that is, to deliver predictions on some but not necessarily\n",
      "all class labels. We propose a formalization of MLC with abstention in terms of\n",
      "a generalized loss minimization problem and present first results for the case\n",
      "of the Hamming loss, rank loss, and F-measure, both theoretical and\n",
      "experimental.\n",
      "\n",
      "**Paper Id :1910.03231 \n",
      "Title :Peer Loss Functions: Learning from Noisy Labels without Knowing Noise\n",
      "  Rates\n",
      "  Learning with noisy labels is a common challenge in supervised learning.\n",
      "Existing approaches often require practitioners to specify noise rates, i.e., a\n",
      "set of parameters controlling the severity of label noises in the problem, and\n",
      "the specifications are either assumed to be given or estimated using additional\n",
      "steps. In this work, we introduce a new family of loss functions that we name\n",
      "as peer loss functions, which enables learning from noisy labels and does not\n",
      "require a priori specification of the noise rates. Peer loss functions work\n",
      "within the standard empirical risk minimization (ERM) framework. We show that,\n",
      "under mild conditions, performing ERM with peer loss functions on the noisy\n",
      "dataset leads to the optimal or a near-optimal classifier as if performing ERM\n",
      "over the clean training data, which we do not have access to. We pair our\n",
      "results with an extensive set of experiments. Peer loss provides a way to\n",
      "simplify model development when facing potentially noisy training labels, and\n",
      "can be promoted as a robust candidate loss function in such situations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.09433 \n",
      "Title :Can Machine Learning Model with Static Features be Fooled: an\n",
      "  Adversarial Machine Learning Approach\n",
      "  The widespread adoption of smartphones dramatically increases the risk of\n",
      "attacks and the spread of mobile malware, especially on the Android platform.\n",
      "Machine learning-based solutions have been already used as a tool to supersede\n",
      "signature-based anti-malware systems. However, malware authors leverage\n",
      "features from malicious and legitimate samples to estimate statistical\n",
      "difference in-order to create adversarial examples. Hence, to evaluate the\n",
      "vulnerability of machine learning algorithms in malware detection, we propose\n",
      "five different attack scenarios to perturb malicious applications (apps). By\n",
      "doing this, the classification algorithm inappropriately fits the discriminant\n",
      "function on the set of data points, eventually yielding a higher\n",
      "misclassification rate. Further, to distinguish the adversarial examples from\n",
      "benign samples, we propose two defense mechanisms to counter attacks. To\n",
      "validate our attacks and solutions, we test our model on three different\n",
      "benchmark datasets. We also test our methods using various classifier\n",
      "algorithms and compare them with the state-of-the-art data poisoning method\n",
      "using the Jacobian matrix. Promising results show that generated adversarial\n",
      "samples can evade detection with a very high probability. Additionally, evasive\n",
      "variants generated by our attack models when used to harden the developed\n",
      "anti-malware system improves the detection rate up to 50% when using the\n",
      "Generative Adversarial Network (GAN) method.\n",
      "\n",
      "**Paper Id :2005.06599 \n",
      "Title :Phishing URL Detection Through Top-level Domain Analysis: A Descriptive\n",
      "  Approach\n",
      "  Phishing is considered to be one of the most prevalent cyber-attacks because\n",
      "of its immense flexibility and alarmingly high success rate. Even with adequate\n",
      "training and high situational awareness, it can still be hard for users to\n",
      "continually be aware of the URL of the website they are visiting. Traditional\n",
      "detection methods rely on blocklists and content analysis, both of which\n",
      "require time-consuming human verification. Thus, there have been attempts\n",
      "focusing on the predictive filtering of such URLs. This study aims to develop a\n",
      "machine-learning model to detect fraudulent URLs which can be used within the\n",
      "Splunk platform. Inspired from similar approaches in the literature, we trained\n",
      "the SVM and Random Forests algorithms using malicious and benign datasets found\n",
      "in the literature and one dataset that we created. We evaluated the algorithms'\n",
      "performance with precision and recall, reaching up to 85% precision and 87%\n",
      "recall in the case of Random Forests while SVM achieved up to 90% precision and\n",
      "88% recall using only descriptive features.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.10797 \n",
      "Title :Machine learning for long-distance quantum communication\n",
      "  Machine learning can help us in solving problems in the context big data\n",
      "analysis and classification, as well as in playing complex games such as Go.\n",
      "But can it also be used to find novel protocols and algorithms for applications\n",
      "such as large-scale quantum communication? Here we show that machine learning\n",
      "can be used to identify central quantum protocols, including teleportation,\n",
      "entanglement purification and the quantum repeater. These schemes are of\n",
      "importance in long-distance quantum communication, and their discovery has\n",
      "shaped the field of quantum information processing. However, the usefulness of\n",
      "learning agents goes beyond the mere re-production of known protocols; the same\n",
      "approach allows one to find improved solutions to long-distance communication\n",
      "problems, in particular when dealing with asymmetric situations where channel\n",
      "noise and segment distance are non-uniform. Our findings are based on the use\n",
      "of projective simulation, a model of a learning agent that combines\n",
      "reinforcement learning and decision making in a physically motivated framework.\n",
      "The learning agent is provided with a universal gate set, and the desired task\n",
      "is specified via a reward scheme. From a technical perspective, the learning\n",
      "agent has to deal with stochastic environments and reactions. We utilize an\n",
      "idea reminiscent of hierarchical skill acquisition, where solutions to\n",
      "sub-problems are learned and re-used in the overall scheme. This is of\n",
      "particular importance in the development of long-distance communication\n",
      "schemes, and opens the way for using machine learning in the design and\n",
      "implementation of quantum networks.\n",
      "\n",
      "**Paper Id :2001.05472 \n",
      "Title :Machine learning transfer efficiencies for noisy quantum walks\n",
      "  Quantum effects are known to provide an advantage in particle transfer across\n",
      "networks. In order to achieve this advantage, requirements on both a graph type\n",
      "and a quantum system coherence must be found. Here we show that the process of\n",
      "finding these requirements can be automated by learning from simulated\n",
      "examples. The automation is done by using a convolutional neural network of a\n",
      "particular type that learns to understand with which network and under which\n",
      "coherence requirements quantum advantage is possible. Our machine learning\n",
      "approach is applied to study noisy quantum walks on cycle graphs of different\n",
      "sizes. We found that it is possible to predict the existence of quantum\n",
      "advantage for the entire decoherence parameter range, even for graphs outside\n",
      "of the training set. Our results are of importance for demonstration of\n",
      "advantage in quantum experiments and pave the way towards automating scientific\n",
      "research and discoveries.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.10900 \n",
      "Title :Learning big Gaussian Bayesian networks: partition, estimation, and\n",
      "  fusion\n",
      "  Structure learning of Bayesian networks has always been a challenging\n",
      "problem. Nowadays, massive-size networks with thousands or more of nodes but\n",
      "fewer samples frequently appear in many areas. We develop a divide-and-conquer\n",
      "framework, called partition-estimation-fusion (PEF), for structure learning of\n",
      "such big networks. The proposed method first partitions nodes into clusters,\n",
      "then learns a subgraph on each cluster of nodes, and finally fuses all learned\n",
      "subgraphs into one Bayesian network. The PEF method is designed in a flexible\n",
      "way so that any structure learning method may be used in the second step to\n",
      "learn a subgraph structure as either a DAG or a CPDAG. In the clustering step,\n",
      "we adapt the hierarchical clustering method to automatically choose a proper\n",
      "number of clusters. In the fusion step, we propose a novel hybrid method that\n",
      "sequentially add edges between subgraphs. Extensive numerical experiments\n",
      "demonstrate the competitive performance of our PEF method, in terms of both\n",
      "speed and accuracy compared to existing methods. Our method can improve the\n",
      "accuracy of structure learning by 20% or more, while reducing running time up\n",
      "to two orders-of-magnitude.\n",
      "\n",
      "**Paper Id :2003.08420 \n",
      "Title :Unsupervised Hierarchical Graph Representation Learning by Mutual\n",
      "  Information Maximization\n",
      "  Graph representation learning based on graph neural networks (GNNs) can\n",
      "greatly improve the performance of downstream tasks, such as node and graph\n",
      "classification. However, the general GNN models do not aggregate node\n",
      "information in a hierarchical manner, and can miss key higher-order structural\n",
      "features of many graphs. The hierarchical aggregation also enables the graph\n",
      "representations to be explainable. In addition, supervised graph representation\n",
      "learning requires labeled data, which is expensive and error-prone. To address\n",
      "these issues, we present an unsupervised graph representation learning method,\n",
      "Unsupervised Hierarchical Graph Representation (UHGR), which can generate\n",
      "hierarchical representations of graphs. Our method focuses on maximizing mutual\n",
      "information between \"local\" and high-level \"global\" representations, which\n",
      "enables us to learn the node embeddings and graph embeddings without any\n",
      "labeled data. To demonstrate the effectiveness of the proposed method, we\n",
      "perform the node and graph classification using the learned node and graph\n",
      "embeddings. The results show that the proposed method achieves comparable\n",
      "results to state-of-the-art supervised methods on several benchmarks. In\n",
      "addition, our visualization of hierarchical representations indicates that our\n",
      "method can capture meaningful and interpretable clusters.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.10959 \n",
      "Title :Crop yield probability density forecasting via quantile random forest\n",
      "  and Epanechnikov Kernel function\n",
      "  A reliable and accurate forecasting model for crop yields is of crucial\n",
      "importance for efficient decision-making process in the agricultural sector.\n",
      "However, due to weather extremes and uncertainties, most forecasting models for\n",
      "crop yield are not reliable and accurate. For measuring the uncertainty and\n",
      "obtaining further information of future crop yields, a probability density\n",
      "forecasting model based on quantile random forest and Epanechnikov kernel\n",
      "function (QRF-SJ) is proposed. The nonlinear structure of random forest is\n",
      "applied to change the quantile regression model for building the probabilistic\n",
      "forecasting model. Epanechnikov kernel function and solve-the equation plug-in\n",
      "approach of Sheather and Jones are used in the kernel density estimation. A\n",
      "case study using the annual crop yield of groundnut and millet in Ghana is\n",
      "presented to illustrate the efficiency and robustness of the proposed\n",
      "technique. The values of the prediction interval coverage probability and\n",
      "prediction interval normalized average width for the two crops show that the\n",
      "constructed prediction intervals capture the observed yields with high coverage\n",
      "probability. The probability density curves show that QRF-SJ method has a very\n",
      "high ability to forecast quality prediction intervals with a higher coverage\n",
      "probability. The feature importance gave a score of the importance of each\n",
      "weather variable in building the quantile regression forest model. The farmer\n",
      "and other stakeholders are able to realize the specific weather variable that\n",
      "affect the yield of a selected crop through feature importance. The proposed\n",
      "method and its application on crop yield dataset are the first of its kind in\n",
      "literature.\n",
      "\n",
      "**Paper Id :2009.10619 \n",
      "Title :An Exponential Factorization Machine with Percentage Error Minimization\n",
      "  to Retail Sales Forecasting\n",
      "  This paper proposes a new approach to sales forecasting for new products with\n",
      "long lead time but short product life cycle. These SKUs are usually sold for\n",
      "one season only, without any replenishments. An exponential factorization\n",
      "machine (EFM) sales forecast model is developed to solve this problem which not\n",
      "only considers SKU attributes, but also pairwise interactions. The EFM model is\n",
      "significantly different from the original Factorization Machines (FM) from\n",
      "two-fold: (1) the attribute-level formulation for explanatory variables and (2)\n",
      "exponential formulation for the positive response variable. The attribute-level\n",
      "formation excludes infeasible intra-attribute interactions and results in more\n",
      "efficient feature engineering comparing with the conventional one-hot encoding,\n",
      "while the exponential formulation is demonstrated more effective than the\n",
      "log-transformation for the positive but not skewed distributed responses. In\n",
      "order to estimate the parameters, percentage error squares (PES) and error\n",
      "squares (ES) are minimized by a proposed adaptive batch gradient descent method\n",
      "over the training set. Real-world data provided by a footwear retailer in\n",
      "Singapore is used for testing the proposed approach. The forecasting\n",
      "performance in terms of both mean absolute percentage error (MAPE) and mean\n",
      "absolute error (MAE) compares favourably with not only off-the-shelf models but\n",
      "also results reported by extant sales and demand forecasting studies. The\n",
      "effectiveness of the proposed approach is also demonstrated by two external\n",
      "public datasets. Moreover, we prove the theoretical relationships between PES\n",
      "and ES minimization, and present an important property of the PES minimization\n",
      "for regression models; that it trains models to underestimate data. This\n",
      "property fits the situation of sales forecasting where unit-holding cost is\n",
      "much greater than the unit-shortage cost.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.12360 \n",
      "Title :Optimizing regularized Cholesky score for order-based learning of\n",
      "  Bayesian networks\n",
      "  Bayesian networks are a class of popular graphical models that encode causal\n",
      "and conditional independence relations among variables by directed acyclic\n",
      "graphs (DAGs). We propose a novel structure learning method, annealing on\n",
      "regularized Cholesky score (ARCS), to search over topological sorts, or\n",
      "permutations of nodes, for a high-scoring Bayesian network. Our scoring\n",
      "function is derived from regularizing Gaussian DAG likelihood, and its\n",
      "optimization gives an alternative formulation of the sparse Cholesky\n",
      "factorization problem from a statistical viewpoint, which is of independent\n",
      "interest. We combine global simulated annealing over permutations with a fast\n",
      "proximal gradient algorithm, operating on triangular matrices of edge\n",
      "coefficients, to compute the score of any permutation. Combined, the two\n",
      "approaches allow us to quickly and effectively search over the space of DAGs\n",
      "without the need to verify the acyclicity constraint or to enumerate possible\n",
      "parent sets given a candidate topological sort. The annealing aspect of the\n",
      "optimization is able to consistently improve the accuracy of DAGs learned by\n",
      "local search algorithms. In addition, we develop several techniques to\n",
      "facilitate the structure learning, including pre-annealing data-driven tuning\n",
      "parameter selection and post-annealing constraint-based structure refinement.\n",
      "Through extensive numerical comparisons, we show that ARCS achieves substantial\n",
      "improvements over existing methods, demonstrating its great potential to learn\n",
      "Bayesian networks from both observational and experimental data.\n",
      "\n",
      "**Paper Id :2011.07225 \n",
      "Title :Reinforced Molecular Optimization with Neighborhood-Controlled Grammars\n",
      "  A major challenge in the pharmaceutical industry is to design novel molecules\n",
      "with specific desired properties, especially when the property evaluation is\n",
      "costly. Here, we propose MNCE-RL, a graph convolutional policy network for\n",
      "molecular optimization with molecular neighborhood-controlled embedding\n",
      "grammars through reinforcement learning. We extend the original\n",
      "neighborhood-controlled embedding grammars to make them applicable to molecular\n",
      "graph generation and design an efficient algorithm to infer grammatical\n",
      "production rules from given molecules. The use of grammars guarantees the\n",
      "validity of the generated molecular structures. By transforming molecular\n",
      "graphs to parse trees with the inferred grammars, the molecular structure\n",
      "generation task is modeled as a Markov decision process where a policy gradient\n",
      "strategy is utilized. In a series of experiments, we demonstrate that our\n",
      "approach achieves state-of-the-art performance in a diverse range of molecular\n",
      "optimization tasks and exhibits significant superiority in optimizing molecular\n",
      "properties with a limited number of property evaluations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1904.13317 \n",
      "Title :A data-efficient geometrically inspired polynomial kernel for robot\n",
      "  inverse dynamics\n",
      "  In this paper, we introduce a novel data-driven inverse dynamics estimator\n",
      "based on Gaussian Process Regression. Driven by the fact that the inverse\n",
      "dynamics can be described as a polynomial function on a suitable input space,\n",
      "we propose the use of a novel kernel, called Geometrically Inspired Polynomial\n",
      "Kernel (GIP). The resulting estimator behaves similarly to model-based\n",
      "approaches as concerns data efficiency. Indeed, we proved that the GIP kernel\n",
      "defines a finite-dimensional Reproducing Kernel Hilbert Space that contains the\n",
      "inverse dynamics function computed through the Rigid Body Dynamics. The\n",
      "proposed kernel is based on the recently introduced Multiplicative Polynomial\n",
      "Kernel, a redefinition of the classical polynomial kernel equipped with a set\n",
      "of parameters that allows for a higher regularization. We tested the proposed\n",
      "approach in a simulated environment, and also in real experiments with a UR10\n",
      "robot. The obtained results confirm that, compared to other data-driven\n",
      "estimators, the proposed approach is more data-efficient and exhibits better\n",
      "generalization properties. Instead, with respect to model-based estimators, our\n",
      "approach requires less prior information and is not affected by model bias.\n",
      "\n",
      "**Paper Id :2004.07210 \n",
      "Title :On Box-Cox Transformation for Image Normality and Pattern Classification\n",
      "  A unique member of the power transformation family is known as the Box-Cox\n",
      "transformation. The latter can be seen as a mathematical operation that leads\n",
      "to finding the optimum lambda ({\\lambda}) value that maximizes the\n",
      "log-likelihood function to transform a data to a normal distribution and to\n",
      "reduce heteroscedasticity. In data analytics, a normality assumption underlies\n",
      "a variety of statistical test models. This technique, however, is best known in\n",
      "statistical analysis to handle one-dimensional data. Herein, this paper\n",
      "revolves around the utility of such a tool as a pre-processing step to\n",
      "transform two-dimensional data, namely, digital images and to study its effect.\n",
      "Moreover, to reduce time complexity, it suffices to estimate the parameter\n",
      "lambda in real-time for large two-dimensional matrices by merely considering\n",
      "their probability density function as a statistical inference of the underlying\n",
      "data distribution. We compare the effect of this light-weight Box-Cox\n",
      "transformation with well-established state-of-the-art low light image\n",
      "enhancement techniques. We also demonstrate the effectiveness of our approach\n",
      "through several test-bed data sets for generic improvement of visual appearance\n",
      "of images and for ameliorating the performance of a colour pattern\n",
      "classification algorithm as an example application. Results with and without\n",
      "the proposed approach, are compared using the AlexNet (transfer deep learning)\n",
      "pretrained model. To the best of our knowledge, this is the first time that the\n",
      "Box-Cox transformation is extended to digital images by exploiting histogram\n",
      "transformation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.00424 \n",
      "Title :An ADMM Based Framework for AutoML Pipeline Configuration\n",
      "  We study the AutoML problem of automatically configuring machine learning\n",
      "pipelines by jointly selecting algorithms and their appropriate\n",
      "hyper-parameters for all steps in supervised learning pipelines. This black-box\n",
      "(gradient-free) optimization with mixed integer & continuous variables is a\n",
      "challenging problem. We propose a novel AutoML scheme by leveraging the\n",
      "alternating direction method of multipliers (ADMM). The proposed framework is\n",
      "able to (i) decompose the optimization problem into easier sub-problems that\n",
      "have a reduced number of variables and circumvent the challenge of mixed\n",
      "variable categories, and (ii) incorporate black-box constraints along-side the\n",
      "black-box optimization objective. We empirically evaluate the flexibility (in\n",
      "utilizing existing AutoML techniques), effectiveness (against open source\n",
      "AutoML toolkits),and unique capability (of executing AutoML with practically\n",
      "motivated black-box constraints) of our proposed scheme on a collection of\n",
      "binary classification data sets from UCI ML& OpenML repositories. We observe\n",
      "that on an average our framework provides significant gains in comparison to\n",
      "other AutoML frameworks (Auto-sklearn & TPOT), highlighting the practical\n",
      "advantages of this framework.\n",
      "\n",
      "**Paper Id :2007.13243 \n",
      "Title :Scalable Derivative-Free Optimization for Nonlinear Least-Squares\n",
      "  Problems\n",
      "  Derivative-free - or zeroth-order - optimization (DFO) has gained recent\n",
      "attention for its ability to solve problems in a variety of application areas,\n",
      "including machine learning, particularly involving objectives which are\n",
      "stochastic and/or expensive to compute. In this work, we develop a novel\n",
      "model-based DFO method for solving nonlinear least-squares problems. We improve\n",
      "on state-of-the-art DFO by performing dimensionality reduction in the\n",
      "observational space using sketching methods, avoiding the construction of a\n",
      "full local model. Our approach has a per-iteration computational cost which is\n",
      "linear in problem dimension in a big data regime, and numerical evidence\n",
      "demonstrates that, compared to existing software, it has dramatically improved\n",
      "runtime performance on overdetermined least-squares problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.00820 \n",
      "Title :On the smoothness of nonlinear system identification\n",
      "  We shed new light on the \\textit{smoothness} of optimization problems arising\n",
      "in prediction error parameter estimation of linear and nonlinear systems. We\n",
      "show that for regions of the parameter space where the model is not\n",
      "contractive, the Lipschitz constant and $\\beta$-smoothness of the objective\n",
      "function might blow up exponentially with the simulation length, making it hard\n",
      "to numerically find minima within those regions or, even, to escape from them.\n",
      "In addition to providing theoretical understanding of this problem, this paper\n",
      "also proposes the use of multiple shooting as a viable solution. The proposed\n",
      "method minimizes the error between a prediction model and the observed values.\n",
      "Rather than running the prediction model over the entire dataset, multiple\n",
      "shooting splits the data into smaller subsets and runs the prediction model\n",
      "over each subset, making the simulation length a design parameter and making it\n",
      "possible to solve problems that would be infeasible using a standard approach.\n",
      "The equivalence to the original problem is obtained by including constraints in\n",
      "the optimization. The new method is illustrated by estimating the parameters of\n",
      "nonlinear systems with chaotic or unstable behavior, as well as neural\n",
      "networks. We also present a comparative analysis of the proposed method with\n",
      "multi-step-ahead prediction error minimization.\n",
      "\n",
      "**Paper Id :1911.08817 \n",
      "Title :Black-box Combinatorial Optimization using Models with Integer-valued\n",
      "  Minima\n",
      "  When a black-box optimization objective can only be evaluated with costly or\n",
      "noisy measurements, most standard optimization algorithms are unsuited to find\n",
      "the optimal solution. Specialized algorithms that deal with exactly this\n",
      "situation make use of surrogate models. These models are usually continuous and\n",
      "smooth, which is beneficial for continuous optimization problems, but not\n",
      "necessarily for combinatorial problems. However, by choosing the basis\n",
      "functions of the surrogate model in a certain way, we show that it can be\n",
      "guaranteed that the optimal solution of the surrogate model is integer. This\n",
      "approach outperforms random search, simulated annealing and one Bayesian\n",
      "optimization algorithm on the problem of finding robust routes for a\n",
      "noise-perturbed traveling salesman benchmark problem, with similar performance\n",
      "as another Bayesian optimization algorithm, and outperforms all compared\n",
      "algorithms on a convex binary optimization problem with a large number of\n",
      "variables.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.01127 \n",
      "Title :Uncertainty-Aware Principal Component Analysis\n",
      "  We present a technique to perform dimensionality reduction on data that is\n",
      "subject to uncertainty. Our method is a generalization of traditional principal\n",
      "component analysis (PCA) to multivariate probability distributions. In\n",
      "comparison to non-linear methods, linear dimensionality reduction techniques\n",
      "have the advantage that the characteristics of such probability distributions\n",
      "remain intact after projection. We derive a representation of the PCA sample\n",
      "covariance matrix that respects potential uncertainty in each of the inputs,\n",
      "building the mathematical foundation of our new method: uncertainty-aware PCA.\n",
      "In addition to the accuracy and performance gained by our approach over\n",
      "sampling-based strategies, our formulation allows us to perform sensitivity\n",
      "analysis with regard to the uncertainty in the data. For this, we propose\n",
      "factor traces as a novel visualization that enables to better understand the\n",
      "influence of uncertainty on the chosen principal components. We provide\n",
      "multiple examples of our technique using real-world datasets. As a special\n",
      "case, we show how to propagate multivariate normal distributions through PCA in\n",
      "closed form. Furthermore, we discuss extensions and limitations of our\n",
      "approach.\n",
      "\n",
      "**Paper Id :1812.05988 \n",
      "Title :Class Mean Vector Component and Discriminant Analysis\n",
      "  The kernel matrix used in kernel methods encodes all the information required\n",
      "for solving complex nonlinear problems defined on data representations in the\n",
      "input space using simple, but implicitly defined, solutions. Spectral analysis\n",
      "on the kernel matrix defines an explicit nonlinear mapping of the input data\n",
      "representations to a subspace of the kernel space, which can be used for\n",
      "directly applying linear methods. However, the selection of the kernel subspace\n",
      "is crucial for the performance of the proceeding processing steps. In this\n",
      "paper, we propose a component analysis method for kernel-based dimensionality\n",
      "reduction that optimally preserves the pair-wise distances of the class means\n",
      "in the feature space. We provide extensive analysis on the connection of the\n",
      "proposed criterion to those used in kernel principal component analysis and\n",
      "kernel discriminant analysis, leading to a discriminant analysis version of the\n",
      "proposed method. Our analysis also provides more insights on the properties of\n",
      "the feature spaces obtained by applying these methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.01258 \n",
      "Title :Disentangling Factors of Variation Using Few Labels\n",
      "  Learning disentangled representations is considered a cornerstone problem in\n",
      "representation learning. Recently, Locatello et al. (2019) demonstrated that\n",
      "unsupervised disentanglement learning without inductive biases is theoretically\n",
      "impossible and that existing inductive biases and unsupervised methods do not\n",
      "allow to consistently learn disentangled representations. However, in many\n",
      "practical settings, one might have access to a limited amount of supervision,\n",
      "for example through manual labeling of (some) factors of variation in a few\n",
      "training examples. In this paper, we investigate the impact of such supervision\n",
      "on state-of-the-art disentanglement methods and perform a large scale study,\n",
      "training over 52000 models under well-defined and reproducible experimental\n",
      "conditions. We observe that a small number of labeled examples (0.01--0.5\\% of\n",
      "the data set), with potentially imprecise and incomplete labels, is sufficient\n",
      "to perform model selection on state-of-the-art unsupervised models. Further, we\n",
      "investigate the benefit of incorporating supervision into the training process.\n",
      "Overall, we empirically validate that with little and imprecise supervision it\n",
      "is possible to reliably learn disentangled representations.\n",
      "\n",
      "**Paper Id :2007.15474 \n",
      "Title :Music FaderNets: Controllable Music Generation Based On High-Level\n",
      "  Features via Low-Level Feature Modelling\n",
      "  High-level musical qualities (such as emotion) are often abstract,\n",
      "subjective, and hard to quantify. Given these difficulties, it is not easy to\n",
      "learn good feature representations with supervised learning techniques, either\n",
      "because of the insufficiency of labels, or the subjectiveness (and hence large\n",
      "variance) in human-annotated labels. In this paper, we present a framework that\n",
      "can learn high-level feature representations with a limited amount of data, by\n",
      "first modelling their corresponding quantifiable low-level attributes. We refer\n",
      "to our proposed framework as Music FaderNets, which is inspired by the fact\n",
      "that low-level attributes can be continuously manipulated by separate \"sliding\n",
      "faders\" through feature disentanglement and latent regularization techniques.\n",
      "High-level features are then inferred from the low-level representations\n",
      "through semi-supervised clustering using Gaussian Mixture Variational\n",
      "Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we\n",
      "show that the \"faders\" of our model are disentangled and change linearly w.r.t.\n",
      "the modelled low-level attributes of the generated output music. Furthermore,\n",
      "we demonstrate that the model successfully learns the intrinsic relationship\n",
      "between arousal and its corresponding low-level attributes (rhythm and note\n",
      "density), with only 1% of the training set being labelled. Finally, using the\n",
      "learnt high-level feature representations, we explore the application of our\n",
      "framework in style transfer tasks across different arousal states. The\n",
      "effectiveness of this approach is verified through a subjective listening test.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.01997 \n",
      "Title :Deep Learning for Sequential Recommendation: Algorithms, Influential\n",
      "  Factors, and Evaluations\n",
      "  In the field of sequential recommendation, deep learning (DL)-based methods\n",
      "have received a lot of attention in the past few years and surpassed\n",
      "traditional models such as Markov chain-based and factorization-based ones.\n",
      "However, there is little systematic study on DL-based methods, especially\n",
      "regarding to how to design an effective DL model for sequential recommendation.\n",
      "In this view, this survey focuses on DL-based sequential recommender systems by\n",
      "taking the aforementioned issues into consideration. Specifically,we illustrate\n",
      "the concept of sequential recommendation, propose a categorization of existing\n",
      "algorithms in terms of three types of behavioral sequence, summarize the key\n",
      "factors affecting the performance of DL-based models, and conduct corresponding\n",
      "evaluations to demonstrate the effects of these factors. We conclude this\n",
      "survey by systematically outlining future directions and challenges in this\n",
      "field.\n",
      "\n",
      "**Paper Id :2002.09841 \n",
      "Title :SetRank: A Setwise Bayesian Approach for Collaborative Ranking from\n",
      "  Implicit Feedback\n",
      "  The recent development of online recommender systems has a focus on\n",
      "collaborative ranking from implicit feedback, such as user clicks and\n",
      "purchases. Different from explicit ratings, which reflect graded user\n",
      "preferences, the implicit feedback only generates positive and unobserved\n",
      "labels. While considerable efforts have been made in this direction, the\n",
      "well-known pairwise and listwise approaches have still been limited by various\n",
      "challenges. Specifically, for the pairwise approaches, the assumption of\n",
      "independent pairwise preference is not always held in practice. Also, the\n",
      "listwise approaches cannot efficiently accommodate \"ties\" due to the\n",
      "precondition of the entire list permutation. To this end, in this paper, we\n",
      "propose a novel setwise Bayesian approach for collaborative ranking, namely\n",
      "SetRank, to inherently accommodate the characteristics of implicit feedback in\n",
      "recommender system. Specifically, SetRank aims at maximizing the posterior\n",
      "probability of novel setwise preference comparisons and can be implemented with\n",
      "matrix factorization and neural networks. Meanwhile, we also present the\n",
      "theoretical analysis of SetRank to show that the bound of excess risk can be\n",
      "proportional to $\\sqrt{M/N}$, where $M$ and $N$ are the numbers of items and\n",
      "users, respectively. Finally, extensive experiments on four real-world datasets\n",
      "clearly validate the superiority of SetRank compared with various\n",
      "state-of-the-art baselines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.02685 \n",
      "Title :Knowing The What But Not The Where in Bayesian Optimization\n",
      "  Bayesian optimization has demonstrated impressive success in finding the\n",
      "optimum input x* and output f* = f(x*) = max f(x) of a black-box function f. In\n",
      "some applications, however, the optimum output f* is known in advance and the\n",
      "goal is to find the corresponding optimum input x*. In this paper, we consider\n",
      "a new setting in BO in which the knowledge of the optimum output f* is\n",
      "available. Our goal is to exploit the knowledge about f* to search for the\n",
      "input x* efficiently. To achieve this goal, we first transform the Gaussian\n",
      "process surrogate using the information about the optimum output. Then, we\n",
      "propose two acquisition functions, called confidence bound minimization and\n",
      "expected regret minimization. We show that our approaches work intuitively and\n",
      "give quantitatively better performance against standard BO methods. We\n",
      "demonstrate real applications in tuning a deep reinforcement learning algorithm\n",
      "on the CartPole problem and XGBoost on Skin Segmentation dataset in which the\n",
      "optimum values are publicly available.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.03297 \n",
      "Title :Interpretable Subgroup Discovery in Treatment Effect Estimation with\n",
      "  Application to Opioid Prescribing Guidelines\n",
      "  The dearth of prescribing guidelines for physicians is one key driver of the\n",
      "current opioid epidemic in the United States. In this work, we analyze medical\n",
      "and pharmaceutical claims data to draw insights on characteristics of patients\n",
      "who are more prone to adverse outcomes after an initial synthetic opioid\n",
      "prescription. Toward this end, we propose a generative model that allows\n",
      "discovery from observational data of subgroups that demonstrate an enhanced or\n",
      "diminished causal effect due to treatment. Our approach models these\n",
      "sub-populations as a mixture distribution, using sparsity to enhance\n",
      "interpretability, while jointly learning nonlinear predictors of the potential\n",
      "outcomes to better adjust for confounding. The approach leads to\n",
      "human-interpretable insights on discovered subgroups, improving the practical\n",
      "utility for decision support\n",
      "\n",
      "**Paper Id :2001.04754 \n",
      "Title :Learning Overlapping Representations for the Estimation of\n",
      "  Individualized Treatment Effects\n",
      "  The choice of making an intervention depends on its potential benefit or harm\n",
      "in comparison to alternatives. Estimating the likely outcome of alternatives\n",
      "from observational data is a challenging problem as all outcomes are never\n",
      "observed, and selection bias precludes the direct comparison of differently\n",
      "intervened groups. Despite their empirical success, we show that algorithms\n",
      "that learn domain-invariant representations of inputs (on which to make\n",
      "predictions) are often inappropriate, and develop generalization bounds that\n",
      "demonstrate the dependence on domain overlap and highlight the need for\n",
      "invertible latent maps. Based on these results, we develop a deep kernel\n",
      "regression algorithm and posterior regularization framework that substantially\n",
      "outperforms the state-of-the-art on a variety of benchmarks data sets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.03970 \n",
      "Title :Reinforcement Learning in Non-Stationary Environments\n",
      "  Reinforcement learning (RL) methods learn optimal decisions in the presence\n",
      "of a stationary environment. However, the stationary assumption on the\n",
      "environment is very restrictive. In many real world problems like traffic\n",
      "signal control, robotic applications, one often encounters situations with\n",
      "non-stationary environments and in these scenarios, RL methods yield\n",
      "sub-optimal decisions. In this paper, we thus consider the problem of\n",
      "developing RL methods that obtain optimal decisions in a non-stationary\n",
      "environment. The goal of this problem is to maximize the long-term discounted\n",
      "reward achieved when the underlying model of the environment changes over time.\n",
      "To achieve this, we first adapt a change point algorithm to detect change in\n",
      "the statistics of the environment and then develop an RL algorithm that\n",
      "maximizes the long-run reward accrued. We illustrate that our change point\n",
      "method detects change in the model of the environment effectively and thus\n",
      "facilitates the RL algorithm in maximizing the long-run reward. We further\n",
      "validate the effectiveness of the proposed solution on non-stationary random\n",
      "Markov decision processes, a sensor energy management problem and a traffic\n",
      "signal control problem.\n",
      "\n",
      "**Paper Id :1911.01546 \n",
      "Title :Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy\n",
      "  While maximizing expected return is the goal in most reinforcement learning\n",
      "approaches, risk-sensitive objectives such as conditional value at risk (CVaR)\n",
      "are more suitable for many high-stakes applications. However, relatively little\n",
      "is known about how to explore to quickly learn policies with good CVaR. In this\n",
      "paper, we present the first algorithm for sample-efficient learning of\n",
      "CVaR-optimal policies in Markov decision processes based on the optimism in the\n",
      "face of uncertainty principle. This method relies on a novel optimistic version\n",
      "of the distributional Bellman operator that moves probability mass from the\n",
      "lower to the upper tail of the return distribution. We prove asymptotic\n",
      "convergence and optimism of this operator for the tabular policy evaluation\n",
      "case. We further demonstrate that our algorithm finds CVaR-optimal policies\n",
      "substantially faster than existing baselines in several simulated environments\n",
      "with discrete and continuous state spaces.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.04071 \n",
      "Title :Survey on Evaluation Methods for Dialogue Systems\n",
      "  In this paper we survey the methods and concepts developed for the evaluation\n",
      "of dialogue systems. Evaluation is a crucial part during the development\n",
      "process. Often, dialogue systems are evaluated by means of human evaluations\n",
      "and questionnaires. However, this tends to be very cost and time intensive.\n",
      "Thus, much work has been put into finding methods, which allow to reduce the\n",
      "involvement of human labour. In this survey, we present the main concepts and\n",
      "methods. For this, we differentiate between the various classes of dialogue\n",
      "systems (task-oriented dialogue systems, conversational dialogue systems, and\n",
      "question-answering dialogue systems). We cover each class by introducing the\n",
      "main technologies developed for the dialogue systems and then by presenting the\n",
      "evaluation methods regarding this class.\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.04232 \n",
      "Title :Automatic Programming of Cellular Automata and Artificial Neural\n",
      "  Networks Guided by Philosophy\n",
      "  Many computer models such as cellular automata and artificial neural networks\n",
      "have been developed and successfully applied. However, in some cases, these\n",
      "models might be restrictive on the possible solutions or their solutions might\n",
      "be difficult to interpret. To overcome this problem, we outline a new approach,\n",
      "the so-called allagmatic method, that automatically programs and executes\n",
      "models with as little limitations as possible while maintaining human\n",
      "interpretability. Earlier we described a metamodel and its building blocks\n",
      "according to the philosophical concepts of structure (spatial dimension) and\n",
      "operation (temporal dimension). They are entity, milieu, and update function\n",
      "that together abstractly describe cellular automata, artificial neural\n",
      "networks, and possibly any kind of computer model. By automatically combining\n",
      "these building blocks in an evolutionary computation, interpretability might be\n",
      "increased by the relationship to the metamodel, and models might be translated\n",
      "into more interpretable models via the metamodel. We propose generic and\n",
      "object-oriented programming to implement the entities and their milieus as\n",
      "dynamic and generic arrays and the update function as a method. We show two\n",
      "experiments where a simple cellular automaton and an artificial neural network\n",
      "are automatically programmed, compiled, and executed. A target state is\n",
      "successfully evolved and learned in the cellular automaton and artificial\n",
      "neural network, respectively. We conclude that the allagmatic method can create\n",
      "and execute cellular automaton and artificial neural network models in an\n",
      "automated manner with the guidance of philosophy.\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.04305 \n",
      "Title :Spectral Reconstruction with Deep Neural Networks\n",
      "  We explore artificial neural networks as a tool for the reconstruction of\n",
      "spectral functions from imaginary time Green's functions, a classic\n",
      "ill-conditioned inverse problem. Our ansatz is based on a supervised learning\n",
      "framework in which prior knowledge is encoded in the training data and the\n",
      "inverse transformation manifold is explicitly parametrised through a neural\n",
      "network. We systematically investigate this novel reconstruction approach,\n",
      "providing a detailed analysis of its performance on physically motivated mock\n",
      "data, and compare it to established methods of Bayesian inference. The\n",
      "reconstruction accuracy is found to be at least comparable, and potentially\n",
      "superior in particular at larger noise levels. We argue that the use of\n",
      "labelled training data in a supervised setting and the freedom in defining an\n",
      "optimisation objective are inherent advantages of the present approach and may\n",
      "lead to significant improvements over state-of-the-art methods in the future.\n",
      "Potential directions for further research are discussed in detail.\n",
      "\n",
      "**Paper Id :2003.01504 \n",
      "Title :Towards Novel Insights in Lattice Field Theory with Explainable Machine\n",
      "  Learning\n",
      "  Machine learning has the potential to aid our understanding of phase\n",
      "structures in lattice quantum field theories through the statistical analysis\n",
      "of Monte Carlo samples. Available algorithms, in particular those based on deep\n",
      "learning, often demonstrate remarkable performance in the search for previously\n",
      "unidentified features, but tend to lack transparency if applied naively. To\n",
      "address these shortcomings, we propose representation learning in combination\n",
      "with interpretability methods as a framework for the identification of\n",
      "observables. More specifically, we investigate action parameter regression as a\n",
      "pretext task while using layer-wise relevance propagation (LRP) to identify the\n",
      "most important observables depending on the location in the phase diagram. The\n",
      "approach is put to work in the context of a scalar Yukawa model in (2+1)d.\n",
      "First, we investigate a multilayer perceptron to determine an importance\n",
      "hierarchy of several predefined, standard observables. The method is then\n",
      "applied directly to the raw field configurations using a convolutional network,\n",
      "demonstrating the ability to reconstruct all order parameters from the learned\n",
      "filter weights. Based on our results, we argue that due to its broad\n",
      "applicability, attribution methods such as LRP could prove a useful and\n",
      "versatile tool in our search for new physical insights. In the case of the\n",
      "Yukawa model, it facilitates the construction of an observable that\n",
      "characterises the symmetric phase.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.04559 \n",
      "Title :ForestDSH: A Universal Hash Design for Discrete Probability\n",
      "  Distributions\n",
      "  In this paper, we consider the problem of classification of $M$ high\n",
      "dimensional queries $y^1,\\cdots,y^M\\in B^S$ to $N$ high dimensional classes\n",
      "$x^1,\\cdots,x^N\\in A^S$ where $A$ and $B$ are discrete alphabets and the\n",
      "probabilistic model that relates data to the classes $P(x,y)$ is known. This\n",
      "problem has applications in various fields including the database search\n",
      "problem in mass spectrometry. The problem is analogous to the nearest neighbor\n",
      "search problem, where the goal is to find the data point in a database that is\n",
      "the most similar to a query point. The state of the art method for solving an\n",
      "approximate version of the nearest neighbor search problem in high dimensions\n",
      "is locality sensitive hashing (LSH). LSH is based on designing hash functions\n",
      "that map near points to the same buckets with a probability higher than random\n",
      "(far) points. To solve our high dimensional classification problem, we\n",
      "introduce distribution sensitive hashes that map jointly generated pairs\n",
      "$(x,y)\\sim P$ to the same bucket with probability higher than random pairs\n",
      "$x\\sim P^A$ and $y\\sim P^B$, where $P^A$ and $P^B$ are the marginal probability\n",
      "distributions of $P$. We design distribution sensitive hashes using a forest of\n",
      "decision trees and we show that the complexity of search grows with\n",
      "$O(N^{\\lambda^*(P)})$ where $\\lambda^*(P)$ is expressed in an analytical form.\n",
      "We further show that the proposed hashes perform faster than state of the art\n",
      "approximate nearest neighbor search methods for a range of probability\n",
      "distributions, in both theory and simulations. Finally, we apply our method to\n",
      "the spectral library search problem in mass spectrometry, and show that it is\n",
      "an order of magnitude faster than the state of the art methods.\n",
      "\n",
      "**Paper Id :2002.05056 \n",
      "Title :Quantum Boosting\n",
      "  Suppose we have a weak learning algorithm $\\mathcal{A}$ for a Boolean-valued\n",
      "problem: $\\mathcal{A}$ produces hypotheses whose bias $\\gamma$ is small, only\n",
      "slightly better than random guessing (this could, for instance, be due to\n",
      "implementing $\\mathcal{A}$ on a noisy device), can we boost the performance of\n",
      "$\\mathcal{A}$ so that $\\mathcal{A}$'s output is correct on $2/3$ of the inputs?\n",
      "  Boosting is a technique that converts a weak and inaccurate machine learning\n",
      "algorithm into a strong accurate learning algorithm. The AdaBoost algorithm by\n",
      "Freund and Schapire (for which they were awarded the G\\\"odel prize in 2003) is\n",
      "one of the widely used boosting algorithms, with many applications in theory\n",
      "and practice. Suppose we have a $\\gamma$-weak learner for a Boolean concept\n",
      "class $C$ that takes time $R(C)$, then the time complexity of AdaBoost scales\n",
      "as $VC(C)\\cdot poly(R(C), 1/\\gamma)$, where $VC(C)$ is the $VC$-dimension of\n",
      "$C$. In this paper, we show how quantum techniques can improve the time\n",
      "complexity of classical AdaBoost. To this end, suppose we have a $\\gamma$-weak\n",
      "quantum learner for a Boolean concept class $C$ that takes time $Q(C)$, we\n",
      "introduce a quantum boosting algorithm whose complexity scales as\n",
      "$\\sqrt{VC(C)}\\cdot poly(Q(C),1/\\gamma);$ thereby achieving a quadratic quantum\n",
      "improvement over classical AdaBoost in terms of $VC(C)$.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.04859 \n",
      "Title :Physically-interpretable classification of biological network dynamics\n",
      "  for complex collective motions\n",
      "  Understanding biological network dynamics is a fundamental issue in various\n",
      "scientific and engineering fields. Network theory is capable of revealing the\n",
      "relationship between elements and their propagation; however, for complex\n",
      "collective motions, the network properties often transiently and complexly\n",
      "change. A fundamental question addressed here pertains to the classification of\n",
      "collective motion network based on physically-interpretable dynamical\n",
      "properties. Here we apply a data-driven spectral analysis called graph dynamic\n",
      "mode decomposition, which obtains the dynamical properties for collective\n",
      "motion classification. Using a ballgame as an example, we classified the\n",
      "strategic collective motions in different global behaviours and discovered\n",
      "that, in addition to the physical properties, the contextual node information\n",
      "was critical for classification. Furthermore, we discovered the label-specific\n",
      "stronger spectra in the relationship among the nearest agents, providing\n",
      "physical and semantic interpretations. Our approach contributes to the\n",
      "understanding of principles of biological complex network dynamics from the\n",
      "perspective of nonlinear dynamical systems.\n",
      "\n",
      "**Paper Id :2011.05516 \n",
      "Title :Probability-Density-Based Deep Learning Paradigm for the Fuzzy Design of\n",
      "  Functional Metastructures\n",
      "  In quantum mechanics, a norm squared wave function can be interpreted as the\n",
      "probability density that describes the likelihood of a particle to be measured\n",
      "in a given position or momentum. This statistical property is at the core of\n",
      "the fuzzy structure of microcosmos. Recently, hybrid neural structures raised\n",
      "intense attention, resulting in various intelligent systems with far-reaching\n",
      "influence. Here, we propose a probability-density-based deep learning paradigm\n",
      "for the fuzzy design of functional meta-structures. In contrast to other\n",
      "inverse design methods, our probability-density-based neural network can\n",
      "efficiently evaluate and accurately capture all plausible meta-structures in a\n",
      "high-dimensional parameter space. Local maxima in probability density\n",
      "distribution correspond to the most likely candidates to meet the desired\n",
      "performances. We verify this universally adaptive approach in but not limited\n",
      "to acoustics by designing multiple meta-structures for each targeted\n",
      "transmission spectrum, with experiments unequivocally demonstrating the\n",
      "effectiveness and generalization of the inverse design.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.05334 \n",
      "Title :Generating Weighted MAX-2-SAT Instances of Tunable Difficulty with\n",
      "  Frustrated Loops\n",
      "  Many optimization problems can be cast into the maximum satisfiability\n",
      "(MAX-SAT) form, and many solvers have been developed for tackling such\n",
      "problems. To evaluate a MAX-SAT solver, it is convenient to generate hard\n",
      "MAX-SAT instances with known solutions. Here, we propose a method of generating\n",
      "weighted MAX-2-SAT instances inspired by the frustrated-loop algorithm used by\n",
      "the quantum annealing community. We extend the algorithm for instances of\n",
      "general bipartite couplings, with the associated optimization problem being the\n",
      "minimization of the restricted Boltzmann machine (RBM) energy over the nodal\n",
      "values, which is useful for effectively pre-training the RBM. The hardness of\n",
      "the generated instances can be tuned through a central parameter known as the\n",
      "frustration index. Two versions of the algorithm are presented: the random- and\n",
      "structured-loop algorithms. For the random-loop algorithm, we provide a\n",
      "thorough theoretical and empirical analysis on its mathematical properties from\n",
      "the perspective of frustration, and observe empirically a double phase\n",
      "transition behavior in the hardness scaling behavior driven by the frustration\n",
      "index. For the structured-loop algorithm, we show that it offers an improvement\n",
      "in hardness over the random-loop algorithm in the regime of high loop density,\n",
      "with the variation of hardness tunable through the concentration of frustrated\n",
      "weights.\n",
      "\n",
      "**Paper Id :2006.03963 \n",
      "Title :Combinatorial Black-Box Optimization with Expert Advice\n",
      "  We consider the problem of black-box function optimization over the boolean\n",
      "hypercube. Despite the vast literature on black-box function optimization over\n",
      "continuous domains, not much attention has been paid to learning models for\n",
      "optimization over combinatorial domains until recently. However, the\n",
      "computational complexity of the recently devised algorithms are prohibitive\n",
      "even for moderate numbers of variables; drawing one sample using the existing\n",
      "algorithms is more expensive than a function evaluation for many black-box\n",
      "functions of interest. To address this problem, we propose a computationally\n",
      "efficient model learning algorithm based on multilinear polynomials and\n",
      "exponential weight updates. In the proposed algorithm, we alternate between\n",
      "simulated annealing with respect to the current polynomial representation and\n",
      "updating the weights using monomial experts' advice. Numerical experiments on\n",
      "various datasets in both unconstrained and sum-constrained boolean optimization\n",
      "indicate the competitive performance of the proposed algorithm, while improving\n",
      "the computational time up to several orders of magnitude compared to\n",
      "state-of-the-art algorithms in the literature.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.05604 \n",
      "Title :Embeddings of Persistence Diagrams into Hilbert Spaces\n",
      "  Since persistence diagrams do not admit an inner product structure, a map\n",
      "into a Hilbert space is needed in order to use kernel methods. It is natural to\n",
      "ask if such maps necessarily distort the metric on persistence diagrams. We\n",
      "show that persistence diagrams with the bottleneck distance do not even admit a\n",
      "coarse embedding into a Hilbert space. As part of our proof, we show that any\n",
      "separable, bounded metric space isometrically embeds into the space of\n",
      "persistence diagrams with the bottleneck distance. As corollaries, we obtain\n",
      "the generalized roundness, negative type, and asymptotic dimension of this\n",
      "space.\n",
      "\n",
      "**Paper Id :1905.13196 \n",
      "Title :Persistent homology detects curvature\n",
      "  In topological data analysis, persistent homology is used to study the \"shape\n",
      "of data\". Persistent homology computations are completely characterized by a\n",
      "set of intervals called a bar code. It is often said that the long intervals\n",
      "represent the \"topological signal\" and the short intervals represent \"noise\".\n",
      "We give evidence to dispute this thesis, showing that the short intervals\n",
      "encode geometric information. Specifically, we prove that persistent homology\n",
      "detects the curvature of disks from which points have been sampled. We describe\n",
      "a general computational framework for solving inverse problems using the\n",
      "average persistence landscape, a continuous mapping from metric spaces with a\n",
      "probability measure to a Hilbert space. In the present application, the average\n",
      "persistence landscapes of points sampled from disks of constant curvature\n",
      "results in a path in this Hilbert space which may be learned using standard\n",
      "tools from statistical and machine learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.07210 \n",
      "Title :Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using\n",
      "  Non-IID Data\n",
      "  This paper proposes a cooperative mechanism for mitigating the performance\n",
      "degradation due to non-independent-and-identically-distributed (non-IID) data\n",
      "in collaborative machine learning (ML), namely federated learning (FL), which\n",
      "trains an ML model using the rich data and computational resources of mobile\n",
      "clients without gathering their data to central systems. The data of mobile\n",
      "clients is typically non-IID owing to diversity among mobile clients' interests\n",
      "and usage, and FL with non-IID data could degrade the model performance.\n",
      "Therefore, to mitigate the degradation induced by non-IID data, we assume that\n",
      "a limited number (e.g., less than 1%) of clients allow their data to be\n",
      "uploaded to a server, and we propose a hybrid learning mechanism referred to as\n",
      "Hybrid-FL, wherein the server updates the model using the data gathered from\n",
      "the clients and aggregates the model with the models trained by clients. The\n",
      "Hybrid-FL solves both client- and data-selection problems via heuristic\n",
      "algorithms, which try to select the optimal sets of clients who train models\n",
      "with their own data, clients who upload their data to the server, and data\n",
      "uploaded to the server. The algorithms increase the number of clients\n",
      "participating in FL and make more data gather in the server IID, thereby\n",
      "improving the prediction accuracy of the aggregated model. Evaluations, which\n",
      "consist of network simulations and ML experiments, demonstrate that the\n",
      "proposed scheme achieves a 13.5% higher classification accuracy than those of\n",
      "the previously proposed schemes for the non-IID case.\n",
      "\n",
      "**Paper Id :2009.10269 \n",
      "Title :An Incentive Mechanism for Federated Learning in Wireless Cellular\n",
      "  network: An Auction Approach\n",
      "  Federated Learning (FL) is a distributed learning framework that can deal\n",
      "with the distributed issue in machine learning and still guarantee high\n",
      "learning performance. However, it is impractical that all users will sacrifice\n",
      "their resources to join the FL algorithm. This motivates us to study the\n",
      "incentive mechanism design for FL. In this paper, we consider a FL system that\n",
      "involves one base station (BS) and multiple mobile users. The mobile users use\n",
      "their own data to train the local machine learning model, and then send the\n",
      "trained models to the BS, which generates the initial model, collects local\n",
      "models and constructs the global model. Then, we formulate the incentive\n",
      "mechanism between the BS and mobile users as an auction game where the BS is an\n",
      "auctioneer and the mobile users are the sellers. In the proposed game, each\n",
      "mobile user submits its bids according to the minimal energy cost that the\n",
      "mobile users experiences in participating in FL. To decide winners in the\n",
      "auction and maximize social welfare, we propose the primal-dual greedy auction\n",
      "mechanism. The proposed mechanism can guarantee three economic properties,\n",
      "namely, truthfulness, individual rationality and efficiency. Finally, numerical\n",
      "results are shown to demonstrate the performance effectiveness of our proposed\n",
      "mechanism.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.07697 \n",
      "Title :Explaining Machine Learning Classifiers through Diverse Counterfactual\n",
      "  Explanations\n",
      "  Post-hoc explanations of machine learning models are crucial for people to\n",
      "understand and act on algorithmic predictions. An intriguing class of\n",
      "explanations is through counterfactuals, hypothetical examples that show people\n",
      "how to obtain a different prediction. We posit that effective counterfactual\n",
      "explanations should satisfy two properties: feasibility of the counterfactual\n",
      "actions given user context and constraints, and diversity among the\n",
      "counterfactuals presented. To this end, we propose a framework for generating\n",
      "and evaluating a diverse set of counterfactual explanations based on\n",
      "determinantal point processes. To evaluate the actionability of\n",
      "counterfactuals, we provide metrics that enable comparison of\n",
      "counterfactual-based methods to other local explanation methods. We further\n",
      "address necessary tradeoffs and point to causal implications in optimizing for\n",
      "counterfactuals. Our experiments on four real-world datasets show that our\n",
      "framework can generate a set of counterfactuals that are diverse and well\n",
      "approximate local decision boundaries, outperforming prior approaches to\n",
      "generating diverse counterfactuals. We provide an implementation of the\n",
      "framework at https://github.com/microsoft/DiCE.\n",
      "\n",
      "**Paper Id :2006.13132 \n",
      "Title :On Counterfactual Explanations under Predictive Multiplicity\n",
      "  Counterfactual explanations are usually obtained by identifying the smallest\n",
      "change made to an input to change a prediction made by a fixed model (hereafter\n",
      "called sparse methods). Recent work, however, has revitalized an old insight:\n",
      "there often does not exist one superior solution to a prediction problem with\n",
      "respect to commonly used measures of interest (e.g. error rate). In fact, often\n",
      "multiple different classifiers give almost equal solutions. This phenomenon is\n",
      "known as predictive multiplicity (Breiman, 2001; Marx et al., 2019). In this\n",
      "work, we derive a general upper bound for the costs of counterfactual\n",
      "explanations under predictive multiplicity. Most notably, it depends on a\n",
      "discrepancy notion between two classifiers, which describes how differently\n",
      "they treat negatively predicted individuals. We then compare sparse and data\n",
      "support approaches empirically on real-world data. The results show that data\n",
      "support methods are more robust to multiplicity of different models. At the\n",
      "same time, we show that those methods have provably higher cost of generating\n",
      "counterfactual explanations under one fixed model. In summary, our theoretical\n",
      "and empiricaln results challenge the commonly held view that counterfactual\n",
      "recommendations should be sparse in general.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.07817 \n",
      "Title :Spatio-Temporal Adversarial Learning for Detecting Unseen Falls\n",
      "  Fall detection is an important problem from both the health and machine\n",
      "learning perspective. A fall can lead to severe injuries, long term impairments\n",
      "or even death in some cases. In terms of machine learning, it presents a\n",
      "severely class imbalance problem with very few or no training data for falls\n",
      "owing to the fact that falls occur rarely. In this paper, we take an alternate\n",
      "philosophy to detect falls in the absence of their training data, by training\n",
      "the classifier on only the normal activities (that are available in abundance)\n",
      "and identifying a fall as an anomaly. To realize such a classifier, we use an\n",
      "adversarial learning framework, which comprises of a spatio-temporal\n",
      "autoencoder for reconstructing input video frames and a spatio-temporal\n",
      "convolution network to discriminate them against original video frames. 3D\n",
      "convolutions are used to learn spatial and temporal features from the input\n",
      "video frames. The adversarial learning of the spatio-temporal autoencoder will\n",
      "enable reconstructing the normal activities of daily living efficiently; thus,\n",
      "rendering detecting unseen falls plausible within this framework. We tested the\n",
      "performance of the proposed framework on camera sensing modalities that may\n",
      "preserve an individual's privacy (fully or partially), such as thermal and\n",
      "depth camera. Our results on three publicly available datasets show that the\n",
      "proposed spatio-temporal adversarial framework performed better than other\n",
      "baseline frame based (or spatial) adversarial learning methods.\n",
      "\n",
      "**Paper Id :2007.13866 \n",
      "Title :se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n",
      "  Residuals in Synthetic Domains\n",
      "  Tracking the 6D pose of objects in video sequences is important for robot\n",
      "manipulation. This task, however, introduces multiple challenges: (i) robot\n",
      "manipulation involves significant occlusions; (ii) data and annotations are\n",
      "troublesome and difficult to collect for 6D poses, which complicates machine\n",
      "learning solutions, and (iii) incremental error drift often accumulates in long\n",
      "term tracking to necessitate re-initialization of the object's pose. This work\n",
      "proposes a data-driven optimization approach for long-term, 6D pose tracking.\n",
      "It aims to identify the optimal relative pose given the current RGB-D\n",
      "observation and a synthetic image conditioned on the previous best estimate and\n",
      "the object's model. The key contribution in this context is a novel neural\n",
      "network architecture, which appropriately disentangles the feature encoding to\n",
      "help reduce domain shift, and an effective 3D orientation representation via\n",
      "Lie Algebra. Consequently, even when the network is trained only with synthetic\n",
      "data can work effectively over real images. Comprehensive experiments over\n",
      "benchmarks - existing ones as well as a new dataset with significant occlusions\n",
      "related to object manipulation - show that the proposed approach achieves\n",
      "consistently robust estimates and outperforms alternatives, even though they\n",
      "have been trained with real images. The approach is also the most\n",
      "computationally efficient among the alternatives and achieves a tracking\n",
      "frequency of 90.9Hz.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.08871 \n",
      "Title :Measuring the effects of confounders in medical supervised\n",
      "  classification problems: the Confounding Index (CI)\n",
      "  Over the years, there has been growing interest in using Machine Learning\n",
      "techniques for biomedical data processing. When tackling these tasks, one needs\n",
      "to bear in mind that biomedical data depends on a variety of characteristics,\n",
      "such as demographic aspects (age, gender, etc) or the acquisition technology,\n",
      "which might be unrelated with the target of the analysis. In supervised tasks,\n",
      "failing to match the ground truth targets with respect to such characteristics,\n",
      "called confounders, may lead to very misleading estimates of the predictive\n",
      "performance. Many strategies have been proposed to handle confounders, ranging\n",
      "from data selection, to normalization techniques, up to the use of training\n",
      "algorithm for learning with imbalanced data. However, all these solutions\n",
      "require the confounders to be known a priori. To this aim, we introduce a novel\n",
      "index that is able to measure the confounding effect of a data attribute in a\n",
      "bias-agnostic way. This index can be used to quantitatively compare the\n",
      "confounding effects of different variables and to inform correction methods\n",
      "such as normalization procedures or ad-hoc-prepared learning algorithms. The\n",
      "effectiveness of this index is validated on both simulated data and real-world\n",
      "neuroimaging data.\n",
      "\n",
      "**Paper Id :2001.05922 \n",
      "Title :Continual Learning for Domain Adaptation in Chest X-ray Classification\n",
      "  Over the last years, Deep Learning has been successfully applied to a broad\n",
      "range of medical applications. Especially in the context of chest X-ray\n",
      "classification, results have been reported which are on par, or even superior\n",
      "to experienced radiologists. Despite this success in controlled experimental\n",
      "environments, it has been noted that the ability of Deep Learning models to\n",
      "generalize to data from a new domain (with potentially different tasks) is\n",
      "often limited. In order to address this challenge, we investigate techniques\n",
      "from the field of Continual Learning (CL) including Joint Training (JT),\n",
      "Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using\n",
      "the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that\n",
      "these methods provide promising options to improve the performance of Deep\n",
      "Learning models on a target domain and to mitigate effectively catastrophic\n",
      "forgetting for the source domain. To this end, the best overall performance was\n",
      "obtained using JT, while for LWF competitive results could be achieved - even\n",
      "without accessing data from the source domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.08883 \n",
      "Title :Explainable Machine Learning for Scientific Insights and Discoveries\n",
      "  Machine learning methods have been remarkably successful for a wide range of\n",
      "application areas in the extraction of essential information from data. An\n",
      "exciting and relatively recent development is the uptake of machine learning in\n",
      "the natural sciences, where the major goal is to obtain novel scientific\n",
      "insights and discoveries from observational or simulated data. A prerequisite\n",
      "for obtaining a scientific outcome is domain knowledge, which is needed to gain\n",
      "explainability, but also to enhance scientific consistency. In this article we\n",
      "review explainable machine learning in view of applications in the natural\n",
      "sciences and discuss three core elements which we identified as relevant in\n",
      "this context: transparency, interpretability, and explainability. With respect\n",
      "to these core elements, we provide a survey of recent scientific works that\n",
      "incorporate machine learning and the way that explainable machine learning is\n",
      "used in combination with domain knowledge from the application areas.\n",
      "\n",
      "**Paper Id :2005.06540 \n",
      "Title :Deep Learning for Political Science\n",
      "  Political science, and social science in general, have traditionally been\n",
      "using computational methods to study areas such as voting behavior, policy\n",
      "making, international conflict, and international development. More recently,\n",
      "increasingly available quantities of data are being combined with improved\n",
      "algorithms and affordable computational resources to predict, learn, and\n",
      "discover new insights from data that is large in volume and variety. New\n",
      "developments in the areas of machine learning, deep learning, natural language\n",
      "processing (NLP), and, more generally, artificial intelligence (AI) are opening\n",
      "up new opportunities for testing theories and evaluating the impact of\n",
      "interventions and programs in a more dynamic and effective way. Applications\n",
      "using large volumes of structured and unstructured data are becoming common in\n",
      "government and industry, and increasingly also in social science research. This\n",
      "chapter offers an introduction to such methods drawing examples from political\n",
      "science. Focusing on the areas where the strengths of the methods coincide with\n",
      "challenges in these fields, the chapter first presents an introduction to AI\n",
      "and its core technology - machine learning, with its rapidly developing\n",
      "subfield of deep learning. The discussion of deep neural networks is\n",
      "illustrated with the NLP tasks that are relevant to political science. The\n",
      "latest advances in deep learning methods for NLP are also reviewed, together\n",
      "with their potential for improving information extraction and pattern\n",
      "recognition from political science texts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "**Paper Id :2004.07210 \n",
      "Title :On Box-Cox Transformation for Image Normality and Pattern Classification\n",
      "  A unique member of the power transformation family is known as the Box-Cox\n",
      "transformation. The latter can be seen as a mathematical operation that leads\n",
      "to finding the optimum lambda ({\\lambda}) value that maximizes the\n",
      "log-likelihood function to transform a data to a normal distribution and to\n",
      "reduce heteroscedasticity. In data analytics, a normality assumption underlies\n",
      "a variety of statistical test models. This technique, however, is best known in\n",
      "statistical analysis to handle one-dimensional data. Herein, this paper\n",
      "revolves around the utility of such a tool as a pre-processing step to\n",
      "transform two-dimensional data, namely, digital images and to study its effect.\n",
      "Moreover, to reduce time complexity, it suffices to estimate the parameter\n",
      "lambda in real-time for large two-dimensional matrices by merely considering\n",
      "their probability density function as a statistical inference of the underlying\n",
      "data distribution. We compare the effect of this light-weight Box-Cox\n",
      "transformation with well-established state-of-the-art low light image\n",
      "enhancement techniques. We also demonstrate the effectiveness of our approach\n",
      "through several test-bed data sets for generic improvement of visual appearance\n",
      "of images and for ameliorating the performance of a colour pattern\n",
      "classification algorithm as an example application. Results with and without\n",
      "the proposed approach, are compared using the AlexNet (transfer deep learning)\n",
      "pretrained model. To the best of our knowledge, this is the first time that the\n",
      "Box-Cox transformation is extended to digital images by exploiting histogram\n",
      "transformation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.09589 \n",
      "Title :Glioma Grade Prediction Using Wavelet Scattering-Based Radiomics\n",
      "  Glioma grading before surgery is very critical for the prognosis prediction\n",
      "and treatment plan making. We present a novel wavelet scattering-based radiomic\n",
      "method to predict noninvasively and accurately the glioma grades. The method\n",
      "consists of wavelet scattering feature extraction, dimensionality reduction,\n",
      "and glioma grade prediction. The dimensionality reduction was achieved using\n",
      "partial least squares (PLS) regression and the glioma grade prediction using\n",
      "support vector machine (SVM), logistic regression (LR) and random forest (RF).\n",
      "The prediction obtained on multimodal magnetic resonance images of 285 patients\n",
      "with well-labeled intratumoral and peritumoral regions showed that the area\n",
      "under the receiver operating characteristic curve (AUC) of glioma grade\n",
      "prediction was increased up to 0.99 when considering both intratumoral and\n",
      "peritumoral features in multimodal images, which represents an increase of\n",
      "about 13% compared to traditional radiomics. In addition, the features\n",
      "extracted from peritumoral regions further increase the accuracy of glioma\n",
      "grading.\n",
      "\n",
      "**Paper Id :1911.01220 \n",
      "Title :Learning-based estimation of dielectric properties and tissue density in\n",
      "  head models for personalized radio-frequency dosimetry\n",
      "  Radio-frequency dosimetry is an important process in human safety and for\n",
      "compliance of related products. Recently, computational human models generated\n",
      "from medical images have often been used for such assessment, especially to\n",
      "consider the inter-variability of subjects. However, the common procedure to\n",
      "develop personalized models is time consuming because it involves excessive\n",
      "segmentation of several components that represent different biological tissues,\n",
      "which limits the inter-variability assessment of radiation safety based on\n",
      "personalized dosimetry. Deep learning methods have been shown to be a powerful\n",
      "approach for pattern recognition and signal analysis. Convolutional neural\n",
      "networks with deep architecture are proven robust for feature extraction and\n",
      "image mapping in several biomedical applications. In this study, we develop a\n",
      "learning-based approach for fast and accurate estimation of the dielectric\n",
      "properties and density of tissues directly from magnetic resonance images in a\n",
      "single shot. The smooth distribution of the dielectric properties in head\n",
      "models, which is realized using a process without tissue segmentation, improves\n",
      "the smoothness of the specific absorption rate (SAR) distribution compared with\n",
      "that in the commonly used procedure. The estimated SAR distributions, as well\n",
      "as that averaged over 10-g of tissue in a cubic shape, are found to be highly\n",
      "consistent with those computed using the conventional methods that employ\n",
      "segmentation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.10227 \n",
      "Title :Low-dimensional statistical manifold embedding of directed graphs\n",
      "  We propose a novel node embedding of directed graphs to statistical\n",
      "manifolds, which is based on a global minimization of pairwise relative entropy\n",
      "and graph geodesics in a non-linear way. Each node is encoded with a\n",
      "probability density function over a measurable space. Furthermore, we analyze\n",
      "the connection between the geometrical properties of such embedding and their\n",
      "efficient learning procedure. Extensive experiments show that our proposed\n",
      "embedding is better in preserving the global geodesic information of graphs, as\n",
      "well as outperforming existing embedding models on directed graphs in a variety\n",
      "of evaluation metrics, in an unsupervised setting.\n",
      "\n",
      "**Paper Id :1906.11641 \n",
      "Title :A global approach for learning sparse Ising models\n",
      "  We consider the problem of learning the link parameters as well as the\n",
      "structure of a binary-valued pairwise Markov model. Under sparsity assumption,\n",
      "we propose a method based on $l_1$- regularized logistic regression, which\n",
      "estimate globally the whole set of edges and link parameters. Unlike the more\n",
      "recent methods discussed in literature that learn the edges and the\n",
      "corresponding link parameters one node at a time, in this work we propose a\n",
      "method that learns all the edges and corresponding link parameters\n",
      "simultaneously for all nodes. The idea behind this proposal is to exploit the\n",
      "reciprocal information of the nodes between each other during the estimation\n",
      "process. Numerical experiments highlight the advantage of this technique and\n",
      "confirm the intuition behind it.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.10702 \n",
      "Title :MDE: Multiple Distance Embeddings for Link Prediction in Knowledge\n",
      "  Graphs\n",
      "  Over the past decade, knowledge graphs became popular for capturing\n",
      "structured domain knowledge. Relational learning models enable the prediction\n",
      "of missing links inside knowledge graphs. More specifically, latent distance\n",
      "approaches model the relationships among entities via a distance between latent\n",
      "representations. Translating embedding models (e.g., TransE) are among the most\n",
      "popular latent distance approaches which use one distance function to learn\n",
      "multiple relation patterns. However, they are mostly inefficient in capturing\n",
      "symmetric relations since the representation vector norm for all the symmetric\n",
      "relations becomes equal to zero. They also lose information when learning\n",
      "relations with reflexive patterns since they become symmetric and transitive.\n",
      "We propose the Multiple Distance Embedding model (MDE) that addresses these\n",
      "limitations and a framework to collaboratively combine variant latent\n",
      "distance-based terms. Our solution is based on two principles: 1) we use a\n",
      "limit-based loss instead of a margin ranking loss and, 2) by learning\n",
      "independent embedding vectors for each of the terms we can collectively train\n",
      "and predict using contradicting distance terms. We further demonstrate that MDE\n",
      "allows modeling relations with (anti)symmetry, inversion, and composition\n",
      "patterns. We propose MDE as a neural network model that allows us to map\n",
      "non-linear relations between the embedding vectors and the expected output of\n",
      "the score function. Our empirical results show that MDE performs competitively\n",
      "to state-of-the-art embedding models on several benchmark datasets.\n",
      "\n",
      "**Paper Id :2006.16365 \n",
      "Title :Multi-Partition Embedding Interaction with Block Term Format for\n",
      "  Knowledge Graph Completion\n",
      "  Knowledge graph completion is an important task that aims to predict the\n",
      "missing relational link between entities. Knowledge graph embedding methods\n",
      "perform this task by representing entities and relations as embedding vectors\n",
      "and modeling their interactions to compute the matching score of each triple.\n",
      "Previous work has usually treated each embedding as a whole and has modeled the\n",
      "interactions between these whole embeddings, potentially making the model\n",
      "excessively expensive or requiring specially designed interaction mechanisms.\n",
      "In this work, we propose the multi-partition embedding interaction (MEI) model\n",
      "with block term format to systematically address this problem. MEI divides each\n",
      "embedding into a multi-partition vector to efficiently restrict the\n",
      "interactions. Each local interaction is modeled with the Tucker tensor format\n",
      "and the full interaction is modeled with the block term tensor format, enabling\n",
      "MEI to control the trade-off between expressiveness and computational cost,\n",
      "learn the interaction mechanisms from data automatically, and achieve\n",
      "state-of-the-art performance on the link prediction task. In addition, we\n",
      "theoretically study the parameter efficiency problem and derive a simple\n",
      "empirically verified criterion for optimal parameter trade-off. We also apply\n",
      "the framework of MEI to provide a new generalized explanation for several\n",
      "specially designed interaction mechanisms in previous models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.10812 \n",
      "Title :Regularity as Regularization: Smooth and Strongly Convex Brenier\n",
      "  Potentials in Optimal Transport\n",
      "  Estimating Wasserstein distances between two high-dimensional densities\n",
      "suffers from the curse of dimensionality: one needs an exponential (wrt\n",
      "dimension) number of samples to ensure that the distance between two empirical\n",
      "measures is comparable to the distance between the original densities.\n",
      "Therefore, optimal transport (OT) can only be used in machine learning if it is\n",
      "substantially regularized. On the other hand, one of the greatest achievements\n",
      "of the OT literature in recent years lies in regularity theory: Caffarelli\n",
      "showed that the OT map between two well behaved measures is Lipschitz, or\n",
      "equivalently when considering 2-Wasserstein distances, that Brenier convex\n",
      "potentials (whose gradient yields an optimal map) are smooth. We propose in\n",
      "this work to draw inspiration from this theory and use regularity as a\n",
      "regularization tool. We give algorithms operating on two discrete measures that\n",
      "can recover nearly optimal transport maps with small distortion, or\n",
      "equivalently, nearly optimal Brenier potentials that are strongly convex and\n",
      "smooth. The problem boils down to solving alternatively a convex QCQP and a\n",
      "discrete OT problem, granting access to the values and gradients of the Brenier\n",
      "potential not only on sampled points, but also out of sample at the cost of\n",
      "solving a simpler QCQP for each evaluation. We propose algorithms to estimate\n",
      "and evaluate transport maps with desired regularity properties, benchmark their\n",
      "statistical performance, apply them to domain adaptation and visualize their\n",
      "action on a color transfer task.\n",
      "\n",
      "**Paper Id :1905.12294 \n",
      "Title :How to iron out rough landscapes and get optimal performances: Averaged\n",
      "  Gradient Descent and its application to tensor PCA\n",
      "  In many high-dimensional estimation problems the main task consists in\n",
      "minimizing a cost function, which is often strongly non-convex when scanned in\n",
      "the space of parameters to be estimated. A standard solution to flatten the\n",
      "corresponding rough landscape consists in summing the losses associated to\n",
      "different data points and obtain a smoother empirical risk. Here we propose a\n",
      "complementary method that works for a single data point. The main idea is that\n",
      "a large amount of the roughness is uncorrelated in different parts of the\n",
      "landscape. One can then substantially reduce the noise by evaluating an\n",
      "empirical average of the gradient obtained as a sum over many random\n",
      "independent positions in the space of parameters to be optimized. We present an\n",
      "algorithm, called Averaged Gradient Descent, based on this idea and we apply it\n",
      "to tensor PCA, which is a very hard estimation problem. We show that Averaged\n",
      "Gradient Descent over-performs physical algorithms such as gradient descent and\n",
      "approximate message passing and matches the best algorithmic thresholds known\n",
      "so far, obtained by tensor unfolding and methods based on sum-of-squares.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.10891 \n",
      "Title :A hybrid model for predicting human physical activity status from\n",
      "  lifelogging data\n",
      "  One trend in the recent healthcare transformations is people are encouraged\n",
      "to monitor and manage their health based on their daily diets and physical\n",
      "activity habits. However, much attention of the use of operational research and\n",
      "analytical models in healthcare has been paid to the systematic level such as\n",
      "country or regional policy making or organisational issues. This paper proposes\n",
      "a model concerned with healthcare analytics at the individual level, which can\n",
      "predict human physical activity status from sequential lifelogging data\n",
      "collected from wearable sensors. The model has a two-stage hybrid structure (in\n",
      "short, MOGP-HMM) -- a multi-objective genetic programming (MOGP) algorithm in\n",
      "the first stage to reduce the dimensions of lifelogging data and a hidden\n",
      "Markov model (HMM) in the second stage for activity status prediction over\n",
      "time. It can be used as a decision support tool to provide real-time\n",
      "monitoring, statistical analysis and personalized advice to individuals,\n",
      "encouraging positive attitudes towards healthy lifestyles. We validate the\n",
      "model with the real data collected from a group of participants in the UK, and\n",
      "compare it with other popular two-stage hybrid models. Our experimental results\n",
      "show that the MOGP-HMM can achieve comparable performance. To the best of our\n",
      "knowledge, this is the very first study that uses the MOGP in the hybrid\n",
      "two-stage structure for individuals' activity status prediction. It fits\n",
      "seamlessly with the current trend in the UK healthcare transformation of\n",
      "patient empowerment as well as contributing to a strategic development for more\n",
      "efficient and cost-effective provision of healthcare.\n",
      "\n",
      "**Paper Id :2004.00959 \n",
      "Title :Neural network based country wise risk prediction of COVID-19\n",
      "  The recent worldwide outbreak of the novel coronavirus (COVID-19) has opened\n",
      "up new challenges to the research community. Artificial intelligence (AI)\n",
      "driven methods can be useful to predict the parameters, risks, and effects of\n",
      "such an epidemic. Such predictions can be helpful to control and prevent the\n",
      "spread of such diseases. The main challenges of applying AI is the small volume\n",
      "of data and the uncertain nature. Here, we propose a shallow long short-term\n",
      "memory (LSTM) based neural network to predict the risk category of a country.\n",
      "We have used a Bayesian optimization framework to optimize and automatically\n",
      "design country-specific networks. The results show that the proposed pipeline\n",
      "outperforms state-of-the-art methods for data of 180 countries and can be a\n",
      "useful tool for such risk categorization. We have also experimented with the\n",
      "trend data and weather data combined for the prediction. The outcome shows that\n",
      "the weather does not have a significant role. The tool can be used to predict\n",
      "long-duration outbreak of such an epidemic such that we can take preventive\n",
      "steps earlier\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.10963 \n",
      "Title :Interpretable deep Gaussian processes with moments\n",
      "  Deep Gaussian Processes (DGPs) combine the expressiveness of Deep Neural\n",
      "Networks (DNNs) with quantified uncertainty of Gaussian Processes (GPs).\n",
      "Expressive power and intractable inference both result from the non-Gaussian\n",
      "distribution over composition functions. We propose interpretable DGP based on\n",
      "approximating DGP as a GP by calculating the exact moments, which additionally\n",
      "identify the heavy-tailed nature of some DGP distributions. Consequently, our\n",
      "approach admits interpretation as both NNs with specified activation functions\n",
      "and as a variational approximation to DGP. We identify the expressivity\n",
      "parameter of DGP and find non-local and non-stationary correlation from DGP\n",
      "composition. We provide general recipes for deriving the effective kernels for\n",
      "DGP of two, three, or infinitely many layers, composed of homogeneous or\n",
      "heterogeneous kernels. Results illustrate the expressiveness of our effective\n",
      "kernels through samples from the prior and inference on simulated and real data\n",
      "and demonstrate advantages of interpretability by analysis of analytic forms,\n",
      "and draw relations and equivalences across kernels.\n",
      "\n",
      "**Paper Id :1907.06673 \n",
      "Title :Quant GANs: Deep Generation of Financial Time Series\n",
      "  Modeling financial time series by stochastic processes is a challenging task\n",
      "and a central area of research in financial mathematics. As an alternative, we\n",
      "introduce Quant GANs, a data-driven model which is inspired by the recent\n",
      "success of generative adversarial networks (GANs). Quant GANs consist of a\n",
      "generator and discriminator function, which utilize temporal convolutional\n",
      "networks (TCNs) and thereby achieve to capture long-range dependencies such as\n",
      "the presence of volatility clusters. The generator function is explicitly\n",
      "constructed such that the induced stochastic process allows a transition to its\n",
      "risk-neutral distribution. Our numerical results highlight that distributional\n",
      "properties for small and large lags are in an excellent agreement and\n",
      "dependence properties such as volatility clusters, leverage effects, and serial\n",
      "autocorrelations can be generated by the generator function of Quant GANs,\n",
      "demonstrably in high fidelity.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.11481 \n",
      "Title :AI Feynman: a Physics-Inspired Method for Symbolic Regression\n",
      "  A core challenge for both physics and artificial intellicence (AI) is\n",
      "symbolic regression: finding a symbolic expression that matches data from an\n",
      "unknown function. Although this problem is likely to be NP-hard in principle,\n",
      "functions of practical interest often exhibit symmetries, separability,\n",
      "compositionality and other simplifying properties. In this spirit, we develop a\n",
      "recursive multidimensional symbolic regression algorithm that combines neural\n",
      "network fitting with a suite of physics-inspired techniques. We apply it to 100\n",
      "equations from the Feynman Lectures on Physics, and it discovers all of them,\n",
      "while previous publicly available software cracks only 71; for a more difficult\n",
      "test set, we improve the state of the art success rate from 15% to 90%.\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.11485 \n",
      "Title :Representation Learning for Dynamic Graphs: A Survey\n",
      "  Graphs arise naturally in many real-world applications including social\n",
      "networks, recommender systems, ontologies, biology, and computational finance.\n",
      "Traditionally, machine learning models for graphs have been mostly designed for\n",
      "static graphs. However, many applications involve evolving graphs. This\n",
      "introduces important challenges for learning and inference since nodes,\n",
      "attributes, and edges change over time. In this survey, we review the recent\n",
      "advances in representation learning for dynamic graphs, including dynamic\n",
      "knowledge graphs. We describe existing models from an encoder-decoder\n",
      "perspective, categorize these encoders and decoders based on the techniques\n",
      "they employ, and analyze the approaches in each category. We also review\n",
      "several prominent applications and widely used datasets and highlight\n",
      "directions for future research.\n",
      "\n",
      "**Paper Id :1903.11835 \n",
      "Title :A Survey on Graph Kernels\n",
      "  Graph kernels have become an established and widely-used technique for\n",
      "solving classification tasks on graphs. This survey gives a comprehensive\n",
      "overview of techniques for kernel-based graph classification developed in the\n",
      "past 15 years. We describe and categorize graph kernels based on properties\n",
      "inherent to their design, such as the nature of their extracted graph features,\n",
      "their method of computation and their applicability to problems in practice. In\n",
      "an extensive experimental evaluation, we study the classification accuracy of a\n",
      "large suite of graph kernels on established benchmarks as well as new datasets.\n",
      "We compare the performance of popular kernels with several baseline methods and\n",
      "study the effect of applying a Gaussian RBF kernel to the metric induced by a\n",
      "graph kernel. In doing so, we find that simple baselines become competitive\n",
      "after this transformation on some datasets. Moreover, we study the extent to\n",
      "which existing graph kernels agree in their predictions (and prediction errors)\n",
      "and obtain a data-driven categorization of kernels as result. Finally, based on\n",
      "our experimental results, we derive a practitioner's guide to kernel-based\n",
      "graph classification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.11503 \n",
      "Title :Body Shape Privacy in Images: Understanding Privacy and Preventing\n",
      "  Automatic Shape Extraction\n",
      "  Modern approaches to pose and body shape estimation have recently achieved\n",
      "strong performance even under challenging real-world conditions. Even from a\n",
      "single image of a clothed person, a realistic looking body shape can be\n",
      "inferred that captures a users' weight group and body shape type well. This\n",
      "opens up a whole spectrum of applications -- in particular in fashion -- where\n",
      "virtual try-on and recommendation systems can make use of these new and\n",
      "automatized cues. However, a realistic depiction of the undressed body is\n",
      "regarded highly private and therefore might not be consented by most people.\n",
      "Hence, we ask if the automatic extraction of such information can be\n",
      "effectively evaded. While adversarial perturbations have been shown to be\n",
      "effective for manipulating the output of machine learning models -- in\n",
      "particular, end-to-end deep learning approaches -- state of the art shape\n",
      "estimation methods are composed of multiple stages. We perform the first\n",
      "investigation of different strategies that can be used to effectively\n",
      "manipulate the automatic shape estimation while preserving the overall\n",
      "appearance of the original image.\n",
      "\n",
      "**Paper Id :1905.07817 \n",
      "Title :Spatio-Temporal Adversarial Learning for Detecting Unseen Falls\n",
      "  Fall detection is an important problem from both the health and machine\n",
      "learning perspective. A fall can lead to severe injuries, long term impairments\n",
      "or even death in some cases. In terms of machine learning, it presents a\n",
      "severely class imbalance problem with very few or no training data for falls\n",
      "owing to the fact that falls occur rarely. In this paper, we take an alternate\n",
      "philosophy to detect falls in the absence of their training data, by training\n",
      "the classifier on only the normal activities (that are available in abundance)\n",
      "and identifying a fall as an anomaly. To realize such a classifier, we use an\n",
      "adversarial learning framework, which comprises of a spatio-temporal\n",
      "autoencoder for reconstructing input video frames and a spatio-temporal\n",
      "convolution network to discriminate them against original video frames. 3D\n",
      "convolutions are used to learn spatial and temporal features from the input\n",
      "video frames. The adversarial learning of the spatio-temporal autoencoder will\n",
      "enable reconstructing the normal activities of daily living efficiently; thus,\n",
      "rendering detecting unseen falls plausible within this framework. We tested the\n",
      "performance of the proposed framework on camera sensing modalities that may\n",
      "preserve an individual's privacy (fully or partially), such as thermal and\n",
      "depth camera. Our results on three publicly available datasets show that the\n",
      "proposed spatio-temporal adversarial framework performed better than other\n",
      "baseline frame based (or spatial) adversarial learning methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.11825 \n",
      "Title :Fast Data-Driven Simulation of Cherenkov Detectors Using Generative\n",
      "  Adversarial Networks\n",
      "  The increasing luminosities of future Large Hadron Collider runs and next\n",
      "generation of collider experiments will require an unprecedented amount of\n",
      "simulated events to be produced. Such large scale productions are extremely\n",
      "demanding in terms of computing resources. Thus new approaches to event\n",
      "generation and simulation of detector responses are needed. In LHCb, the\n",
      "accurate simulation of Cherenkov detectors takes a sizeable fraction of CPU\n",
      "time. An alternative approach is described here, when one generates high-level\n",
      "reconstructed observables using a generative neural network to bypass low level\n",
      "details. This network is trained to reproduce the particle species likelihood\n",
      "function values based on the track kinematic parameters and detector occupancy.\n",
      "The fast simulation is trained using real data samples collected by LHCb during\n",
      "run 2. We demonstrate that this approach provides high-fidelity results.\n",
      "\n",
      "**Paper Id :1911.05797 \n",
      "Title :AI-optimized detector design for the future Electron-Ion Collider: the\n",
      "  dual-radiator RICH case\n",
      "  Advanced detector R&D requires performing computationally intensive and\n",
      "detailed simulations as part of the detector-design optimization process. We\n",
      "propose a general approach to this process based on Bayesian optimization and\n",
      "machine learning that encodes detector requirements. As a case study, we focus\n",
      "on the design of the dual-radiator Ring Imaging Cherenkov (dRICH) detector\n",
      "under development as part of the particle-identification system at the future\n",
      "Electron-Ion Collider (EIC). The EIC is a US-led frontier accelerator project\n",
      "for nuclear physics, which has been proposed to further explore the structure\n",
      "and interactions of nuclear matter at the scale of sea quarks and gluons. We\n",
      "show that the detector design obtained with our automated and highly\n",
      "parallelized framework outperforms the baseline dRICH design within the\n",
      "assumptions of the current model. Our approach can be applied to any detector\n",
      "R&D, provided that realistic simulations are available.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.11931 \n",
      "Title :Adversarial Domain Adaptation Being Aware of Class Relationships\n",
      "  Adversarial training is a useful approach to promote the learning of\n",
      "transferable representations across the source and target domains, which has\n",
      "been widely applied for domain adaptation (DA) tasks based on deep neural\n",
      "networks. Until very recently, existing adversarial domain adaptation (ADA)\n",
      "methods ignore the useful information from the label space, which is an\n",
      "important factor accountable for the complicated data distributions associated\n",
      "with different semantic classes. Especially, the inter-class semantic\n",
      "relationships have been rarely considered and discussed in the current work of\n",
      "transfer learning. In this paper, we propose a novel relationship-aware\n",
      "adversarial domain adaptation (RADA) algorithm, which first utilizes a single\n",
      "multi-class domain discriminator to enforce the learning of inter-class\n",
      "dependency structure during domain-adversarial training and then aligns this\n",
      "structure with the inter-class dependencies that are characterized from\n",
      "training the label predictor on source domain. Specifically, we impose a\n",
      "regularization term to penalize the structure discrepancy between the\n",
      "inter-class dependencies respectively estimated from domain discriminator and\n",
      "label predictor. Through this alignment, our proposed method makes the\n",
      "adversarial domain adaptation aware of the class relationships. Empirical\n",
      "studies show that the incorporation of class relationships significantly\n",
      "improves the performance on benchmark datasets.\n",
      "\n",
      "**Paper Id :2002.07366 \n",
      "Title :Adversarial Deep Network Embedding for Cross-network Node Classification\n",
      "  In this paper, the task of cross-network node classification, which leverages\n",
      "the abundant labeled nodes from a source network to help classify unlabeled\n",
      "nodes in a target network, is studied. The existing domain adaptation\n",
      "algorithms generally fail to model the network structural information, and the\n",
      "current network embedding models mainly focus on single-network applications.\n",
      "Thus, both of them cannot be directly applied to solve the cross-network node\n",
      "classification problem. This motivates us to propose an adversarial\n",
      "cross-network deep network embedding (ACDNE) model to integrate adversarial\n",
      "domain adaptation with deep network embedding so as to learn network-invariant\n",
      "node representations that can also well preserve the network structural\n",
      "information. In ACDNE, the deep network embedding module utilizes two feature\n",
      "extractors to jointly preserve attributed affinity and topological proximities\n",
      "between nodes. In addition, a node classifier is incorporated to make node\n",
      "representations label-discriminative. Moreover, an adversarial domain\n",
      "adaptation technique is employed to make node representations\n",
      "network-invariant. Extensive experimental results demonstrate that the proposed\n",
      "ACDNE model achieves the state-of-the-art performance in cross-network node\n",
      "classification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.12294 \n",
      "Title :How to iron out rough landscapes and get optimal performances: Averaged\n",
      "  Gradient Descent and its application to tensor PCA\n",
      "  In many high-dimensional estimation problems the main task consists in\n",
      "minimizing a cost function, which is often strongly non-convex when scanned in\n",
      "the space of parameters to be estimated. A standard solution to flatten the\n",
      "corresponding rough landscape consists in summing the losses associated to\n",
      "different data points and obtain a smoother empirical risk. Here we propose a\n",
      "complementary method that works for a single data point. The main idea is that\n",
      "a large amount of the roughness is uncorrelated in different parts of the\n",
      "landscape. One can then substantially reduce the noise by evaluating an\n",
      "empirical average of the gradient obtained as a sum over many random\n",
      "independent positions in the space of parameters to be optimized. We present an\n",
      "algorithm, called Averaged Gradient Descent, based on this idea and we apply it\n",
      "to tensor PCA, which is a very hard estimation problem. We show that Averaged\n",
      "Gradient Descent over-performs physical algorithms such as gradient descent and\n",
      "approximate message passing and matches the best algorithmic thresholds known\n",
      "so far, obtained by tensor unfolding and methods based on sum-of-squares.\n",
      "\n",
      "**Paper Id :1905.10812 \n",
      "Title :Regularity as Regularization: Smooth and Strongly Convex Brenier\n",
      "  Potentials in Optimal Transport\n",
      "  Estimating Wasserstein distances between two high-dimensional densities\n",
      "suffers from the curse of dimensionality: one needs an exponential (wrt\n",
      "dimension) number of samples to ensure that the distance between two empirical\n",
      "measures is comparable to the distance between the original densities.\n",
      "Therefore, optimal transport (OT) can only be used in machine learning if it is\n",
      "substantially regularized. On the other hand, one of the greatest achievements\n",
      "of the OT literature in recent years lies in regularity theory: Caffarelli\n",
      "showed that the OT map between two well behaved measures is Lipschitz, or\n",
      "equivalently when considering 2-Wasserstein distances, that Brenier convex\n",
      "potentials (whose gradient yields an optimal map) are smooth. We propose in\n",
      "this work to draw inspiration from this theory and use regularity as a\n",
      "regularization tool. We give algorithms operating on two discrete measures that\n",
      "can recover nearly optimal transport maps with small distortion, or\n",
      "equivalently, nearly optimal Brenier potentials that are strongly convex and\n",
      "smooth. The problem boils down to solving alternatively a convex QCQP and a\n",
      "discrete OT problem, granting access to the values and gradients of the Brenier\n",
      "potential not only on sampled points, but also out of sample at the cost of\n",
      "solving a simpler QCQP for each evaluation. We propose algorithms to estimate\n",
      "and evaluate transport maps with desired regularity properties, benchmark their\n",
      "statistical performance, apply them to domain adaptation and visualize their\n",
      "action on a color transfer task.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.12692 \n",
      "Title :Vector-Valued Graph Trend Filtering with Non-Convex Penalties\n",
      "  This work studies the denoising of piecewise smooth graph signals that\n",
      "exhibit inhomogeneous levels of smoothness over a graph, where the value at\n",
      "each node can be vector-valued. We extend the graph trend filtering framework\n",
      "to denoising vector-valued graph signals with a family of non-convex\n",
      "regularizers, which exhibit superior recovery performance over existing convex\n",
      "regularizers. Using an oracle inequality, we establish the statistical error\n",
      "rates of first-order stationary points of the proposed non-convex method for\n",
      "generic graphs. Furthermore, we present an ADMM-based algorithm to solve the\n",
      "proposed method and establish its convergence. Numerical experiments are\n",
      "conducted on both synthetic and real-world data for denoising, support\n",
      "recovery, event detection, and semi-supervised classification.\n",
      "\n",
      "**Paper Id :1811.05076 \n",
      "Title :Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n",
      "  and its Statistical Optimality\n",
      "  We consider the problem of decomposing a higher-order tensor with binary\n",
      "entries. Such data problems arise frequently in applications such as\n",
      "neuroimaging, recommendation system, topic modeling, and sensor network\n",
      "localization. We propose a multilinear Bernoulli model, develop a\n",
      "rank-constrained likelihood-based estimation method, and obtain the theoretical\n",
      "accuracy guarantees. In contrast to continuous-valued problems, the binary\n",
      "tensor problem exhibits an interesting phase transition phenomenon according to\n",
      "the signal-to-noise ratio. The error bound for the parameter tensor estimation\n",
      "is established, and we show that the obtained rate is minimax optimal under the\n",
      "considered model. Furthermore, we develop an alternating optimization algorithm\n",
      "with convergence guarantees. The efficacy of our approach is demonstrated\n",
      "through both simulations and analyses of multiple data sets on the tasks of\n",
      "tensor completion and clustering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.12762 \n",
      "Title :Securing Connected & Autonomous Vehicles: Challenges Posed by\n",
      "  Adversarial Machine Learning and The Way Forward\n",
      "  Connected and autonomous vehicles (CAVs) will form the backbone of future\n",
      "next-generation intelligent transportation systems (ITS) providing travel\n",
      "comfort, road safety, along with a number of value-added services. Such a\n",
      "transformation---which will be fuelled by concomitant advances in technologies\n",
      "for machine learning (ML) and wireless communications---will enable a future\n",
      "vehicular ecosystem that is better featured and more efficient. However, there\n",
      "are lurking security problems related to the use of ML in such a critical\n",
      "setting where an incorrect ML decision may not only be a nuisance but can lead\n",
      "to loss of precious lives. In this paper, we present an in-depth overview of\n",
      "the various challenges associated with the application of ML in vehicular\n",
      "networks. In addition, we formulate the ML pipeline of CAVs and present various\n",
      "potential security issues associated with the adoption of ML methods. In\n",
      "particular, we focus on the perspective of adversarial ML attacks on CAVs and\n",
      "outline a solution to defend against adversarial attacks in multiple settings.\n",
      "\n",
      "**Paper Id :1908.08649 \n",
      "Title :Adversary-resilient Distributed and Decentralized Statistical Inference\n",
      "  and Machine Learning: An Overview of Recent Advances Under the Byzantine\n",
      "  Threat Model\n",
      "  While the last few decades have witnessed a huge body of work devoted to\n",
      "inference and learning in distributed and decentralized setups, much of this\n",
      "work assumes a non-adversarial setting in which individual nodes---apart from\n",
      "occasional statistical failures---operate as intended within the algorithmic\n",
      "framework. In recent years, however, cybersecurity threats from malicious\n",
      "non-state actors and rogue entities have forced practitioners and researchers\n",
      "to rethink the robustness of distributed and decentralized algorithms against\n",
      "adversarial attacks. As a result, we now have a plethora of algorithmic\n",
      "approaches that guarantee robustness of distributed and/or decentralized\n",
      "inference and learning under different adversarial threat models. Driven in\n",
      "part by the world's growing appetite for data-driven decision making, however,\n",
      "securing of distributed/decentralized frameworks for inference and learning\n",
      "against adversarial threats remains a rapidly evolving research area. In this\n",
      "article, we provide an overview of some of the most recent developments in this\n",
      "area under the threat model of Byzantine attacks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.13196 \n",
      "Title :Persistent homology detects curvature\n",
      "  In topological data analysis, persistent homology is used to study the \"shape\n",
      "of data\". Persistent homology computations are completely characterized by a\n",
      "set of intervals called a bar code. It is often said that the long intervals\n",
      "represent the \"topological signal\" and the short intervals represent \"noise\".\n",
      "We give evidence to dispute this thesis, showing that the short intervals\n",
      "encode geometric information. Specifically, we prove that persistent homology\n",
      "detects the curvature of disks from which points have been sampled. We describe\n",
      "a general computational framework for solving inverse problems using the\n",
      "average persistence landscape, a continuous mapping from metric spaces with a\n",
      "probability measure to a Hilbert space. In the present application, the average\n",
      "persistence landscapes of points sampled from disks of constant curvature\n",
      "results in a path in this Hilbert space which may be learned using standard\n",
      "tools from statistical and machine learning.\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.13209 \n",
      "Title :AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n",
      "  Architectures\n",
      "  Learning to represent videos is a very challenging task both algorithmically\n",
      "and computationally. Standard video CNN architectures have been designed by\n",
      "directly extending architectures devised for image understanding to include the\n",
      "time dimension, using modules such as 3D convolutions, or by using two-stream\n",
      "design to capture both appearance and motion in videos. We interpret a video\n",
      "CNN as a collection of multi-stream convolutional blocks connected to each\n",
      "other, and propose the approach of automatically finding neural architectures\n",
      "with better connectivity and spatio-temporal interactions for video\n",
      "understanding. This is done by evolving a population of overly-connected\n",
      "architectures guided by connection weight learning. Architectures combining\n",
      "representations that abstract different input types (i.e., RGB and optical\n",
      "flow) at multiple temporal resolutions are searched for, allowing different\n",
      "types or sources of information to interact with each other. Our method,\n",
      "referred to as AssembleNet, outperforms prior approaches on public video\n",
      "datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n",
      "34.27% accuracy on Moments-in-Time.\n",
      "\n",
      "**Paper Id :2008.04852 \n",
      "Title :GeLaTO: Generative Latent Textured Objects\n",
      "  Accurate modeling of 3D objects exhibiting transparency, reflections and thin\n",
      "structures is an extremely challenging problem. Inspired by billboards and\n",
      "geometric proxies used in computer graphics, this paper proposes Generative\n",
      "Latent Textured Objects (GeLaTO), a compact representation that combines a set\n",
      "of coarse shape proxies defining low frequency geometry with learned neural\n",
      "textures, to encode both medium and fine scale geometry as well as\n",
      "view-dependent appearance. To generate the proxies' textures, we learn a joint\n",
      "latent space allowing category-level appearance and geometry interpolation. The\n",
      "proxies are independently rasterized with their corresponding neural texture\n",
      "and composited using a U-Net, which generates an output photorealistic image\n",
      "including an alpha map. We demonstrate the effectiveness of our approach by\n",
      "reconstructing complex objects from a sparse set of views. We show results on a\n",
      "dataset of real images of eyeglasses frames, which are particularly challenging\n",
      "to reconstruct using classical methods. We also demonstrate that these coarse\n",
      "proxies can be handcrafted when the underlying object geometry is easy to\n",
      "model, like eyeglasses, or generated using a neural network for more complex\n",
      "categories, such as cars.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.13402 \n",
      "Title :Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep\n",
      "  Model-Based RL for Sparse Cost Robotic Tasks\n",
      "  Reinforcement learning (RL) for robotics is challenging due to the difficulty\n",
      "in hand-engineering a dense cost function, which can lead to unintended\n",
      "behavior, and dynamical uncertainty, which makes exploration and constraint\n",
      "satisfaction challenging. We address these issues with a new model-based\n",
      "reinforcement learning algorithm, Safety Augmented Value Estimation from\n",
      "Demonstrations (SAVED), which uses supervision that only identifies task\n",
      "completion and a modest set of suboptimal demonstrations to constrain\n",
      "exploration and learn efficiently while handling complex constraints. We then\n",
      "compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms\n",
      "on 6 standard simulation benchmarks involving navigation and manipulation and a\n",
      "physical knot-tying task on the da Vinci surgical robot. Results suggest that\n",
      "SAVED outperforms prior methods in terms of success rate, constraint\n",
      "satisfaction, and sample efficiency, making it feasible to safely learn a\n",
      "control policy directly on a real robot in less than an hour. For tasks on the\n",
      "robot, baselines succeed less than 5% of the time while SAVED has a success\n",
      "rate of over 75% in the first 50 training iterations. Code and supplementary\n",
      "material is available at https://tinyurl.com/saved-rl.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.13613 \n",
      "Title :Regression Networks for Meta-Learning Few-Shot Classification\n",
      "  We propose regression networks for the problem of few-shot classification,\n",
      "where a classifier must generalize to new classes not seen in the training set,\n",
      "given only a small number of examples of each class. In high dimensional\n",
      "embedding spaces the direction of data generally contains richer information\n",
      "than magnitude. Next to this, state-of-the-art few-shot metric methods that\n",
      "compare distances with aggregated class representations, have shown superior\n",
      "performance. Combining these two insights, we propose to meta-learn\n",
      "classification of embedded points by regressing the closest approximation in\n",
      "every class subspace while using the regression error as a distance metric.\n",
      "Similarly to recent approaches for few-shot learning, regression networks\n",
      "reflect a simple inductive bias that is beneficial in this limited-data regime\n",
      "and they achieve excellent results, especially when more aggregate class\n",
      "representations can be formed with multiple shots.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.13667 \n",
      "Title :Partial Scanning Transmission Electron Microscopy with Deep Learning\n",
      "  Compressed sensing algorithms are used to decrease electron microscope scan\n",
      "time and electron beam exposure with minimal information loss. Following\n",
      "successful applications of deep learning to compressed sensing, we have\n",
      "developed a two-stage multiscale generative adversarial neural network to\n",
      "complete realistic 512$\\times$512 scanning transmission electron micrographs\n",
      "from spiral, jittered gridlike, and other partial scans. For spiral scans and\n",
      "mean squared error based pre-training, this enables electron beam coverage to\n",
      "be decreased by 17.9$\\times$ with a 3.8\\% test set root mean squared intensity\n",
      "error, and by 87.0$\\times$ with a 6.2\\% error. Our generator networks are\n",
      "trained on partial scans created from a new dataset of 16227 scanning\n",
      "transmission electron micrographs. High performance is achieved with adaptive\n",
      "learning rate clipping of loss spikes and an auxiliary trainer network. Our\n",
      "source code, new dataset, and pre-trained models have been made publicly\n",
      "available at https://github.com/Jeffrey-Ede/partial-STEM\n",
      "\n",
      "**Paper Id :1906.09060 \n",
      "Title :Adaptive Learning Rate Clipping Stabilizes Learning\n",
      "  Artificial neural network training with stochastic gradient descent can be\n",
      "destabilized by \"bad batches\" with high losses. This is often problematic for\n",
      "training with small batch sizes, high order loss functions or unstably high\n",
      "learning rates. To stabilize learning, we have developed adaptive learning rate\n",
      "clipping (ALRC) to limit backpropagated losses to a number of standard\n",
      "deviations above their running means. ALRC is designed to complement existing\n",
      "learning algorithms: Our algorithm is computationally inexpensive, can be\n",
      "applied to any loss function or batch size, is robust to hyperparameter choices\n",
      "and does not affect backpropagated gradient distributions. Experiments with\n",
      "CIFAR-10 supersampling show that ALCR decreases errors for unstable mean\n",
      "quartic error training while stable mean squared error training is unaffected.\n",
      "We also show that ALRC decreases unstable mean squared errors for partial\n",
      "scanning transmission electron micrograph completion. Our source code is\n",
      "publicly available at https://github.com/Jeffrey-Ede/ALRC\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1905.13741 \n",
      "Title :Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular\n",
      "  string representation\n",
      "  The discovery of novel materials and functional molecules can help to solve\n",
      "some of society's most urgent challenges, ranging from efficient energy\n",
      "harvesting and storage to uncovering novel pharmaceutical drug candidates.\n",
      "Traditionally matter engineering -- generally denoted as inverse design -- was\n",
      "based massively on human intuition and high-throughput virtual screening. The\n",
      "last few years have seen the emergence of significant interest in\n",
      "computer-inspired designs based on evolutionary or deep learning methods. The\n",
      "major challenge here is that the standard strings molecular representation\n",
      "SMILES shows substantial weaknesses in that task because large fractions of\n",
      "strings do not correspond to valid molecules. Here, we solve this problem at a\n",
      "fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a\n",
      "string-based representation of molecules which is 100\\% robust. Every SELFIES\n",
      "string corresponds to a valid molecule, and SELFIES can represent every\n",
      "molecule. SELFIES can be directly applied in arbitrary machine learning models\n",
      "without the adaptation of the models; each of the generated molecule candidates\n",
      "is valid. In our experiments, the model's internal memory stores two orders of\n",
      "magnitude more diverse molecules than a similar test with SMILES. Furthermore,\n",
      "as all molecules are valid, it allows for explanation and interpretation of the\n",
      "internal working of the generative models.\n",
      "\n",
      "**Paper Id :2002.12826 \n",
      "Title :A Deep Generative Model for Fragment-Based Molecule Generation\n",
      "  Molecule generation is a challenging open problem in cheminformatics.\n",
      "Currently, deep generative approaches addressing the challenge belong to two\n",
      "broad categories, differing in how molecules are represented. One approach\n",
      "encodes molecular graphs as strings of text, and learns their corresponding\n",
      "character-based language model. Another, more expressive, approach operates\n",
      "directly on the molecular graph. In this work, we address two limitations of\n",
      "the former: generation of invalid and duplicate molecules. To improve validity\n",
      "rates, we develop a language model for small molecular substructures called\n",
      "fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug\n",
      "Design. In other words, we generate molecules fragment by fragment, instead of\n",
      "atom by atom. To improve uniqueness rates, we present a frequency-based masking\n",
      "strategy that helps generate molecules with infrequent fragments. We show\n",
      "experimentally that our model largely outperforms other language model-based\n",
      "competitors, reaching state-of-the-art performances typical of graph-based\n",
      "approaches. Moreover, generated molecules display molecular properties similar\n",
      "to those in the training sample, even in absence of explicit task-specific\n",
      "supervision.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.00150 \n",
      "Title :Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training\n",
      "  Neural Networks\n",
      "  Handling missing data is one of the most fundamental problems in machine\n",
      "learning. Among many approaches, the simplest and most intuitive way is zero\n",
      "imputation, which treats the value of a missing entry simply as zero. However,\n",
      "many studies have experimentally confirmed that zero imputation results in\n",
      "suboptimal performances in training neural networks. Yet, none of the existing\n",
      "work has explained what brings such performance degradations. In this paper, we\n",
      "introduce the variable sparsity problem (VSP), which describes a phenomenon\n",
      "where the output of a predictive model largely varies with respect to the rate\n",
      "of missingness in the given input, and show that it adversarially affects the\n",
      "model performance. We first theoretically analyze this phenomenon and propose a\n",
      "simple yet effective technique to handle missingness, which we refer to as\n",
      "Sparsity Normalization (SN), that directly targets and resolves the VSP. We\n",
      "further experimentally validate SN on diverse benchmark datasets, to show that\n",
      "debiasing the effect of input-level sparsity improves the performance and\n",
      "stabilizes the training of neural networks.\n",
      "\n",
      "**Paper Id :1911.09450 \n",
      "Title :Few Shot Network Compression via Cross Distillation\n",
      "  Model compression has been widely adopted to obtain light-weighted deep\n",
      "neural networks. Most prevalent methods, however, require fine-tuning with\n",
      "sufficient training data to ensure accuracy, which could be challenged by\n",
      "privacy and security issues. As a compromise between privacy and performance,\n",
      "in this paper we investigate few shot network compression: given few samples\n",
      "per class, how can we effectively compress the network with negligible\n",
      "performance drop? The core challenge of few shot network compression lies in\n",
      "high estimation errors from the original network during inference, since the\n",
      "compressed network can easily over-fits on the few training instances. The\n",
      "estimation errors could propagate and accumulate layer-wisely and finally\n",
      "deteriorate the network output. To address the problem, we propose cross\n",
      "distillation, a novel layer-wise knowledge distillation approach. By\n",
      "interweaving hidden layers of teacher and student network, layer-wisely\n",
      "accumulated estimation errors can be effectively reduced.The proposed method\n",
      "offers a general framework compatible with prevalent network compression\n",
      "techniques such as pruning. Extensive experiments on benchmark datasets\n",
      "demonstrate that cross distillation can significantly improve the student\n",
      "network's accuracy when only a few training instances are available.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.00588 \n",
      "Title :Quantifying Point-Prediction Uncertainty in Neural Networks via Residual\n",
      "  Estimation with an I/O Kernel\n",
      "  Neural Networks (NNs) have been extensively used for a wide spectrum of\n",
      "real-world regression tasks, where the goal is to predict a numerical outcome\n",
      "such as revenue, effectiveness, or a quantitative result. In many such tasks,\n",
      "the point prediction is not enough: the uncertainty (i.e. risk or confidence)\n",
      "of that prediction must also be estimated. Standard NNs, which are most often\n",
      "used in such tasks, do not provide uncertainty information. Existing approaches\n",
      "address this issue by combining Bayesian models with NNs, but these models are\n",
      "hard to implement, more expensive to train, and usually do not predict as\n",
      "accurately as standard NNs. In this paper, a new framework (RIO) is developed\n",
      "that makes it possible to estimate uncertainty in any pretrained standard NN.\n",
      "The behavior of the NN is captured by modeling its prediction residuals with a\n",
      "Gaussian Process, whose kernel includes both the NN's input and its output. The\n",
      "framework is evaluated in twelve real-world datasets, where it is found to (1)\n",
      "provide reliable estimates of uncertainty, (2) reduce the error of the point\n",
      "predictions, and (3) scale well to large datasets. Given that RIO can be\n",
      "applied to any standard NN without modifications to model architecture or\n",
      "training pipeline, it provides an important ingredient for building real-world\n",
      "NN applications.\n",
      "\n",
      "**Paper Id :2002.08274 \n",
      "Title :Residual Correlation in Graph Neural Network Regression\n",
      "  A graph neural network transforms features in each vertex's neighborhood into\n",
      "a vector representation of the vertex. Afterward, each vertex's representation\n",
      "is used independently for predicting its label. This standard pipeline\n",
      "implicitly assumes that vertex labels are conditionally independent given their\n",
      "neighborhood features. However, this is a strong assumption, and we show that\n",
      "it is far from true on many real-world graph datasets. Focusing on regression\n",
      "tasks, we find that this conditional independence assumption severely limits\n",
      "predictive power. This should not be that surprising, given that traditional\n",
      "graph-based semi-supervised learning methods such as label propagation work in\n",
      "the opposite fashion by explicitly modeling the correlation in predicted\n",
      "outcomes.\n",
      "  Here, we address this problem with an interpretable and efficient framework\n",
      "that can improve any graph neural network architecture simply by exploiting\n",
      "correlation structure in the regression residuals. In particular, we model the\n",
      "joint distribution of residuals on vertices with a parameterized multivariate\n",
      "Gaussian, and estimate the parameters by maximizing the marginal likelihood of\n",
      "the observed labels. Our framework achieves substantially higher accuracy than\n",
      "competing baselines, and the learned parameters can be interpreted as the\n",
      "strength of correlation among connected vertices. Furthermore, we develop\n",
      "linear time algorithms for low-variance, unbiased model parameter estimates,\n",
      "allowing us to scale to large networks. We also provide a basic version of our\n",
      "method that makes stronger assumptions on correlation structure but is painless\n",
      "to implement, often leading to great practical performance with minimal\n",
      "overhead.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.01103 \n",
      "Title :Attractive versus truncated repulsive supercooled liquids: The dynamics\n",
      "  is encoded in the pair correlation function\n",
      "  We compare glassy dynamics in two liquids that differ in the form of their\n",
      "interaction potentials. Both systems have the same repulsive interactions but\n",
      "one has also an attractive part in the potential. These two systems exhibit\n",
      "very different dynamics despite having nearly identical pair correlation\n",
      "functions. We demonstrate that a properly weighted integral of the pair\n",
      "correlation function, which amplifies the subtle differences between the two\n",
      "systems, correctly captures their dynamical differences. The weights are\n",
      "obtained from a standard machine learning algorithm.\n",
      "\n",
      "**Paper Id :1910.12752 \n",
      "Title :Analytical classical density functionals from an equation learning\n",
      "  network\n",
      "  We explore the feasibility of using machine learning methods to obtain an\n",
      "analytic form of the classical free energy functional for two model fluids,\n",
      "hard rods and Lennard--Jones, in one dimension . The Equation Learning Network\n",
      "proposed in Ref. 1 is suitably modified to construct free energy densities\n",
      "which are functions of a set of weighted densities and which are built from a\n",
      "small number of basis functions with flexible combination rules. This setup\n",
      "considerably enlarges the functional space used in the machine learning\n",
      "optimization as compared to previous work 2 where the functional is limited to\n",
      "a simple polynomial form. As a result, we find a good approximation for the\n",
      "exact hard rod functional and its direct correlation function. For the\n",
      "Lennard--Jones fluid, we let the network learn (i) the full excess free energy\n",
      "functional and (ii) the excess free energy functional related to interparticle\n",
      "attractions. Both functionals show a good agreement with simulated density\n",
      "profiles for thermodynamic parameters inside and outside the training region.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.01251 \n",
      "Title :The Extended Dawid-Skene Model: Fusing Information from Multiple Data\n",
      "  Schemas\n",
      "  While label fusion from multiple noisy annotations is a well understood\n",
      "concept in data wrangling (tackled for example by the Dawid-Skene (DS) model),\n",
      "we consider the extended problem of carrying out learning when the labels\n",
      "themselves are not consistently annotated with the same schema. We show that\n",
      "even if annotators use disparate, albeit related, label-sets, we can still draw\n",
      "inferences for the underlying full label-set. We propose the Inter-Schema\n",
      "AdapteR (ISAR) to translate the fully-specified label-set to the one used by\n",
      "each annotator, enabling learning under such heterogeneous schemas, without the\n",
      "need to re-annotate the data. We apply our method to a mouse behavioural\n",
      "dataset, achieving significant gains (compared with DS) in out-of-sample\n",
      "log-likelihood (-3.40 to -2.39) and F1-score (0.785 to 0.864).\n",
      "\n",
      "**Paper Id :2001.05922 \n",
      "Title :Continual Learning for Domain Adaptation in Chest X-ray Classification\n",
      "  Over the last years, Deep Learning has been successfully applied to a broad\n",
      "range of medical applications. Especially in the context of chest X-ray\n",
      "classification, results have been reported which are on par, or even superior\n",
      "to experienced radiologists. Despite this success in controlled experimental\n",
      "environments, it has been noted that the ability of Deep Learning models to\n",
      "generalize to data from a new domain (with potentially different tasks) is\n",
      "often limited. In order to address this challenge, we investigate techniques\n",
      "from the field of Continual Learning (CL) including Joint Training (JT),\n",
      "Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using\n",
      "the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that\n",
      "these methods provide promising options to improve the performance of Deep\n",
      "Learning models on a target domain and to mitigate effectively catastrophic\n",
      "forgetting for the source domain. To this end, the best overall performance was\n",
      "obtained using JT, while for LWF competitive results could be achieved - even\n",
      "without accessing data from the source domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.01470 \n",
      "Title :Options as responses: Grounding behavioural hierarchies in multi-agent\n",
      "  RL\n",
      "  This paper investigates generalisation in multi-agent games, where the\n",
      "generality of the agent can be evaluated by playing against opponents it hasn't\n",
      "seen during training. We propose two new games with concealed information and\n",
      "complex, non-transitive reward structure (think rock/paper/scissors). It turns\n",
      "out that most current deep reinforcement learning methods fail to efficiently\n",
      "explore the strategy space, thus learning policies that generalise poorly to\n",
      "unseen opponents. We then propose a novel hierarchical agent architecture,\n",
      "where the hierarchy is grounded in the game-theoretic structure of the game --\n",
      "the top level chooses strategic responses to opponents, while the low level\n",
      "implements them into policy over primitive actions. This grounding facilitates\n",
      "credit assignment across the levels of hierarchy. Our experiments show that the\n",
      "proposed hierarchical agent is capable of generalisation to unseen opponents,\n",
      "while conventional baselines fail to generalise whatsoever.\n",
      "\n",
      "**Paper Id :1901.03887 \n",
      "Title :Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement\n",
      "  Learning through Memory-driven Communication\n",
      "  Deep reinforcement learning algorithms have recently been used to train\n",
      "multiple interacting agents in a centralised manner whilst keeping their\n",
      "execution decentralised. When the agents can only acquire partial observations\n",
      "and are faced with tasks requiring coordination and synchronisation skills,\n",
      "inter-agent communication plays an essential role. In this work, we propose a\n",
      "framework for multi-agent training using deep deterministic policy gradients\n",
      "that enables concurrent, end-to-end learning of an explicit communication\n",
      "protocol through a memory device. During training, the agents learn to perform\n",
      "read and write operations enabling them to infer a shared representation of the\n",
      "world. We empirically demonstrate that concurrent learning of the communication\n",
      "device and individual policies can improve inter-agent coordination and\n",
      "performance in small-scale systems. Our experimental results show that the\n",
      "proposed method achieves superior performance in scenarios with up to six\n",
      "agents. We illustrate how different communication patterns can emerge on six\n",
      "different tasks of increasing complexity. Furthermore, we study the effects of\n",
      "corrupting the communication channel, provide a visualisation of the\n",
      "time-varying memory content as the underlying task is being solved and validate\n",
      "the building blocks of the proposed memory device through ablation studies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.01493 \n",
      "Title :Constructing Energy-efficient Mixed-precision Neural Networks through\n",
      "  Principal Component Analysis for Edge Intelligence\n",
      "  The `Internet of Things' has brought increased demand for AI-based edge\n",
      "computing in applications ranging from healthcare monitoring systems to\n",
      "autonomous vehicles. Quantization is a powerful tool to address the growing\n",
      "computational cost of such applications, and yields significant compression\n",
      "over full-precision networks. However, quantization can result in substantial\n",
      "loss of performance for complex image classification tasks. To address this, we\n",
      "propose a Principal Component Analysis (PCA) driven methodology to identify the\n",
      "important layers of a binary network, and design mixed-precision networks. The\n",
      "proposed Hybrid-Net achieves a more than 10% improvement in classification\n",
      "accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures\n",
      "on CIFAR-100 and ImageNet datasets while still achieving up to 94% of the\n",
      "energy-efficiency of XNOR-Nets. This work furthers the feasibility of using\n",
      "highly compressed neural networks for energy-efficient neural computing in edge\n",
      "devices.\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "**Paper Id :2007.13243 \n",
      "Title :Scalable Derivative-Free Optimization for Nonlinear Least-Squares\n",
      "  Problems\n",
      "  Derivative-free - or zeroth-order - optimization (DFO) has gained recent\n",
      "attention for its ability to solve problems in a variety of application areas,\n",
      "including machine learning, particularly involving objectives which are\n",
      "stochastic and/or expensive to compute. In this work, we develop a novel\n",
      "model-based DFO method for solving nonlinear least-squares problems. We improve\n",
      "on state-of-the-art DFO by performing dimensionality reduction in the\n",
      "observational space using sketching methods, avoiding the construction of a\n",
      "full local model. Our approach has a per-iteration computational cost which is\n",
      "linear in problem dimension in a big data regime, and numerical evidence\n",
      "demonstrates that, compared to existing software, it has dramatically improved\n",
      "runtime performance on overdetermined least-squares problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.02416 \n",
      "Title :Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models\n",
      "  To scale non-parametric extensions of probabilistic topic models such as\n",
      "Latent Dirichlet allocation to larger data sets, practitioners rely\n",
      "increasingly on parallel and distributed systems. In this work, we study\n",
      "data-parallel training for the hierarchical Dirichlet process (HDP) topic\n",
      "model. Based upon a representation of certain conditional distributions within\n",
      "an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic\n",
      "model. This sampler utilizes all available sources of sparsity found in natural\n",
      "language - an important way to make computation efficient. We benchmark our\n",
      "method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using\n",
      "a single multi-core machine in under four days.\n",
      "\n",
      "**Paper Id :2011.04640 \n",
      "Title :Scaling Hidden Markov Language Models\n",
      "  The hidden Markov model (HMM) is a fundamental tool for sequence modeling\n",
      "that cleanly separates the hidden state from the emission structure. However,\n",
      "this separation makes it difficult to fit HMMs to large datasets in modern NLP,\n",
      "and they have fallen out of use due to very poor performance compared to fully\n",
      "observed models. This work revisits the challenge of scaling HMMs to language\n",
      "modeling datasets, taking ideas from recent approaches to neural modeling. We\n",
      "propose methods for scaling HMMs to massive state spaces while maintaining\n",
      "efficient exact inference, a compact parameterization, and effective\n",
      "regularization. Experiments show that this approach leads to models that are\n",
      "more accurate than previous HMM and n-gram-based methods, making progress\n",
      "towards the performance of state-of-the-art neural models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.03388 \n",
      "Title :Protocol for implementing quantum nonparametric learning with trapped\n",
      "  ions\n",
      "  Nonparametric learning is able to make reliable predictions by extracting\n",
      "information from similarities between a new set of input data and all samples.\n",
      "Here we point out a quantum paradigm of nonparametric learning which offers an\n",
      "exponential speedup over the sample size. By encoding data into quantum feature\n",
      "space, similarity between the data is defined as an inner product of quantum\n",
      "states. A quantum training state is introduced to superpose all data of\n",
      "samples, encoding relevant information for learning in its bipartite\n",
      "entanglement spectrum. We demonstrate that a trained state for prediction can\n",
      "be obtained by entanglement spectrum transformation, using quantum matrix\n",
      "toolbox. We further work out a feasible protocol to implement the quantum\n",
      "nonparametric learning with trapped ions, and demonstrate the power of quantum\n",
      "superposition for machine learning.\n",
      "\n",
      "**Paper Id :1910.08798 \n",
      "Title :Data classification by quantum radial basis function networks\n",
      "  Radial basis function (RBF) network is a third layered neural network that is\n",
      "widely used in function approximation and data classification. Here we propose\n",
      "a quantum model of the RBF network. Similar to the classical case, we still use\n",
      "the radial basis functions as the activation functions. Quantum linear\n",
      "algebraic techniques and coherent states can be applied to implement these\n",
      "functions. Differently, we define the state of the weight as a tensor product\n",
      "of single-qubit states. This gives a simple approach to implement the quantum\n",
      "RBF network in the quantum circuits. Theoretically, we prove that the training\n",
      "is almost quadratic faster than the classical one. Numerically, we demonstrate\n",
      "that the quantum RBF network can solve binary classification problems as good\n",
      "as the classical RBF network. While the time used for training is much shorter.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.06283 \n",
      "Title :Support vector machines on the D-Wave quantum annealer\n",
      "  Kernel-based support vector machines (SVMs) are supervised machine learning\n",
      "algorithms for classification and regression problems. We introduce a method to\n",
      "train SVMs on a D-Wave 2000Q quantum annealer and study its performance in\n",
      "comparison to SVMs trained on conventional computers. The method is applied to\n",
      "both synthetic data and real data obtained from biology experiments. We find\n",
      "that the quantum annealer produces an ensemble of different solutions that\n",
      "often generalizes better to unseen data than the single global minimum of an\n",
      "SVM trained on a conventional computer, especially in cases where only limited\n",
      "training data is available. For cases with more training data than currently\n",
      "fits on the quantum annealer, we show that a combination of classifiers for\n",
      "subsets of the data almost always produces stronger joint classifiers than the\n",
      "conventional SVM for the same parameters.\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.06843 \n",
      "Title :Predicting Research Trends with Semantic and Neural Networks with an\n",
      "  application in Quantum Physics\n",
      "  The vast and growing number of publications in all disciplines of science\n",
      "cannot be comprehended by a single human researcher. As a consequence,\n",
      "researchers have to specialize in narrow sub-disciplines, which makes it\n",
      "challenging to uncover scientific connections beyond the own field of research.\n",
      "Thus access to structured knowledge from a large corpus of publications could\n",
      "help pushing the frontiers of science. Here we demonstrate a method to build a\n",
      "semantic network from published scientific literature, which we call SemNet. We\n",
      "use SemNet to predict future trends in research and to inspire new,\n",
      "personalized and surprising seeds of ideas in science. We apply it in the\n",
      "discipline of quantum physics, which has seen an unprecedented growth of\n",
      "activity in recent years. In SemNet, scientific knowledge is represented as an\n",
      "evolving network using the content of 750,000 scientific papers published since\n",
      "1919. The nodes of the network correspond to physical concepts, and links\n",
      "between two nodes are drawn when two physical concepts are concurrently studied\n",
      "in research articles. We identify influential and prize-winning research topics\n",
      "from the past inside SemNet thus confirm that it stores useful semantic\n",
      "knowledge. We train a deep neural network using states of SemNet of the past,\n",
      "to predict future developments in quantum physics research, and confirm high\n",
      "quality predictions using historic data. With the neural network and\n",
      "theoretical network tools we are able to suggest new, personalized,\n",
      "out-of-the-box ideas, by identifying pairs of concepts which have unique and\n",
      "extremal semantic network properties. Finally, we consider possible future\n",
      "developments and implications of our findings.\n",
      "\n",
      "**Paper Id :1912.11084 \n",
      "Title :Where Are We? Using Scopus to Map the Literature at the Intersection\n",
      "  Between Artificial Intelligence and Research on Crime\n",
      "  Research on Artificial Intelligence (AI) applications has spread over many\n",
      "scientific disciplines. Scientists have tested the power of intelligent\n",
      "algorithms developed to predict (or learn from) natural, physical and social\n",
      "phenomena. This also applies to crime-related research problems. Nonetheless,\n",
      "studies that map the current state of the art at the intersection between AI\n",
      "and crime are lacking. What are the current research trends in terms of topics\n",
      "in this area? What is the structure of scientific collaboration when\n",
      "considering works investigating criminal issues using machine learning, deep\n",
      "learning, and AI in general? What are the most active countries in this\n",
      "specific scientific sphere? Using data retrieved from the Scopus database, this\n",
      "work quantitatively analyzes 692 published works at the intersection between AI\n",
      "and crime employing network science to respond to these questions. Results show\n",
      "that researchers are mainly focusing on cyber-related criminal topics and that\n",
      "relevant themes such as algorithmic discrimination, fairness, and ethics are\n",
      "considerably overlooked. Furthermore, data highlight the extremely disconnected\n",
      "structure of co-authorship networks. Such disconnectedness may represent a\n",
      "substantial obstacle to a more solid community of scientists interested in\n",
      "these topics. Additionally, the graph of scientific collaboration indicates\n",
      "that countries that are more prone to engage in international partnerships are\n",
      "generally less central in the network. This means that scholars working in\n",
      "highly productive countries (e.g. the United States, China) tend to mostly\n",
      "collaborate domestically. Finally, current issues and future developments\n",
      "within this scientific area are also discussed.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.07300 \n",
      "Title :Linear Lower Bounds and Conditioning of Differentiable Games\n",
      "  Recent successes of game-theoretic formulations in ML have caused a\n",
      "resurgence of research interest in differentiable games. Overwhelmingly, that\n",
      "research focuses on methods and upper bounds on their speed of convergence. In\n",
      "this work, we approach the question of fundamental iteration complexity by\n",
      "providing lower bounds to complement the linear (i.e. geometric) upper bounds\n",
      "observed in the literature on a wide class of problems. We cast saddle-point\n",
      "and min-max problems as 2-player games. We leverage tools from single-objective\n",
      "convex optimisation to propose new linear lower bounds for convex-concave\n",
      "games. Notably, we give a linear lower bound for $n$-player differentiable\n",
      "games, by using the spectral properties of the update operator. We then propose\n",
      "a new definition of the condition number arising from our lower bound analysis.\n",
      "Unlike past definitions, our condition number captures the fact that linear\n",
      "rates are possible in games, even in the absence of strong convexity or strong\n",
      "concavity in the variables.\n",
      "\n",
      "**Paper Id :1908.05699 \n",
      "Title :Convergence of Gradient Methods on Bilinear Zero-Sum Games\n",
      "  Min-max formulations have attracted great attention in the ML community due\n",
      "to the rise of deep generative models and adversarial methods, while\n",
      "understanding the dynamics of gradient algorithms for solving such formulations\n",
      "has remained a grand challenge. As a first step, we restrict to bilinear\n",
      "zero-sum games and give a systematic analysis of popular gradient updates, for\n",
      "both simultaneous and alternating versions. We provide exact conditions for\n",
      "their convergence and find the optimal parameter setup and convergence rates.\n",
      "In particular, our results offer formal evidence that alternating updates\n",
      "converge \"better\" than simultaneous ones.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.07315 \n",
      "Title :Evolutionary Reinforcement Learning for Sample-Efficient Multiagent\n",
      "  Coordination\n",
      "  Many cooperative multiagent reinforcement learning environments provide\n",
      "agents with a sparse team-based reward, as well as a dense agent-specific\n",
      "reward that incentivizes learning basic skills. Training policies solely on the\n",
      "team-based reward is often difficult due to its sparsity. Furthermore, relying\n",
      "solely on the agent-specific reward is sub-optimal because it usually does not\n",
      "capture the team coordination objective. A common approach is to use reward\n",
      "shaping to construct a proxy reward by combining the individual rewards.\n",
      "However, this requires manual tuning for each environment. We introduce\n",
      "Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training\n",
      "platform that handles the two objectives separately through two optimization\n",
      "processes. An evolutionary algorithm maximizes the sparse team-based objective\n",
      "through neuroevolution on a population of teams. Concurrently, a gradient-based\n",
      "optimizer trains policies to only maximize the dense agent-specific rewards.\n",
      "The gradient-based policies are periodically added to the evolutionary\n",
      "population as a way of information transfer between the two optimization\n",
      "processes. This enables the evolutionary algorithm to use skills learned via\n",
      "the agent-specific rewards toward optimizing the global objective. Results\n",
      "demonstrate that MERL significantly outperforms state-of-the-art methods, such\n",
      "as MADDPG, on a number of difficult coordination benchmarks.\n",
      "\n",
      "**Paper Id :2004.03267 \n",
      "Title :Guided Dialog Policy Learning without Adversarial Learning in the Loop\n",
      "  Reinforcement Learning (RL) methods have emerged as a popular choice for\n",
      "training an efficient and effective dialogue policy. However, these methods\n",
      "suffer from sparse and unstable reward signals returned by a user simulator\n",
      "only when a dialogue finishes. Besides, the reward signal is manually designed\n",
      "by human experts, which requires domain knowledge. Recently, a number of\n",
      "adversarial learning methods have been proposed to learn the reward function\n",
      "together with the dialogue policy. However, to alternatively update the\n",
      "dialogue policy and the reward model on the fly, we are limited to\n",
      "policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the\n",
      "alternating training of a dialogue agent and the reward model can easily get\n",
      "stuck in local optima or result in mode collapse. To overcome the listed\n",
      "issues, we propose to decompose the adversarial training into two steps. First,\n",
      "we train the discriminator with an auxiliary dialogue generator and then\n",
      "incorporate a derived reward model into a common RL method to guide the\n",
      "dialogue policy learning. This approach is applicable to both on-policy and\n",
      "off-policy RL methods. Based on our extensive experimentation, we can conclude\n",
      "the proposed method: (1) achieves a remarkable task success rate using both\n",
      "on-policy and off-policy RL methods; and (2) has the potential to transfer\n",
      "knowledge from existing domains to a new domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.08829 \n",
      "Title :Data-driven prediction of a multi-scale Lorenz 96 chaotic system using\n",
      "  deep learning methods: Reservoir computing, ANN, and RNN-LSTM\n",
      "  In this paper, the performance of three deep learning methods for predicting\n",
      "short-term evolution and for reproducing the long-term statistics of a\n",
      "multi-scale spatio-temporal Lorenz 96 system is examined. The methods are: echo\n",
      "state network (a type of reservoir computing, RC-ESN), deep feed-forward\n",
      "artificial neural network (ANN), and recurrent neural network with long\n",
      "short-term memory (RNN-LSTM). This Lorenz 96 system has three tiers of\n",
      "nonlinearly interacting variables representing slow/large-scale ($X$),\n",
      "intermediate ($Y$), and fast/small-scale ($Z$) processes. For training or\n",
      "testing, only $X$ is available; $Y$ and $Z$ are never known or used. We show\n",
      "that RC-ESN substantially outperforms ANN and RNN-LSTM for short-term\n",
      "prediction, e.g., accurately forecasting the chaotic trajectories for hundreds\n",
      "of numerical solver's time steps, equivalent to several Lyapunov timescales.\n",
      "The RNN-LSTM and ANN show some prediction skills as well; RNN-LSTM bests ANN.\n",
      "Furthermore, even after losing the trajectory, data predicted by RC-ESN and\n",
      "RNN-LSTM have probability density functions (PDFs) that closely match the true\n",
      "PDF, even at the tails. The PDF of the data predicted using ANN, however,\n",
      "deviates from the true PDF. Implications, caveats, and applications to\n",
      "data-driven and data-assisted surrogate modeling of complex nonlinear dynamical\n",
      "systems such as weather/climate are discussed.\n",
      "\n",
      "**Paper Id :2007.00936 \n",
      "Title :Deep Neural Networks for Nonlinear Model Order Reduction of Unsteady\n",
      "  Flows\n",
      "  Unsteady fluid systems are nonlinear high-dimensional dynamical systems that\n",
      "may exhibit multiple complex phenomena both in time and space. Reduced Order\n",
      "Modeling (ROM) of fluid flows has been an active research topic in the recent\n",
      "decade with the primary goal to decompose complex flows to a set of features\n",
      "most important for future state prediction and control, typically using a\n",
      "dimensionality reduction technique. In this work, a novel data-driven technique\n",
      "based on the power of deep neural networks for reduced order modeling of the\n",
      "unsteady fluid flows is introduced. An autoencoder network is used for\n",
      "nonlinear dimension reduction and feature extraction as an alternative for\n",
      "singular value decomposition (SVD). Then, the extracted features are used as an\n",
      "input for long short-term memory network (LSTM) to predict the velocity field\n",
      "at future time instances. The proposed autoencoder-LSTM method is compared with\n",
      "non-intrusive reduced order models based on dynamic mode decomposition (DMD)\n",
      "and proper orthogonal decomposition (POD). Moreover, an autoencoder-DMD\n",
      "algorithm is introduced for reduced order modeling, which uses the autoencoder\n",
      "network for dimensionality reduction rather than SVD rank truncation. Results\n",
      "show that the autoencoder-LSTM method is considerably capable of predicting\n",
      "fluid flow evolution, where higher values for coefficient of determination\n",
      "$R^{2}$ are obtained using autoencoder-LSTM compared to other models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.09060 \n",
      "Title :Adaptive Learning Rate Clipping Stabilizes Learning\n",
      "  Artificial neural network training with stochastic gradient descent can be\n",
      "destabilized by \"bad batches\" with high losses. This is often problematic for\n",
      "training with small batch sizes, high order loss functions or unstably high\n",
      "learning rates. To stabilize learning, we have developed adaptive learning rate\n",
      "clipping (ALRC) to limit backpropagated losses to a number of standard\n",
      "deviations above their running means. ALRC is designed to complement existing\n",
      "learning algorithms: Our algorithm is computationally inexpensive, can be\n",
      "applied to any loss function or batch size, is robust to hyperparameter choices\n",
      "and does not affect backpropagated gradient distributions. Experiments with\n",
      "CIFAR-10 supersampling show that ALCR decreases errors for unstable mean\n",
      "quartic error training while stable mean squared error training is unaffected.\n",
      "We also show that ALRC decreases unstable mean squared errors for partial\n",
      "scanning transmission electron micrograph completion. Our source code is\n",
      "publicly available at https://github.com/Jeffrey-Ede/ALRC\n",
      "\n",
      "**Paper Id :2001.05559 \n",
      "Title :Mode-Assisted Unsupervised Learning of Restricted Boltzmann Machines\n",
      "  Restricted Boltzmann machines (RBMs) are a powerful class of generative\n",
      "models, but their training requires computing a gradient that, unlike\n",
      "supervised backpropagation on typical loss functions, is notoriously difficult\n",
      "even to approximate. Here, we show that properly combining standard gradient\n",
      "updates with an off-gradient direction, constructed from samples of the RBM\n",
      "ground state (mode), improves their training dramatically over traditional\n",
      "gradient methods. This approach, which we call mode training, promotes faster\n",
      "training and stability, in addition to lower converged relative entropy (KL\n",
      "divergence). Along with the proofs of stability and convergence of this method,\n",
      "we also demonstrate its efficacy on synthetic datasets where we can compute KL\n",
      "divergences exactly, as well as on a larger machine learning standard, MNIST.\n",
      "The mode training we suggest is quite versatile, as it can be applied in\n",
      "conjunction with any given gradient method, and is easily extended to more\n",
      "general energy-based neural network structures such as deep, convolutional and\n",
      "unrestricted Boltzmann machines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.09230 \n",
      "Title :Modeling and Forecasting Art Movements with CGANs\n",
      "  Conditional Generative Adversarial Networks~(CGAN) are a recent and popular\n",
      "method for generating samples from a probability distribution conditioned on\n",
      "latent information. The latent information often comes in the form of a\n",
      "discrete label from a small set. We propose a novel method for training CGANs\n",
      "which allows us to condition on a sequence of continuous latent distributions\n",
      "$f^{(1)}, \\ldots, f^{(K)}$. This training allows CGANs to generate samples from\n",
      "a sequence of distributions. We apply our method to paintings from a sequence\n",
      "of artistic movements, where each movement is considered to be its own\n",
      "distribution. Exploiting the temporal aspect of the data, a vector\n",
      "autoregressive (VAR) model is fitted to the means of the latent distributions\n",
      "that we learn, and used for one-step-ahead forecasting, to predict the latent\n",
      "distribution of a future art movement $f^{{(K+1)}}$. Realisations from this\n",
      "distribution can be used by the CGAN to generate \"future\" paintings. In\n",
      "experiments, this novel methodology generates accurate predictions of the\n",
      "evolution of art. The training set consists of a large dataset of past\n",
      "paintings. While there is no agreement on exactly what current art period we\n",
      "find ourselves in, we test on plausible candidate sets of present art, and show\n",
      "that the mean distance to our predictions is small.\n",
      "\n",
      "**Paper Id :2005.10374 \n",
      "Title :Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric\n",
      "  Fields with a Generative Adversarial Network\n",
      "  Generative adversarial networks (GANs) have been recently adopted for\n",
      "super-resolution, an application closely related to what is referred to as\n",
      "\"downscaling\" in the atmospheric sciences: improving the spatial resolution of\n",
      "low-resolution images. The ability of conditional GANs to generate an ensemble\n",
      "of solutions for a given input lends itself naturally to stochastic\n",
      "downscaling, but the stochastic nature of GANs is not usually considered in\n",
      "super-resolution applications. Here, we introduce a recurrent, stochastic\n",
      "super-resolution GAN that can generate ensembles of time-evolving\n",
      "high-resolution atmospheric fields for an input consisting of a low-resolution\n",
      "sequence of images of the same field. We test the GAN using two datasets, one\n",
      "consisting of radar-measured precipitation from Switzerland, the other of cloud\n",
      "optical thickness derived from the Geostationary Earth Observing Satellite 16\n",
      "(GOES-16). We find that the GAN can generate realistic, temporally consistent\n",
      "super-resolution sequences for both datasets. The statistical properties of the\n",
      "generated ensemble are analyzed using rank statistics, a method adapted from\n",
      "ensemble weather forecasting; these analyses indicate that the GAN produces\n",
      "close to the correct amount of variability in its outputs. As the GAN generator\n",
      "is fully convolutional, it can be applied after training to input images larger\n",
      "than the images used to train it. It is also able to generate time series much\n",
      "longer than the training sequences, as demonstrated by applying the generator\n",
      "to a three-month dataset of the precipitation radar data. The source code to\n",
      "our GAN is available at https://github.com/jleinonen/downscaling-rnn-gan.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.09679 \n",
      "Title :The Value of Collaboration in Convex Machine Learning with Differential\n",
      "  Privacy\n",
      "  In this paper, we apply machine learning to distributed private data owned by\n",
      "multiple data owners, entities with access to non-overlapping training\n",
      "datasets. We use noisy, differentially-private gradients to minimize the\n",
      "fitness cost of the machine learning model using stochastic gradient descent.\n",
      "We quantify the quality of the trained model, using the fitness cost, as a\n",
      "function of privacy budget and size of the distributed datasets to capture the\n",
      "trade-off between privacy and utility in machine learning. This way, we can\n",
      "predict the outcome of collaboration among privacy-aware data owners prior to\n",
      "executing potentially computationally-expensive machine learning algorithms.\n",
      "Particularly, we show that the difference between the fitness of the trained\n",
      "machine learning model using differentially-private gradient queries and the\n",
      "fitness of the trained machine model in the absence of any privacy concerns is\n",
      "inversely proportional to the size of the training datasets squared and the\n",
      "privacy budget squared. We successfully validate the performance prediction\n",
      "with the actual performance of the proposed privacy-aware learning algorithms,\n",
      "applied to: financial datasets for determining interest rates of loans using\n",
      "regression; and detecting credit card frauds using support vector machines.\n",
      "\n",
      "**Paper Id :1912.12576 \n",
      "Title :Privacy-Preserving Public Release of Datasets for Support Vector Machine\n",
      "  Classification\n",
      "  We consider the problem of publicly releasing a dataset for support vector\n",
      "machine classification while not infringing on the privacy of data subjects\n",
      "(i.e., individuals whose private information is stored in the dataset). The\n",
      "dataset is systematically obfuscated using an additive noise for privacy\n",
      "protection. Motivated by the Cramer-Rao bound, inverse of the trace of the\n",
      "Fisher information matrix is used as a measure of the privacy. Conditions are\n",
      "established for ensuring that the classifier extracted from the original\n",
      "dataset and the obfuscated one are close to each other (capturing the utility).\n",
      "The optimal noise distribution is determined by maximizing a weighted sum of\n",
      "the measures of privacy and utility. The optimal privacy-preserving noise is\n",
      "proved to achieve local differential privacy. The results are generalized to a\n",
      "broader class of optimization-based supervised machine learning algorithms.\n",
      "Applicability of the methodology is demonstrated on multiple datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.09831 \n",
      "Title :Foolproof Cooperative Learning\n",
      "  This paper extends the notion of learning equilibrium in game theory from\n",
      "matrix games to stochastic games. We introduce Foolproof Cooperative Learning\n",
      "(FCL), an algorithm that converges to a Tit-for-Tat behavior. It allows\n",
      "cooperative strategies when played against itself while being not exploitable\n",
      "by selfish players. We prove that in repeated symmetric games, this algorithm\n",
      "is a learning equilibrium. We illustrate the behavior of FCL on symmetric\n",
      "matrix and grid games, and its robustness to selfish learners.\n",
      "\n",
      "**Paper Id :2001.04678 \n",
      "Title :Smooth markets: A basic mechanism for organizing gradient-based learners\n",
      "  With the success of modern machine learning, it is becoming increasingly\n",
      "important to understand and control how learning algorithms interact.\n",
      "Unfortunately, negative results from game theory show there is little hope of\n",
      "understanding or controlling general n-player games. We therefore introduce\n",
      "smooth markets (SM-games), a class of n-player games with pairwise zero sum\n",
      "interactions. SM-games codify a common design pattern in machine learning that\n",
      "includes (some) GANs, adversarial training, and other recent algorithms. We\n",
      "show that SM-games are amenable to analysis and optimization using first-order\n",
      "methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.10155 \n",
      "Title :Machine Learning Phase Transitions with a Quantum Processor\n",
      "  Machine learning has emerged as a promising approach to study the properties\n",
      "of many-body systems. Recently proposed as a tool to classify phases of matter,\n",
      "the approach relies on classical simulation methods$-$such as Monte\n",
      "Carlo$-$which are known to experience an exponential slowdown when simulating\n",
      "certain quantum systems. To overcome this slowdown while still leveraging\n",
      "machine learning, we propose a variational quantum algorithm which merges\n",
      "quantum simulation and quantum machine learning to classify phases of matter.\n",
      "Our classifier is directly fed labeled states recovered by the variational\n",
      "quantum eigensolver algorithm, thereby avoiding the data reading slowdown\n",
      "experienced in many applications of quantum enhanced machine learning. We\n",
      "propose families of variational ansatz states that are inspired directly by\n",
      "tensor networks. This allows us to use tools from tensor network theory to\n",
      "explain properties of the phase diagrams the presented method recovers.\n",
      "Finally, we propose a nearest-neighbour (checkerboard) quantum neural network.\n",
      "This majority vote quantum classifier is successfully trained to recognize\n",
      "phases of matter with $99\\%$ accuracy for the transverse field Ising model and\n",
      "$94\\%$ accuracy for the XXZ model. These findings suggest that our merger\n",
      "between quantum simulation and quantum enhanced machine learning offers a\n",
      "fertile ground to develop computational insights into quantum systems.\n",
      "\n",
      "**Paper Id :1902.09216 \n",
      "Title :Revealing quantum chaos with machine learning\n",
      "  Understanding properties of quantum matter is an outstanding challenge in\n",
      "science. In this paper, we demonstrate how machine-learning methods can be\n",
      "successfully applied for the classification of various regimes in\n",
      "single-particle and many-body systems. We realize neural network algorithms\n",
      "that perform a classification between regular and chaotic behavior in quantum\n",
      "billiard models with remarkably high accuracy. We use the variational\n",
      "autoencoder for autosupervised classification of regular/chaotic wave\n",
      "functions, as well as demonstrating that variational autoencoders could be used\n",
      "as a tool for detection of anomalous quantum states, such as quantum scars. By\n",
      "taking this method further, we show that machine learning techniques allow us\n",
      "to pin down the transition from integrability to many-body quantum chaos in\n",
      "Heisenberg XXZ spin chains. For both cases, we confirm the existence of\n",
      "universal W shapes that characterize the transition. Our results pave the way\n",
      "for exploring the power of machine learning tools for revealing exotic\n",
      "phenomena in quantum many-body systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.10652 \n",
      "Title :Monte Carlo Gradient Estimation in Machine Learning\n",
      "  This paper is a broad and accessible survey of the methods we have at our\n",
      "disposal for Monte Carlo gradient estimation in machine learning and across the\n",
      "statistical sciences: the problem of computing the gradient of an expectation\n",
      "of a function with respect to parameters defining the distribution that is\n",
      "integrated; the problem of sensitivity analysis. In machine learning research,\n",
      "this gradient problem lies at the core of many learning problems, in\n",
      "supervised, unsupervised and reinforcement learning. We will generally seek to\n",
      "rewrite such gradients in a form that allows for Monte Carlo estimation,\n",
      "allowing them to be easily and efficiently used and analysed. We explore three\n",
      "strategies--the pathwise, score function, and measure-valued gradient\n",
      "estimators--exploring their historical development, derivation, and underlying\n",
      "assumptions. We describe their use in other fields, show how they are related\n",
      "and can be combined, and expand on their possible generalisations. Wherever\n",
      "Monte Carlo gradient estimators have been derived and deployed in the past,\n",
      "important advances have followed. A deeper and more widely-held understanding\n",
      "of this problem will lead to further advances, and it is these advances that we\n",
      "wish to support.\n",
      "\n",
      "**Paper Id :2009.07608 \n",
      "Title :Deep Learning in Photoacoustic Tomography: Current approaches and future\n",
      "  directions\n",
      "  Biomedical photoacoustic tomography, which can provide high resolution 3D\n",
      "soft tissue images based on the optical absorption, has advanced to the stage\n",
      "at which translation from the laboratory to clinical settings is becoming\n",
      "possible. The need for rapid image formation and the practical restrictions on\n",
      "data acquisition that arise from the constraints of a clinical workflow are\n",
      "presenting new image reconstruction challenges. There are many classical\n",
      "approaches to image reconstruction, but ameliorating the effects of incomplete\n",
      "or imperfect data through the incorporation of accurate priors is challenging\n",
      "and leads to slow algorithms. Recently, the application of Deep Learning, or\n",
      "deep neural networks, to this problem has received a great deal of attention.\n",
      "This paper reviews the literature on learned image reconstruction, summarising\n",
      "the current trends, and explains how these new approaches fit within, and to\n",
      "some extent have arisen from, a framework that encompasses classical\n",
      "reconstruction methods. In particular, it shows how these new techniques can be\n",
      "understood from a Bayesian perspective, providing useful insights. The paper\n",
      "also provides a concise tutorial demonstration of three prototypical approaches\n",
      "to learned image reconstruction. The code and data sets for these\n",
      "demonstrations are available to researchers. It is anticipated that it is in in\n",
      "vivo applications - where data may be sparse, fast imaging critical and priors\n",
      "difficult to construct by hand - that Deep Learning will have the most impact.\n",
      "With this in mind, the paper concludes with some indications of possible future\n",
      "research directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.10773 \n",
      "Title :Are Adversarial Perturbations a Showstopper for ML-Based CAD? A Case\n",
      "  Study on CNN-Based Lithographic Hotspot Detection\n",
      "  There is substantial interest in the use of machine learning (ML) based\n",
      "techniques throughout the electronic computer-aided design (CAD) flow,\n",
      "particularly those based on deep learning. However, while deep learning methods\n",
      "have surpassed state-of-the-art performance in several applications, they have\n",
      "exhibited intrinsic susceptibility to adversarial perturbations --- small but\n",
      "deliberate alterations to the input of a neural network, precipitating\n",
      "incorrect predictions. In this paper, we seek to investigate whether\n",
      "adversarial perturbations pose risks to ML-based CAD tools, and if so, how\n",
      "these risks can be mitigated. To this end, we use a motivating case study of\n",
      "lithographic hotspot detection, for which convolutional neural networks (CNN)\n",
      "have shown great promise. In this context, we show the first adversarial\n",
      "perturbation attacks on state-of-the-art CNN-based hotspot detectors;\n",
      "specifically, we show that small (on average 0.5% modified area), functionality\n",
      "preserving and design-constraint satisfying changes to a layout can nonetheless\n",
      "trick a CNN-based hotspot detector into predicting the modified layout as\n",
      "hotspot free (with up to 99.7% success). We propose an adversarial retraining\n",
      "strategy to improve the robustness of CNN-based hotspot detection and show that\n",
      "this strategy significantly improves robustness (by a factor of ~3) against\n",
      "adversarial attacks without compromising classification accuracy.\n",
      "\n",
      "**Paper Id :2005.14124 \n",
      "Title :Active Fuzzing for Testing and Securing Cyber-Physical Systems\n",
      "  Cyber-physical systems (CPSs) in critical infrastructure face a pervasive\n",
      "threat from attackers, motivating research into a variety of countermeasures\n",
      "for securing them. Assessing the effectiveness of these countermeasures is\n",
      "challenging, however, as realistic benchmarks of attacks are difficult to\n",
      "manually construct, blindly testing is ineffective due to the enormous search\n",
      "spaces and resource requirements, and intelligent fuzzing approaches require\n",
      "impractical amounts of data and network access. In this work, we propose active\n",
      "fuzzing, an automatic approach for finding test suites of packet-level CPS\n",
      "network attacks, targeting scenarios in which attackers can observe sensors and\n",
      "manipulate packets, but have no existing knowledge about the payload encodings.\n",
      "Our approach learns regression models for predicting sensor values that will\n",
      "result from sampled network packets, and uses these predictions to guide a\n",
      "search for payload manipulations (i.e. bit flips) most likely to drive the CPS\n",
      "into an unsafe state. Key to our solution is the use of online active learning,\n",
      "which iteratively updates the models by sampling payloads that are estimated to\n",
      "maximally improve them. We evaluate the efficacy of active fuzzing by\n",
      "implementing it for a water purification plant testbed, finding it can\n",
      "automatically discover a test suite of flow, pressure, and over/underflow\n",
      "attacks, all with substantially less time, data, and network access than the\n",
      "most comparable approach. Finally, we demonstrate that our prediction models\n",
      "can also be utilised as countermeasures themselves, implementing them as\n",
      "anomaly detectors and early warning systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.11152 \n",
      "Title :Modulating Surrogates for Bayesian Optimization\n",
      "  Bayesian optimization (BO) methods often rely on the assumption that the\n",
      "objective function is well-behaved, but in practice, this is seldom true for\n",
      "real-world objectives even if noise-free observations can be collected. Common\n",
      "approaches, which try to model the objective as precisely as possible, often\n",
      "fail to make progress by spending too many evaluations modeling irrelevant\n",
      "details. We address this issue by proposing surrogate models that focus on the\n",
      "well-behaved structure in the objective function, which is informative for\n",
      "search, while ignoring detrimental structure that is challenging to model from\n",
      "few observations. First, we demonstrate that surrogate models with appropriate\n",
      "noise distributions can absorb challenging structures in the objective function\n",
      "by treating them as irreducible uncertainty. Secondly, we show that a latent\n",
      "Gaussian process is an excellent surrogate for this purpose, comparing with\n",
      "Gaussian processes with standard noise distributions. We perform numerous\n",
      "experiments on a range of BO benchmarks and find that our approach improves\n",
      "reliability and performance when faced with challenging objective functions.\n",
      "\n",
      "**Paper Id :1910.03231 \n",
      "Title :Peer Loss Functions: Learning from Noisy Labels without Knowing Noise\n",
      "  Rates\n",
      "  Learning with noisy labels is a common challenge in supervised learning.\n",
      "Existing approaches often require practitioners to specify noise rates, i.e., a\n",
      "set of parameters controlling the severity of label noises in the problem, and\n",
      "the specifications are either assumed to be given or estimated using additional\n",
      "steps. In this work, we introduce a new family of loss functions that we name\n",
      "as peer loss functions, which enables learning from noisy labels and does not\n",
      "require a priori specification of the noise rates. Peer loss functions work\n",
      "within the standard empirical risk minimization (ERM) framework. We show that,\n",
      "under mild conditions, performing ERM with peer loss functions on the noisy\n",
      "dataset leads to the optimal or a near-optimal classifier as if performing ERM\n",
      "over the clean training data, which we do not have access to. We pair our\n",
      "results with an extensive set of experiments. Peer loss provides a way to\n",
      "simplify model development when facing potentially noisy training labels, and\n",
      "can be promoted as a robust candidate loss function in such situations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.11259 \n",
      "Title :Reachability Deficits in Quantum Approximate Optimization\n",
      "  The quantum approximate optimization algorithm (QAOA) has rapidly become a\n",
      "cornerstone of contemporary quantum algorithm development. Despite a growing\n",
      "range of applications, only a few results have been developed towards\n",
      "understanding the algorithms ultimate limitations. Here we report that QAOA\n",
      "exhibits a strong dependence on a problem instances constraint to variable\n",
      "ratio$-$this problem density places a limiting restriction on the algorithms\n",
      "capacity to minimize a corresponding objective function (and hence solve\n",
      "optimization problem instances). Such $reachability~deficits$ persist even in\n",
      "the absence of barren plateaus [McClean et al., 2018] and are outside of the\n",
      "recently reported level-1 QAOA limitations [Hastings 2019]. These findings are\n",
      "among the first to determine strong limitations on variational quantum\n",
      "approximate optimization.\n",
      "\n",
      "**Paper Id :2005.00544 \n",
      "Title :Variational Quantum Eigensolver for Frustrated Quantum Systems\n",
      "  Hybrid quantum-classical algorithms have been proposed as a potentially\n",
      "viable application of quantum computers. A particular example - the variational\n",
      "quantum eigensolver, or VQE - is designed to determine a global minimum in an\n",
      "energy landscape specified by a quantum Hamiltonian, which makes it appealing\n",
      "for the needs of quantum chemistry. Experimental realizations have been\n",
      "reported in recent years and theoretical estimates of its efficiency are a\n",
      "subject of intense effort. Here we consider the performance of the VQE\n",
      "technique for a Hubbard-like model describing a one-dimensional chain of\n",
      "fermions with competing nearest- and next-nearest-neighbor interactions. We\n",
      "find that recovering the VQE solution allows one to obtain the correlation\n",
      "function of the ground state consistent with the exact result. We also study\n",
      "the barren plateau phenomenon for the Hamiltonian in question and find that the\n",
      "severity of this effect depends on the encoding of fermions to qubits. Our\n",
      "results are consistent with the current knowledge about the barren plateaus in\n",
      "quantum optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.11416 \n",
      "Title :Clustering by the way of atomic fission\n",
      "  Cluster analysis which focuses on the grouping and categorization of similar\n",
      "elements is widely used in various fields of research. Inspired by the\n",
      "phenomenon of atomic fission, a novel density-based clustering algorithm is\n",
      "proposed in this paper, called fission clustering (FC). It focuses on mining\n",
      "the dense families of a dataset and utilizes the information of the distance\n",
      "matrix to fissure clustering dataset into subsets. When we face the dataset\n",
      "which has a few points surround the dense families of clusters, K-nearest\n",
      "neighbors local density indicator is applied to distinguish and remove the\n",
      "points of sparse areas so as to obtain a dense subset that is constituted by\n",
      "the dense families of clusters. A number of frequently-used datasets were used\n",
      "to test the performance of this clustering approach, and to compare the results\n",
      "with those of algorithms. The proposed algorithm is found to outperform other\n",
      "algorithms in speed and accuracy.\n",
      "\n",
      "**Paper Id :2008.09994 \n",
      "Title :Discriminative Residual Analysis for Image Set Classification with\n",
      "  Posture and Age Variations\n",
      "  Image set recognition has been widely applied in many practical problems like\n",
      "real-time video retrieval and image caption tasks. Due to its superior\n",
      "performance, it has grown into a significant topic in recent years. However,\n",
      "images with complicated variations, e.g., postures and human ages, are\n",
      "difficult to address, as these variations are continuous and gradual with\n",
      "respect to image appearance. Consequently, the crucial point of image set\n",
      "recognition is to mine the intrinsic connection or structural information from\n",
      "the image batches with variations. In this work, a Discriminant Residual\n",
      "Analysis (DRA) method is proposed to improve the classification performance by\n",
      "discovering discriminant features in related and unrelated groups.\n",
      "Specifically, DRA attempts to obtain a powerful projection which casts the\n",
      "residual representations into a discriminant subspace. Such a projection\n",
      "subspace is expected to magnify the useful information of the input space as\n",
      "much as possible, then the relation between the training set and the test set\n",
      "described by the given metric or distance will be more precise in the\n",
      "discriminant subspace. We also propose a nonfeasance strategy by defining\n",
      "another approach to construct the unrelated groups, which help to reduce\n",
      "furthermore the cost of sampling errors. Two regularization approaches are used\n",
      "to deal with the probable small sample size problem. Extensive experiments are\n",
      "conducted on benchmark databases, and the results show superiority and\n",
      "efficiency of the new methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.11641 \n",
      "Title :A global approach for learning sparse Ising models\n",
      "  We consider the problem of learning the link parameters as well as the\n",
      "structure of a binary-valued pairwise Markov model. Under sparsity assumption,\n",
      "we propose a method based on $l_1$- regularized logistic regression, which\n",
      "estimate globally the whole set of edges and link parameters. Unlike the more\n",
      "recent methods discussed in literature that learn the edges and the\n",
      "corresponding link parameters one node at a time, in this work we propose a\n",
      "method that learns all the edges and corresponding link parameters\n",
      "simultaneously for all nodes. The idea behind this proposal is to exploit the\n",
      "reciprocal information of the nodes between each other during the estimation\n",
      "process. Numerical experiments highlight the advantage of this technique and\n",
      "confirm the intuition behind it.\n",
      "\n",
      "**Paper Id :1904.05333 \n",
      "Title :Bayesian estimation of the latent dimension and communities in\n",
      "  stochastic blockmodels\n",
      "  Spectral embedding of adjacency or Laplacian matrices of undirected graphs is\n",
      "a common technique for representing a network in a lower dimensional latent\n",
      "space, with optimal theoretical guarantees. The embedding can be used to\n",
      "estimate the community structure of the network, with strong consistency\n",
      "results in the stochastic blockmodel framework. One of the main practical\n",
      "limitations of standard algorithms for community detection from spectral\n",
      "embeddings is that the number of communities and the latent dimension of the\n",
      "embedding must be specified in advance. In this article, a novel Bayesian model\n",
      "for simultaneous and automatic selection of the appropriate dimension of the\n",
      "latent space and the number of blocks is proposed. Extensions to directed and\n",
      "bipartite graphs are discussed. The model is tested on simulated and real world\n",
      "network data, showing promising performance for recovering latent community\n",
      "structure.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.11889 \n",
      "Title :Deep Eyedentification: Biometric Identification using Micro-Movements of\n",
      "  the Eye\n",
      "  We study involuntary micro-movements of the eye for biometric identification.\n",
      "While prior studies extract lower-frequency macro-movements from the output of\n",
      "video-based eye-tracking systems and engineer explicit features of these\n",
      "macro-movements, we develop a deep convolutional architecture that processes\n",
      "the raw eye-tracking signal. Compared to prior work, the network attains a\n",
      "lower error rate by one order of magnitude and is faster by two orders of\n",
      "magnitude: it identifies users accurately within seconds.\n",
      "\n",
      "**Paper Id :2006.01644 \n",
      "Title :Learning Efficient Representations of Mouse Movements to Predict User\n",
      "  Attention\n",
      "  Tracking mouse cursor movements can be used to predict user attention on\n",
      "heterogeneous page layouts like SERPs. So far, previous work has relied heavily\n",
      "on handcrafted features, which is a time-consuming approach that often requires\n",
      "domain expertise. We investigate different representations of mouse cursor\n",
      "movements, including time series, heatmaps, and trajectory-based images, to\n",
      "build and contrast both recurrent and convolutional neural networks that can\n",
      "predict user attention to direct displays, such as SERP advertisements. Our\n",
      "models are trained over raw mouse cursor data and achieve competitive\n",
      "performance. We conclude that neural network models should be adopted for\n",
      "downstream tasks involving mouse cursor movements, since they can provide an\n",
      "invaluable implicit feedback signal for re-ranking and evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1906.12282 \n",
      "Title :Synaptic Delays for Temporal Feature Detection in Dynamic Neuromorphic\n",
      "  Processors\n",
      "  Spiking neural networks implemented in dynamic neuromorphic processors are\n",
      "well suited for spatiotemporal feature detection and learning, for example in\n",
      "ultra low-power embedded intelligence and deep edge applications. Such pattern\n",
      "recognition networks naturally involve a combination of dynamic delay\n",
      "mechanisms and coincidence detection. Inspired by an auditory feature detection\n",
      "circuit in crickets, featuring a delayed excitation by postinhibitory rebound,\n",
      "we investigate disynaptic delay elements formed by inhibitory-excitatory pairs\n",
      "of dynamic synapses. We configure such disynaptic delay elements in the\n",
      "DYNAP-SE neuromorphic processor and characterize the distribution of delayed\n",
      "excitations resulting from device mismatch. Furthermore, we present a network\n",
      "that mimics the auditory feature detection circuit of crickets and demonstrate\n",
      "how varying synapse weights, input noise and processor temperature affects the\n",
      "circuit. Interestingly, we find that the disynaptic delay elements can be\n",
      "configured such that the timing and magnitude of the delayed postsynaptic\n",
      "excitation depend mainly on the efficacy of the inhibitory and excitatory\n",
      "synapses, respectively. Delay elements of this kind can be implemented in other\n",
      "reconfigurable dynamic neuromorphic processors and opens up for synapse level\n",
      "temporal feature tuning with large fan-in and flexible delays of order 10-100\n",
      "ms.\n",
      "\n",
      "**Paper Id :2003.12319 \n",
      "Title :Boolean learning under noise-perturbations in hardware neural networks\n",
      "  A high efficiency hardware integration of neural networks benefits from\n",
      "realizing nonlinearity, network connectivity and learning fully in a physical\n",
      "substrate. Multiple systems have recently implemented some or all of these\n",
      "operations, yet the focus was placed on addressing technological challenges.\n",
      "Fundamental questions regarding learning in hardware neural networks remain\n",
      "largely unexplored. Noise in particular is unavoidable in such architectures,\n",
      "and here we investigate its interaction with a learning algorithm using an\n",
      "opto-electronic recurrent neural network. We find that noise strongly modifies\n",
      "the system's path during convergence, and surprisingly fully decorrelates the\n",
      "final readout weight matrices. This highlights the importance of understanding\n",
      "architecture, noise and learning algorithm as interacting players, and\n",
      "therefore identifies the need for mathematical tools for noisy, analogue system\n",
      "optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.00865 \n",
      "Title :Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale\n",
      "  Bayesian Deep Learning\n",
      "  We propose Radial Bayesian Neural Networks (BNNs): a variational approximate\n",
      "posterior for BNNs which scales well to large models while maintaining a\n",
      "distribution over weight-space with full support. Other scalable Bayesian deep\n",
      "learning methods, like MC dropout or deep ensembles, have discrete support-they\n",
      "assign zero probability to almost all of the weight-space. Unlike these\n",
      "discrete support methods, Radial BNNs' full support makes them suitable for use\n",
      "as a prior for sequential inference. In addition, they solve the conceptual\n",
      "challenges with the a priori implausibility of weight distributions with\n",
      "discrete support. The Radial BNN is motivated by avoiding a sampling problem in\n",
      "'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble'\n",
      "pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are\n",
      "robust to hyperparameters and can be efficiently applied to a challenging\n",
      "real-world medical application without needing ad-hoc tweaks and intensive\n",
      "tuning. In fact, in this setting Radial BNNs out-perform discrete-support\n",
      "methods like MC dropout. Lastly, by using Radial BNNs as a theoretically\n",
      "principled, robust alternative to MFVI we make significant strides in a\n",
      "Bayesian continual learning evaluation.\n",
      "\n",
      "**Paper Id :2008.03209 \n",
      "Title :Investigating maximum likelihood based training of infinite mixtures for\n",
      "  uncertainty quantification\n",
      "  Uncertainty quantification in neural networks gained a lot of attention in\n",
      "the past years. The most popular approaches, Bayesian neural networks (BNNs),\n",
      "Monte Carlo dropout, and deep ensembles have one thing in common: they are all\n",
      "based on some kind of mixture model. While the BNNs build infinite mixture\n",
      "models and are derived via variational inference, the latter two build finite\n",
      "mixtures trained with the maximum likelihood method. In this work we\n",
      "investigate the effect of training an infinite mixture distribution with the\n",
      "maximum likelihood method instead of variational inference. We find that the\n",
      "proposed objective leads to stochastic networks with an increased predictive\n",
      "variance, which improves uncertainty based identification of\n",
      "miss-classification and robustness against adversarial attacks in comparison to\n",
      "a standard BNN with equivalent network structure. The new model also displays\n",
      "higher entropy on out-of-distribution data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.01898 \n",
      "Title :Cryo-EM reconstruction of continuous heterogeneity by Laplacian spectral\n",
      "  volumes\n",
      "  Single-particle electron cryomicroscopy is an essential tool for\n",
      "high-resolution 3D reconstruction of proteins and other biological\n",
      "macromolecules. An important challenge in cryo-EM is the reconstruction of\n",
      "non-rigid molecules with parts that move and deform. Traditional reconstruction\n",
      "methods fail in these cases, resulting in smeared reconstructions of the moving\n",
      "parts. This poses a major obstacle for structural biologists, who need\n",
      "high-resolution reconstructions of entire macromolecules, moving parts\n",
      "included. To address this challenge, we present a new method for the\n",
      "reconstruction of macromolecules exhibiting continuous heterogeneity. The\n",
      "proposed method uses projection images from multiple viewing directions to\n",
      "construct a graph Laplacian through which the manifold of three-dimensional\n",
      "conformations is analyzed. The 3D molecular structures are then expanded in a\n",
      "basis of Laplacian eigenvectors, using a novel generalized tomographic\n",
      "reconstruction algorithm to compute the expansion coefficients. These\n",
      "coefficients, which we name spectral volumes, provide a high-resolution\n",
      "visualization of the molecular dynamics. We provide a theoretical analysis and\n",
      "evaluate the method empirically on several simulated data sets.\n",
      "\n",
      "**Paper Id :1909.05215 \n",
      "Title :Reconstructing continuous distributions of 3D protein structure from\n",
      "  cryo-EM images\n",
      "  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining\n",
      "the structure of proteins and other macromolecular complexes at near-atomic\n",
      "resolution. In single particle cryo-EM, the central problem is to reconstruct\n",
      "the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and\n",
      "randomly oriented two-dimensional projections. However, the imaged protein\n",
      "complexes may exhibit structural variability, which complicates reconstruction\n",
      "and is typically addressed using discrete clustering approaches that fail to\n",
      "capture the full range of protein dynamics. Here, we introduce a novel method\n",
      "for cryo-EM reconstruction that extends naturally to modeling continuous\n",
      "generative factors of structural heterogeneity. This method encodes structures\n",
      "in Fourier space using coordinate-based deep neural networks, and trains these\n",
      "networks from unlabeled 2D cryo-EM images by combining exact inference over\n",
      "image orientation with variational inference for structural heterogeneity. We\n",
      "demonstrate that the proposed method, termed cryoDRGN, can perform ab initio\n",
      "reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image\n",
      "data. To our knowledge, cryoDRGN is the first neural network-based approach for\n",
      "cryo-EM reconstruction and the first end-to-end method for directly\n",
      "reconstructing continuous ensembles of protein structures from cryo-EM images.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.02177 \n",
      "Title :Adaptive Approximation and Generalization of Deep Neural Network with\n",
      "  Intrinsic Dimensionality\n",
      "  In this study, we prove that an intrinsic low dimensionality of covariates is\n",
      "the main factor that determines the performance of deep neural networks (DNNs).\n",
      "DNNs generally provide outstanding empirical performance. Hence, numerous\n",
      "studies have actively investigated the theoretical properties of DNNs to\n",
      "understand their underlying mechanisms. In particular, the behavior of DNNs in\n",
      "terms of high-dimensional data is one of the most critical questions. However,\n",
      "this issue has not been sufficiently investigated from the aspect of\n",
      "covariates, although high-dimensional data have practically low intrinsic\n",
      "dimensionality. In this study, we derive bounds for an approximation error and\n",
      "a generalization error regarding DNNs with intrinsically low dimensional\n",
      "covariates. We apply the notion of the Minkowski dimension and develop a novel\n",
      "proof technique. Consequently, we show that convergence rates of the errors by\n",
      "DNNs do not depend on the nominal high dimensionality of data, but on its lower\n",
      "intrinsic dimension. We further prove that the rate is optimal in the minimax\n",
      "sense. We identify an advantage of DNNs by showing that DNNs can handle a\n",
      "broader class of intrinsic low dimensional data than other adaptive estimators.\n",
      "Finally, we conduct a numerical simulation to validate the theoretical results.\n",
      "\n",
      "**Paper Id :2006.14117 \n",
      "Title :Fast Learning of Graph Neural Networks with Guaranteed Generalizability:\n",
      "  One-hidden-layer Case\n",
      "  Although graph neural networks (GNNs) have made great progress recently on\n",
      "learning from graph-structured data in practice, their theoretical guarantee on\n",
      "generalizability remains elusive in the literature. In this paper, we provide a\n",
      "theoretically-grounded generalizability analysis of GNNs with one hidden layer\n",
      "for both regression and binary classification problems. Under the assumption\n",
      "that there exists a ground-truth GNN model (with zero generalization error),\n",
      "the objective of GNN learning is to estimate the ground-truth GNN parameters\n",
      "from the training data. To achieve this objective, we propose a learning\n",
      "algorithm that is built on tensor initialization and accelerated gradient\n",
      "descent. We then show that the proposed learning algorithm converges to the\n",
      "ground-truth GNN model for the regression problem, and to a model sufficiently\n",
      "close to the ground-truth for the binary classification problem. Moreover, for\n",
      "both cases, the convergence rate of the proposed learning algorithm is proven\n",
      "to be linear and faster than the vanilla gradient descent algorithm. We further\n",
      "explore the relationship between the sample complexity of GNNs and their\n",
      "underlying graph properties. Lastly, we provide numerical experiments to\n",
      "demonstrate the validity of our analysis and the effectiveness of the proposed\n",
      "learning algorithm for GNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.02586 \n",
      "Title :Structure fusion based on graph convolutional networks for\n",
      "  semi-supervised classification\n",
      "  Suffering from the multi-view data diversity and complexity for\n",
      "semi-supervised classification, most of existing graph convolutional networks\n",
      "focus on the networks architecture construction or the salient graph structure\n",
      "preservation, and ignore the the complete graph structure for semi-supervised\n",
      "classification contribution. To mine the more complete distribution structure\n",
      "from multi-view data with the consideration of the specificity and the\n",
      "commonality, we propose structure fusion based on graph convolutional networks\n",
      "(SF-GCN) for improving the performance of semi-supervised classification.\n",
      "SF-GCN can not only retain the special characteristic of each view data by\n",
      "spectral embedding, but also capture the common style of multi-view data by\n",
      "distance metric between multi-graph structures. Suppose the linear relationship\n",
      "between multi-graph structures, we can construct the optimization function of\n",
      "structure fusion model by balancing the specificity loss and the commonality\n",
      "loss. By solving this function, we can simultaneously obtain the fusion\n",
      "spectral embedding from the multi-view data and the fusion structure as\n",
      "adjacent matrix to input graph convolutional networks for semi-supervised\n",
      "classification. Experiments demonstrate that the performance of SF-GCN\n",
      "outperforms that of the state of the arts on three challenging datasets, which\n",
      "are Cora,Citeseer and Pubmed in citation networks.\n",
      "\n",
      "**Paper Id :2009.13299 \n",
      "Title :Learning to Match Jobs with Resumes from Sparse Interaction Data using\n",
      "  Multi-View Co-Teaching Network\n",
      "  With the ever-increasing growth of online recruitment data, job-resume\n",
      "matching has become an important task to automatically match jobs with suitable\n",
      "resumes. This task is typically casted as a supervised text matching problem.\n",
      "Supervised learning is powerful when the labeled data is sufficient. However,\n",
      "on online recruitment platforms, job-resume interaction data is sparse and\n",
      "noisy, which affects the performance of job-resume match algorithms. To\n",
      "alleviate these problems, in this paper, we propose a novel multi-view\n",
      "co-teaching network from sparse interaction data for job-resume matching. Our\n",
      "network consists of two major components, namely text-based matching model and\n",
      "relation-based matching model. The two parts capture semantic compatibility in\n",
      "two different views, and complement each other. In order to address the\n",
      "challenges from sparse and noisy data, we design two specific strategies to\n",
      "combine the two components. First, two components share the learned parameters\n",
      "or representations, so that the original representations of each component can\n",
      "be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the\n",
      "influence of noise in training data. The core idea is to let the two components\n",
      "help each other by selecting more reliable training instances. The two\n",
      "strategies focus on representation enhancement and data enhancement,\n",
      "respectively. Compared with pure text-based matching models, the proposed\n",
      "approach is able to learn better data representations from limited or even\n",
      "sparse interaction data, which is more resistible to noise in training data.\n",
      "Experiment results have demonstrated that our model is able to outperform\n",
      "state-of-the-art methods for job-resume matching.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.02929 \n",
      "Title :Improved local search for graph edit distance\n",
      "  The graph edit distance (GED) measures the dissimilarity between two graphs\n",
      "as the minimal cost of a sequence of elementary operations transforming one\n",
      "graph into another. This measure is fundamental in many areas such as\n",
      "structural pattern recognition or classification. However, exactly computing\n",
      "GED is NP-hard. Among different classes of heuristic algorithms that were\n",
      "proposed to compute approximate solutions, local search based algorithms\n",
      "provide the tightest upper bounds for GED. In this paper, we present K-REFINE\n",
      "and RANDPOST. K-REFINE generalizes and improves an existing local search\n",
      "algorithm and performs particularly well on small graphs. RANDPOST is a general\n",
      "warm start framework that stochastically generates promising initial solutions\n",
      "to be used by any local search based GED algorithm. It is particularly\n",
      "efficient on large graphs. An extensive empirical evaluation demonstrates that\n",
      "both K-REFINE and RANDPOST perform excellently in practice.\n",
      "\n",
      "**Paper Id :2002.01793 \n",
      "Title :Proximity Preserving Binary Code using Signed Graph-Cut\n",
      "  We introduce a binary embedding framework, called Proximity Preserving Code\n",
      "(PPC), which learns similarity and dissimilarity between data points to create\n",
      "a compact and affinity-preserving binary code. This code can be used to apply\n",
      "fast and memory-efficient approximation to nearest-neighbor searches. Our\n",
      "framework is flexible, enabling different proximity definitions between data\n",
      "points. In contrast to previous methods that extract binary codes based on\n",
      "unsigned graph partitioning, our system models the attractive and repulsive\n",
      "forces in the data by incorporating positive and negative graph weights. The\n",
      "proposed framework is shown to boil down to finding the minimal cut of a signed\n",
      "graph, a problem known to be NP-hard. We offer an efficient approximation and\n",
      "achieve superior results by constructing the code bit after bit. We show that\n",
      "the proposed approximation is superior to the commonly used spectral methods\n",
      "with respect to both accuracy and complexity. Thus, it is useful for many other\n",
      "problems that can be translated into signed graph cut.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.02957 \n",
      "Title :Detecting and Diagnosing Adversarial Images with Class-Conditional\n",
      "  Capsule Reconstructions\n",
      "  Adversarial examples raise questions about whether neural network models are\n",
      "sensitive to the same visual features as humans. In this paper, we first detect\n",
      "adversarial examples or otherwise corrupted images based on a class-conditional\n",
      "reconstruction of the input. To specifically attack our detection mechanism, we\n",
      "propose the Reconstructive Attack which seeks both to cause a misclassification\n",
      "and a low reconstruction error. This reconstructive attack produces undetected\n",
      "adversarial examples but with much smaller success rate. Among all these\n",
      "attacks, we find that CapsNets always perform better than convolutional\n",
      "networks. Then, we diagnose the adversarial examples for CapsNets and find that\n",
      "the success of the reconstructive attack is highly related to the visual\n",
      "similarity between the source and target class. Additionally, the resulting\n",
      "perturbations can cause the input image to appear visually more like the target\n",
      "class and hence become non-adversarial. This suggests that CapsNets use\n",
      "features that are more aligned with human perception and have the potential to\n",
      "address the central issue raised by adversarial examples.\n",
      "\n",
      "**Paper Id :1904.09433 \n",
      "Title :Can Machine Learning Model with Static Features be Fooled: an\n",
      "  Adversarial Machine Learning Approach\n",
      "  The widespread adoption of smartphones dramatically increases the risk of\n",
      "attacks and the spread of mobile malware, especially on the Android platform.\n",
      "Machine learning-based solutions have been already used as a tool to supersede\n",
      "signature-based anti-malware systems. However, malware authors leverage\n",
      "features from malicious and legitimate samples to estimate statistical\n",
      "difference in-order to create adversarial examples. Hence, to evaluate the\n",
      "vulnerability of machine learning algorithms in malware detection, we propose\n",
      "five different attack scenarios to perturb malicious applications (apps). By\n",
      "doing this, the classification algorithm inappropriately fits the discriminant\n",
      "function on the set of data points, eventually yielding a higher\n",
      "misclassification rate. Further, to distinguish the adversarial examples from\n",
      "benign samples, we propose two defense mechanisms to counter attacks. To\n",
      "validate our attacks and solutions, we test our model on three different\n",
      "benchmark datasets. We also test our methods using various classifier\n",
      "algorithms and compare them with the state-of-the-art data poisoning method\n",
      "using the Jacobian matrix. Promising results show that generated adversarial\n",
      "samples can evade detection with a very high probability. Additionally, evasive\n",
      "variants generated by our attack models when used to harden the developed\n",
      "anti-malware system improves the detection rate up to 50% when using the\n",
      "Generative Adversarial Network (GAN) method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.03063 \n",
      "Title :MRI Super-Resolution with Ensemble Learning and Complementary Priors\n",
      "  Magnetic resonance imaging (MRI) is a widely used medical imaging modality.\n",
      "However, due to the limitations in hardware, scan time, and throughput, it is\n",
      "often clinically challenging to obtain high-quality MR images. The\n",
      "super-resolution approach is potentially promising to improve MR image quality\n",
      "without any hardware upgrade. In this paper, we propose an ensemble learning\n",
      "and deep learning framework for MR image super-resolution. In our study, we\n",
      "first enlarged low resolution images using 5 commonly used super-resolution\n",
      "algorithms and obtained differentially enlarged image datasets with\n",
      "complementary priors. Then, a generative adversarial network (GAN) is trained\n",
      "with each dataset to generate super-resolution MR images. Finally, a\n",
      "convolutional neural network is used for ensemble learning that synergizes the\n",
      "outputs of GANs into the final MR super-resolution images. According to our\n",
      "results, the ensemble learning results outcome any one of GAN outputs. Compared\n",
      "with some state-of-the-art deep learning-based super-resolution methods, our\n",
      "approach is advantageous in suppressing artifacts and keeping more image\n",
      "details.\n",
      "\n",
      "**Paper Id :1908.01612 \n",
      "Title :Multi-Contrast Super-Resolution MRI Through a Progressive Network\n",
      "  Magnetic resonance imaging (MRI) is widely used for screening, diagnosis,\n",
      "image-guided therapy, and scientific research. A significant advantage of MRI\n",
      "over other imaging modalities such as computed tomography (CT) and nuclear\n",
      "imaging is that it clearly shows soft tissues in multi-contrasts. Compared with\n",
      "other medical image super-resolution (SR) methods that are in a single\n",
      "contrast, multi-contrast super-resolution studies can synergize multiple\n",
      "contrast images to achieve better super-resolution results. In this paper, we\n",
      "propose a one-level non-progressive neural network for low up-sampling\n",
      "multi-contrast super-resolution and a two-level progressive network for high\n",
      "up-sampling multi-contrast super-resolution. Multi-contrast information is\n",
      "combined in high-level feature space. Our experimental results demonstrate that\n",
      "the proposed networks can produce MRI super-resolution images with good image\n",
      "quality and outperform other multi-contrast super-resolution methods in terms\n",
      "of structural similarity and peak signal-to-noise ratio. Also, the progressive\n",
      "network produces a better SR image quality than the non-progressive network,\n",
      "even if the original low-resolution images were highly down-sampled.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.03137 \n",
      "Title :Precision annealing Monte Carlo methods for statistical data\n",
      "  assimilation and machine learning\n",
      "  In statistical data assimilation (SDA) and supervised machine learning (ML),\n",
      "we wish to transfer information from observations to a model of the processes\n",
      "underlying those observations. For SDA, the model consists of a set of\n",
      "differential equations that describe the dynamics of a physical system. For ML,\n",
      "the model is usually constructed using other strategies. In this paper, we\n",
      "develop a systematic formulation based on Monte Carlo sampling to achieve such\n",
      "information transfer. Following the derivation of an appropriate target\n",
      "distribution, we present the formulation based on the standard\n",
      "Metropolis-Hasting (MH) procedure and the Hamiltonian Monte Carlo (HMC) method\n",
      "for performing the high dimensional integrals that appear. To the extensive\n",
      "literature on MH and HMC, we add (1) an annealing method using a hyperparameter\n",
      "that governs the precision of the model to identify and explore the highest\n",
      "probability regions of phase space dominating those integrals, and (2) a\n",
      "strategy for initializing the state space search. The efficacy of the proposed\n",
      "formulation is demonstrated using a nonlinear dynamical model with chaotic\n",
      "solutions widely used in geophysics.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.03211 \n",
      "Title :Convolutional dictionary learning based auto-encoders for natural\n",
      "  exponential-family distributions\n",
      "  We introduce a class of auto-encoder neural networks tailored to data from\n",
      "the natural exponential family (e.g., count data). The architectures are\n",
      "inspired by the problem of learning the filters in a convolutional generative\n",
      "model with sparsity constraints, often referred to as convolutional dictionary\n",
      "learning (CDL). Our work is the first to combine ideas from convolutional\n",
      "generative models and deep learning for data that are naturally modeled with a\n",
      "non-Gaussian distribution (e.g., binomial and Poisson). This perspective\n",
      "provides us with a scalable and flexible framework that can be re-purposed for\n",
      "a wide range of tasks and assumptions on the generative model. Specifically,\n",
      "the iterative optimization procedure for solving CDL, an unsupervised task, is\n",
      "mapped to an unfolded and constrained neural network, with iterative\n",
      "adjustments to the inputs to account for the generative distribution. We also\n",
      "show that the framework can easily be extended for discriminative training,\n",
      "appropriate for a supervised task. We demonstrate 1) that fitting the\n",
      "generative model to learn, in an unsupervised fashion, the latent stimulus that\n",
      "underlies neural spiking data leads to better goodness-of-fit compared to other\n",
      "baselines, 2) competitive performance compared to state-of-the-art algorithms\n",
      "for supervised Poisson image denoising, with significantly fewer parameters,\n",
      "and 3) gradient dynamics of shallow binomial auto-encoder.\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.04240 \n",
      "Title :Bayesian deep learning with hierarchical prior: Predictions from limited\n",
      "  and noisy data\n",
      "  Datasets in engineering applications are often limited and contaminated,\n",
      "mainly due to unavoidable measurement noise and signal distortion. Thus, using\n",
      "conventional data-driven approaches to build a reliable discriminative model,\n",
      "and further applying this identified surrogate to uncertainty analysis remains\n",
      "to be very challenging. A deep learning approach is presented to provide\n",
      "predictions based on limited and noisy data. To address noise perturbation, the\n",
      "Bayesian learning method that naturally facilitates an automatic updating\n",
      "mechanism is considered to quantify and propagate model uncertainties into\n",
      "predictive quantities. Specifically, hierarchical Bayesian modeling (HBM) is\n",
      "first adopted to describe model uncertainties, which allows the prior\n",
      "assumption to be less subjective, while also makes the proposed surrogate more\n",
      "robust. Next, the Bayesian inference is seamlessly integrated into the DL\n",
      "framework, which in turn supports probabilistic programming by yielding a\n",
      "probability distribution of the quantities of interest rather than their point\n",
      "estimates. Variational inference (VI) is implemented for the posterior\n",
      "distribution analysis where the intractable marginalization of the likelihood\n",
      "function over parameter space is framed in an optimization format, and\n",
      "stochastic gradient descent method is applied to solve this optimization\n",
      "problem. Finally, Monte Carlo simulation is used to obtain an unbiased\n",
      "estimator in the predictive phase of Bayesian inference, where the proposed\n",
      "Bayesian deep learning (BDL) scheme is able to offer confidence bounds for the\n",
      "output estimation by analyzing propagated uncertainties. The effectiveness of\n",
      "Bayesian shrinkage is demonstrated in improving predictive performance using\n",
      "contaminated data, and various examples are provided to illustrate concepts,\n",
      "methodologies, and algorithms of this proposed BDL modeling technique.\n",
      "\n",
      "**Paper Id :2002.07217 \n",
      "Title :Decision-Making with Auto-Encoding Variational Bayes\n",
      "  To make decisions based on a model fit with auto-encoding variational Bayes\n",
      "(AEVB), practitioners often let the variational distribution serve as a\n",
      "surrogate for the posterior distribution. This approach yields biased estimates\n",
      "of the expected risk, and therefore leads to poor decisions for two reasons.\n",
      "First, the model fit with AEVB may not equal the underlying data distribution.\n",
      "Second, the variational distribution may not equal the posterior distribution\n",
      "under the fitted model. We explore how fitting the variational distribution\n",
      "based on several objective functions other than the ELBO, while continuing to\n",
      "fit the generative model based on the ELBO, affects the quality of downstream\n",
      "decisions. For the probabilistic principal component analysis model, we\n",
      "investigate how importance sampling error, as well as the bias of the model\n",
      "parameter estimates, varies across several approximate posteriors when used as\n",
      "proposal distributions. Our theoretical results suggest that a posterior\n",
      "approximation distinct from the variational distribution should be used for\n",
      "making decisions. Motivated by these theoretical results, we propose learning\n",
      "several approximate proposals for the best model and combining them using\n",
      "multiple importance sampling for decision-making. In addition to toy examples,\n",
      "we present a full-fledged case study of single-cell RNA sequencing. In this\n",
      "challenging instance of multiple hypothesis testing, our proposed approach\n",
      "surpasses the current state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.04553 \n",
      "Title :Neural Reasoning, Fast and Slow, for Video Question Answering\n",
      "  What does it take to design a machine that learns to answer natural questions\n",
      "about a video? A Video QA system must simultaneously understand language,\n",
      "represent visual content over space-time, and iteratively transform these\n",
      "representations in response to lingual content in the query, and finally\n",
      "arriving at a sensible answer. While recent advances in lingual and visual\n",
      "question answering have enabled sophisticated representations and neural\n",
      "reasoning mechanisms, major challenges in Video QA remain on dynamic grounding\n",
      "of concepts, relations and actions to support the reasoning process. Inspired\n",
      "by the dual-process account of human reasoning, we design a dual process neural\n",
      "architecture, which is composed of a question-guided video processing module\n",
      "(System 1, fast and reactive) followed by a generic reasoning module (System 2,\n",
      "slow and deliberative). System 1 is a hierarchical model that encodes visual\n",
      "patterns about objects, actions and relations in space-time given the textual\n",
      "cues from the question. The encoded representation is a set of high-level\n",
      "visual features, which are then passed to System 2. Here multi-step inference\n",
      "follows to iteratively chain visual elements as instructed by the textual\n",
      "elements. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets\n",
      "(real), demonstrating competitive results, with a large margin in the case of\n",
      "multi-step reasoning.\n",
      "\n",
      "**Paper Id :1905.13209 \n",
      "Title :AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n",
      "  Architectures\n",
      "  Learning to represent videos is a very challenging task both algorithmically\n",
      "and computationally. Standard video CNN architectures have been designed by\n",
      "directly extending architectures devised for image understanding to include the\n",
      "time dimension, using modules such as 3D convolutions, or by using two-stream\n",
      "design to capture both appearance and motion in videos. We interpret a video\n",
      "CNN as a collection of multi-stream convolutional blocks connected to each\n",
      "other, and propose the approach of automatically finding neural architectures\n",
      "with better connectivity and spatio-temporal interactions for video\n",
      "understanding. This is done by evolving a population of overly-connected\n",
      "architectures guided by connection weight learning. Architectures combining\n",
      "representations that abstract different input types (i.e., RGB and optical\n",
      "flow) at multiple temporal resolutions are searched for, allowing different\n",
      "types or sources of information to interact with each other. Our method,\n",
      "referred to as AssembleNet, outperforms prior approaches on public video\n",
      "datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n",
      "34.27% accuracy on Moments-in-Time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.04710 \n",
      "Title :Trust-Region Variational Inference with Gaussian Mixture Models\n",
      "  Many methods for machine learning rely on approximate inference from\n",
      "intractable probability distributions. Variational inference approximates such\n",
      "distributions by tractable models that can be subsequently used for approximate\n",
      "inference. Learning sufficiently accurate approximations requires a rich model\n",
      "family and careful exploration of the relevant modes of the target\n",
      "distribution. We propose a method for learning accurate GMM approximations of\n",
      "intractable probability distributions based on insights from policy search by\n",
      "using information-geometric trust regions for principled exploration. For\n",
      "efficient improvement of the GMM approximation, we derive a lower bound on the\n",
      "corresponding optimization objective enabling us to update the components\n",
      "independently. Our use of the lower bound ensures convergence to a stationary\n",
      "point of the original objective. The number of components is adapted online by\n",
      "adding new components in promising regions and by deleting components with\n",
      "negligible weight. We demonstrate on several domains that we can learn\n",
      "approximations of complex, multimodal distributions with a quality that is\n",
      "unmet by previous variational inference methods, and that the GMM approximation\n",
      "can be used for drawing samples that are on par with samples created by\n",
      "state-of-the-art MCMC samplers while requiring up to three orders of magnitude\n",
      "less computational resources.\n",
      "\n",
      "**Paper Id :2009.00666 \n",
      "Title :Robust, Accurate Stochastic Optimization for Variational Inference\n",
      "  We consider the problem of fitting variational posterior approximations using\n",
      "stochastic optimization methods. The performance of these approximations\n",
      "depends on (1) how well the variational family matches the true posterior\n",
      "distribution,(2) the choice of divergence, and (3) the optimization of the\n",
      "variational objective. We show that even in the best-case scenario when the\n",
      "exact posterior belongs to the assumed variational family, common stochastic\n",
      "optimization methods lead to poor variational approximations if the problem\n",
      "dimension is moderately large. We also demonstrate that these methods are not\n",
      "robust across diverse model types. Motivated by these findings, we develop a\n",
      "more robust and accurate stochastic optimization framework by viewing the\n",
      "underlying optimization algorithm as producing a Markov chain. Our approach is\n",
      "theoretically motivated and includes a diagnostic for convergence and a novel\n",
      "stopping rule, both of which are robust to noisy evaluations of the objective\n",
      "function. We show empirically that the proposed framework works well on a\n",
      "diverse set of models: it can automatically detect stochastic optimization\n",
      "failure or inaccurate variational approximation\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.06011 \n",
      "Title :Extracting Interpretable Physical Parameters from Spatiotemporal Systems\n",
      "  using Unsupervised Learning\n",
      "  Experimental data is often affected by uncontrolled variables that make\n",
      "analysis and interpretation difficult. For spatiotemporal systems, this problem\n",
      "is further exacerbated by their intricate dynamics. Modern machine learning\n",
      "methods are particularly well-suited for analyzing and modeling complex\n",
      "datasets, but to be effective in science, the result needs to be interpretable.\n",
      "We demonstrate an unsupervised learning technique for extracting interpretable\n",
      "physical parameters from noisy spatiotemporal data and for building a\n",
      "transferable model of the system. In particular, we implement a\n",
      "physics-informed architecture based on variational autoencoders that is\n",
      "designed for analyzing systems governed by partial differential equations\n",
      "(PDEs). The architecture is trained end-to-end and extracts latent parameters\n",
      "that parameterize the dynamics of a learned predictive model for the system. To\n",
      "test our method, we train our model on simulated data from a variety of PDEs\n",
      "with varying dynamical parameters that act as uncontrolled variables. Numerical\n",
      "experiments show that our method can accurately identify relevant parameters\n",
      "and extract them from raw and even noisy spatiotemporal data (tested with\n",
      "roughly 10% added noise). These extracted parameters correlate well (linearly\n",
      "with $R^2 > 0.95$) with the ground truth physical parameters used to generate\n",
      "the datasets. We then apply this method to nonlinear fiber propagation data,\n",
      "generated by an ab-initio simulation, to demonstrate its capabilities on a more\n",
      "realistic dataset. Our method for discovering interpretable latent parameters\n",
      "in spatiotemporal systems will allow us to better analyze and understand\n",
      "real-world phenomena and datasets, which often have unknown and uncontrolled\n",
      "variables that alter the system dynamics and cause varying behaviors that are\n",
      "difficult to disentangle.\n",
      "\n",
      "**Paper Id :2011.02838 \n",
      "Title :Real-time parameter inference in reduced-order flame models with\n",
      "  heteroscedastic Bayesian neural network ensembles\n",
      "  The estimation of model parameters with uncertainties from observed data is a\n",
      "ubiquitous inverse problem in science and engineering. In this paper, we\n",
      "suggest an inexpensive and easy to implement parameter estimation technique\n",
      "that uses a heteroscedastic Bayesian Neural Network trained using anchored\n",
      "ensembling. The heteroscedastic aleatoric error of the network models the\n",
      "irreducible uncertainty due to parameter degeneracies in our inverse problem,\n",
      "while the epistemic uncertainty of the Bayesian model captures uncertainties\n",
      "which may arise from an input observation's out-of-distribution nature. We use\n",
      "this tool to perform real-time parameter inference in a 6 parameter G-equation\n",
      "model of a ducted, premixed flame from observations of acoustically excited\n",
      "flames. We train our networks on a library of 2.1 million simulated flame\n",
      "videos. Results on the test dataset of simulated flames show that the network\n",
      "recovers flame model parameters, with the correlation coefficient between\n",
      "predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated\n",
      "uncertainty estimates. The trained neural networks are then used to infer model\n",
      "parameters from real videos of a premixed Bunsen flame captured using a\n",
      "high-speed camera in our lab. Re-simulation using inferred parameters shows\n",
      "excellent agreement between the real and simulated flames. Compared to Ensemble\n",
      "Kalman Filter-based tools that have been proposed for this problem in the\n",
      "combustion literature, our neural network ensemble achieves better\n",
      "data-efficiency and our sub-millisecond inference times represent a savings on\n",
      "computational costs by several orders of magnitude. This allows us to calibrate\n",
      "our reduced-order flame model in real-time and predict the thermoacoustic\n",
      "instability behaviour of the flame more accurately.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.06382 \n",
      "Title :Dynamical Systems as Temporal Feature Spaces\n",
      "  Parameterized state space models in the form of recurrent networks are often\n",
      "used in machine learning to learn from data streams exhibiting temporal\n",
      "dependencies. To break the black box nature of such models it is important to\n",
      "understand the dynamical features of the input driving time series that are\n",
      "formed in the state space. We propose a framework for rigorous analysis of such\n",
      "state representations in vanishing memory state space models such as echo state\n",
      "networks (ESN). In particular, we consider the state space a temporal feature\n",
      "space and the readout mapping from the state space a kernel machine operating\n",
      "in that feature space. We show that: (1) The usual ESN strategy of randomly\n",
      "generating input-to-state, as well as state coupling leads to shallow memory\n",
      "time series representations, corresponding to cross-correlation operator with\n",
      "fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic\n",
      "coupling yields a constrained dynamic kernel matching the input time series\n",
      "with straightforward exponentially decaying motifs or exponentially decaying\n",
      "motifs of the highest frequency; (3) Simple cycle high-dimensional reservoir\n",
      "topology specified only through two free parameters can implement deep memory\n",
      "dynamic kernels with a rich variety of matching motifs. We quantify richness of\n",
      "feature representations imposed by dynamic kernels and demonstrate that for\n",
      "dynamic kernel associated with cycle reservoir topology, the kernel richness\n",
      "undergoes a phase transition close to the edge of stability.\n",
      "\n",
      "**Paper Id :2002.05909 \n",
      "Title :Deep reconstruction of strange attractors from time series\n",
      "  Experimental measurements of physical systems often have a limited number of\n",
      "independent channels, causing essential dynamical variables to remain\n",
      "unobserved. However, many popular methods for unsupervised inference of latent\n",
      "dynamics from experimental data implicitly assume that the measurements have\n",
      "higher intrinsic dimensionality than the underlying system---making coordinate\n",
      "identification a dimensionality reduction problem. Here, we study the opposite\n",
      "limit, in which hidden governing coordinates must be inferred from only a\n",
      "low-dimensional time series of measurements. Inspired by classical analysis\n",
      "techniques for partial observations of chaotic attractors, we introduce a\n",
      "general embedding technique for univariate and multivariate time series,\n",
      "consisting of an autoencoder trained with a novel latent-space loss function.\n",
      "We show that our technique reconstructs the strange attractors of synthetic and\n",
      "real-world systems better than existing techniques, and that it creates\n",
      "consistent, predictive representations of even stochastic systems. We conclude\n",
      "by using our technique to discover dynamical attractors in diverse systems such\n",
      "as patient electrocardiograms, household electricity usage, neural spiking, and\n",
      "eruptions of the Old Faithful geyser---demonstrating diverse applications of\n",
      "our technique for exploratory data analysis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.06589 \n",
      "Title :Experimental quantum homodyne tomography via machine learning\n",
      "  Complete characterization of states and processes that occur within quantum\n",
      "devices is crucial for understanding and testing their potential to outperform\n",
      "classical technologies for communications and computing. However, solving this\n",
      "task with current state-of-the-art techniques becomes unwieldy for large and\n",
      "complex quantum systems. Here we realize and experimentally demonstrate a\n",
      "method for complete characterization of a quantum harmonic oscillator based on\n",
      "an artificial neural network known as the restricted Boltzmann machine. We\n",
      "apply the method to optical homodyne tomography and show it to allow full\n",
      "estimation of quantum states based on a smaller amount of experimental data\n",
      "compared to state-of-the-art methods. We link this advantage to reduced\n",
      "overfitting. Although our experiment is in the optical domain, our method\n",
      "provides a way of exploring quantum resources in a broad class of large-scale\n",
      "physical systems, such as superconducting circuits, atomic and molecular\n",
      "ensembles, and optomechanical systems.\n",
      "\n",
      "**Paper Id :1902.04057 \n",
      "Title :Deep autoregressive models for the efficient variational simulation of\n",
      "  many-body quantum systems\n",
      "  Artificial Neural Networks were recently shown to be an efficient\n",
      "representation of highly-entangled many-body quantum states. In practical\n",
      "applications, neural-network states inherit numerical schemes used in\n",
      "Variational Monte Carlo, most notably the use of Markov-Chain Monte-Carlo\n",
      "(MCMC) sampling to estimate quantum expectations. The local stochastic sampling\n",
      "in MCMC caps the potential advantages of neural networks in two ways: (i) Its\n",
      "intrinsic computational cost sets stringent practical limits on the width and\n",
      "depth of the networks, and therefore limits their expressive capacity; (ii) Its\n",
      "difficulty in generating precise and uncorrelated samples can result in\n",
      "estimations of observables that are very far from their true value. Inspired by\n",
      "the state-of-the-art generative models used in machine learning, we propose a\n",
      "specialized Neural Network architecture that supports efficient and exact\n",
      "sampling, completely circumventing the need for Markov Chain sampling. We\n",
      "demonstrate our approach for two-dimensional interacting spin models,\n",
      "showcasing the ability to obtain accurate results on larger system sizes than\n",
      "those currently accessible to neural-network quantum states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.06673 \n",
      "Title :Quant GANs: Deep Generation of Financial Time Series\n",
      "  Modeling financial time series by stochastic processes is a challenging task\n",
      "and a central area of research in financial mathematics. As an alternative, we\n",
      "introduce Quant GANs, a data-driven model which is inspired by the recent\n",
      "success of generative adversarial networks (GANs). Quant GANs consist of a\n",
      "generator and discriminator function, which utilize temporal convolutional\n",
      "networks (TCNs) and thereby achieve to capture long-range dependencies such as\n",
      "the presence of volatility clusters. The generator function is explicitly\n",
      "constructed such that the induced stochastic process allows a transition to its\n",
      "risk-neutral distribution. Our numerical results highlight that distributional\n",
      "properties for small and large lags are in an excellent agreement and\n",
      "dependence properties such as volatility clusters, leverage effects, and serial\n",
      "autocorrelations can be generated by the generator function of Quant GANs,\n",
      "demonstrably in high fidelity.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.06814 \n",
      "Title :A Quantum-inspired Algorithm for General Minimum Conical Hull Problems\n",
      "  A wide range of fundamental machine learning tasks that are addressed by the\n",
      "maximum a posteriori estimation can be reduced to a general minimum conical\n",
      "hull problem. The best-known solution to tackle general minimum conical hull\n",
      "problems is the divide-and-conquer anchoring learning scheme (DCA), whose\n",
      "runtime complexity is polynomial in size. However, big data is pushing these\n",
      "polynomial algorithms to their performance limits. In this paper, we propose a\n",
      "sublinear classical algorithm to tackle general minimum conical hull problems\n",
      "when the input has stored in a sample-based low-overhead data structure. The\n",
      "algorithm's runtime complexity is polynomial in the rank and polylogarithmic in\n",
      "size. The proposed algorithm achieves the exponential speedup over DCA and,\n",
      "therefore, provides advantages for high dimensional problems.\n",
      "\n",
      "**Paper Id :1812.08491 \n",
      "Title :cuPC: CUDA-based Parallel PC Algorithm for Causal Structure Learning on\n",
      "  GPU\n",
      "  The main goal in many fields in the empirical sciences is to discover causal\n",
      "relationships among a set of variables from observational data. PC algorithm is\n",
      "one of the promising solutions to learn underlying causal structure by\n",
      "performing a number of conditional independence tests. In this paper, we\n",
      "propose a novel GPU-based parallel algorithm, called cuPC, to execute an\n",
      "order-independent version of PC. The proposed solution has two variants, cuPC-E\n",
      "and cuPC-S, which parallelize PC in two different ways for multivariate normal\n",
      "distribution. Experimental results show the scalability of the proposed\n",
      "algorithms with respect to the number of variables, the number of samples, and\n",
      "different graph densities. For instance, in one of the most challenging\n",
      "datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On\n",
      "average, cuPC-E and cuPC-S achieve 500 X and 1300 X speedup, respectively,\n",
      "compared to serial implementation on CPU. The source code of cuPC is available\n",
      "online [1].\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.06870 \n",
      "Title :Light Multi-segment Activation for Model Compression\n",
      "  Model compression has become necessary when applying neural networks (NN)\n",
      "into many real application tasks that can accept slightly-reduced model\n",
      "accuracy with strict tolerance to model complexity. Recently, Knowledge\n",
      "Distillation, which distills the knowledge from well-trained and highly complex\n",
      "teacher model into a compact student model, has been widely used for model\n",
      "compression. However, under the strict requirement on the resource cost, it is\n",
      "quite challenging to achieve comparable performance with the teacher model,\n",
      "essentially due to the drastically-reduced expressiveness ability of the\n",
      "compact student model. Inspired by the nature of the expressiveness ability in\n",
      "Neural Networks, we propose to use multi-segment activation, which can\n",
      "significantly improve the expressiveness ability with very little cost, in the\n",
      "compact student model. Specifically, we propose a highly efficient\n",
      "multi-segment activation, called Light Multi-segment Activation (LMA), which\n",
      "can rapidly produce multiple linear regions with very few parameters by\n",
      "leveraging the statistical information. With using LMA, the compact student\n",
      "model is capable of achieving much better performance effectively and\n",
      "efficiently, than the ReLU-equipped one with same model scale. Furthermore, the\n",
      "proposed method is compatible with other model compression techniques, such as\n",
      "quantization, which means they can be used jointly for better compression\n",
      "performance. Experiments on state-of-the-art NN architectures over the\n",
      "real-world tasks demonstrate the effectiveness and extensibility of the LMA.\n",
      "\n",
      "**Paper Id :2010.04950 \n",
      "Title :A Model Compression Method with Matrix Product Operators for Speech\n",
      "  Enhancement\n",
      "  The deep neural network (DNN) based speech enhancement approaches have\n",
      "achieved promising performance. However, the number of parameters involved in\n",
      "these methods is usually enormous for the real applications of speech\n",
      "enhancement on the device with the limited resources. This seriously restricts\n",
      "the applications. To deal with this issue, model compression techniques are\n",
      "being widely studied. In this paper, we propose a model compression method\n",
      "based on matrix product operators (MPO) to substantially reduce the number of\n",
      "parameters in DNN models for speech enhancement. In this method, the weight\n",
      "matrices in the linear transformations of neural network model are replaced by\n",
      "the MPO decomposition format before training. In experiment, this process is\n",
      "applied to the causal neural network models, such as the feedforward multilayer\n",
      "perceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM\n",
      "models with/without compression are then utilized to estimate the ideal ratio\n",
      "mask for monaural speech enhancement. The experimental results show that our\n",
      "proposed MPO-based method outperforms the widely-used pruning method for speech\n",
      "enhancement under various compression rates, and further improvement can be\n",
      "achieved with respect to low compression rates. Our proposal provides an\n",
      "effective model compression method for speech enhancement, especially in\n",
      "cloud-free application.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.08349 \n",
      "Title :Convergence of Edge Computing and Deep Learning: A Comprehensive Survey\n",
      "  Ubiquitous sensors and smart devices from factories and communities are\n",
      "generating massive amounts of data, and ever-increasing computing power is\n",
      "driving the core of computation and services from the cloud to the edge of the\n",
      "network. As an important enabler broadly changing people's lives, from face\n",
      "recognition to ambitious smart factories and cities, developments of artificial\n",
      "intelligence (especially deep learning, DL) based applications and services are\n",
      "thriving. However, due to efficiency and latency issues, the current cloud\n",
      "computing service architecture hinders the vision of \"providing artificial\n",
      "intelligence for every person and every organization at everywhere\". Thus,\n",
      "unleashing DL services using resources at the network edge near the data\n",
      "sources has emerged as a desirable solution. Therefore, edge intelligence,\n",
      "aiming to facilitate the deployment of DL services by edge computing, has\n",
      "received significant attention. In addition, DL, as the representative\n",
      "technique of artificial intelligence, can be integrated into edge computing\n",
      "frameworks to build intelligent edge for dynamic, adaptive edge maintenance and\n",
      "management. With regard to mutually beneficial edge intelligence and\n",
      "intelligent edge, this paper introduces and discusses: 1) the application\n",
      "scenarios of both; 2) the practical implementation methods and enabling\n",
      "technologies, namely DL training and inference in the customized edge computing\n",
      "framework; 3) challenges and future trends of more pervasive and fine-grained\n",
      "intelligence. We believe that by consolidating information scattered across the\n",
      "communication, networking, and DL areas, this survey can help readers to\n",
      "understand the connections between enabling technologies while promoting\n",
      "further discussions on the fusion of edge intelligence and intelligent edge,\n",
      "i.e., Edge DL.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.09063 \n",
      "Title :Fast Convolutional Dictionary Learning off the Grid\n",
      "  Given a continuous-time signal that can be modeled as the superposition of\n",
      "localized, time-shifted events from multiple sources, the goal of Convolutional\n",
      "Dictionary Learning (CDL) is to identify the location of the events--by\n",
      "Convolutional Sparse Coding (CSC)--and learn the template for each source--by\n",
      "Convolutional Dictionary Update (CDU). In practice, because we observe samples\n",
      "of the continuous-time signal on a uniformly-sampled grid in discrete time,\n",
      "classical CSC methods can only produce estimates of the times when the events\n",
      "occur on this grid, which degrades the performance of the CDU. We introduce a\n",
      "CDL framework that significantly reduces the errors arising from performing the\n",
      "estimation in discrete time. Specifically, we construct an expanded dictionary\n",
      "that comprises, not only discrete-time shifts of the templates, but also\n",
      "interpolated variants, obtained by bandlimited interpolation, that account for\n",
      "continuous-time shifts. For CSC, we develop a novel computationally efficient\n",
      "CSC algorithm, termed Convolutional Orthogonal Matching Pursuit with\n",
      "interpolated dictionary (COMP-INTERP). We benchmarked COMP-INTERP to\n",
      "Contiunuous Basis Pursuit (CBP), the state-of-the-art CSC algorithm for\n",
      "estimating off-the-grid events, and demonstrate, on simulated data, that 1)\n",
      "COMP-INTERP achieves a similar level of accuracy, and 2) is two orders of\n",
      "magnitude faster. For CDU, we derive a novel procedure to update the templates\n",
      "given sparse codes that can occur both on and off the discrete-time grid. We\n",
      "also show that 3) dictionary update with the overcomplete dictionary yields\n",
      "more accurate templates. Finally, we apply the algorithms to the spike sorting\n",
      "problem on electrophysiology recording and show their competitive performance.\n",
      "\n",
      "**Paper Id :1907.11374 \n",
      "Title :Deep-learning-based Optimization of the Under-sampling Pattern in MRI\n",
      "  In compressed sensing MRI (CS-MRI), k-space measurements are under-sampled to\n",
      "achieve accelerated scan times. CS-MRI presents two fundamental problems: (1)\n",
      "where to sample and (2) how to reconstruct an under-sampled scan. In this\n",
      "paper, we tackle both problems simultaneously for the specific case of 2D\n",
      "Cartesian sampling, using a novel end-to-end learning framework that we call\n",
      "LOUPE (Learning-based Optimization of the Under-sampling PattErn). Our method\n",
      "trains a neural network model on a set of full-resolution MRI scans, which are\n",
      "retrospectively under-sampled on a 2D Cartesian grid and forwarded to an\n",
      "anti-aliasing (a.k.a. reconstruction) model that computes a reconstruction,\n",
      "which is in turn compared with the input. This formulation enables a\n",
      "data-driven optimized under-sampling pattern at a given sparsity level. In our\n",
      "experiments, we demonstrate that LOUPE-optimized under-sampling masks are\n",
      "data-dependent, varying significantly with the imaged anatomy, and perform well\n",
      "with different reconstruction methods. We present empirical results obtained\n",
      "with a large-scale, publicly available knee MRI dataset, where LOUPE offered\n",
      "superior reconstruction quality across different conditions. Even with an\n",
      "aggressive 8-fold acceleration rate, LOUPE's reconstructions contained much of\n",
      "the anatomical detail that was missed by alternative masks and reconstruction\n",
      "methods. Our experiments also show how LOUPE yielded optimal under-sampling\n",
      "patterns that were significantly different for brain vs knee MRI scans. Our\n",
      "code is made freely available at https://github.com/cagladbahadir/LOUPE/.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.09613 \n",
      "Title :Incremental and Decremental Fuzzy Bounded Twin Support Vector Machine\n",
      "  In this paper we present an incremental variant of the Twin Support Vector\n",
      "Machine (TWSVM) called Fuzzy Bounded Twin Support Vector Machine (FBTWSVM) to\n",
      "deal with large datasets and learning from data streams. We combine the TWSVM\n",
      "with a fuzzy membership function, so that each input has a different\n",
      "contribution to each hyperplane in a binary classifier. To solve the pair of\n",
      "quadratic programming problems (QPPs) we use a dual coordinate descent\n",
      "algorithm with a shrinking strategy, and to obtain a robust classification with\n",
      "a fast training we propose the use of a Fourier Gaussian approximation function\n",
      "with our linear FBTWSVM. Inspired by the shrinking technique, the incremental\n",
      "algorithm re-utilizes part of the training method with some heuristics, while\n",
      "the decremental procedure is based on a scored window. The FBTWSVM is also\n",
      "extended for multi-class problems by combining binary classifiers using a\n",
      "Directed Acyclic Graph (DAG) approach. Moreover, we analyzed the theoretical\n",
      "foundations properties of the proposed approach and its extension, and the\n",
      "experimental results on benchmark datasets indicate that the FBTWSVM has a fast\n",
      "training and retraining process while maintaining a robust classification\n",
      "performance.\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.09623 \n",
      "Title :Doubly robust off-policy evaluation with shrinkage\n",
      "  We propose a new framework for designing estimators for off-policy evaluation\n",
      "in contextual bandits. Our approach is based on the asymptotically optimal\n",
      "doubly robust estimator, but we shrink the importance weights to minimize a\n",
      "bound on the mean squared error, which results in a better bias-variance\n",
      "tradeoff in finite samples. We use this optimization-based framework to obtain\n",
      "three estimators: (a) a weight-clipping estimator, (b) a new weight-shrinkage\n",
      "estimator, and (c) the first shrinkage-based estimator for combinatorial action\n",
      "sets. Extensive experiments in both standard and combinatorial bandit benchmark\n",
      "problems show that our estimators are highly adaptive and typically outperform\n",
      "state-of-the-art methods.\n",
      "\n",
      "**Paper Id :1902.02495 \n",
      "Title :Cost-Effective Incentive Allocation via Structured Counterfactual\n",
      "  Inference\n",
      "  We address a practical problem ubiquitous in modern marketing campaigns, in\n",
      "which a central agent tries to learn a policy for allocating strategic\n",
      "financial incentives to customers and observes only bandit feedback. In\n",
      "contrast to traditional policy optimization frameworks, we take into account\n",
      "the additional reward structure and budget constraints common in this setting,\n",
      "and develop a new two-step method for solving this constrained counterfactual\n",
      "policy optimization problem. Our method first casts the reward estimation\n",
      "problem as a domain adaptation problem with supplementary structure, and then\n",
      "subsequently uses the estimators for optimizing the policy with constraints. We\n",
      "also establish theoretical error bounds for our estimation procedure and we\n",
      "empirically show that the approach leads to significant improvement on both\n",
      "synthetic and real datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "**Paper Id :2002.10986 \n",
      "Title :A Deep Learning Framework for Simulation and Defect Prediction Applied\n",
      "  in Microelectronics\n",
      "  The prediction of upcoming events in industrial processes has been a\n",
      "long-standing research goal since it enables optimization of manufacturing\n",
      "parameters, planning of equipment maintenance and more importantly prediction\n",
      "and eventually prevention of defects. While existing approaches have\n",
      "accomplished substantial progress, they are mostly limited to processing of one\n",
      "dimensional signals or require parameter tuning to model environmental\n",
      "parameters. In this paper, we propose an alternative approach based on deep\n",
      "neural networks that simulates changes in the 3D structure of a monitored\n",
      "object in a batch based on previous 3D measurements. In particular, we propose\n",
      "an architecture based on 3D Convolutional Neural Networks (3DCNN) in order to\n",
      "model the geometric variations in manufacturing parameters and predict upcoming\n",
      "events related to sub-optimal performance. We validate our framework on a\n",
      "microelectronics use-case using the recently published PCB scans dataset where\n",
      "we simulate changes on the shape and volume of glue deposited on an Liquid\n",
      "Crystal Polymer (LCP) substrate before the attachment of integrated circuits\n",
      "(IC). Experimental evaluation examines the impact of different choices in the\n",
      "cost function during training and shows that the proposed method can be\n",
      "efficiently used for defect prediction.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.10290 \n",
      "Title :Quantum Compressed Sensing with Unsupervised Tensor-Network Machine\n",
      "  Learning\n",
      "  We propose tensor-network compressed sensing (TNCS) by combining the ideas of\n",
      "compressed sensing, tensor network (TN), and machine learning, which permits\n",
      "novel and efficient quantum communications of realistic data. The strategy is\n",
      "to use the unsupervised TN machine learning algorithm to obtain the entangled\n",
      "state $|\\Psi \\rangle$ that describes the probability distribution of a huge\n",
      "amount of classical information considered to be communicated. To transfer a\n",
      "specific piece of information with $|\\Psi \\rangle$, our proposal is to encode\n",
      "such information in the separable state with the minimal distance to the\n",
      "measured state $|\\Phi \\rangle$ that is obtained by partially measuring on\n",
      "$|\\Psi \\rangle$ in a designed way. To this end, a measuring protocol analogous\n",
      "to the compressed sensing with neural-network machine learning is suggested,\n",
      "where the measurements are designed to minimize uncertainty of information from\n",
      "the probability distribution given by $|\\Phi \\rangle$. In this way, those who\n",
      "have $|\\Phi \\rangle$ can reliably access the information by simply measuring on\n",
      "$|\\Phi \\rangle$. We propose q-sparsity to characterize the sparsity of quantum\n",
      "states and the efficiency of the quantum communications by TNCS. The high\n",
      "q-sparsity is essentially due to the fact that the TN states describing nicely\n",
      "the probability distribution obey the area law of entanglement entropy. Testing\n",
      "on realistic datasets (hand-written digits and fashion images), TNCS is shown\n",
      "to possess high efficiency and accuracy, where the security of communications\n",
      "is guaranteed by the fundamental quantum principles.\n",
      "\n",
      "**Paper Id :1909.11887 \n",
      "Title :Information Scrambling in Quantum Neural Networks\n",
      "  The quantum neural network is one of the promising applications for near-term\n",
      "noisy intermediate-scale quantum computers. A quantum neural network distills\n",
      "the information from the input wavefunction into the output qubits. In this\n",
      "Letter, we show that this process can also be viewed from the opposite\n",
      "direction: the quantum information in the output qubits is scrambled into the\n",
      "input. This observation motivates us to use the tripartite information, a\n",
      "quantity recently developed to characterize information scrambling, to diagnose\n",
      "the training dynamics of quantum neural networks. We empirically find strong\n",
      "correlation between the dynamical behavior of the tripartite information and\n",
      "the loss function in the training process, from which we identify that the\n",
      "training process has two stages for randomly initialized networks. In the early\n",
      "stage, the network performance improves rapidly and the tripartite information\n",
      "increases linearly with a universal slope, meaning that the neural network\n",
      "becomes less scrambled than the random unitary. In the latter stage, the\n",
      "network performance improves slowly while the tripartite information decreases.\n",
      "We present evidences that the network constructs local correlations in the\n",
      "early stage and learns large-scale structures in the latter stage. We believe\n",
      "this two-stage training dynamics is universal and is applicable to a wide range\n",
      "of problems. Our work builds bridges between two research subjects of quantum\n",
      "neural networks and information scrambling, which opens up a new perspective to\n",
      "understand quantum neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.10552 \n",
      "Title :A neural network oracle for quantum nonlocality problems in networks\n",
      "  Characterizing quantum nonlocality in networks is a challenging, but\n",
      "important problem. Using quantum sources one can achieve distributions which\n",
      "are unattainable classically. A key point in investigations is to decide\n",
      "whether an observed probability distribution can be reproduced using only\n",
      "classical resources. This causal inference task is challenging even for simple\n",
      "networks, both analytically and using standard numerical techniques. We propose\n",
      "to use neural networks as numerical tools to overcome these challenges, by\n",
      "learning the classical strategies required to reproduce a distribution. As\n",
      "such, the neural network acts as an oracle, demonstrating that a behavior is\n",
      "classical if it can be learned. We apply our method to several examples in the\n",
      "triangle configuration. After demonstrating that the method is consistent with\n",
      "previously known results, we give solid evidence that the distribution\n",
      "presented in [N. Gisin, Entropy 21(3), 325 (2019)] is indeed nonlocal as\n",
      "conjectured. Finally we examine the genuinely nonlocal distribution presented\n",
      "in [M.-O. Renou et al., PRL 123, 140401 (2019)], and, guided by the findings of\n",
      "the neural network, conjecture nonlocality in a new range of parameters in\n",
      "these distributions. The method allows us to get an estimate on the noise\n",
      "robustness of all examined distributions.\n",
      "\n",
      "**Paper Id :2006.13222 \n",
      "Title :Certified variational quantum algorithms for eigenstate preparation\n",
      "  Solutions to many-body problem instances often involve an intractable number\n",
      "of degrees of freedom and admit no known approximations in general form. In\n",
      "practice, representing quantum-mechanical states of a given Hamiltonian using\n",
      "available numerical methods, in particular those based on variational Monte\n",
      "Carlo simulations, become exponentially more challenging with increasing system\n",
      "size. Recently quantum algorithms implemented as variational models have been\n",
      "proposed to accelerate such simulations. The variational ansatz states are\n",
      "characterized by a polynomial number of parameters devised in a way to minimize\n",
      "the expectation value of a given Hamiltonian, which is emulated by local\n",
      "measurements. In this study, we develop a means to certify the termination of\n",
      "variational algorithms. We demonstrate our approach by applying it to three\n",
      "models: the transverse field Ising model, the model of one-dimensional spinless\n",
      "fermions with competing interactions, and the Schwinger model of quantum\n",
      "electrodynamics. By means of comparison, we observe that our approach shows\n",
      "better performance near critical points in these models. We hence take a\n",
      "further step to improve the applicability and to certify the results of\n",
      "variational quantum simulators.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.11374 \n",
      "Title :Deep-learning-based Optimization of the Under-sampling Pattern in MRI\n",
      "  In compressed sensing MRI (CS-MRI), k-space measurements are under-sampled to\n",
      "achieve accelerated scan times. CS-MRI presents two fundamental problems: (1)\n",
      "where to sample and (2) how to reconstruct an under-sampled scan. In this\n",
      "paper, we tackle both problems simultaneously for the specific case of 2D\n",
      "Cartesian sampling, using a novel end-to-end learning framework that we call\n",
      "LOUPE (Learning-based Optimization of the Under-sampling PattErn). Our method\n",
      "trains a neural network model on a set of full-resolution MRI scans, which are\n",
      "retrospectively under-sampled on a 2D Cartesian grid and forwarded to an\n",
      "anti-aliasing (a.k.a. reconstruction) model that computes a reconstruction,\n",
      "which is in turn compared with the input. This formulation enables a\n",
      "data-driven optimized under-sampling pattern at a given sparsity level. In our\n",
      "experiments, we demonstrate that LOUPE-optimized under-sampling masks are\n",
      "data-dependent, varying significantly with the imaged anatomy, and perform well\n",
      "with different reconstruction methods. We present empirical results obtained\n",
      "with a large-scale, publicly available knee MRI dataset, where LOUPE offered\n",
      "superior reconstruction quality across different conditions. Even with an\n",
      "aggressive 8-fold acceleration rate, LOUPE's reconstructions contained much of\n",
      "the anatomical detail that was missed by alternative masks and reconstruction\n",
      "methods. Our experiments also show how LOUPE yielded optimal under-sampling\n",
      "patterns that were significantly different for brain vs knee MRI scans. Our\n",
      "code is made freely available at https://github.com/cagladbahadir/LOUPE/.\n",
      "\n",
      "**Paper Id :1910.09116 \n",
      "Title :Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without\n",
      "  Fully-Sampled Data\n",
      "  Deep learning (DL) has emerged as a tool for improving accelerated MRI\n",
      "reconstruction. A common strategy among DL methods is the physics-based\n",
      "approach, where a regularized iterative algorithm alternating between data\n",
      "consistency and a regularizer is unrolled for a finite number of iterations.\n",
      "This unrolled network is then trained end-to-end in a supervised manner, using\n",
      "fully-sampled data as ground truth for the network output. However, in a number\n",
      "of scenarios, it is difficult to obtain fully-sampled datasets, due to\n",
      "physiological constraints such as organ motion or physical constraints such as\n",
      "signal decay. In this work, we tackle this issue and propose a self-supervised\n",
      "learning strategy that enables physics-based DL reconstruction without\n",
      "fully-sampled data. Our approach is to divide the acquired sub-sampled points\n",
      "for each scan into training and validation subsets. During training, data\n",
      "consistency is enforced over the training subset, while the validation subset\n",
      "is used to define the loss function. Results show that the proposed\n",
      "self-supervised learning method successfully reconstructs images without\n",
      "fully-sampled data, performing similarly to the supervised approach that is\n",
      "trained with fully-sampled references. This has implications for physics-based\n",
      "inverse problem approaches for other settings, where fully-sampled data is not\n",
      "available or possible to acquire.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.11985 \n",
      "Title :The Wang-Landau Algorithm as Stochastic Optimization and Its\n",
      "  Acceleration\n",
      "  We show that the Wang-Landau algorithm can be formulated as a stochastic\n",
      "gradient descent algorithm minimizing a smooth and convex objective function,\n",
      "of which the gradient is estimated using Markov chain Monte Carlo iterations.\n",
      "The optimization formulation provides us a new way to establish the convergence\n",
      "rate of the Wang-Landau algorithm, by exploiting the fact that almost surely,\n",
      "the density estimates (on the logarithmic scale) remain in a compact set, upon\n",
      "which the objective function is strongly convex. The optimization viewpoint\n",
      "motivates us to improve the efficiency of the Wang-Landau algorithm using\n",
      "popular tools including the momentum method and the adaptive learning rate\n",
      "method. We demonstrate the accelerated Wang-Landau algorithm on a\n",
      "two-dimensional Ising model and a two-dimensional ten-state Potts model.\n",
      "\n",
      "**Paper Id :1910.11561 \n",
      "Title :Convergence Analysis of Block Coordinate Algorithms with Determinantal\n",
      "  Sampling\n",
      "  We analyze the convergence rate of the randomized Newton-like method\n",
      "introduced by Qu et. al. (2016) for smooth and convex objectives, which uses\n",
      "random coordinate blocks of a Hessian-over-approximation matrix $\\bM$ instead\n",
      "of the true Hessian. The convergence analysis of the algorithm is challenging\n",
      "because of its complex dependence on the structure of $\\bM$. However, we show\n",
      "that when the coordinate blocks are sampled with probability proportional to\n",
      "their determinant, the convergence rate depends solely on the eigenvalue\n",
      "distribution of matrix $\\bM$, and has an analytically tractable form. To do so,\n",
      "we derive a fundamental new expectation formula for determinantal point\n",
      "processes. We show that determinantal sampling allows us to reason about the\n",
      "optimal subset size of blocks in terms of the spectrum of $\\bM$. Additionally,\n",
      "we provide a numerical evaluation of our analysis, demonstrating cases where\n",
      "determinantal sampling is superior or on par with uniform sampling.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.12340 \n",
      "Title :Bandit Convex Optimization in Non-stationary Environments\n",
      "  Bandit Convex Optimization (BCO) is a fundamental framework for modeling\n",
      "sequential decision-making with partial information, where the only feedback\n",
      "available to the player is the one-point or two-point function values. In this\n",
      "paper, we investigate BCO in non-stationary environments and choose the\n",
      "\\emph{dynamic regret} as the performance measure, which is defined as the\n",
      "difference between the cumulative loss incurred by the algorithm and that of\n",
      "any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the\n",
      "path-length of the comparator sequence that reflects the non-stationarity of\n",
      "environments. We propose a novel algorithm that achieves\n",
      "$O(T^{3/4}(1+P_T)^{1/2})$ and $O(T^{1/2}(1+P_T)^{1/2})$ dynamic regret\n",
      "respectively for the one-point and two-point feedback models. The latter result\n",
      "is optimal, matching the $\\Omega(T^{1/2}(1+P_T)^{1/2})$ lower bound established\n",
      "in this paper. Notably, our algorithm is more adaptive to non-stationary\n",
      "environments since it does not require prior knowledge of the path-length $P_T$\n",
      "ahead of time, which is generally unknown.\n",
      "\n",
      "**Paper Id :2010.00081 \n",
      "Title :Stage-wise Conservative Linear Bandits\n",
      "  We study stage-wise conservative linear stochastic bandits: an instance of\n",
      "bandit optimization, which accounts for (unknown) safety constraints that\n",
      "appear in applications such as online advertising and medical trials. At each\n",
      "stage, the learner must choose actions that not only maximize cumulative reward\n",
      "across the entire time horizon but further satisfy a linear baseline constraint\n",
      "that takes the form of a lower bound on the instantaneous reward. For this\n",
      "problem, we present two novel algorithms, stage-wise conservative linear\n",
      "Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that\n",
      "respect the baseline constraints and enjoy probabilistic regret bounds of order\n",
      "O(\\sqrt{T} \\log^{3/2}T) and O(\\sqrt{T} \\log T), respectively. Notably, the\n",
      "proposed algorithms can be adjusted with only minor modifications to tackle\n",
      "different problem variations, such as constraints with bandit-feedback, or an\n",
      "unknown sequence of baseline actions. We discuss these and other improvements\n",
      "over the state-of-the-art. For instance, compared to existing solutions, we\n",
      "show that SCLTS plays the (non-optimal) baseline action at most O(\\log{T})\n",
      "times (compared to O(\\sqrt{T})). Finally, we make connections to another\n",
      "studied form of safety constraints that takes the form of an upper bound on the\n",
      "instantaneous reward. While this incurs additional complexity to the learning\n",
      "process as the optimal action is not guaranteed to belong to the safe set at\n",
      "each round, we show that SCLUCB can properly adjust in this setting via a\n",
      "simple modification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.12724 \n",
      "Title :Classical and Quantum Algorithms for Tensor Principal Component Analysis\n",
      "  We present classical and quantum algorithms based on spectral methods for a\n",
      "problem in tensor principal component analysis. The quantum algorithm achieves\n",
      "a quartic speedup while using exponentially smaller space than the fastest\n",
      "classical spectral algorithm, and a super-polynomial speedup over classical\n",
      "algorithms that use only polynomial space. The classical algorithms that we\n",
      "present are related to, but slightly different from those presented recently in\n",
      "Ref. 1. In particular, we have an improved threshold for recovery and the\n",
      "algorithms we present work for both even and odd order tensors. These results\n",
      "suggest that large-scale inference problems are a promising future application\n",
      "for quantum computers.\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1907.13266 \n",
      "Title :PrecoderNet: Hybrid Beamforming for Millimeter Wave Systems with Deep\n",
      "  Reinforcement Learning\n",
      "  In this letter, we investigate the hybrid beamforming for millimeter wave\n",
      "massive multiple-input multiple-output (MIMO) system based on deep\n",
      "reinforcement learning (DRL). Imperfect channel state information (CSI) is\n",
      "assumed to be available at the base station (BS). To achieve high spectral\n",
      "efficiency with low time consumption, we propose a novel DRL-based method\n",
      "called PrecoderNet to design the digital precoder and analog combiner. The DRL\n",
      "agent takes the digital beamformer and analog combiner of the previous learning\n",
      "iteration as state, and these matrices of current learning iteration as action.\n",
      "Simulation results demonstrate that the PrecoderNet performs well in spectral\n",
      "efficiency, bit error rate (BER), as well as time consumption, and is robust to\n",
      "the CSI imperfection.\n",
      "\n",
      "**Paper Id :2001.11085 \n",
      "Title :Deep Channel Learning For Large Intelligent Surfaces Aided mm-Wave\n",
      "  Massive MIMO Systems\n",
      "  This letter presents the first work introducing a deep learning (DL)\n",
      "framework for channel estimation in large intelligent surface (LIS) assisted\n",
      "massive MIMO (multiple-input multiple-output) systems. A twin convolutional\n",
      "neural network (CNN) architecture is designed and it is fed with the received\n",
      "pilot signals to estimate both direct and cascaded channels. In a multi-user\n",
      "scenario, each user has access to the CNN to estimate its own channel. The\n",
      "performance of the proposed DL approach is evaluated and compared with\n",
      "state-of-the-art DL-based techniques and its superior performance is\n",
      "demonstrated.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.00176 \n",
      "Title :FairSight: Visual Analytics for Fairness in Decision Making\n",
      "  Data-driven decision making related to individuals has become increasingly\n",
      "pervasive, but the issue concerning the potential discrimination has been\n",
      "raised by recent studies. In response, researchers have made efforts to propose\n",
      "and implement fairness measures and algorithms, but those efforts have not been\n",
      "translated to the real-world practice of data-driven decision making. As such,\n",
      "there is still an urgent need to create a viable tool to facilitate fair\n",
      "decision making. We propose FairSight, a visual analytic system to address this\n",
      "need; it is designed to achieve different notions of fairness in ranking\n",
      "decisions through identifying the required actions -- understanding, measuring,\n",
      "diagnosing and mitigating biases -- that together lead to fairer decision\n",
      "making. Through a case study and user study, we demonstrate that the proposed\n",
      "visual analytic and diagnostic modules in the system are effective in\n",
      "understanding the fairness-aware decision pipeline and obtaining more fair\n",
      "outcomes.\n",
      "\n",
      "**Paper Id :2005.12379 \n",
      "Title :Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias?\n",
      "  An Empirical Study on Model Fairness\n",
      "  Machine learning models are increasingly being used in important\n",
      "decision-making software such as approving bank loans, recommending criminal\n",
      "sentencing, hiring employees, and so on. It is important to ensure the fairness\n",
      "of these models so that no discrimination is made based on protected attribute\n",
      "(e.g., race, sex, age) while decision making. Algorithms have been developed to\n",
      "measure unfairness and mitigate them to a certain extent. In this paper, we\n",
      "have focused on the empirical evaluation of fairness and mitigations on\n",
      "real-world machine learning models. We have created a benchmark of 40 top-rated\n",
      "models from Kaggle used for 5 different tasks, and then using a comprehensive\n",
      "set of fairness metrics, evaluated their fairness. Then, we have applied 7\n",
      "mitigation techniques on these models and analyzed the fairness, mitigation\n",
      "results, and impacts on performance. We have found that some model optimization\n",
      "techniques result in inducing unfairness in the models. On the other hand,\n",
      "although there are some fairness control mechanisms in machine learning\n",
      "libraries, they are not documented. The mitigation algorithm also exhibit\n",
      "common patterns such as mitigation in the post-processing is often costly (in\n",
      "terms of performance) and mitigation in the pre-processing stage is preferred\n",
      "in most cases. We have also presented different trade-off choices of fairness\n",
      "mitigation decisions. Our study suggests future research directions to reduce\n",
      "the gap between theoretical fairness aware algorithms and the software\n",
      "engineering methods to leverage them in practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.00617 \n",
      "Title :A self-organizing fuzzy neural network for sequence learning\n",
      "  In this paper, a new self-organizing fuzzy neural network model is presented\n",
      "which is able to learn and reproduce different sequences accurately. Sequence\n",
      "learning is important in performing skillful tasks, such as writing and playing\n",
      "piano. The structure of the proposed network is composed of two parts:\n",
      "1-sequence identifier which computes a novel sequence identity value based on\n",
      "initial samples of a sequence, and detects the sequence identity based on\n",
      "proper fuzzy rules, and 2-sequence locator, which locates the input sample in\n",
      "the sequence. Therefore, by integrating outputs of these two parts in fuzzy\n",
      "rules, the network is able to produce the proper output based on current state\n",
      "of the sequence. To learn the proposed structure, a gradual learning procedure\n",
      "is proposed. First, learning is performed by adding new fuzzy rules, based on\n",
      "coverage measure, using available correct data. Next, the initialized\n",
      "parameters are fine-tuned, by gradient descent algorithm, based on fed back\n",
      "approximated network output as the next input. The proposed method has a\n",
      "dynamic structure which is able to learn new sequences online. The proposed\n",
      "method is used to learn and reproduce different sequences simultaneously which\n",
      "is the novelty of this method.\n",
      "\n",
      "**Paper Id :2004.04573 \n",
      "Title :Backprojection for Training Feedforward Neural Networks in the Input and\n",
      "  Feature Spaces\n",
      "  After the tremendous development of neural networks trained by\n",
      "backpropagation, it is a good time to develop other algorithms for training\n",
      "neural networks to gain more insights into networks. In this paper, we propose\n",
      "a new algorithm for training feedforward neural networks which is fairly faster\n",
      "than backpropagation. This method is based on projection and reconstruction\n",
      "where, at every layer, the projected data and reconstructed labels are forced\n",
      "to be similar and the weights are tuned accordingly layer by layer. The\n",
      "proposed algorithm can be used for both input and feature spaces, named as\n",
      "backprojection and kernel backprojection, respectively. This algorithm gives an\n",
      "insight to networks with a projection-based perspective. The experiments on\n",
      "synthetic datasets show the effectiveness of the proposed method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.01092 \n",
      "Title :Machine-learning based three-qubit gate for realization of a Toffoli\n",
      "  gate with cQED-based transmon systems\n",
      "  We use machine learning techniques to design a 50 ns three-qubit flux-tunable\n",
      "controlled-controlled-phase gate with fidelity of >99.99% for nearest-neighbor\n",
      "coupled transmons in circuit quantum electrodynamics architectures. We explain\n",
      "our gate design procedure where we enforce realistic constraints, and analyze\n",
      "the new gate's robustness under decoherence, distortion, and random noise. Our\n",
      "controlled-controlled-phase gate in combination with two single-qubit gates\n",
      "realizes a Toffoli gate which is widely used in quantum circuits, logic\n",
      "synthesis, quantum error correction, and quantum games.\n",
      "\n",
      "**Paper Id :2004.04743 \n",
      "Title :Topological Quantum Compiling with Reinforcement Learning\n",
      "  Quantum compiling, a process that decomposes the quantum algorithm into a\n",
      "series of hardware-compatible commands or elementary gates, is of fundamental\n",
      "importance for quantum computing. We introduce an efficient algorithm based on\n",
      "deep reinforcement learning that compiles an arbitrary single-qubit gate into a\n",
      "sequence of elementary gates from a finite universal set. It generates\n",
      "near-optimal gate sequences with given accuracy and is generally applicable to\n",
      "various scenarios, independent of the hardware-feasible universal set and free\n",
      "from using ancillary qubits. For concreteness, we apply this algorithm to the\n",
      "case of topological compiling of Fibonacci anyons and obtain near-optimal\n",
      "braiding sequences for arbitrary single-qubit unitaries. Our algorithm may\n",
      "carry over to other challenging quantum discrete problems, thus opening up a\n",
      "new avenue for intriguing applications of deep learning in quantum physics.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.01612 \n",
      "Title :Multi-Contrast Super-Resolution MRI Through a Progressive Network\n",
      "  Magnetic resonance imaging (MRI) is widely used for screening, diagnosis,\n",
      "image-guided therapy, and scientific research. A significant advantage of MRI\n",
      "over other imaging modalities such as computed tomography (CT) and nuclear\n",
      "imaging is that it clearly shows soft tissues in multi-contrasts. Compared with\n",
      "other medical image super-resolution (SR) methods that are in a single\n",
      "contrast, multi-contrast super-resolution studies can synergize multiple\n",
      "contrast images to achieve better super-resolution results. In this paper, we\n",
      "propose a one-level non-progressive neural network for low up-sampling\n",
      "multi-contrast super-resolution and a two-level progressive network for high\n",
      "up-sampling multi-contrast super-resolution. Multi-contrast information is\n",
      "combined in high-level feature space. Our experimental results demonstrate that\n",
      "the proposed networks can produce MRI super-resolution images with good image\n",
      "quality and outperform other multi-contrast super-resolution methods in terms\n",
      "of structural similarity and peak signal-to-noise ratio. Also, the progressive\n",
      "network produces a better SR image quality than the non-progressive network,\n",
      "even if the original low-resolution images were highly down-sampled.\n",
      "\n",
      "**Paper Id :1907.03063 \n",
      "Title :MRI Super-Resolution with Ensemble Learning and Complementary Priors\n",
      "  Magnetic resonance imaging (MRI) is a widely used medical imaging modality.\n",
      "However, due to the limitations in hardware, scan time, and throughput, it is\n",
      "often clinically challenging to obtain high-quality MR images. The\n",
      "super-resolution approach is potentially promising to improve MR image quality\n",
      "without any hardware upgrade. In this paper, we propose an ensemble learning\n",
      "and deep learning framework for MR image super-resolution. In our study, we\n",
      "first enlarged low resolution images using 5 commonly used super-resolution\n",
      "algorithms and obtained differentially enlarged image datasets with\n",
      "complementary priors. Then, a generative adversarial network (GAN) is trained\n",
      "with each dataset to generate super-resolution MR images. Finally, a\n",
      "convolutional neural network is used for ensemble learning that synergizes the\n",
      "outputs of GANs into the final MR super-resolution images. According to our\n",
      "results, the ensemble learning results outcome any one of GAN outputs. Compared\n",
      "with some state-of-the-art deep learning-based super-resolution methods, our\n",
      "approach is advantageous in suppressing artifacts and keeping more image\n",
      "details.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.01786 \n",
      "Title :Stochastic data-driven model predictive control using Gaussian processes\n",
      "  Nonlinear model predictive control (NMPC) is one of the few control methods\n",
      "that can handle multivariable nonlinear controlsystems with constraints.\n",
      "Gaussian processes (GPs) present a powerful tool to identify the required plant\n",
      "model and quantifythe residual uncertainty of the plant-model mismatch. It is\n",
      "crucial to consider this uncertainty, since it may lead to worsecontrol\n",
      "performance and constraint violations. In this paper we propose a new method to\n",
      "design a GP-based NMPC algorithmfor finite horizon control problems. The method\n",
      "generates Monte Carlo samples of the GP offline for constraint tighteningusing\n",
      "back-offs. The tightened constraints then guarantee the satisfaction of chance\n",
      "constraints online. Advantages of our proposed approach over existing methods\n",
      "include fast online evaluation, consideration of closed-loop behaviour, and\n",
      "thepossibility to alleviate conservativeness by considering both online\n",
      "learning and state dependency of the uncertainty. The algorithm is verified on\n",
      "a challenging semi-batch bioprocess case study.\n",
      "\n",
      "**Paper Id :1912.10360 \n",
      "Title :Safe and Fast Tracking on a Robot Manipulator: Robust MPC and Neural\n",
      "  Network Control\n",
      "  Fast feedback control and safety guarantees are essential in modern robotics.\n",
      "We present an approach that achieves both by combining novel robust model\n",
      "predictive control (MPC) with function approximation via (deep) neural networks\n",
      "(NNs). The result is a new approach for complex tasks with nonlinear,\n",
      "uncertain, and constrained dynamics as are common in robotics. Specifically, we\n",
      "leverage recent results in MPC research to propose a new robust setpoint\n",
      "tracking MPC algorithm, which achieves reliable and safe tracking of a dynamic\n",
      "setpoint while guaranteeing stability and constraint satisfaction. The\n",
      "presented robust MPC scheme constitutes a one-layer approach that unifies the\n",
      "often separated planning and control layers, by directly computing the control\n",
      "command based on a reference and possibly obstacle positions. As a separate\n",
      "contribution, we show how the computation time of the MPC can be drastically\n",
      "reduced by approximating the MPC law with a NN controller. The NN is trained\n",
      "and validated from offline samples of the MPC, yielding statistical guarantees,\n",
      "and used in lieu thereof at run time. Our experiments on a state-of-the-art\n",
      "robot manipulator are the first to show that both the proposed robust and\n",
      "approximate MPC schemes scale to real-world robotic systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.03620 \n",
      "Title :Learning physics-based reduced-order models for a single-injector\n",
      "  combustion process\n",
      "  This paper presents a physics-based data-driven method to learn predictive\n",
      "reduced-order models (ROMs) from high-fidelity simulations, and illustrates it\n",
      "in the challenging context of a single-injector combustion process. The method\n",
      "combines the perspectives of model reduction and machine learning. Model\n",
      "reduction brings in the physics of the problem, constraining the ROM\n",
      "predictions to lie on a subspace defined by the governing equations. This is\n",
      "achieved by defining the ROM in proper orthogonal decomposition (POD)\n",
      "coordinates, which embed the rich physics information contained in solution\n",
      "snapshots of a high-fidelity computational fluid dynamics (CFD) model. The\n",
      "machine learning perspective brings the flexibility to use transformed physical\n",
      "variables to define the POD basis. This is in contrast to traditional model\n",
      "reduction approaches that are constrained to use the physical variables of the\n",
      "high-fidelity code. Combining the two perspectives, the approach identifies a\n",
      "set of transformed physical variables that expose quadratic structure in the\n",
      "combustion governing equations and learns a quadratic ROM from transformed\n",
      "snapshot data. This learning does not require access to the high-fidelity model\n",
      "implementation. Numerical experiments show that the ROM accurately predicts\n",
      "temperature, pressure, velocity, species concentrations, and the limit-cycle\n",
      "amplitude, with speedups of more than five orders of magnitude over\n",
      "high-fidelity models. Our ROM simulation is shown to be predictive 200% past\n",
      "the training interval. Moreover, ROM-predicted pressure traces accurately match\n",
      "the phase of the pressure signal and yield good approximations of the\n",
      "limit-cycle amplitude.\n",
      "\n",
      "**Paper Id :1912.08177 \n",
      "Title :Lift & Learn: Physics-informed machine learning for large-scale\n",
      "  nonlinear dynamical systems\n",
      "  We present Lift & Learn, a physics-informed method for learning\n",
      "low-dimensional models for large-scale dynamical systems. The method exploits\n",
      "knowledge of a system's governing equations to identify a coordinate\n",
      "transformation in which the system dynamics have quadratic structure. This\n",
      "transformation is called a lifting map because it often adds auxiliary\n",
      "variables to the system state. The lifting map is applied to data obtained by\n",
      "evaluating a model for the original nonlinear system. This lifted data is\n",
      "projected onto its leading principal components, and low-dimensional linear and\n",
      "quadratic matrix operators are fit to the lifted reduced data using a\n",
      "least-squares operator inference procedure. Analysis of our method shows that\n",
      "the Lift & Learn models are able to capture the system physics in the lifted\n",
      "coordinates at least as accurately as traditional intrusive model reduction\n",
      "approaches. This preservation of system physics makes the Lift & Learn models\n",
      "robust to changes in inputs. Numerical experiments on the FitzHugh-Nagumo\n",
      "neuron activation model and the compressible Euler equations demonstrate the\n",
      "generalizability of our model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.04347 \n",
      "Title :Enforcing Perceptual Consistency on Generative Adversarial Networks by\n",
      "  Using the Normalised Laplacian Pyramid Distance\n",
      "  In recent years there has been a growing interest in image generation through\n",
      "deep learning. While an important part of the evaluation of the generated\n",
      "images usually involves visual inspection, the inclusion of human perception as\n",
      "a factor in the training process is often overlooked. In this paper we propose\n",
      "an alternative perceptual regulariser for image-to-image translation using\n",
      "conditional generative adversarial networks (cGANs). To do so automatically\n",
      "(avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance\n",
      "(NLPD) to measure the perceptual similarity between the generated image and the\n",
      "original image. The NLPD is based on the principle of normalising the value of\n",
      "coefficients with respect to a local estimate of mean energy at different\n",
      "scales and has already been successfully tested in different experiments\n",
      "involving human perception. We compare this regulariser with the originally\n",
      "proposed L1 distance and note that when using NLPD the generated images contain\n",
      "more realistic values for both local and global contrast. We found that using\n",
      "NLPD as a regulariser improves image segmentation accuracy on generated images\n",
      "as well as improving two no-reference image quality metrics.\n",
      "\n",
      "**Paper Id :1910.11664 \n",
      "Title :SPICE: Self-supervised Pitch Estimation\n",
      "  We propose a model to estimate the fundamental frequency in monophonic audio,\n",
      "often referred to as pitch estimation. We acknowledge the fact that obtaining\n",
      "ground truth annotations at the required temporal and frequency resolution is a\n",
      "particularly daunting task. Therefore, we propose to adopt a self-supervised\n",
      "learning technique, which is able to estimate pitch without any form of\n",
      "supervision. The key observation is that pitch shift maps to a simple\n",
      "translation when the audio signal is analysed through the lens of the\n",
      "constant-Q transform (CQT). We design a self-supervised task by feeding two\n",
      "shifted slices of the CQT to the same convolutional encoder, and require that\n",
      "the difference in the outputs is proportional to the corresponding difference\n",
      "in pitch. In addition, we introduce a small model head on top of the encoder,\n",
      "which is able to determine the confidence of the pitch estimate, so as to\n",
      "distinguish between voiced and unvoiced audio. Our results show that the\n",
      "proposed method is able to estimate pitch at a level of accuracy comparable to\n",
      "fully supervised models, both on clean and noisy audio samples, although it\n",
      "does not require access to large labeled datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.04710 \n",
      "Title :metric-learn: Metric Learning Algorithms in Python\n",
      "  metric-learn is an open source Python package implementing supervised and\n",
      "weakly-supervised distance metric learning algorithms. As part of\n",
      "scikit-learn-contrib, it provides a unified interface compatible with\n",
      "scikit-learn which allows to easily perform cross-validation, model selection,\n",
      "and pipelining with other machine learning estimators. metric-learn is\n",
      "thoroughly tested and available on PyPi under the MIT licence.\n",
      "\n",
      "**Paper Id :1909.10389 \n",
      "Title :Machine Learning Pipelines with Modern Big Data Tools for High Energy\n",
      "  Physics\n",
      "  The effective utilization at scale of complex machine learning (ML)\n",
      "techniques for HEP use cases poses several technological challenges, most\n",
      "importantly on the actual implementation of dedicated end-to-end data\n",
      "pipelines. A solution to these challenges is presented, which allows training\n",
      "neural network classifiers using solutions from the Big Data and data science\n",
      "ecosystems, integrated with tools, software, and platforms common in the HEP\n",
      "environment. In particular, Apache Spark is exploited for data preparation and\n",
      "feature engineering, running the corresponding (Python) code interactively on\n",
      "Jupyter notebooks. Key integrations and libraries that make Spark capable of\n",
      "ingesting data stored using ROOT format and accessed via the XRootD protocol,\n",
      "are described and discussed. Training of the neural network models, defined\n",
      "using the Keras API, is performed in a distributed fashion on Spark clusters by\n",
      "using BigDL with Analytics Zoo and also by using TensorFlow, notably for\n",
      "distributed training on CPU and GPU resourcess. The implementation and the\n",
      "results of the distributed training are described in detail in this work.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.05699 \n",
      "Title :Convergence of Gradient Methods on Bilinear Zero-Sum Games\n",
      "  Min-max formulations have attracted great attention in the ML community due\n",
      "to the rise of deep generative models and adversarial methods, while\n",
      "understanding the dynamics of gradient algorithms for solving such formulations\n",
      "has remained a grand challenge. As a first step, we restrict to bilinear\n",
      "zero-sum games and give a systematic analysis of popular gradient updates, for\n",
      "both simultaneous and alternating versions. We provide exact conditions for\n",
      "their convergence and find the optimal parameter setup and convergence rates.\n",
      "In particular, our results offer formal evidence that alternating updates\n",
      "converge \"better\" than simultaneous ones.\n",
      "\n",
      "**Paper Id :1906.07300 \n",
      "Title :Linear Lower Bounds and Conditioning of Differentiable Games\n",
      "  Recent successes of game-theoretic formulations in ML have caused a\n",
      "resurgence of research interest in differentiable games. Overwhelmingly, that\n",
      "research focuses on methods and upper bounds on their speed of convergence. In\n",
      "this work, we approach the question of fundamental iteration complexity by\n",
      "providing lower bounds to complement the linear (i.e. geometric) upper bounds\n",
      "observed in the literature on a wide class of problems. We cast saddle-point\n",
      "and min-max problems as 2-player games. We leverage tools from single-objective\n",
      "convex optimisation to propose new linear lower bounds for convex-concave\n",
      "games. Notably, we give a linear lower bound for $n$-player differentiable\n",
      "games, by using the spectral properties of the update operator. We then propose\n",
      "a new definition of the condition number arising from our lower bound analysis.\n",
      "Unlike past definitions, our condition number captures the fact that linear\n",
      "rates are possible in games, even in the absence of strong convexity or strong\n",
      "concavity in the variables.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.06180 \n",
      "Title :Multi-View Broad Learning System for Primate Oculomotor Decision\n",
      "  Decoding\n",
      "  Multi-view learning improves the learning performance by utilizing multi-view\n",
      "data: data collected from multiple sources, or feature sets extracted from the\n",
      "same data source. This approach is suitable for primate brain state decoding\n",
      "using cortical neural signals. This is because the complementary components of\n",
      "simultaneously recorded neural signals, local field potentials (LFPs) and\n",
      "action potentials (spikes), can be treated as two views. In this paper, we\n",
      "extended broad learning system (BLS), a recently proposed wide neural network\n",
      "architecture, from single-view learning to multi-view learning, and validated\n",
      "its performance in decoding monkeys' oculomotor decision from medial frontal\n",
      "LFPs and spikes. We demonstrated that medial frontal LFPs and spikes in\n",
      "non-human primate do contain complementary information about the oculomotor\n",
      "decision, and that the proposed multi-view BLS is a more effective approach for\n",
      "decoding the oculomotor decision than several classical and state-of-the-art\n",
      "single-view and multi-view learning approaches.\n",
      "\n",
      "**Paper Id :1910.05878 \n",
      "Title :Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces\n",
      "  Transfer learning makes use of data or knowledge in one problem to help solve\n",
      "a different, yet related, problem. It is particularly useful in brain-computer\n",
      "interfaces (BCIs), for coping with variations among different subjects and/or\n",
      "tasks. This paper considers offline unsupervised cross-subject\n",
      "electroencephalogram (EEG) classification, i.e., we have labeled EEG trials\n",
      "from one or more source subjects, but only unlabeled EEG trials from the target\n",
      "subject. We propose a novel manifold embedded knowledge transfer (MEKT)\n",
      "approach, which first aligns the covariance matrices of the EEG trials in the\n",
      "Riemannian manifold, extracts features in the tangent space, and then performs\n",
      "domain adaptation by minimizing the joint probability distribution shift\n",
      "between the source and the target domains, while preserving their geometric\n",
      "structures. MEKT can cope with one or multiple source domains, and can be\n",
      "computed efficiently. We also propose a domain transferability estimation (DTE)\n",
      "approach to identify the most beneficial source domains, in case there are a\n",
      "large number of source domains. Experiments on four EEG datasets from two\n",
      "different BCI paradigms demonstrated that MEKT outperformed several\n",
      "state-of-the-art transfer learning approaches, and DTE can reduce more than\n",
      "half of the computational cost when the number of source subjects is large,\n",
      "with little sacrifice of classification accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.06655 \n",
      "Title :Quantum Expectation-Maximization Algorithm\n",
      "  Clustering algorithms are a cornerstone of machine learning applications.\n",
      "Recently, a quantum algorithm for clustering based on the k-means algorithm has\n",
      "been proposed by Kerenidis, Landman, Luongo and Prakash. Based on their work,\n",
      "we propose a quantum expectation-maximization (EM) algorithm for Gaussian\n",
      "mixture models (GMMs). The robustness and quantum speedup of the algorithm is\n",
      "demonstrated. We also show numerically the advantage of GMM over k-means for\n",
      "non-trivial cluster data.\n",
      "\n",
      "**Paper Id :1804.10905 \n",
      "Title :An Investigation on Support Vector Clustering for Big Data in Quantum\n",
      "  Paradigm\n",
      "  The support vector clustering algorithm is a well-known clustering algorithm\n",
      "based on support vector machines using Gaussian or polynomial kernels. The\n",
      "classical support vector clustering algorithm works well in general, but its\n",
      "performance degrades when applied on big data. In this paper, we have\n",
      "investigated the performance of support vector clustering algorithm implemented\n",
      "in a quantum paradigm for possible run-time improvements. We have developed and\n",
      "analyzed a quantum version of the support vector clustering algorithm. The\n",
      "proposed approach is based on the quantum support vector machine and quantum\n",
      "kernels (i.e., Gaussian and polynomial). The proposed quantum version of the\n",
      "SVM clustering method demonstrates a significant speed-up gain on the overall\n",
      "run-time complexity as compared to the classical counterpart.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.06660 \n",
      "Title :Learning to play the Chess Variant Crazyhouse above World Champion Level\n",
      "  with Deep Neural Networks and Human Data\n",
      "  Deep neural networks have been successfully applied in learning the board\n",
      "games Go, chess and shogi without prior knowledge by making use of\n",
      "reinforcement learning. Although starting from zero knowledge has been shown to\n",
      "yield impressive results, it is associated with high computationally costs\n",
      "especially for complex games. With this paper, we present CrazyAra which is a\n",
      "neural network based engine solely trained in supervised manner for the chess\n",
      "variant crazyhouse. Crazyhouse is a game with a higher branching factor than\n",
      "chess and there is only limited data of lower quality available compared to\n",
      "AlphaGo. Therefore, we focus on improving efficiency in multiple aspects while\n",
      "relying on low computational resources. These improvements include\n",
      "modifications in the neural network design and training configuration, the\n",
      "introduction of a data normalization step and a more sample efficient\n",
      "Monte-Carlo tree search which has a lower chance to blunder. After training on\n",
      "569,537 human games for 1.5 days we achieve a move prediction accuracy of\n",
      "60.4%. During development, versions of CrazyAra played professional human\n",
      "players. Most notably, CrazyAra achieved a four to one win over 2017 crazyhouse\n",
      "world champion Justin Tan (aka LM Jann Lee) who is more than 400 Elo higher\n",
      "rated compared to the average player in our training set. Furthermore, we test\n",
      "the playing strength of CrazyAra on CPU against all participants of the second\n",
      "Crazyhouse Computer Championships 2017, winning against twelve of the thirteen\n",
      "participants. Finally, for CrazyAraFish we continue training our model on\n",
      "generated engine games. In ten long-time control matches playing Stockfish 10,\n",
      "CrazyAraFish wins three games and draws one out of ten matches.\n",
      "\n",
      "**Paper Id :2003.02372 \n",
      "Title :Dynamic Experience Replay\n",
      "  We present a novel technique called Dynamic Experience Replay (DER) that\n",
      "allows Reinforcement Learning (RL) algorithms to use experience replay samples\n",
      "not only from human demonstrations but also successful transitions generated by\n",
      "RL agents during training and therefore improve training efficiency. It can be\n",
      "combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and\n",
      "their distributed versions. We build upon Ape-X DDPG and demonstrate our\n",
      "approach on robotic tight-fitting joint assembly tasks, based on force/torque\n",
      "and Cartesian pose observations. In particular, we run experiments on two\n",
      "different tasks: peg-in-hole and lap-joint. In each case, we compare different\n",
      "replay buffer structures and how DER affects them. Our ablation studies show\n",
      "that Dynamic Experience Replay is a crucial ingredient that either largely\n",
      "shortens the training time in these challenging environments or solves the\n",
      "tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies\n",
      "learned purely in simulation can be deployed successfully on the real robot.\n",
      "The video presenting our experiments is available at\n",
      "https://sites.google.com/site/dynamicexperiencereplay\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.08649 \n",
      "Title :Adversary-resilient Distributed and Decentralized Statistical Inference\n",
      "  and Machine Learning: An Overview of Recent Advances Under the Byzantine\n",
      "  Threat Model\n",
      "  While the last few decades have witnessed a huge body of work devoted to\n",
      "inference and learning in distributed and decentralized setups, much of this\n",
      "work assumes a non-adversarial setting in which individual nodes---apart from\n",
      "occasional statistical failures---operate as intended within the algorithmic\n",
      "framework. In recent years, however, cybersecurity threats from malicious\n",
      "non-state actors and rogue entities have forced practitioners and researchers\n",
      "to rethink the robustness of distributed and decentralized algorithms against\n",
      "adversarial attacks. As a result, we now have a plethora of algorithmic\n",
      "approaches that guarantee robustness of distributed and/or decentralized\n",
      "inference and learning under different adversarial threat models. Driven in\n",
      "part by the world's growing appetite for data-driven decision making, however,\n",
      "securing of distributed/decentralized frameworks for inference and learning\n",
      "against adversarial threats remains a rapidly evolving research area. In this\n",
      "article, we provide an overview of some of the most recent developments in this\n",
      "area under the threat model of Byzantine attacks.\n",
      "\n",
      "**Paper Id :1905.12762 \n",
      "Title :Securing Connected & Autonomous Vehicles: Challenges Posed by\n",
      "  Adversarial Machine Learning and The Way Forward\n",
      "  Connected and autonomous vehicles (CAVs) will form the backbone of future\n",
      "next-generation intelligent transportation systems (ITS) providing travel\n",
      "comfort, road safety, along with a number of value-added services. Such a\n",
      "transformation---which will be fuelled by concomitant advances in technologies\n",
      "for machine learning (ML) and wireless communications---will enable a future\n",
      "vehicular ecosystem that is better featured and more efficient. However, there\n",
      "are lurking security problems related to the use of ML in such a critical\n",
      "setting where an incorrect ML decision may not only be a nuisance but can lead\n",
      "to loss of precious lives. In this paper, we present an in-depth overview of\n",
      "the various challenges associated with the application of ML in vehicular\n",
      "networks. In addition, we formulate the ML pipeline of CAVs and present various\n",
      "potential security issues associated with the adoption of ML methods. In\n",
      "particular, we focus on the perspective of adversarial ML attacks on CAVs and\n",
      "outline a solution to defend against adversarial attacks in multiple settings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.09345 \n",
      "Title :Almost Tune-Free Variance Reduction\n",
      "  The variance reduction class of algorithms including the representative ones,\n",
      "SVRG and SARAH, have well documented merits for empirical risk minimization\n",
      "problems. However, they require grid search to tune parameters (step size and\n",
      "the number of iterations per inner loop) for optimal performance. This work\n",
      "introduces `almost tune-free' SVRG and SARAH schemes equipped with i)\n",
      "Barzilai-Borwein (BB) step sizes; ii) averaging; and, iii) the inner loop\n",
      "length adjusted to the BB step sizes. In particular, SVRG, SARAH, and their BB\n",
      "variants are first reexamined through an `estimate sequence' lens to enable new\n",
      "averaging methods that tighten their convergence rates theoretically, and\n",
      "improve their performance empirically when the step size or the inner loop\n",
      "length is chosen large. Then a simple yet effective means to adjust the number\n",
      "of iterations per inner loop is developed to enhance the merits of the proposed\n",
      "averaging schemes and BB step sizes. Numerical tests corroborate the proposed\n",
      "methods.\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.09487 \n",
      "Title :Stochastic dynamical modeling of turbulent flows\n",
      "  Advanced measurement techniques and high performance computing have made\n",
      "large data sets available for a wide range of turbulent flows that arise in\n",
      "engineering applications. Drawing on this abundance of data, dynamical models\n",
      "can be constructed to reproduce structural and statistical features of\n",
      "turbulent flows, opening the way to the design of effective model-based flow\n",
      "control strategies. This review describes a framework for completing\n",
      "second-order statistics of turbulent flows by models that are based on the\n",
      "Navier-Stokes equations linearized around the turbulent mean velocity. Systems\n",
      "theory and convex optimization are combined to address the inherent uncertainty\n",
      "in the dynamics and the statistics of the flow by seeking a suitable\n",
      "parsimonious correction to the prior linearized model. Specifically, dynamical\n",
      "couplings between states of the linearized model dictate structural constraints\n",
      "on the statistics of flow fluctuations. Thence, colored-in-time stochastic\n",
      "forcing that drives the linearized model is sought to account for and reconcile\n",
      "dynamics with available data (i.e., partially known second order statistics).\n",
      "The number of dynamical degrees of freedom that are directly affected by\n",
      "stochastic excitation is minimized as a measure of model parsimony. The\n",
      "spectral content of the resulting colored-in-time stochastic contribution can\n",
      "alternatively be seen to arise from a low-rank structural perturbation of the\n",
      "linearized dynamical generator, pointing to suitable dynamical corrections that\n",
      "may account for the absence of the nonlinear interactions in the linearized\n",
      "model.\n",
      "\n",
      "**Paper Id :1908.03620 \n",
      "Title :Learning physics-based reduced-order models for a single-injector\n",
      "  combustion process\n",
      "  This paper presents a physics-based data-driven method to learn predictive\n",
      "reduced-order models (ROMs) from high-fidelity simulations, and illustrates it\n",
      "in the challenging context of a single-injector combustion process. The method\n",
      "combines the perspectives of model reduction and machine learning. Model\n",
      "reduction brings in the physics of the problem, constraining the ROM\n",
      "predictions to lie on a subspace defined by the governing equations. This is\n",
      "achieved by defining the ROM in proper orthogonal decomposition (POD)\n",
      "coordinates, which embed the rich physics information contained in solution\n",
      "snapshots of a high-fidelity computational fluid dynamics (CFD) model. The\n",
      "machine learning perspective brings the flexibility to use transformed physical\n",
      "variables to define the POD basis. This is in contrast to traditional model\n",
      "reduction approaches that are constrained to use the physical variables of the\n",
      "high-fidelity code. Combining the two perspectives, the approach identifies a\n",
      "set of transformed physical variables that expose quadratic structure in the\n",
      "combustion governing equations and learns a quadratic ROM from transformed\n",
      "snapshot data. This learning does not require access to the high-fidelity model\n",
      "implementation. Numerical experiments show that the ROM accurately predicts\n",
      "temperature, pressure, velocity, species concentrations, and the limit-cycle\n",
      "amplitude, with speedups of more than five orders of magnitude over\n",
      "high-fidelity models. Our ROM simulation is shown to be predictive 200% past\n",
      "the training interval. Moreover, ROM-predicted pressure traces accurately match\n",
      "the phase of the pressure signal and yield good approximations of the\n",
      "limit-cycle amplitude.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1908.10525 \n",
      "Title :Linear Convergence of Adaptive Stochastic Gradient Descent\n",
      "  We prove that the norm version of the adaptive stochastic gradient method\n",
      "(AdaGrad-Norm) achieves a linear convergence rate for a subset of either\n",
      "strongly convex functions or non-convex functions that satisfy the Polyak\n",
      "Lojasiewicz (PL) inequality. The paper introduces the notion of Restricted\n",
      "Uniform Inequality of Gradients (RUIG)---which is a measure of the\n",
      "balanced-ness of the stochastic gradient norms---to depict the landscape of a\n",
      "function. RUIG plays a key role in proving the robustness of AdaGrad-Norm to\n",
      "its hyper-parameter tuning in the stochastic setting. On top of RUIG, we\n",
      "develop a two-stage framework to prove the linear convergence of AdaGrad-Norm\n",
      "without knowing the parameters of the objective functions. This framework can\n",
      "likely be extended to other adaptive stepsize algorithms. The numerical\n",
      "experiments validate the theory and suggest future directions for improvement.\n",
      "\n",
      "**Paper Id :1907.11985 \n",
      "Title :The Wang-Landau Algorithm as Stochastic Optimization and Its\n",
      "  Acceleration\n",
      "  We show that the Wang-Landau algorithm can be formulated as a stochastic\n",
      "gradient descent algorithm minimizing a smooth and convex objective function,\n",
      "of which the gradient is estimated using Markov chain Monte Carlo iterations.\n",
      "The optimization formulation provides us a new way to establish the convergence\n",
      "rate of the Wang-Landau algorithm, by exploiting the fact that almost surely,\n",
      "the density estimates (on the logarithmic scale) remain in a compact set, upon\n",
      "which the objective function is strongly convex. The optimization viewpoint\n",
      "motivates us to improve the efficiency of the Wang-Landau algorithm using\n",
      "popular tools including the momentum method and the adaptive learning rate\n",
      "method. We demonstrate the accelerated Wang-Landau algorithm on a\n",
      "two-dimensional Ising model and a two-dimensional ten-state Potts model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.01135 \n",
      "Title :HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep\n",
      "  Learning Techniques on HTML Analysis\n",
      "  Recently, the development and implementation of phishing attacks require\n",
      "little technical skills and costs. This uprising has led to an ever-growing\n",
      "number of phishing attacks on the World Wide Web. Consequently, proactive\n",
      "techniques to fight phishing attacks have become extremely necessary. In this\n",
      "paper, we propose HTMLPhish, a deep learning based data-driven end-to-end\n",
      "automatic phishing web page classification approach. Specifically, HTMLPhish\n",
      "receives the content of the HTML document of a web page and employs\n",
      "Convolutional Neural Networks (CNNs) to learn the semantic dependencies in the\n",
      "textual contents of the HTML. The CNNs learn appropriate feature\n",
      "representations from the HTML document embeddings without extensive manual\n",
      "feature engineering. Furthermore, our proposed approach of the concatenation of\n",
      "the word and character embeddings allows our model to manage new features and\n",
      "ensure easy extrapolation to test data. We conduct comprehensive experiments on\n",
      "a dataset of more than 50,000 HTML documents that provides a distribution of\n",
      "phishing to benign web pages obtainable in the real-world that yields over 93\n",
      "percent Accuracy and True Positive Rate. Also, HTMLPhish is a completely\n",
      "language-independent and client-side strategy which can, therefore, conduct web\n",
      "page phishing detection regardless of the textual language.\n",
      "\n",
      "**Paper Id :2010.01309 \n",
      "Title :Personality Trait Detection Using Bagged SVM over BERT Word Embedding\n",
      "  Ensembles\n",
      "  Recently, the automatic prediction of personality traits has received\n",
      "increasing attention and has emerged as a hot topic within the field of\n",
      "affective computing. In this work, we present a novel deep learning-based\n",
      "approach for automated personality detection from text. We leverage state of\n",
      "the art advances in natural language understanding, namely the BERT language\n",
      "model to extract contextualized word embeddings from textual data for automated\n",
      "author personality detection. Our primary goal is to develop a computationally\n",
      "efficient, high-performance personality prediction model which can be easily\n",
      "used by a large number of people without access to huge computation resources.\n",
      "Our extensive experiments with this ideology in mind, led us to develop a novel\n",
      "model which feeds contextualized embeddings along with psycholinguistic\n",
      "features toa Bagged-SVM classifier for personality trait prediction. Our model\n",
      "outperforms the previous state of the art by 1.04% and, at the same time is\n",
      "significantly more computationally efficient to train. We report our results on\n",
      "the famous gold standard Essays dataset for personality detection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.02108 \n",
      "Title :Quantum Natural Gradient\n",
      "  A quantum generalization of Natural Gradient Descent is presented as part of\n",
      "a general-purpose optimization framework for variational quantum circuits. The\n",
      "optimization dynamics is interpreted as moving in the steepest descent\n",
      "direction with respect to the Quantum Information Geometry, corresponding to\n",
      "the real part of the Quantum Geometric Tensor (QGT), also known as the\n",
      "Fubini-Study metric tensor. An efficient algorithm is presented for computing a\n",
      "block-diagonal approximation to the Fubini-Study metric tensor for parametrized\n",
      "quantum circuits, which may be of independent interest.\n",
      "\n",
      "**Paper Id :1907.11985 \n",
      "Title :The Wang-Landau Algorithm as Stochastic Optimization and Its\n",
      "  Acceleration\n",
      "  We show that the Wang-Landau algorithm can be formulated as a stochastic\n",
      "gradient descent algorithm minimizing a smooth and convex objective function,\n",
      "of which the gradient is estimated using Markov chain Monte Carlo iterations.\n",
      "The optimization formulation provides us a new way to establish the convergence\n",
      "rate of the Wang-Landau algorithm, by exploiting the fact that almost surely,\n",
      "the density estimates (on the logarithmic scale) remain in a compact set, upon\n",
      "which the objective function is strongly convex. The optimization viewpoint\n",
      "motivates us to improve the efficiency of the Wang-Landau algorithm using\n",
      "popular tools including the momentum method and the adaptive learning rate\n",
      "method. We demonstrate the accelerated Wang-Landau algorithm on a\n",
      "two-dimensional Ising model and a two-dimensional ten-state Potts model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.02119 \n",
      "Title :Inductive-bias-driven Reinforcement Learning For Efficient Schedules in\n",
      "  Heterogeneous Clusters\n",
      "  The problem of scheduling of workloads onto heterogeneous processors (e.g.,\n",
      "CPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\n",
      "system schedulers rely on application/system-specific heuristics that have to\n",
      "be built on a case-by-case basis. Recent work has demonstrated ML techniques\n",
      "for automating the heuristic search by using black-box approaches which require\n",
      "significant training data and time, which make them challenging to use in\n",
      "practice. This paper presents Symphony, a scheduling framework that addresses\n",
      "the challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n",
      "(RL) model for scheduling, which inherently models the resource dependencies\n",
      "identified from the system architecture; and (ii) a sampling-based technique to\n",
      "compute the gradients of a Bayesian model without performing full probabilistic\n",
      "inference. Together, these techniques reduce both the amount of training data\n",
      "and the time required to produce scheduling policies that significantly\n",
      "outperform black-box approaches by up to 2.2x.\n",
      "\n",
      "**Paper Id :1810.05934 \n",
      "Title :A System for Massively Parallel Hyperparameter Tuning\n",
      "  Modern learning models are characterized by large hyperparameter spaces and\n",
      "long training times. These properties, coupled with the rise of parallel\n",
      "computing and the growing demand to productionize machine learning workloads,\n",
      "motivate the need to develop mature hyperparameter optimization functionality\n",
      "in distributed computing settings. We address this challenge by first\n",
      "introducing a simple and robust hyperparameter optimization algorithm called\n",
      "ASHA, which exploits parallelism and aggressive early-stopping to tackle\n",
      "large-scale hyperparameter optimization problems. Our extensive empirical\n",
      "results show that ASHA outperforms existing state-of-the-art hyperparameter\n",
      "optimization methods; scales linearly with the number of workers in distributed\n",
      "settings; and is suitable for massive parallelism, as demonstrated on a task\n",
      "with 500 workers. We then describe several design decisions we encountered,\n",
      "along with our associated solutions, when integrating ASHA in Determined AI's\n",
      "end-to-end production-quality machine learning system that offers\n",
      "hyperparameter tuning as a service.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.02249 \n",
      "Title :Generative Machine Learning for Robust Free-Space Communication\n",
      "  Realistic free-space optical communications systems suffer from turbulent\n",
      "propagation of light through the atmosphere and detector noise at the receiver,\n",
      "which can significantly degrade the optical mode quality of the received state,\n",
      "increase cross-talk between modes, and correspondingly increase the symbol\n",
      "error ratio (SER) of the system. In order to overcome these obstacles, we\n",
      "develop a state-of-the-art generative machine learning (GML) and convolutional\n",
      "neural network (CNN) system in combination, and demonstrate its efficacy in a\n",
      "free-space optical (FSO) communications setting. The system corrects for the\n",
      "distortion effects due to turbulence and reduces detector noise, resulting in\n",
      "significantly lowered SERs and cross-talk at the output of the receiver, while\n",
      "requiring no feedback. This scheme is straightforward to scale, and may provide\n",
      "a concrete and cost effective technique to establishing long range classical\n",
      "and quantum communication links in the near future.\n",
      "\n",
      "**Paper Id :2001.11085 \n",
      "Title :Deep Channel Learning For Large Intelligent Surfaces Aided mm-Wave\n",
      "  Massive MIMO Systems\n",
      "  This letter presents the first work introducing a deep learning (DL)\n",
      "framework for channel estimation in large intelligent surface (LIS) assisted\n",
      "massive MIMO (multiple-input multiple-output) systems. A twin convolutional\n",
      "neural network (CNN) architecture is designed and it is fed with the received\n",
      "pilot signals to estimate both direct and cascaded channels. In a multi-user\n",
      "scenario, each user has access to the CNN to estimate its own channel. The\n",
      "performance of the proposed DL approach is evaluated and compared with\n",
      "state-of-the-art DL-based techniques and its superior performance is\n",
      "demonstrated.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.02487 \n",
      "Title :Ab-Initio Solution of the Many-Electron Schr\\\"odinger Equation with Deep\n",
      "  Neural Networks\n",
      "  Given access to accurate solutions of the many-electron Schr\\\"odinger\n",
      "equation, nearly all chemistry could be derived from first principles. Exact\n",
      "wavefunctions of interesting chemical systems are out of reach because they are\n",
      "NP-hard to compute in general, but approximations can be found using\n",
      "polynomially-scaling algorithms. The key challenge for many of these algorithms\n",
      "is the choice of wavefunction approximation, or Ansatz, which must trade off\n",
      "between efficiency and accuracy. Neural networks have shown impressive power as\n",
      "accurate practical function approximators and promise as a compact wavefunction\n",
      "Ansatz for spin systems, but problems in electronic structure require\n",
      "wavefunctions that obey Fermi-Dirac statistics. Here we introduce a novel deep\n",
      "learning architecture, the Fermionic Neural Network, as a powerful wavefunction\n",
      "Ansatz for many-electron systems. The Fermionic Neural Network is able to\n",
      "achieve accuracy beyond other variational quantum Monte Carlo Ans\\\"atze on a\n",
      "variety of atoms and small molecules. Using no data other than atomic positions\n",
      "and charges, we predict the dissociation curves of the nitrogen molecule and\n",
      "hydrogen chain, two challenging strongly-correlated systems, to significantly\n",
      "higher accuracy than the coupled cluster method, widely considered the most\n",
      "accurate scalable method for quantum chemistry at equilibrium geometry. This\n",
      "demonstrates that deep neural networks can improve the accuracy of variational\n",
      "quantum Monte Carlo to the point where it outperforms other ab-initio quantum\n",
      "chemistry methods, opening the possibility of accurate direct optimisation of\n",
      "wavefunctions for previously intractable molecules and solids.\n",
      "\n",
      "**Paper Id :2005.00544 \n",
      "Title :Variational Quantum Eigensolver for Frustrated Quantum Systems\n",
      "  Hybrid quantum-classical algorithms have been proposed as a potentially\n",
      "viable application of quantum computers. A particular example - the variational\n",
      "quantum eigensolver, or VQE - is designed to determine a global minimum in an\n",
      "energy landscape specified by a quantum Hamiltonian, which makes it appealing\n",
      "for the needs of quantum chemistry. Experimental realizations have been\n",
      "reported in recent years and theoretical estimates of its efficiency are a\n",
      "subject of intense effort. Here we consider the performance of the VQE\n",
      "technique for a Hubbard-like model describing a one-dimensional chain of\n",
      "fermions with competing nearest- and next-nearest-neighbor interactions. We\n",
      "find that recovering the VQE solution allows one to obtain the correlation\n",
      "function of the ground state consistent with the exact result. We also study\n",
      "the barren plateau phenomenon for the Hamiltonian in question and find that the\n",
      "severity of this effect depends on the encoding of fermions to qubits. Our\n",
      "results are consistent with the current knowledge about the barren plateaus in\n",
      "quantum optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.02729 \n",
      "Title :A Baseline for Few-Shot Image Classification\n",
      "  Fine-tuning a deep network trained with the standard cross-entropy loss is a\n",
      "strong baseline for few-shot learning. When fine-tuned transductively, this\n",
      "outperforms the current state-of-the-art on standard datasets such as\n",
      "Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same\n",
      "hyper-parameters. The simplicity of this approach enables us to demonstrate the\n",
      "first few-shot learning results on the ImageNet-21k dataset. We find that using\n",
      "a large number of meta-training classes results in high few-shot accuracies\n",
      "even for a large number of few-shot classes. We do not advocate our approach as\n",
      "the solution for few-shot learning, but simply use the results to highlight\n",
      "limitations of current benchmarks and few-shot protocols. We perform extensive\n",
      "studies on benchmark datasets to propose a metric that quantifies the\n",
      "\"hardness\" of a few-shot episode. This metric can be used to report the\n",
      "performance of few-shot algorithms in a more systematic way.\n",
      "\n",
      "**Paper Id :1901.09054 \n",
      "Title :Deep Learning on Small Datasets without Pre-Training using Cosine Loss\n",
      "  Two things seem to be indisputable in the contemporary deep learning\n",
      "discourse: 1. The categorical cross-entropy loss after softmax activation is\n",
      "the method of choice for classification. 2. Training a CNN classifier from\n",
      "scratch on small datasets does not work well. In contrast to this, we show that\n",
      "the cosine loss function provides significantly better performance than\n",
      "cross-entropy on datasets with only a handful of samples per class. For\n",
      "example, the accuracy achieved on the CUB-200-2011 dataset without pre-training\n",
      "is by 30% higher than with the cross-entropy loss. Further experiments on other\n",
      "popular datasets confirm our findings. Moreover, we demonstrate that\n",
      "integrating prior knowledge in the form of class hierarchies is straightforward\n",
      "with the cosine loss and improves classification performance further.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.03681 \n",
      "Title :Outlier Detection in High Dimensional Data\n",
      "  High-dimensional data poses unique challenges in outlier detection process.\n",
      "Most of the existing algorithms fail to properly address the issues stemming\n",
      "from a large number of features. In particular, outlier detection algorithms\n",
      "perform poorly on data set of small size with a large number of features. In\n",
      "this paper, we propose a novel outlier detection algorithm based on principal\n",
      "component analysis and kernel density estimation. The proposed method is\n",
      "designed to address the challenges of dealing with high-dimensional data by\n",
      "projecting the original data onto a smaller space and using the innate\n",
      "structure of the data to calculate anomaly scores for each data point.\n",
      "Numerical experiments on synthetic and real-life data show that our method\n",
      "performs well on high-dimensional data. In particular, the proposed method\n",
      "outperforms the benchmark methods as measured by the $F_1$-score. Our method\n",
      "also produces better-than-average execution times compared to the benchmark\n",
      "methods.\n",
      "\n",
      "**Paper Id :2009.10343 \n",
      "Title :Gamma distribution-based sampling for imbalanced data\n",
      "  Imbalanced class distribution is a common problem in a number of fields\n",
      "including medical diagnostics, fraud detection, and others. It causes bias in\n",
      "classification algorithms leading to poor performance on the minority class\n",
      "data. In this paper, we propose a novel method for balancing the class\n",
      "distribution in data through intelligent resampling of the minority class\n",
      "instances. The proposed method is based on generating new minority instances in\n",
      "the neighborhood of the existing minority points via a gamma distribution. Our\n",
      "method offers a natural and coherent approach to balancing the data. We conduct\n",
      "a comprehensive numerical analysis of the new sampling technique. The\n",
      "experimental results show that the proposed method outperforms the existing\n",
      "state-of-the-art methods for imbalanced data. Concretely, the new sampling\n",
      "technique produces the best results on 12 out of 24 real life as well as\n",
      "synthetic datasets. For comparison, the SMOTE method achieves the top score on\n",
      "only 1 dataset. We conclude that the new technique offers a simple yet\n",
      "effective sampling approach to balance data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.04019 \n",
      "Title :Forecaster: A Graph Transformer for Forecasting Spatial and\n",
      "  Time-Dependent Data\n",
      "  Spatial and time-dependent data is of interest in many applications. This\n",
      "task is difficult due to its complex spatial dependency, long-range temporal\n",
      "dependency, data non-stationarity, and data heterogeneity. To address these\n",
      "challenges, we propose Forecaster, a graph Transformer architecture.\n",
      "Specifically, we start by learning the structure of the graph that\n",
      "parsimoniously represents the spatial dependency between the data at different\n",
      "locations. Based on the topology of the graph, we sparsify the Transformer to\n",
      "account for the strength of spatial dependency, long-range temporal dependency,\n",
      "data non-stationarity, and data heterogeneity. We evaluate Forecaster in the\n",
      "problem of forecasting taxi ride-hailing demand and show that our proposed\n",
      "architecture significantly outperforms the state-of-the-art baselines.\n",
      "\n",
      "**Paper Id :1903.07789 \n",
      "Title :Predicting Citywide Crowd Flows in Irregular Regions Using Multi-View\n",
      "  Graph Convolutional Networks\n",
      "  Being able to predict the crowd flows in each and every part of a city,\n",
      "especially in irregular regions, is strategically important for traffic\n",
      "control, risk assessment, and public safety. However, it is very challenging\n",
      "because of interactions and spatial correlations between different regions. In\n",
      "addition, it is affected by many factors: i) multiple temporal correlations\n",
      "among different time intervals: closeness, period, trend; ii) complex external\n",
      "influential factors: weather, events; iii) meta features: time of the day, day\n",
      "of the week, and so on. In this paper, we formulate crowd flow forecasting in\n",
      "irregular regions as a spatio-temporal graph (STG) prediction problem in which\n",
      "each node represents a region with time-varying flows. By extending graph\n",
      "convolution to handle the spatial information, we propose using spatial graph\n",
      "convolution to build a multi-view graph convolutional network (MVGCN) for the\n",
      "crowd flow forecasting problem, where different views can capture different\n",
      "factors as mentioned above. We evaluate MVGCN using four real-world datasets\n",
      "(taxicabs and bikes) and extensive experimental results show that our approach\n",
      "outperforms the adaptations of state-of-the-art methods. And we have developed\n",
      "a crowd flow forecasting system for irregular regions that can now be used\n",
      "internally.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.04305 \n",
      "Title :Inverse Ising inference from high-temperature re-weighting of\n",
      "  observations\n",
      "  Maximum Likelihood Estimation (MLE) is the bread and butter of system\n",
      "inference for stochastic systems. In some generality, MLE will converge to the\n",
      "correct model in the infinite data limit. In the context of physical approaches\n",
      "to system inference, such as Boltzmann machines, MLE requires the arduous\n",
      "computation of partition functions summing over all configurations, both\n",
      "observed and unobserved. We present here a conceptually and computationally\n",
      "transparent data-driven approach to system inference that is based on the\n",
      "simple question: How should the Boltzmann weights of observed configurations be\n",
      "modified to make the probability distribution of observed configurations close\n",
      "to a flat distribution? This algorithm gives accurate inference by using only\n",
      "observed configurations for systems with a large number of degrees of freedom\n",
      "where other approaches are intractable.\n",
      "\n",
      "**Paper Id :2004.12157 \n",
      "Title :A Bayesian machine scientist to aid in the solution of challenging\n",
      "  scientific problems\n",
      "  Closed-form, interpretable mathematical models have been instrumental for\n",
      "advancing our understanding of the world; with the data revolution, we may now\n",
      "be in a position to uncover new such models for many systems from physics to\n",
      "the social sciences. However, to deal with increasing amounts of data, we need\n",
      "\"machine scientists\" that are able to extract these models automatically from\n",
      "data. Here, we introduce a Bayesian machine scientist, which establishes the\n",
      "plausibility of models using explicit approximations to the exact marginal\n",
      "posterior over models and establishes its prior expectations about models by\n",
      "learning from a large empirical corpus of mathematical expressions. It explores\n",
      "the space of models using Markov chain Monte Carlo. We show that this approach\n",
      "uncovers accurate models for synthetic and real data and provides out-of-sample\n",
      "predictions that are more accurate than those of existing approaches and of\n",
      "other nonparametric methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.04421 \n",
      "Title :Privacy-Preserving Bandits\n",
      "  Contextual bandit algorithms~(CBAs) often rely on personal data to provide\n",
      "recommendations. Centralized CBA agents utilize potentially sensitive data from\n",
      "recent interactions to provide personalization to end-users. Keeping the\n",
      "sensitive data locally, by running a local agent on the user's device, protects\n",
      "the user's privacy, however, the agent requires longer to produce useful\n",
      "recommendations, as it does not leverage feedback from other users. This paper\n",
      "proposes a technique we call Privacy-Preserving Bandits (P2B); a system that\n",
      "updates local agents by collecting feedback from other local agents in a\n",
      "differentially-private manner. Comparisons of our proposed approach with a\n",
      "non-private, as well as a fully-private (local) system, show competitive\n",
      "performance on both synthetic benchmarks and real-world data. Specifically, we\n",
      "observed only a decrease of 2.6% and 3.6% in multi-label classification\n",
      "accuracy, and a CTR increase of 0.0025 in online advertising for a privacy\n",
      "budget $\\epsilon \\approx 0.693$. These results suggest P2B is an effective\n",
      "approach to challenges arising in on-device privacy-preserving personalization.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.04719 \n",
      "Title :Neural Belief Reasoner\n",
      "  This paper proposes a new generative model called neural belief reasoner\n",
      "(NBR). It differs from previous models in that it specifies a belief function\n",
      "rather than a probability distribution. Its implementation consists of neural\n",
      "networks, fuzzy-set operations and belief-function operations, and\n",
      "query-answering, sample-generation and training algorithms are presented. This\n",
      "paper studies NBR in two tasks. The first is a synthetic unsupervised-learning\n",
      "task, which demonstrates NBR's ability to perform multi-hop reasoning,\n",
      "reasoning with uncertainty and reasoning about conflicting information. The\n",
      "second is supervised learning: a robust MNIST classifier for 4 and 9, which is\n",
      "the most challenging pair of digits. This classifier needs no adversarial\n",
      "training, and it substantially exceeds the state of the art in adversarial\n",
      "robustness as measured by the L2 metric, while at the same time maintains 99.1%\n",
      "accuracy on natural images.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.05215 \n",
      "Title :Reconstructing continuous distributions of 3D protein structure from\n",
      "  cryo-EM images\n",
      "  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining\n",
      "the structure of proteins and other macromolecular complexes at near-atomic\n",
      "resolution. In single particle cryo-EM, the central problem is to reconstruct\n",
      "the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and\n",
      "randomly oriented two-dimensional projections. However, the imaged protein\n",
      "complexes may exhibit structural variability, which complicates reconstruction\n",
      "and is typically addressed using discrete clustering approaches that fail to\n",
      "capture the full range of protein dynamics. Here, we introduce a novel method\n",
      "for cryo-EM reconstruction that extends naturally to modeling continuous\n",
      "generative factors of structural heterogeneity. This method encodes structures\n",
      "in Fourier space using coordinate-based deep neural networks, and trains these\n",
      "networks from unlabeled 2D cryo-EM images by combining exact inference over\n",
      "image orientation with variational inference for structural heterogeneity. We\n",
      "demonstrate that the proposed method, termed cryoDRGN, can perform ab initio\n",
      "reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image\n",
      "data. To our knowledge, cryoDRGN is the first neural network-based approach for\n",
      "cryo-EM reconstruction and the first end-to-end method for directly\n",
      "reconstructing continuous ensembles of protein structures from cryo-EM images.\n",
      "\n",
      "**Paper Id :1907.01898 \n",
      "Title :Cryo-EM reconstruction of continuous heterogeneity by Laplacian spectral\n",
      "  volumes\n",
      "  Single-particle electron cryomicroscopy is an essential tool for\n",
      "high-resolution 3D reconstruction of proteins and other biological\n",
      "macromolecules. An important challenge in cryo-EM is the reconstruction of\n",
      "non-rigid molecules with parts that move and deform. Traditional reconstruction\n",
      "methods fail in these cases, resulting in smeared reconstructions of the moving\n",
      "parts. This poses a major obstacle for structural biologists, who need\n",
      "high-resolution reconstructions of entire macromolecules, moving parts\n",
      "included. To address this challenge, we present a new method for the\n",
      "reconstruction of macromolecules exhibiting continuous heterogeneity. The\n",
      "proposed method uses projection images from multiple viewing directions to\n",
      "construct a graph Laplacian through which the manifold of three-dimensional\n",
      "conformations is analyzed. The 3D molecular structures are then expanded in a\n",
      "basis of Laplacian eigenvectors, using a novel generalized tomographic\n",
      "reconstruction algorithm to compute the expansion coefficients. These\n",
      "coefficients, which we name spectral volumes, provide a high-resolution\n",
      "visualization of the molecular dynamics. We provide a theoretical analysis and\n",
      "evaluate the method empirically on several simulated data sets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.07578 \n",
      "Title :Stacking Models for Nearly Optimal Link Prediction in Complex Networks\n",
      "  Most real-world networks are incompletely observed. Algorithms that can\n",
      "accurately predict which links are missing can dramatically speedup the\n",
      "collection of network data and improve the validity of network models. Many\n",
      "algorithms now exist for predicting missing links, given a partially observed\n",
      "network, but it has remained unknown whether a single best predictor exists,\n",
      "how link predictability varies across methods and networks from different\n",
      "domains, and how close to optimality current methods are. We answer these\n",
      "questions by systematically evaluating 203 individual link predictor\n",
      "algorithms, representing three popular families of methods, applied to a large\n",
      "corpus of 548 structurally diverse networks from six scientific domains. We\n",
      "first show that individual algorithms exhibit a broad diversity of prediction\n",
      "errors, such that no one predictor or family is best, or worst, across all\n",
      "realistic inputs. We then exploit this diversity via meta-learning to construct\n",
      "a series of \"stacked\" models that combine predictors into a single algorithm.\n",
      "Applied to a broad range of synthetic networks, for which we may analytically\n",
      "calculate optimal performance, these stacked models achieve optimal or nearly\n",
      "optimal levels of accuracy. Applied to real-world networks, stacked models are\n",
      "also superior, but their accuracy varies strongly by domain, suggesting that\n",
      "link prediction may be fundamentally easier in social networks than in\n",
      "biological or technological networks. These results indicate that the\n",
      "state-of-the-art for link prediction comes from combining individual\n",
      "algorithms, which achieves nearly optimal predictions. We close with a brief\n",
      "discussion of limitations and opportunities for further improvement of these\n",
      "results.\n",
      "\n",
      "**Paper Id :2004.04704 \n",
      "Title :Heuristics for Link Prediction in Multiplex Networks\n",
      "  Link prediction, or the inference of future or missing connections between\n",
      "entities, is a well-studied problem in network analysis. A multitude of\n",
      "heuristics exist for link prediction in ordinary networks with a single type of\n",
      "connection. However, link prediction in multiplex networks, or networks with\n",
      "multiple types of connections, is not a well understood problem. We propose a\n",
      "novel general framework and three families of heuristics for multiplex network\n",
      "link prediction that are simple, interpretable, and take advantage of the rich\n",
      "connection type correlation structure that exists in many real world networks.\n",
      "We further derive a theoretical threshold for determining when to use a\n",
      "different connection type based on the number of links that overlap with an\n",
      "Erdos-Renyi random graph. Through experiments with simulated and real world\n",
      "scientific collaboration, transportation and global trade networks, we\n",
      "demonstrate that the proposed heuristics show increased performance with the\n",
      "richness of connection type correlation structure and significantly outperform\n",
      "their baseline heuristics for ordinary networks with a single connection type.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "**Paper Id :1906.01493 \n",
      "Title :Constructing Energy-efficient Mixed-precision Neural Networks through\n",
      "  Principal Component Analysis for Edge Intelligence\n",
      "  The `Internet of Things' has brought increased demand for AI-based edge\n",
      "computing in applications ranging from healthcare monitoring systems to\n",
      "autonomous vehicles. Quantization is a powerful tool to address the growing\n",
      "computational cost of such applications, and yields significant compression\n",
      "over full-precision networks. However, quantization can result in substantial\n",
      "loss of performance for complex image classification tasks. To address this, we\n",
      "propose a Principal Component Analysis (PCA) driven methodology to identify the\n",
      "important layers of a binary network, and design mixed-precision networks. The\n",
      "proposed Hybrid-Net achieves a more than 10% improvement in classification\n",
      "accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures\n",
      "on CIFAR-100 and ImageNet datasets while still achieving up to 94% of the\n",
      "energy-efficiency of XNOR-Nets. This work furthers the feasibility of using\n",
      "highly compressed neural networks for energy-efficient neural computing in edge\n",
      "devices.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.10387 \n",
      "Title :An Adversarial Approach to Private Flocking in Mobile Robot Teams\n",
      "  Privacy is an important facet of defence against adversaries. In this letter,\n",
      "we introduce the problem of private flocking. We consider a team of mobile\n",
      "robots flocking in the presence of an adversary, who is able to observe all\n",
      "robots' trajectories, and who is interested in identifying the leader. We\n",
      "present a method that generates private flocking controllers that hide the\n",
      "identity of the leader robot. Our approach towards privacy leverages a\n",
      "data-driven adversarial co-optimization scheme. We design a mechanism that\n",
      "optimizes flocking control parameters, such that leader inference is hindered.\n",
      "As the flocking performance improves, we successively train an adversarial\n",
      "discriminator that tries to infer the identity of the leader robot. To evaluate\n",
      "the performance of our co-optimization scheme, we investigate different classes\n",
      "of reference trajectories. Although it is reasonable to assume that there is an\n",
      "inherent trade-off between flocking performance and privacy, our results\n",
      "demonstrate that we are able to achieve high flocking performance and\n",
      "simultaneously reduce the risk of revealing the leader.\n",
      "\n",
      "**Paper Id :2010.13032 \n",
      "Title :Byzantine Resilient Distributed Multi-Task Learning\n",
      "  Distributed multi-task learning provides significant advantages in\n",
      "multi-agent networks with heterogeneous data sources where agents aim to learn\n",
      "distinct but correlated models simultaneously. However, distributed algorithms\n",
      "for learning relatedness among tasks are not resilient in the presence of\n",
      "Byzantine agents. In this paper, we present an approach for Byzantine resilient\n",
      "distributed multi-task learning. We propose an efficient online weight\n",
      "assignment rule by measuring the accumulated loss using an agent's data and its\n",
      "neighbors' models. A small accumulated loss indicates a large similarity\n",
      "between the two tasks. In order to ensure the Byzantine resilience of the\n",
      "aggregation at a normal agent, we introduce a step for filtering out larger\n",
      "losses. We analyze the approach for convex models and show that normal agents\n",
      "converge resiliently towards their true targets. Further, an agent's learning\n",
      "performance using the proposed weight assignment rule is guaranteed to be at\n",
      "least as good as in the non-cooperative case as measured by the expected\n",
      "regret. Finally, we demonstrate the approach using three case studies,\n",
      "including regression and classification problems, and show that our method\n",
      "exhibits good empirical performance for non-convex models, such as\n",
      "convolutional neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.10389 \n",
      "Title :Machine Learning Pipelines with Modern Big Data Tools for High Energy\n",
      "  Physics\n",
      "  The effective utilization at scale of complex machine learning (ML)\n",
      "techniques for HEP use cases poses several technological challenges, most\n",
      "importantly on the actual implementation of dedicated end-to-end data\n",
      "pipelines. A solution to these challenges is presented, which allows training\n",
      "neural network classifiers using solutions from the Big Data and data science\n",
      "ecosystems, integrated with tools, software, and platforms common in the HEP\n",
      "environment. In particular, Apache Spark is exploited for data preparation and\n",
      "feature engineering, running the corresponding (Python) code interactively on\n",
      "Jupyter notebooks. Key integrations and libraries that make Spark capable of\n",
      "ingesting data stored using ROOT format and accessed via the XRootD protocol,\n",
      "are described and discussed. Training of the neural network models, defined\n",
      "using the Keras API, is performed in a distributed fashion on Spark clusters by\n",
      "using BigDL with Analytics Zoo and also by using TensorFlow, notably for\n",
      "distributed training on CPU and GPU resourcess. The implementation and the\n",
      "results of the distributed training are described in detail in this work.\n",
      "\n",
      "**Paper Id :1904.06517 \n",
      "Title :Improving detection of protein-ligand binding sites with 3D segmentation\n",
      "  In recent years machine learning (ML) took bio- and cheminformatics fields by\n",
      "storm, providing new solutions for a vast repertoire of problems related to\n",
      "protein sequence, structure, and interactions analysis. ML techniques, deep\n",
      "neural networks especially, were proven more effective than classical models\n",
      "for tasks like predicting binding affinity for molecular complex. In this work\n",
      "we investigated the earlier stage of drug discovery process - finding druggable\n",
      "pockets on protein surface, that can be later used to design active molecules.\n",
      "For this purpose we developed a 3D fully convolutional neural network capable\n",
      "of binding site segmentation. Our solution has high prediction accuracy and\n",
      "provides intuitive representations of the results, which makes it easy to\n",
      "incorporate into drug discovery projects. The model's source code, together\n",
      "with scripts for most common use-cases is freely available at\n",
      "http://gitlab.com/cheminfIBB/kalasanty\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.10572 \n",
      "Title :Hypernym Detection Using Strict Partial Order Networks\n",
      "  This paper introduces Strict Partial Order Networks (SPON), a novel neural\n",
      "network architecture designed to enforce asymmetry and transitive properties as\n",
      "soft constraints. We apply it to induce hypernymy relations by training with\n",
      "is-a pairs. We also present an augmented variant of SPON that can generalize\n",
      "type information learned for in-vocabulary terms to previously unseen ones. An\n",
      "extensive evaluation over eleven benchmarks across different tasks shows that\n",
      "SPON consistently either outperforms or attains the state of the art on all but\n",
      "one of these benchmarks.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.10707 \n",
      "Title :Invariant Transform Experience Replay: Data Augmentation for Deep\n",
      "  Reinforcement Learning\n",
      "  Deep Reinforcement Learning (RL) is a promising approach for adaptive robot\n",
      "control, but its current application to robotics is currently hindered by high\n",
      "sample requirements. To alleviate this issue, we propose to exploit the\n",
      "symmetries present in robotic tasks. Intuitively, symmetries from observed\n",
      "trajectories define transformations that leave the space of feasible RL\n",
      "trajectories invariant and can be used to generate new feasible trajectories,\n",
      "which could be used for training. Based on this data augmentation idea, we\n",
      "formulate a general framework, called Invariant Transform Experience Replay\n",
      "that we present with two techniques: (i) Kaleidoscope Experience Replay\n",
      "exploits reflectional symmetries and (ii) Goal-augmented Experience Replay\n",
      "which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI\n",
      "Gym, our experimental results show significant increases in learning rates and\n",
      "success rates. Particularly, we attain a 13, 3, and 5 times speedup in the\n",
      "pushing, sliding, and pick-and-place tasks respectively in the multi-goal\n",
      "setting. Performance gains are also observed in similar tasks with obstacles\n",
      "and we successfully deployed a trained policy on a real Baxter robot. Our work\n",
      "demonstrates that invariant transformations on RL trajectories are a promising\n",
      "methodology to speed up learning in deep RL.\n",
      "\n",
      "**Paper Id :2003.02372 \n",
      "Title :Dynamic Experience Replay\n",
      "  We present a novel technique called Dynamic Experience Replay (DER) that\n",
      "allows Reinforcement Learning (RL) algorithms to use experience replay samples\n",
      "not only from human demonstrations but also successful transitions generated by\n",
      "RL agents during training and therefore improve training efficiency. It can be\n",
      "combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and\n",
      "their distributed versions. We build upon Ape-X DDPG and demonstrate our\n",
      "approach on robotic tight-fitting joint assembly tasks, based on force/torque\n",
      "and Cartesian pose observations. In particular, we run experiments on two\n",
      "different tasks: peg-in-hole and lap-joint. In each case, we compare different\n",
      "replay buffer structures and how DER affects them. Our ablation studies show\n",
      "that Dynamic Experience Replay is a crucial ingredient that either largely\n",
      "shortens the training time in these challenging environments or solves the\n",
      "tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies\n",
      "learned purely in simulation can be deployed successfully on the real robot.\n",
      "The video presenting our experiments is available at\n",
      "https://sites.google.com/site/dynamicexperiencereplay\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.11197 \n",
      "Title :Graph-Partitioning-Based Diffusion Convolutional Recurrent Neural\n",
      "  Network for Large-Scale Traffic Forecasting\n",
      "  Traffic forecasting approaches are critical to developing adaptive strategies\n",
      "for mobility. Traffic patterns have complex spatial and temporal dependencies\n",
      "that make accurate forecasting on large highway networks a challenging task.\n",
      "Recently, diffusion convolutional recurrent neural networks (DCRNNs) have\n",
      "achieved state-of-the-art results in traffic forecasting by capturing the\n",
      "spatiotemporal dynamics of the traffic. Despite the promising results, however,\n",
      "applying DCRNNs for large highway networks still remains elusive because of\n",
      "computational and memory bottlenecks. We present an approach for implementing a\n",
      "DCRNN for a large highway network that overcomes these limitations. Our\n",
      "approach uses a graph-partitioning method to decompose a large highway network\n",
      "into smaller networks and trains them independently. We demonstrate the\n",
      "efficacy of the graph-partitioning-based DCRNN approach to model the traffic on\n",
      "a large California highway network with 11,160 sensor locations. We develop an\n",
      "overlapping nodes approach for the graph-partitioning-based DCRNN to include\n",
      "sensor locations from partitions that are geographically close to a given\n",
      "partition. Furthermore, we demonstrate that the DCRNN model can be used to\n",
      "forecast the speed and flow simultaneously and that the forecasted values\n",
      "preserve fundamental traffic flow dynamics. Our approach to developing DCRNN\n",
      "models that represent large highway networks can be a potential core capability\n",
      "in advanced highway traffic monitoring systems, where a trained DCRNN model\n",
      "forecasting traffic at all sensor locations can be used to adjust traffic\n",
      "management strategies proactively based on anticipated future conditions.\n",
      "\n",
      "**Paper Id :2011.02179 \n",
      "Title :Node-Centric Graph Learning from Data for Brain State Identification\n",
      "  Data-driven graph learning models a network by determining the strength of\n",
      "connections between its nodes. The data refers to a graph signal which\n",
      "associates a value with each graph node. Existing graph learning methods either\n",
      "use simplified models for the graph signal, or they are prohibitively expensive\n",
      "in terms of computational and memory requirements. This is particularly true\n",
      "when the number of nodes is high or there are temporal changes in the network.\n",
      "In order to consider richer models with a reasonable computational\n",
      "tractability, we introduce a graph learning method based on representation\n",
      "learning on graphs. Representation learning generates an embedding for each\n",
      "graph node, taking the information from neighbouring nodes into account. Our\n",
      "graph learning method further modifies the embeddings to compute the graph\n",
      "similarity matrix. In this work, graph learning is used to examine brain\n",
      "networks for brain state identification. We infer time-varying brain graphs\n",
      "from an extensive dataset of intracranial electroencephalographic (iEEG)\n",
      "signals from ten patients. We then apply the graphs as input to a classifier to\n",
      "distinguish seizure vs. non-seizure brain states. Using the binary\n",
      "classification metric of area under the receiver operating characteristic curve\n",
      "(AUC), this approach yields an average of 9.13 percent improvement when\n",
      "compared to two widely used brain network modeling methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.11212 \n",
      "Title :Augmenting the Pathology Lab: An Intelligent Whole Slide Image\n",
      "  Classification System for the Real World\n",
      "  Standard of care diagnostic procedure for suspected skin cancer is\n",
      "microscopic examination of hematoxylin \\& eosin stained tissue by a\n",
      "pathologist. Areas of high inter-pathologist discordance and rising biopsy\n",
      "rates necessitate higher efficiency and diagnostic reproducibility. We present\n",
      "and validate a deep learning system which classifies digitized dermatopathology\n",
      "slides into 4 categories. The system is developed using 5,070 images from a\n",
      "single lab, and tested on an uncurated set of 13,537 images from 3 test labs,\n",
      "using whole slide scanners manufactured by 3 different vendors. The system's\n",
      "use of deep-learning-based confidence scoring as a criterion to consider the\n",
      "result as accurate yields an accuracy of up to 98\\%, and makes it adoptable in\n",
      "a real-world setting. Without confidence scoring, the system achieved an\n",
      "accuracy of 78\\%. We anticipate that our deep learning system will serve as a\n",
      "foundation enabling faster diagnosis of skin cancer, identification of cases\n",
      "for specialist review, and targeted diagnostic classifications.\n",
      "\n",
      "**Paper Id :1904.01949 \n",
      "Title :Automatic diagnosis of the 12-lead ECG using a deep neural network\n",
      "  The role of automatic electrocardiogram (ECG) analysis in clinical practice\n",
      "is limited by the accuracy of existing models. Deep Neural Networks (DNNs) are\n",
      "models composed of stacked transformations that learn tasks by examples. This\n",
      "technology has recently achieved striking success in a variety of task and\n",
      "there are great expectations on how it might improve clinical practice. Here we\n",
      "present a DNN model trained in a dataset with more than 2 million labeled exams\n",
      "analyzed by the Telehealth Network of Minas Gerais and collected under the\n",
      "scope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The\n",
      "DNN outperform cardiology resident medical doctors in recognizing 6 types of\n",
      "abnormalities in 12-lead ECG recordings, with F1 scores above 80% and\n",
      "specificity over 99%. These results indicate ECG analysis based on DNNs,\n",
      "previously studied in a single-lead setup, generalizes well to 12-lead exams,\n",
      "taking the technology closer to the standard clinical practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.11655 \n",
      "Title :Augmenting Genetic Algorithms with Deep Neural Networks for Exploring\n",
      "  the Chemical Space\n",
      "  Challenges in natural sciences can often be phrased as optimization problems.\n",
      "Machine learning techniques have recently been applied to solve such problems.\n",
      "One example in chemistry is the design of tailor-made organic materials and\n",
      "molecules, which requires efficient methods to explore the chemical space. We\n",
      "present a genetic algorithm (GA) that is enhanced with a neural network (DNN)\n",
      "based discriminator model to improve the diversity of generated molecules and\n",
      "at the same time steer the GA. We show that our algorithm outperforms other\n",
      "generative models in optimization tasks. We furthermore present a way to\n",
      "increase interpretability of genetic algorithms, which helped us to derive\n",
      "design principles.\n",
      "\n",
      "**Paper Id :2004.06874 \n",
      "Title :Understanding Aesthetic Evaluation using Deep Learning\n",
      "  A bottleneck in any evolutionary art system is aesthetic evaluation. Many\n",
      "different methods have been proposed to automate the evaluation of aesthetics,\n",
      "including measures of symmetry, coherence, complexity, contrast and grouping.\n",
      "The interactive genetic algorithm (IGA) relies on human-in-the-loop, subjective\n",
      "evaluation of aesthetics, but limits possibilities for large search due to user\n",
      "fatigue and small population sizes. In this paper we look at how recent\n",
      "advances in deep learning can assist in automating personal aesthetic\n",
      "judgement. Using a leading artist's computer art dataset, we use dimensionality\n",
      "reduction methods to visualise both genotype and phenotype space in order to\n",
      "support the exploration of new territory in any generative system.\n",
      "Convolutional Neural Networks trained on the user's prior aesthetic evaluations\n",
      "are used to suggest new possibilities similar or between known high quality\n",
      "genotype-phenotype mappings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.11723 \n",
      "Title :Revisit Knowledge Distillation: a Teacher-free Framework\n",
      "  Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome\n",
      "teacher model into a lightweight student model. Its success is generally\n",
      "attributed to the privileged information on similarities among categories\n",
      "provided by the teacher model, and in this sense, only strong teacher models\n",
      "are deployed to teach weaker students in practice. In this work, we challenge\n",
      "this common belief by following experimental observations: 1) beyond the\n",
      "acknowledgment that the teacher can improve the student, the student can also\n",
      "enhance the teacher significantly by reversing the KD procedure; 2) a\n",
      "poorly-trained teacher with much lower accuracy than the student can still\n",
      "improve the latter significantly. To explain these observations, we provide a\n",
      "theoretical analysis of the relationships between KD and label smoothing\n",
      "regularization. We prove that 1) KD is a type of learned label smoothing\n",
      "regularization and 2) label smoothing regularization provides a virtual teacher\n",
      "model for KD. From these results, we argue that the success of KD is not fully\n",
      "due to the similarity information between categories, but also to the\n",
      "regularization of soft targets, which is equally or even more important.\n",
      "  Based on these analyses, we further propose a novel Teacher-free Knowledge\n",
      "Distillation (Tf-KD) framework, where a student model learns from itself or\n",
      "manually-designed regularization distribution. The Tf-KD achieves comparable\n",
      "performance with normal KD from a superior teacher, which is well applied when\n",
      "teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly\n",
      "deployed for training deep neural networks. Without any extra computation cost,\n",
      "Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established\n",
      "baseline models, which is superior to label smoothing regularization. The codes\n",
      "are in: \\url{https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation}\n",
      "\n",
      "**Paper Id :1911.03437 \n",
      "Title :SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language\n",
      "  Models through Principled Regularized Optimization\n",
      "  Transfer learning has fundamentally changed the landscape of natural language\n",
      "processing (NLP) research. Many existing state-of-the-art models are first\n",
      "pre-trained on a large text corpus and then fine-tuned on downstream tasks.\n",
      "However, due to limited data resources from downstream tasks and the extremely\n",
      "large capacity of pre-trained models, aggressive fine-tuning often causes the\n",
      "adapted model to overfit the data of downstream tasks and forget the knowledge\n",
      "of the pre-trained model. To address the above issue in a more principled\n",
      "manner, we propose a new computational framework for robust and efficient\n",
      "fine-tuning for pre-trained language models. Specifically, our proposed\n",
      "framework contains two important ingredients: 1. Smoothness-inducing\n",
      "regularization, which effectively manages the capacity of the model; 2. Bregman\n",
      "proximal point optimization, which is a class of trust-region methods and can\n",
      "prevent knowledge forgetting. Our experiments demonstrate that our proposed\n",
      "method achieves the state-of-the-art performance on multiple NLP benchmarks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.11887 \n",
      "Title :Information Scrambling in Quantum Neural Networks\n",
      "  The quantum neural network is one of the promising applications for near-term\n",
      "noisy intermediate-scale quantum computers. A quantum neural network distills\n",
      "the information from the input wavefunction into the output qubits. In this\n",
      "Letter, we show that this process can also be viewed from the opposite\n",
      "direction: the quantum information in the output qubits is scrambled into the\n",
      "input. This observation motivates us to use the tripartite information, a\n",
      "quantity recently developed to characterize information scrambling, to diagnose\n",
      "the training dynamics of quantum neural networks. We empirically find strong\n",
      "correlation between the dynamical behavior of the tripartite information and\n",
      "the loss function in the training process, from which we identify that the\n",
      "training process has two stages for randomly initialized networks. In the early\n",
      "stage, the network performance improves rapidly and the tripartite information\n",
      "increases linearly with a universal slope, meaning that the neural network\n",
      "becomes less scrambled than the random unitary. In the latter stage, the\n",
      "network performance improves slowly while the tripartite information decreases.\n",
      "We present evidences that the network constructs local correlations in the\n",
      "early stage and learns large-scale structures in the latter stage. We believe\n",
      "this two-stage training dynamics is universal and is applicable to a wide range\n",
      "of problems. Our work builds bridges between two research subjects of quantum\n",
      "neural networks and information scrambling, which opens up a new perspective to\n",
      "understand quantum neural networks.\n",
      "\n",
      "**Paper Id :2011.07929 \n",
      "Title :On the equivalence of molecular graph convolution and molecular wave\n",
      "  function with poor basis set\n",
      "  In this study, we demonstrate that the linear combination of atomic orbitals\n",
      "(LCAO), an approximation of quantum physics introduced by Pauling and\n",
      "Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)\n",
      "for molecules. However, GCNs involve unnecessary nonlinearity and deep\n",
      "architecture. We also verify that molecular GCNs are based on a poor basis\n",
      "function set compared with the standard one used in theoretical calculations or\n",
      "quantum chemical simulations. From these observations, we describe the quantum\n",
      "deep field (QDF), a machine learning (ML) model based on an underlying quantum\n",
      "physics, in particular the density functional theory (DFT). We believe that the\n",
      "QDF model can be easily understood because it can be regarded as a single\n",
      "linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to\n",
      "learn an energy functional and a Hohenberg--Kohn map that have nonlinearities\n",
      "inherent in quantum physics and the DFT. For molecular energy prediction tasks,\n",
      "we demonstrated the viability of an ``extrapolation,'' in which we trained a\n",
      "QDF model with small molecules, tested it with large molecules, and achieved\n",
      "high extrapolation performance. This will lead to reliable and practical\n",
      "applications for discovering effective materials. The implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.12009 \n",
      "Title :Complex Network based Supervised Keyword Extractor\n",
      "  In this paper, we present a supervised framework for automatic keyword\n",
      "extraction from single document. We model the text as complex network, and\n",
      "construct the feature set by extracting select node properties from it. Several\n",
      "node properties have been exploited by unsupervised, graph-based keyword\n",
      "extraction methods to discriminate keywords from non-keywords. We exploit the\n",
      "complex interplay of node properties to design a supervised keyword extraction\n",
      "method. The training set is created from the feature set by assigning a label\n",
      "to each candidate keyword depending on whether the candidate is listed as a\n",
      "gold-standard keyword or not. Since the number of keywords in a document is\n",
      "much less than non-keywords, the curated training set is naturally imbalanced.\n",
      "We train a binary classifier to predict keywords after balancing the training\n",
      "set. The model is trained using two public datasets from scientific domain and\n",
      "tested using three unseen scientific corpora and one news corpus. Comparative\n",
      "study of the results with several recent keyword and keyphrase extraction\n",
      "methods establishes that the proposed method performs better in most cases.\n",
      "This substantiates our claim that graph-theoretic properties of words are\n",
      "effective discriminators between keywords and non-keywords. We support our\n",
      "argument by showing that the improved performance of the proposed method is\n",
      "statistically significant for all datasets. We also evaluate the effectiveness\n",
      "of the pre-trained model on Hindi and Assamese language documents. We observe\n",
      "that the model performs equally well for the cross-language text even though it\n",
      "was trained only on English language documents. This shows that the proposed\n",
      "method is independent of the domain, collection, and language of the training\n",
      "corpora.\n",
      "\n",
      "**Paper Id :1910.13321 \n",
      "Title :Semantic Object Accuracy for Generative Text-to-Image Synthesis\n",
      "  Generative adversarial networks conditioned on textual image descriptions are\n",
      "capable of generating realistic-looking images. However, current methods still\n",
      "struggle to generate images based on complex image captions from a\n",
      "heterogeneous domain. Furthermore, quantitatively evaluating these\n",
      "text-to-image models is challenging, as most evaluation metrics only judge\n",
      "image quality but not the conformity between the image and its caption. To\n",
      "address these challenges we introduce a new model that explicitly models\n",
      "individual objects within an image and a new evaluation metric called Semantic\n",
      "Object Accuracy (SOA) that specifically evaluates images given an image\n",
      "caption. The SOA uses a pre-trained object detector to evaluate if a generated\n",
      "image contains objects that are mentioned in the image caption, e.g. whether an\n",
      "image generated from \"a car driving down the street\" contains a car. We perform\n",
      "a user study comparing several text-to-image models and show that our SOA\n",
      "metric ranks the models the same way as humans, whereas other metrics such as\n",
      "the Inception Score do not. Our evaluation also shows that models which\n",
      "explicitly model objects outperform models which only model global image\n",
      "characteristics.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.12200 \n",
      "Title :Scaling data-driven robotics with reward sketching and batch\n",
      "  reinforcement learning\n",
      "  We present a framework for data-driven robotics that makes use of a large\n",
      "dataset of recorded robot experience and scales to several tasks using learned\n",
      "reward functions. We show how to apply this framework to accomplish three\n",
      "different object manipulation tasks on a real robot platform. Given\n",
      "demonstrations of a task together with task-agnostic recorded experience, we\n",
      "use a special form of human annotation as supervision to learn a reward\n",
      "function, which enables us to deal with real-world tasks where the reward\n",
      "signal cannot be acquired directly. Learned rewards are used in combination\n",
      "with a large dataset of experience from different tasks to learn a robot policy\n",
      "offline using batch RL. We show that using our approach it is possible to train\n",
      "agents to perform a variety of challenging manipulation tasks including\n",
      "stacking rigid objects and handling cloth.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.12892 \n",
      "Title :Automated curricula through setter-solver interactions\n",
      "  Reinforcement learning algorithms use correlations between policies and\n",
      "rewards to improve agent performance. But in dynamic or sparsely rewarding\n",
      "environments these correlations are often too small, or rewarding events are\n",
      "too infrequent to make learning feasible. Human education instead relies on\n",
      "curricula--the breakdown of tasks into simpler, static challenges with dense\n",
      "rewards--to build up to complex behaviors. While curricula are also useful for\n",
      "artificial agents, hand-crafting them is time consuming. This has lead\n",
      "researchers to explore automatic curriculum generation. Here we explore\n",
      "automatic curriculum generation in rich, dynamic environments. Using a\n",
      "setter-solver paradigm we show the importance of considering goal validity,\n",
      "goal feasibility, and goal coverage to construct useful curricula. We\n",
      "demonstrate the success of our approach in rich but sparsely rewarding 2D and\n",
      "3D environments, where an agent is tasked to achieve a single goal selected\n",
      "from a set of possible goals that varies between episodes, and identify\n",
      "challenges for future work. Finally, we demonstrate the value of a novel\n",
      "technique that guides agents towards a desired goal distribution. Altogether,\n",
      "these results represent a substantial step towards applying automatic task\n",
      "curricula to learn complex, otherwise unlearnable goals, and to our knowledge\n",
      "are the first to demonstrate automated curriculum generation for\n",
      "goal-conditioned agents in environments where the possible goals vary between\n",
      "episodes.\n",
      "\n",
      "**Paper Id :2002.06306 \n",
      "Title :Jelly Bean World: A Testbed for Never-Ending Learning\n",
      "  Machine learning has shown growing success in recent years. However, current\n",
      "machine learning systems are highly specialized, trained for particular\n",
      "problems or domains, and typically on a single narrow dataset. Human learning,\n",
      "on the other hand, is highly general and adaptable. Never-ending learning is a\n",
      "machine learning paradigm that aims to bridge this gap, with the goal of\n",
      "encouraging researchers to design machine learning systems that can learn to\n",
      "perform a wider variety of inter-related tasks in more complex environments. To\n",
      "date, there is no environment or testbed to facilitate the development and\n",
      "evaluation of never-ending learning systems. To this end, we propose the Jelly\n",
      "Bean World testbed. The Jelly Bean World allows experimentation over\n",
      "two-dimensional grid worlds which are filled with items and in which agents can\n",
      "navigate. This testbed provides environments that are sufficiently complex and\n",
      "where more generally intelligent algorithms ought to perform better than\n",
      "current state-of-the-art reinforcement learning approaches. It does so by\n",
      "producing non-stationary environments and facilitating experimentation with\n",
      "multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope\n",
      "that this new freely-available software will prompt new research and interest\n",
      "in the development and evaluation of never-ending learning systems and more\n",
      "broadly, general intelligence systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.13334 \n",
      "Title :Symplectic Recurrent Neural Networks\n",
      "  We propose Symplectic Recurrent Neural Networks (SRNNs) as learning\n",
      "algorithms that capture the dynamics of physical systems from observed\n",
      "trajectories. An SRNN models the Hamiltonian function of the system by a neural\n",
      "network and furthermore leverages symplectic integration, multiple-step\n",
      "training and initial state optimization to address the challenging numerical\n",
      "issues associated with Hamiltonian systems. We show that SRNNs succeed reliably\n",
      "on complex and noisy Hamiltonian systems. We also show how to augment the SRNN\n",
      "integration scheme in order to handle stiff dynamical systems such as bouncing\n",
      "billiards.\n",
      "\n",
      "**Paper Id :1910.09349 \n",
      "Title :Variational Integrator Networks for Physically Structured Embeddings\n",
      "  Learning workable representations of dynamical systems is becoming an\n",
      "increasingly important problem in a number of application areas. By leveraging\n",
      "recent work connecting deep neural networks to systems of differential\n",
      "equations, we propose \\emph{variational integrator networks}, a class of neural\n",
      "network architectures designed to preserve the geometric structure of physical\n",
      "systems. This class of network architectures facilitates accurate long-term\n",
      "prediction, interpretability, and data-efficient learning, while still\n",
      "remaining highly flexible and capable of modeling complex behavior. We\n",
      "demonstrate that they can accurately learn dynamical systems from both noisy\n",
      "observations in phase space and from image pixels within which the unknown\n",
      "dynamics are embedded.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.13408 \n",
      "Title :Multi-classifier prediction of knee osteoarthritis progression from\n",
      "  incomplete imbalanced longitudinal data\n",
      "  Conventional inclusion criteria used in osteoarthritis clinical trials are\n",
      "not very effective in selecting patients who would benefit from a therapy being\n",
      "tested. Typically majority of selected patients show no or limited disease\n",
      "progression during a trial period. As a consequence, the effect of the tested\n",
      "treatment cannot be observed, and the efforts and resources invested in running\n",
      "the trial are not rewarded. This could be avoided, if selection criteria were\n",
      "more predictive of the future disease progression.\n",
      "  In this article, we formulated the patient selection problem as a multi-class\n",
      "classification task, with classes based on clinically relevant measures of\n",
      "progression (over a time scale typical for clinical trials). Using data from\n",
      "two long-term knee osteoarthritis studies OAI and CHECK, we tested multiple\n",
      "algorithms and learning process configurations (including multi-classifier\n",
      "approaches, cost-sensitive learning, and feature selection), to identify the\n",
      "best performing machine learning models. We examined the behaviour of the best\n",
      "models, with respect to prediction errors and the impact of used features, to\n",
      "confirm their clinical relevance. We found that the model-based selection\n",
      "outperforms the conventional inclusion criteria, reducing by 20-25% the number\n",
      "of patients who show no progression. This result might lead to more efficient\n",
      "clinical trials.\n",
      "\n",
      "**Paper Id :2011.09801 \n",
      "Title :Novel Classification of Ischemic Heart Disease Using Artificial Neural\n",
      "  Network\n",
      "  Ischemic heart disease (IHD), particularly in its chronic stable form, is a\n",
      "subtle pathology due to its silent behavior before developing in unstable\n",
      "angina, myocardial infarction or sudden cardiac death. Machine learning\n",
      "techniques applied to parameters extracted form heart rate variability (HRV)\n",
      "signal seem to be a valuable support in the early diagnosis of some cardiac\n",
      "diseases. However, so far, IHD patients were identified using Artificial Neural\n",
      "Networks (ANNs) applied to a limited number of HRV parameters and only to very\n",
      "few subjects. In this study, we used several linear and non-linear HRV\n",
      "parameters applied to ANNs, in order to confirm these results on a large cohort\n",
      "of 965 sample of subjects and to identify which features could discriminate IHD\n",
      "patients with high accuracy. By using principal component analysis and stepwise\n",
      "regression, we reduced the original 17 parameters to five, used as inputs, for\n",
      "a series of ANNs. The highest accuracy of 82% was achieved using meanRR, LFn,\n",
      "SD1, gender and age parameters and two hidden neurons.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1909.13768 \n",
      "Title :Backpropagation in the Simply Typed Lambda-calculus with Linear Negation\n",
      "  Backpropagation is a classic automatic differentiation algorithm computing\n",
      "the gradient of functions specified by a certain class of simple, first-order\n",
      "programs, called computational graphs. It is a fundamental tool in several\n",
      "fields, most notably machine learning, where it is the key for efficiently\n",
      "training (deep) neural networks. Recent years have witnessed the quick growth\n",
      "of a research field called differentiable programming, the aim of which is to\n",
      "express computational graphs more synthetically and modularly by resorting to\n",
      "actual programming languages endowed with control flow operators and\n",
      "higher-order combinators, such as map and fold. In this paper, we extend the\n",
      "backpropagation algorithm to a paradigmatic example of such a programming\n",
      "language: we define a compositional program transformation from the\n",
      "simply-typed lambda-calculus to itself augmented with a notion of linear\n",
      "negation, and prove that this computes the gradient of the source program with\n",
      "the same efficiency as first-order backpropagation. The transformation is\n",
      "completely effect-free and thus provides a purely logical understanding of the\n",
      "dynamics of backpropagation.\n",
      "\n",
      "**Paper Id :2001.05559 \n",
      "Title :Mode-Assisted Unsupervised Learning of Restricted Boltzmann Machines\n",
      "  Restricted Boltzmann machines (RBMs) are a powerful class of generative\n",
      "models, but their training requires computing a gradient that, unlike\n",
      "supervised backpropagation on typical loss functions, is notoriously difficult\n",
      "even to approximate. Here, we show that properly combining standard gradient\n",
      "updates with an off-gradient direction, constructed from samples of the RBM\n",
      "ground state (mode), improves their training dramatically over traditional\n",
      "gradient methods. This approach, which we call mode training, promotes faster\n",
      "training and stability, in addition to lower converged relative entropy (KL\n",
      "divergence). Along with the proofs of stability and convergence of this method,\n",
      "we also demonstrate its efficacy on synthetic datasets where we can compute KL\n",
      "divergences exactly, as well as on a larger machine learning standard, MNIST.\n",
      "The mode training we suggest is quite versatile, as it can be applied in\n",
      "conjunction with any given gradient method, and is easily extended to more\n",
      "general energy-based neural network structures such as deep, convolutional and\n",
      "unrestricted Boltzmann machines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.00024 \n",
      "Title :Neural Canonical Transformation with Symplectic Flows\n",
      "  Canonical transformation plays a fundamental role in simplifying and solving\n",
      "classical Hamiltonian systems. We construct flexible and powerful canonical\n",
      "transformations as generative models using symplectic neural networks. The\n",
      "model transforms physical variables towards a latent representation with an\n",
      "independent harmonic oscillator Hamiltonian. Correspondingly, the phase space\n",
      "density of the physical system flows towards a factorized Gaussian distribution\n",
      "in the latent space. Since the canonical transformation preserves the\n",
      "Hamiltonian evolution, the model captures nonlinear collective modes in the\n",
      "learned latent representation. We present an efficient implementation of\n",
      "symplectic neural coordinate transformations and two ways to train the model.\n",
      "The variational free energy calculation is based on the analytical form of\n",
      "physical Hamiltonian. While the phase space density estimation only requires\n",
      "samples in the coordinate space for separable Hamiltonians. We demonstrate\n",
      "appealing features of neural canonical transformation using toy problems\n",
      "including two-dimensional ring potential and harmonic chain. Finally, we apply\n",
      "the approach to real-world problems such as identifying slow collective modes\n",
      "in alanine dipeptide and conceptual compression of the MNIST dataset.\n",
      "\n",
      "**Paper Id :1912.08177 \n",
      "Title :Lift & Learn: Physics-informed machine learning for large-scale\n",
      "  nonlinear dynamical systems\n",
      "  We present Lift & Learn, a physics-informed method for learning\n",
      "low-dimensional models for large-scale dynamical systems. The method exploits\n",
      "knowledge of a system's governing equations to identify a coordinate\n",
      "transformation in which the system dynamics have quadratic structure. This\n",
      "transformation is called a lifting map because it often adds auxiliary\n",
      "variables to the system state. The lifting map is applied to data obtained by\n",
      "evaluating a model for the original nonlinear system. This lifted data is\n",
      "projected onto its leading principal components, and low-dimensional linear and\n",
      "quadratic matrix operators are fit to the lifted reduced data using a\n",
      "least-squares operator inference procedure. Analysis of our method shows that\n",
      "the Lift & Learn models are able to capture the system physics in the lifted\n",
      "coordinates at least as accurately as traditional intrusive model reduction\n",
      "approaches. This preservation of system physics makes the Lift & Learn models\n",
      "robust to changes in inputs. Numerical experiments on the FitzHugh-Nagumo\n",
      "neuron activation model and the compressible Euler equations demonstrate the\n",
      "generalizability of our model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.00189 \n",
      "Title :The Non-IID Data Quagmire of Decentralized Machine Learning\n",
      "  Many large-scale machine learning (ML) applications need to perform\n",
      "decentralized learning over datasets generated at different devices and\n",
      "locations. Such datasets pose a significant challenge to decentralized learning\n",
      "because their different contexts result in significant data distribution skew\n",
      "across devices/locations. In this paper, we take a step toward better\n",
      "understanding this challenge by presenting a detailed experimental study of\n",
      "decentralized DNN training on a common type of data skew: skewed distribution\n",
      "of data labels across devices/locations. Our study shows that: (i) skewed data\n",
      "labels are a fundamental and pervasive problem for decentralized learning,\n",
      "causing significant accuracy loss across many ML applications, DNN models,\n",
      "training datasets, and decentralized learning algorithms; (ii) the problem is\n",
      "particularly challenging for DNN models with batch normalization; and (iii) the\n",
      "degree of data skew is a key determinant of the difficulty of the problem.\n",
      "Based on these findings, we present SkewScout, a system-level approach that\n",
      "adapts the communication frequency of decentralized learning algorithms to the\n",
      "(skew-induced) accuracy loss between data partitions. We also show that group\n",
      "normalization can recover much of the accuracy loss of batch normalization.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.00452 \n",
      "Title :On the Equivalence between Positional Node Embeddings and Structural\n",
      "  Graph Representations\n",
      "  This work provides the first unifying theoretical framework for node\n",
      "(positional) embeddings and structural graph representations, bridging methods\n",
      "like matrix factorization and graph neural networks. Using invariant theory, we\n",
      "show that the relationship between structural representations and node\n",
      "embeddings is analogous to that of a distribution and its samples. We prove\n",
      "that all tasks that can be performed by node embeddings can also be performed\n",
      "by structural representations and vice-versa. We also show that the concept of\n",
      "transductive and inductive learning is unrelated to node embeddings and graph\n",
      "representations, clearing another source of confusion in the literature.\n",
      "Finally, we introduce new practical guidelines to generating and using node\n",
      "embeddings, which fixes significant shortcomings of standard operating\n",
      "procedures used today.\n",
      "\n",
      "**Paper Id :2003.12635 \n",
      "Title :The impossibility of low rank representations for triangle-rich complex\n",
      "  networks\n",
      "  The study of complex networks is a significant development in modern science,\n",
      "and has enriched the social sciences, biology, physics, and computer science.\n",
      "Models and algorithms for such networks are pervasive in our society, and\n",
      "impact human behavior via social networks, search engines, and recommender\n",
      "systems to name a few. A widely used algorithmic technique for modeling such\n",
      "complex networks is to construct a low-dimensional Euclidean embedding of the\n",
      "vertices of the network, where proximity of vertices is interpreted as the\n",
      "likelihood of an edge. Contrary to the common view, we argue that such graph\n",
      "embeddings do not}capture salient properties of complex networks. The two\n",
      "properties we focus on are low degree and large clustering coefficients, which\n",
      "have been widely established to be empirically true for real-world networks. We\n",
      "mathematically prove that any embedding (that uses dot products to measure\n",
      "similarity) that can successfully create these two properties must have rank\n",
      "nearly linear in the number of vertices. Among other implications, this\n",
      "establishes that popular embedding techniques such as Singular Value\n",
      "Decomposition and node2vec fail to capture significant structural aspects of\n",
      "real-world complex networks. Furthermore, we empirically study a number of\n",
      "different embedding techniques based on dot product, and show that they all\n",
      "fail to capture the triangle structure.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.00821 \n",
      "Title :Near-Convex Archetypal Analysis\n",
      "  Nonnegative matrix factorization (NMF) is a widely used linear dimensionality\n",
      "reduction technique for nonnegative data. NMF requires that each data point is\n",
      "approximated by a convex combination of basis elements. Archetypal analysis\n",
      "(AA), also referred to as convex NMF, is a well-known NMF variant imposing that\n",
      "the basis elements are themselves convex combinations of the data points. AA\n",
      "has the advantage to be more interpretable than NMF because the basis elements\n",
      "are directly constructed from the data points. However, it usually suffers from\n",
      "a high data fitting error because the basis elements are constrained to be\n",
      "contained in the convex cone of the data points. In this letter, we introduce\n",
      "near-convex archetypal analysis (NCAA) which combines the advantages of both AA\n",
      "and NMF. As for AA, the basis vectors are required to be linear combinations of\n",
      "the data points and hence are easily interpretable. As for NMF, the additional\n",
      "flexibility in choosing the basis elements allows NCAA to have a low data\n",
      "fitting error. We show that NCAA compares favorably with a state-of-the-art\n",
      "minimum-volume NMF method on synthetic datasets and on a real-world\n",
      "hyperspectral image.\n",
      "\n",
      "**Paper Id :2001.02568 \n",
      "Title :A Group Norm Regularized Factorization Model for Subspace Segmentation\n",
      "  Subspace segmentation assumes that data comes from the union of different\n",
      "subspaces and the purpose of segmentation is to partition the data into the\n",
      "corresponding subspace. Low-rank representation (LRR) is a classic\n",
      "spectral-type method for solving subspace segmentation problems, that is, one\n",
      "first obtains an affinity matrix by solving a LRR model and then performs\n",
      "spectral clustering for segmentation. This paper proposes a group norm\n",
      "regularized factorization model (GNRFM) inspired by the LRR model for subspace\n",
      "segmentation and then designs an Accelerated Augmented Lagrangian Method (AALM)\n",
      "algorithm to solve this model. Specifically, we adopt group norm regularization\n",
      "to make the columns of the factor matrix sparse, thereby achieving a purpose of\n",
      "low rank, which means no Singular Value Decompositions (SVD) are required and\n",
      "the computational complexity of each step is greatly reduced. We obtain\n",
      "affinity matrices by using different LRR models and then performing cluster\n",
      "testing on different sets of synthetic noisy data and real data, respectively.\n",
      "Compared with traditional models and algorithms, the proposed method is faster\n",
      "and more robust to noise, so the final clustering results are better. Moreover,\n",
      "the numerical results show that our algorithm converges fast and only requires\n",
      "approximately ten iterations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.01155 \n",
      "Title :Stochastic gradient descent for hybrid quantum-classical optimization\n",
      "  Within the context of hybrid quantum-classical optimization, gradient descent\n",
      "based optimizers typically require the evaluation of expectation values with\n",
      "respect to the outcome of parameterized quantum circuits. In this work, we\n",
      "explore the consequences of the prior observation that estimation of these\n",
      "quantities on quantum hardware results in a form of stochastic gradient descent\n",
      "optimization. We formalize this notion, which allows us to show that in many\n",
      "relevant cases, including VQE, QAOA and certain quantum classifiers, estimating\n",
      "expectation values with $k$ measurement outcomes results in optimization\n",
      "algorithms whose convergence properties can be rigorously well understood, for\n",
      "any value of $k$. In fact, even using single measurement outcomes for the\n",
      "estimation of expectation values is sufficient. Moreover, in many settings the\n",
      "required gradients can be expressed as linear combinations of expectation\n",
      "values -- originating, e.g., from a sum over local terms of a Hamiltonian, a\n",
      "parameter shift rule, or a sum over data-set instances -- and we show that in\n",
      "these cases $k$-shot expectation value estimation can be combined with sampling\n",
      "over terms of the linear combination, to obtain ``doubly stochastic'' gradient\n",
      "descent optimizers. For all algorithms we prove convergence guarantees,\n",
      "providing a framework for the derivation of rigorous optimization results in\n",
      "the context of near-term quantum devices. Additionally, we explore numerically\n",
      "these methods on benchmark VQE, QAOA and quantum-enhanced machine learning\n",
      "tasks and show that treating the stochastic settings as hyper-parameters allows\n",
      "for state-of-the-art results with significantly fewer circuit executions and\n",
      "measurements.\n",
      "\n",
      "**Paper Id :2002.01068 \n",
      "Title :Policy Gradient based Quantum Approximate Optimization Algorithm\n",
      "  The quantum approximate optimization algorithm (QAOA), as a hybrid\n",
      "quantum/classical algorithm, has received much interest recently. QAOA can also\n",
      "be viewed as a variational ansatz for quantum control. However, its direct\n",
      "application to emergent quantum technology encounters additional physical\n",
      "constraints: (i) the states of the quantum system are not observable; (ii)\n",
      "obtaining the derivatives of the objective function can be computationally\n",
      "expensive or even inaccessible in experiments, and (iii) the values of the\n",
      "objective function may be sensitive to various sources of uncertainty, as is\n",
      "the case for noisy intermediate-scale quantum (NISQ) devices. Taking such\n",
      "constraints into account, we show that policy-gradient-based reinforcement\n",
      "learning (RL) algorithms are well suited for optimizing the variational\n",
      "parameters of QAOA in a noise-robust fashion, opening up the way for developing\n",
      "RL techniques for continuous quantum control. This is advantageous to help\n",
      "mitigate and monitor the potentially unknown sources of errors in modern\n",
      "quantum simulators. We analyze the performance of the algorithm for quantum\n",
      "state transfer problems in single- and multi-qubit systems, subject to various\n",
      "sources of noise such as error terms in the Hamiltonian, or quantum uncertainty\n",
      "in the measurement process. We show that, in noisy setups, it is capable of\n",
      "outperforming state-of-the-art existing optimization algorithms.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.01178 \n",
      "Title :Neural Network Applications in Earthquake Prediction (1994-2019):\n",
      "  Meta-Analytic Insight on their Limitations\n",
      "  In the last few years, deep learning has solved seemingly intractable\n",
      "problems, boosting the hope to find approximate solutions to problems that now\n",
      "are considered unsolvable. Earthquake prediction, the Grail of Seismology, is,\n",
      "in this context of continuous exciting discoveries, an obvious choice for deep\n",
      "learning exploration. We review the entire literature of artificial neural\n",
      "network (ANN) applications for earthquake prediction (77 articles, 1994-2019\n",
      "period) and find two emerging trends: an increasing interest in this domain,\n",
      "and a complexification of ANN models over time, towards deep learning. Despite\n",
      "apparent positive results observed in this corpus, we demonstrate that simpler\n",
      "models seem to offer similar predictive powers, if not better ones. Due to the\n",
      "structured, tabulated nature of earthquake catalogues, and the limited number\n",
      "of features so far considered, simpler and more transparent machine learning\n",
      "models seem preferable at the present stage of research. Those baseline models\n",
      "follow first physical principles and are consistent with the known empirical\n",
      "laws of Statistical Seismology, which have minimal abilities to predict large\n",
      "earthquakes.\n",
      "\n",
      "**Paper Id :1911.00890 \n",
      "Title :Mean-field inference methods for neural networks\n",
      "  Machine learning algorithms relying on deep neural networks recently allowed\n",
      "a great leap forward in artificial intelligence. Despite the popularity of\n",
      "their applications, the efficiency of these algorithms remains largely\n",
      "unexplained from a theoretical point of view. The mathematical description of\n",
      "learning problems involves very large collections of interacting random\n",
      "variables, difficult to handle analytically as well as numerically. This\n",
      "complexity is precisely the object of study of statistical physics. Its\n",
      "mission, originally pointed towards natural systems, is to understand how\n",
      "macroscopic behaviors arise from microscopic laws. Mean-field methods are one\n",
      "type of approximation strategy developed in this view. We review a selection of\n",
      "classical mean-field methods and recent progress relevant for inference in\n",
      "neural networks. In particular, we remind the principles of derivations of\n",
      "high-temperature expansions, the replica method and message passing algorithms,\n",
      "highlighting their equivalences and complementarities. We also provide\n",
      "references for past and current directions of research on neural networks\n",
      "relying on mean-field methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.01432 \n",
      "Title :The Bouncer Problem: Challenges to Remote Explainability\n",
      "  The concept of explainability is envisioned to satisfy society's demands for\n",
      "transparency on machine learning decisions. The concept is simple: like humans,\n",
      "algorithms should explain the rationale behind their decisions so that their\n",
      "fairness can be assessed. While this approach is promising in a local context\n",
      "(e.g. to explain a model during debugging at training time), we argue that this\n",
      "reasoning cannot simply be transposed in a remote context, where a trained\n",
      "model by a service provider is only accessible through its API. This is\n",
      "problematic as it constitutes precisely the target use-case requiring\n",
      "transparency from a societal perspective. Through an analogy with a club\n",
      "bouncer (which may provide untruthful explanations upon customer reject), we\n",
      "show that providing explanations cannot prevent a remote service from lying\n",
      "about the true reasons leading to its decisions. More precisely, we prove the\n",
      "impossibility of remote explainability for single explanations, by constructing\n",
      "an attack on explanations that hides discriminatory features to the querying\n",
      "user. We provide an example implementation of this attack. We then show that\n",
      "the probability that an observer spots the attack, using several explanations\n",
      "for attempting to find incoherences, is low in practical settings. This\n",
      "undermines the very concept of remote explainability in general.\n",
      "\n",
      "**Paper Id :2003.06005 \n",
      "Title :Model Agnostic Multilevel Explanations\n",
      "  In recent years, post-hoc local instance-level and global dataset-level\n",
      "explainability of black-box models has received a lot of attention. Much less\n",
      "attention has been given to obtaining insights at intermediate or group levels,\n",
      "which is a need outlined in recent works that study the challenges in realizing\n",
      "the guidelines in the General Data Protection Regulation (GDPR). In this paper,\n",
      "we propose a meta-method that, given a typical local explainability method, can\n",
      "build a multilevel explanation tree. The leaves of this tree correspond to the\n",
      "local explanations, the root corresponds to the global explanation, and\n",
      "intermediate levels correspond to explanations for groups of data points that\n",
      "it automatically clusters. The method can also leverage side information, where\n",
      "users can specify points for which they may want the explanations to be\n",
      "similar. We argue that such a multilevel structure can also be an effective\n",
      "form of communication, where one could obtain few explanations that\n",
      "characterize the entire dataset by considering an appropriate level in our\n",
      "explanation tree. Explanations for novel test points can be cost-efficiently\n",
      "obtained by associating them with the closest training points. When the local\n",
      "explainability technique is generalized additive (viz. LIME, GAMs), we develop\n",
      "a fast approximate algorithm for building the multilevel tree and study its\n",
      "convergence behavior. We validate the effectiveness of the proposed technique\n",
      "based on two human studies -- one with experts and the other with non-expert\n",
      "users -- on real world datasets, and show that we produce high fidelity sparse\n",
      "explanations on several other public datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.02333 \n",
      "Title :The Role of Neural Network Activation Functions\n",
      "  A wide variety of activation functions have been proposed for neural\n",
      "networks. The Rectified Linear Unit (ReLU) is especially popular today. There\n",
      "are many practical reasons that motivate the use of the ReLU. This paper\n",
      "provides new theoretical characterizations that support the use of the ReLU,\n",
      "its variants such as the leaky ReLU, as well as other activation functions in\n",
      "the case of univariate, single-hidden layer feedforward neural networks. Our\n",
      "results also explain the importance of commonly used strategies in the design\n",
      "and training of neural networks such as \"weight decay\" and \"path-norm\"\n",
      "regularization, and provide a new justification for the use of \"skip\n",
      "connections\" in network architectures. These new insights are obtained through\n",
      "the lens of spline theory. In particular, we show how neural network training\n",
      "problems are related to infinite-dimensional optimizations posed over Banach\n",
      "spaces of functions whose solutions are well-known to be fractional and\n",
      "polynomial splines, where the particular Banach space (which controls the order\n",
      "of the spline) depends on the choice of activation function.\n",
      "\n",
      "**Paper Id :1708.06633 \n",
      "Title :Nonparametric regression using deep neural networks with ReLU activation\n",
      "  function\n",
      "  Consider the multivariate nonparametric regression model. It is shown that\n",
      "estimators based on sparsely connected deep neural networks with ReLU\n",
      "activation function and properly chosen network architecture achieve the\n",
      "minimax rates of convergence (up to $\\log n$-factors) under a general\n",
      "composition assumption on the regression function. The framework includes many\n",
      "well-studied structural constraints such as (generalized) additive models.\n",
      "While there is a lot of flexibility in the network architecture, the tuning\n",
      "parameter is the sparsity of the network. Specifically, we consider large\n",
      "networks with number of potential network parameters exceeding the sample size.\n",
      "The analysis gives some insights into why multilayer feedforward neural\n",
      "networks perform well in practice. Interestingly, for ReLU activation function\n",
      "the depth (number of layers) of the neural network architectures plays an\n",
      "important role and our theory suggests that for nonparametric regression,\n",
      "scaling the network depth with the sample size is natural. It is also shown\n",
      "that under the composition assumption wavelet estimators can only achieve\n",
      "suboptimal rates.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.02420 \n",
      "Title :Deep learning-based development of personalized human head model with\n",
      "  non-uniform conductivity for brain stimulation\n",
      "  Electromagnetic stimulation of the human brain is a key tool for the\n",
      "neurophysiological characterization and diagnosis of several neurological\n",
      "disorders. Transcranial magnetic stimulation (TMS) is one procedure that is\n",
      "commonly used clinically. However, personalized TMS requires a pipeline for\n",
      "accurate head model generation to provide target-specific stimulation. This\n",
      "process includes intensive segmentation of several head tissues based on\n",
      "magnetic resonance imaging (MRI), which has significant potential for\n",
      "segmentation error, especially for low-contrast tissues. Additionally, a\n",
      "uniform electrical conductivity is assigned to each tissue in the model, which\n",
      "is an unrealistic assumption based on conventional volume conductor modeling.\n",
      "This paper proposes a novel approach to the automatic estimation of electric\n",
      "conductivity in the human head for volume conductor models without anatomical\n",
      "segmentation. A convolutional neural network is designed to estimate\n",
      "personalized electrical conductivity values based on anatomical information\n",
      "obtained from T1- and T2-weighted MRI scans. This approach can avoid the\n",
      "time-consuming process of tissue segmentation and maximize the advantages of\n",
      "position-dependent conductivity assignment based on water content values\n",
      "estimated from MRI intensity values. The computational results of the proposed\n",
      "approach provide similar but smoother electric field results for the brain when\n",
      "compared to conventional approaches.\n",
      "\n",
      "**Paper Id :2002.05487 \n",
      "Title :End-to-end semantic segmentation of personalized deep brain structures\n",
      "  for non-invasive brain stimulation\n",
      "  Electro-stimulation or modulation of deep brain regions is commonly used in\n",
      "clinical procedures for the treatment of several nervous system disorders. In\n",
      "particular, transcranial direct current stimulation (tDCS) is widely used as an\n",
      "affordable clinical application that is applied through electrodes attached to\n",
      "the scalp. However, it is difficult to determine the amount and distribution of\n",
      "the electric field (EF) in the different brain regions due to anatomical\n",
      "complexity and high inter-subject variability. Personalized tDCS is an emerging\n",
      "clinical procedure that is used to tolerate electrode montage for accurate\n",
      "targeting. This procedure is guided by computational head models generated from\n",
      "anatomical images such as MRI. Distribution of the EF in segmented head models\n",
      "can be calculated through simulation studies. Therefore, fast, accurate, and\n",
      "feasible segmentation of different brain structures would lead to a better\n",
      "adjustment for customized tDCS studies. In this study, a single-encoder\n",
      "multi-decoders convolutional neural network is proposed for deep brain\n",
      "segmentation. The proposed architecture is trained to segment seven deep brain\n",
      "structures using T1-weighted MRI. Network generated models are compared with a\n",
      "reference model constructed using a semi-automatic method, and it presents a\n",
      "high matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate\n",
      "(DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution\n",
      "during tDCS in generated and reference models matched well each other,\n",
      "suggesting its potential usefulness in clinical practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.02498 \n",
      "Title :Predicting popularity of EV charging infrastructure from GIS data\n",
      "  The availability of charging infrastructure is essential for large-scale\n",
      "adoption of electric vehicles (EV). Charging patterns and the utilization of\n",
      "infrastructure have consequences not only for the energy demand, loading local\n",
      "power grids but influence the economic returns, parking policies and further\n",
      "adoption of EVs. We develop a data-driven approach that is exploiting\n",
      "predictors compiled from GIS data describing the urban context and urban\n",
      "activities near charging infrastructure to explore correlations with a\n",
      "comprehensive set of indicators measuring the performance of charging\n",
      "infrastructure. The best fit was identified for the size of the unique group of\n",
      "visitors (popularity) attracted by the charging infrastructure. Consecutively,\n",
      "charging infrastructure is ranked by popularity. The question of whether or not\n",
      "a given charging spot belongs to the top tier is posed as a binary\n",
      "classification problem and predictive performance of logistic regression\n",
      "regularized with an l-1 penalty, random forests and gradient boosted regression\n",
      "trees is evaluated. Obtained results indicate that the collected predictors\n",
      "contain information that can be used to predict the popularity of charging\n",
      "infrastructure. The significance of predictors and how they are linked with the\n",
      "popularity are explored as well. The proposed methodology can be used to inform\n",
      "charging infrastructure deployment strategies.\n",
      "\n",
      "**Paper Id :1905.10891 \n",
      "Title :A hybrid model for predicting human physical activity status from\n",
      "  lifelogging data\n",
      "  One trend in the recent healthcare transformations is people are encouraged\n",
      "to monitor and manage their health based on their daily diets and physical\n",
      "activity habits. However, much attention of the use of operational research and\n",
      "analytical models in healthcare has been paid to the systematic level such as\n",
      "country or regional policy making or organisational issues. This paper proposes\n",
      "a model concerned with healthcare analytics at the individual level, which can\n",
      "predict human physical activity status from sequential lifelogging data\n",
      "collected from wearable sensors. The model has a two-stage hybrid structure (in\n",
      "short, MOGP-HMM) -- a multi-objective genetic programming (MOGP) algorithm in\n",
      "the first stage to reduce the dimensions of lifelogging data and a hidden\n",
      "Markov model (HMM) in the second stage for activity status prediction over\n",
      "time. It can be used as a decision support tool to provide real-time\n",
      "monitoring, statistical analysis and personalized advice to individuals,\n",
      "encouraging positive attitudes towards healthy lifestyles. We validate the\n",
      "model with the real data collected from a group of participants in the UK, and\n",
      "compare it with other popular two-stage hybrid models. Our experimental results\n",
      "show that the MOGP-HMM can achieve comparable performance. To the best of our\n",
      "knowledge, this is the very first study that uses the MOGP in the hybrid\n",
      "two-stage structure for individuals' activity status prediction. It fits\n",
      "seamlessly with the current trend in the UK healthcare transformation of\n",
      "patient empowerment as well as contributing to a strategic development for more\n",
      "efficient and cost-effective provision of healthcare.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.02787 \n",
      "Title :Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping\n",
      "  The distributional perspective on reinforcement learning (RL) has given rise\n",
      "to a series of successful Q-learning algorithms, resulting in state-of-the-art\n",
      "performance in arcade game environments. However, it has not yet been analyzed\n",
      "how these findings from a discrete setting translate to complex practical\n",
      "applications characterized by noisy, high dimensional and continuous\n",
      "state-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a\n",
      "distributional variant of the recently introduced distributed Q-learning\n",
      "algorithm for continuous domains, and examine its behaviour in a series of\n",
      "simulated and real vision-based robotic grasping tasks. The absence of an actor\n",
      "in Q2-Opt allows us to directly draw a parallel to the previous discrete\n",
      "experiments in the literature without the additional complexities induced by an\n",
      "actor-critic architecture. We demonstrate that Q2-Opt achieves a superior\n",
      "vision-based object grasping success rate, while also being more sample\n",
      "efficient. The distributional formulation also allows us to experiment with\n",
      "various risk distortion metrics that give us an indication of how robots can\n",
      "concretely manage risk in practice using a Deep RL control policy. As an\n",
      "additional contribution, we perform batch RL experiments in our virtual\n",
      "environment and compare them with the latest findings from discrete settings.\n",
      "Surprisingly, we find that the previous batch RL findings from the literature\n",
      "obtained on arcade game environments do not generalise to our setup.\n",
      "\n",
      "**Paper Id :1909.10707 \n",
      "Title :Invariant Transform Experience Replay: Data Augmentation for Deep\n",
      "  Reinforcement Learning\n",
      "  Deep Reinforcement Learning (RL) is a promising approach for adaptive robot\n",
      "control, but its current application to robotics is currently hindered by high\n",
      "sample requirements. To alleviate this issue, we propose to exploit the\n",
      "symmetries present in robotic tasks. Intuitively, symmetries from observed\n",
      "trajectories define transformations that leave the space of feasible RL\n",
      "trajectories invariant and can be used to generate new feasible trajectories,\n",
      "which could be used for training. Based on this data augmentation idea, we\n",
      "formulate a general framework, called Invariant Transform Experience Replay\n",
      "that we present with two techniques: (i) Kaleidoscope Experience Replay\n",
      "exploits reflectional symmetries and (ii) Goal-augmented Experience Replay\n",
      "which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI\n",
      "Gym, our experimental results show significant increases in learning rates and\n",
      "success rates. Particularly, we attain a 13, 3, and 5 times speedup in the\n",
      "pushing, sliding, and pick-and-place tasks respectively in the multi-goal\n",
      "setting. Performance gains are also observed in similar tasks with obstacles\n",
      "and we successfully deployed a trained policy on a real Baxter robot. Our work\n",
      "demonstrates that invariant transformations on RL trajectories are a promising\n",
      "methodology to speed up learning in deep RL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.03143 \n",
      "Title :On Polyhedral and Second-Order Cone Decompositions of Semidefinite\n",
      "  Optimization Problems\n",
      "  We study a cutting-plane method for semidefinite optimization problems\n",
      "(SDOs), and supply a proof of the method's convergence, under a boundedness\n",
      "assumption. By relating the method's rate of convergence to an initial outer\n",
      "approximation's diameter, we argue that the method performs well when\n",
      "initialized with a second-order-cone approximation, instead of a linear\n",
      "approximation. We invoke the method to provide bound gaps of 0.5-6.5% for\n",
      "sparse PCA problems with $1000$s of covariates, and solve nuclear norm problems\n",
      "over 500x500 matrices.\n",
      "\n",
      "**Paper Id :2002.04756 \n",
      "Title :Average-case Acceleration Through Spectral Density Estimation\n",
      "  We develop a framework for the average-case analysis of random quadratic\n",
      "problems and derive algorithms that are optimal under this analysis. This\n",
      "yields a new class of methods that achieve acceleration given a model of the\n",
      "Hessian's eigenvalue distribution. We develop explicit algorithms for the\n",
      "uniform, Marchenko-Pastur, and exponential distributions. These methods are\n",
      "momentum-based algorithms, whose hyper-parameters can be estimated without\n",
      "knowledge of the Hessian's smallest singular value, in contrast with classical\n",
      "accelerated methods like Nesterov acceleration and Polyak momentum. Through\n",
      "empirical benchmarks on quadratic and logistic regression problems, we identify\n",
      "regimes in which the the proposed methods improve over classical (worst-case)\n",
      "accelerated methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.03231 \n",
      "Title :Peer Loss Functions: Learning from Noisy Labels without Knowing Noise\n",
      "  Rates\n",
      "  Learning with noisy labels is a common challenge in supervised learning.\n",
      "Existing approaches often require practitioners to specify noise rates, i.e., a\n",
      "set of parameters controlling the severity of label noises in the problem, and\n",
      "the specifications are either assumed to be given or estimated using additional\n",
      "steps. In this work, we introduce a new family of loss functions that we name\n",
      "as peer loss functions, which enables learning from noisy labels and does not\n",
      "require a priori specification of the noise rates. Peer loss functions work\n",
      "within the standard empirical risk minimization (ERM) framework. We show that,\n",
      "under mild conditions, performing ERM with peer loss functions on the noisy\n",
      "dataset leads to the optimal or a near-optimal classifier as if performing ERM\n",
      "over the clean training data, which we do not have access to. We pair our\n",
      "results with an extensive set of experiments. Peer loss provides a way to\n",
      "simplify model development when facing potentially noisy training labels, and\n",
      "can be promoted as a robust candidate loss function in such situations.\n",
      "\n",
      "**Paper Id :2007.03511 \n",
      "Title :Estimating Generalization under Distribution Shifts via Domain-Invariant\n",
      "  Representations\n",
      "  When machine learning models are deployed on a test distribution different\n",
      "from the training distribution, they can perform poorly, but overestimate their\n",
      "performance. In this work, we aim to better estimate a model's performance\n",
      "under distribution shift, without supervision. To do so, we use a set of\n",
      "domain-invariant predictors as a proxy for the unknown, true target labels.\n",
      "Since the error of the resulting risk estimate depends on the target risk of\n",
      "the proxy model, we study generalization of domain-invariant representations\n",
      "and show that the complexity of the latent representation has a significant\n",
      "influence on the target risk. Empirically, our approach (1) enables self-tuning\n",
      "of domain adaptation models, and (2) accurately estimates the target error of\n",
      "given models under distribution shift. Other applications include model\n",
      "selection, deciding early stopping and error detection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.03644 \n",
      "Title :Stochastic triangular mesh mapping: A terrain mapping technique for\n",
      "  autonomous mobile robots\n",
      "  For mobile robots to operate autonomously in general environments, perception\n",
      "is required in the form of a dense metric map. For this purpose, we present the\n",
      "stochastic triangular mesh (STM) mapping technique: a 2.5-D representation of\n",
      "the surface of the environment using a continuous mesh of triangular surface\n",
      "elements, where each surface element models the mean plane and roughness of the\n",
      "underlying surface. In contrast to existing mapping techniques, a STM map\n",
      "models the structure of the environment by ensuring a continuous model, while\n",
      "also being able to be incrementally updated with linear computational cost in\n",
      "the number of measurements. We reduce the effect of uncertainty in the robot\n",
      "pose (position and orientation) by using landmark-relative submaps. The\n",
      "uncertainty in the measurements and robot pose are accounted for by the use of\n",
      "Bayesian inference techniques during the map update. We demonstrate that a STM\n",
      "map can be used with sensors that generate point measurements, such as light\n",
      "detection and ranging (LiDAR) sensors and stereo cameras. We show that a STM\n",
      "map is a more accurate model than the only comparable online surface mapping\n",
      "technique$\\unicode{x2014}$a standard elevation map$\\unicode{x2014}$and we also\n",
      "provide qualitative results on practical datasets.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.04464 \n",
      "Title :PAC-Bayesian Contrastive Unsupervised Representation Learning\n",
      "  Contrastive unsupervised representation learning (CURL) is the\n",
      "state-of-the-art technique to learn representations (as a set of features) from\n",
      "unlabelled data. While CURL has collected several empirical successes recently,\n",
      "theoretical understanding of its performance was still missing. In a recent\n",
      "work, Arora et al. (2019) provide the first generalisation bounds for CURL,\n",
      "relying on a Rademacher complexity. We extend their framework to the flexible\n",
      "PAC-Bayes setting, allowing us to deal with the non-iid setting. We present\n",
      "PAC-Bayesian generalisation bounds for CURL, which are then used to derive a\n",
      "new representation learning algorithm. Numerical experiments on real-life\n",
      "datasets illustrate that our algorithm achieves competitive accuracy, and\n",
      "yields non-vacuous generalisation bounds.\n",
      "\n",
      "**Paper Id :1910.02787 \n",
      "Title :Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping\n",
      "  The distributional perspective on reinforcement learning (RL) has given rise\n",
      "to a series of successful Q-learning algorithms, resulting in state-of-the-art\n",
      "performance in arcade game environments. However, it has not yet been analyzed\n",
      "how these findings from a discrete setting translate to complex practical\n",
      "applications characterized by noisy, high dimensional and continuous\n",
      "state-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a\n",
      "distributional variant of the recently introduced distributed Q-learning\n",
      "algorithm for continuous domains, and examine its behaviour in a series of\n",
      "simulated and real vision-based robotic grasping tasks. The absence of an actor\n",
      "in Q2-Opt allows us to directly draw a parallel to the previous discrete\n",
      "experiments in the literature without the additional complexities induced by an\n",
      "actor-critic architecture. We demonstrate that Q2-Opt achieves a superior\n",
      "vision-based object grasping success rate, while also being more sample\n",
      "efficient. The distributional formulation also allows us to experiment with\n",
      "various risk distortion metrics that give us an indication of how robots can\n",
      "concretely manage risk in practice using a Deep RL control policy. As an\n",
      "additional contribution, we perform batch RL experiments in our virtual\n",
      "environment and compare them with the latest findings from discrete settings.\n",
      "Surprisingly, we find that the previous batch RL findings from the literature\n",
      "obtained on arcade game environments do not generalise to our setup.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.04817 \n",
      "Title :Estimation of Bounds on Potential Outcomes For Decision Making\n",
      "  Estimation of individual treatment effects is commonly used as the basis for\n",
      "contextual decision making in fields such as healthcare, education, and\n",
      "economics. However, it is often sufficient for the decision maker to have\n",
      "estimates of upper and lower bounds on the potential outcomes of decision\n",
      "alternatives to assess risks and benefits. We show that, in such cases, we can\n",
      "improve sample efficiency by estimating simple functions that bound these\n",
      "outcomes instead of estimating their conditional expectations, which may be\n",
      "complex and hard to estimate. Our analysis highlights a trade-off between the\n",
      "complexity of the learning task and the confidence with which the learned\n",
      "bounds hold. Guided by these findings, we develop an algorithm for learning\n",
      "upper and lower bounds on potential outcomes which optimize an objective\n",
      "function defined by the decision maker, subject to the probability that bounds\n",
      "are violated being small. Using a clinical dataset and a well-known causality\n",
      "benchmark, we demonstrate that our algorithm outperforms baselines, providing\n",
      "tighter, more reliable bounds.\n",
      "\n",
      "**Paper Id :2001.04754 \n",
      "Title :Learning Overlapping Representations for the Estimation of\n",
      "  Individualized Treatment Effects\n",
      "  The choice of making an intervention depends on its potential benefit or harm\n",
      "in comparison to alternatives. Estimating the likely outcome of alternatives\n",
      "from observational data is a challenging problem as all outcomes are never\n",
      "observed, and selection bias precludes the direct comparison of differently\n",
      "intervened groups. Despite their empirical success, we show that algorithms\n",
      "that learn domain-invariant representations of inputs (on which to make\n",
      "predictions) are often inappropriate, and develop generalization bounds that\n",
      "demonstrate the dependence on domain overlap and highlight the need for\n",
      "invertible latent maps. Based on these results, we develop a deep kernel\n",
      "regression algorithm and posterior regularization framework that substantially\n",
      "outperforms the state-of-the-art on a variety of benchmarks data sets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.05493 \n",
      "Title :Deep Transfer Learning for Source Code Modeling\n",
      "  In recent years, deep learning models have shown great potential in source\n",
      "code modeling and analysis. Generally, deep learning-based approaches are\n",
      "problem-specific and data-hungry. A challenging issue of these approaches is\n",
      "that they require training from starch for a different related problem. In this\n",
      "work, we propose a transfer learning-based approach that significantly improves\n",
      "the performance of deep learning-based source code models. In contrast to\n",
      "traditional learning paradigms, transfer learning can transfer the knowledge\n",
      "learned in solving one problem into another related problem. First, we present\n",
      "two recurrent neural network-based models RNN and GRU for the purpose of\n",
      "transfer learning in the domain of source code modeling. Next, via transfer\n",
      "learning, these pre-trained (RNN and GRU) models are used as feature\n",
      "extractors. Then, these extracted features are combined into attention learner\n",
      "for different downstream tasks. The attention learner leverages from the\n",
      "learned knowledge of pre-trained models and fine-tunes them for a specific\n",
      "downstream task. We evaluate the performance of the proposed approach with\n",
      "extensive experiments with the source code suggestion task. The results\n",
      "indicate that the proposed approach outperforms the state-of-the-art models in\n",
      "terms of accuracy, precision, recall, and F-measure without training the models\n",
      "from scratch.\n",
      "\n",
      "**Paper Id :2002.12455 \n",
      "Title :Is the Meta-Learning Idea Able to Improve the Generalization of Deep\n",
      "  Neural Networks on the Standard Supervised Learning?\n",
      "  Substantial efforts have been made on improving the generalization abilities\n",
      "of deep neural networks (DNNs) in order to obtain better performances without\n",
      "introducing more parameters. On the other hand, meta-learning approaches\n",
      "exhibit powerful generalization on new tasks in few-shot learning. Intuitively,\n",
      "few-shot learning is more challenging than the standard supervised learning as\n",
      "each target class only has a very few or no training samples. The natural\n",
      "question that arises is whether the meta-learning idea can be used for\n",
      "improving the generalization of DNNs on the standard supervised learning. In\n",
      "this paper, we propose a novel meta-learning based training procedure (MLTP)\n",
      "for DNNs and demonstrate that the meta-learning idea can indeed improve the\n",
      "generalization abilities of DNNs. MLTP simulates the meta-training process by\n",
      "considering a batch of training samples as a task. The key idea is that the\n",
      "gradient descent step for improving the current task performance should also\n",
      "improve a new task performance, which is ignored by the current standard\n",
      "procedure for training neural networks. MLTP also benefits from all the\n",
      "existing training techniques such as dropout, weight decay, and batch\n",
      "normalization. We evaluate MLTP by training a variety of small and large neural\n",
      "networks on three benchmark datasets, i.e., CIFAR-10, CIFAR-100, and Tiny\n",
      "ImageNet. The experimental results show a consistently improved generalization\n",
      "performance on all the DNNs with different sizes, which verifies the promise of\n",
      "MLTP and demonstrates that the meta-learning idea is indeed able to improve the\n",
      "generalization of DNNs on the standard supervised learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.05744 \n",
      "Title :Powering Hidden Markov Model by Neural Network based Generative Models\n",
      "  Hidden Markov model (HMM) has been successfully used for sequential data\n",
      "modeling problems. In this work, we propose to power the modeling capacity of\n",
      "HMM by bringing in neural network based generative models. The proposed model\n",
      "is termed as GenHMM. In the proposed GenHMM, each HMM hidden state is\n",
      "associated with a neural network based generative model that has tractability\n",
      "of exact likelihood and provides efficient likelihood computation. A generative\n",
      "model in GenHMM consists of mixture of generators that are realized by flow\n",
      "models. A learning algorithm for GenHMM is proposed in expectation-maximization\n",
      "framework. The convergence of the learning GenHMM is analyzed. We demonstrate\n",
      "the efficiency of GenHMM by classification tasks on practical sequential data.\n",
      "Code available at https://github.com/FirstHandScientist/genhmm.\n",
      "\n",
      "**Paper Id :1911.02945 \n",
      "Title :J-MoDL: Joint Model-Based Deep Learning for Optimized Sampling and\n",
      "  Reconstruction\n",
      "  Modern MRI schemes, which rely on compressed sensing or deep learning\n",
      "algorithms to recover MRI data from undersampled multichannel Fourier\n",
      "measurements, are widely used to reduce scan time. The image quality of these\n",
      "approaches is heavily dependent on the sampling pattern. We introduce a\n",
      "continuous strategy to jointly optimize the sampling pattern and network\n",
      "parameters. We use a multichannel forward model, consisting of a non-uniform\n",
      "Fourier transform with continuously defined sampling locations, to realize the\n",
      "data consistency block within a model-based deep learning image reconstruction\n",
      "scheme. This approach facilitates the joint and continuous optimization of the\n",
      "sampling pattern and the CNN parameters to improve image quality. We observe\n",
      "that the joint optimization of the sampling patterns and the reconstruction\n",
      "module significantly improves the performance of most deep learning\n",
      "reconstruction algorithms. The source code of the proposed joint learning\n",
      "framework is available at https://github.com/hkaggarwal/J-MoDL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.05769 \n",
      "Title :Large Deviation Analysis of Function Sensitivity in Random Deep Neural\n",
      "  Networks\n",
      "  Mean field theory has been successfully used to analyze deep neural networks\n",
      "(DNN) in the infinite size limit. Given the finite size of realistic DNN, we\n",
      "utilize the large deviation theory and path integral analysis to study the\n",
      "deviation of functions represented by DNN from their typical mean field\n",
      "solutions. The parameter perturbations investigated include weight\n",
      "sparsification (dilution) and binarization, which are commonly used in model\n",
      "simplification, for both ReLU and sign activation functions. We find that\n",
      "random networks with ReLU activation are more robust to parameter perturbations\n",
      "with respect to their counterparts with sign activation, which arguably is\n",
      "reflected in the simplicity of the functions they generate.\n",
      "\n",
      "**Paper Id :1907.06673 \n",
      "Title :Quant GANs: Deep Generation of Financial Time Series\n",
      "  Modeling financial time series by stochastic processes is a challenging task\n",
      "and a central area of research in financial mathematics. As an alternative, we\n",
      "introduce Quant GANs, a data-driven model which is inspired by the recent\n",
      "success of generative adversarial networks (GANs). Quant GANs consist of a\n",
      "generator and discriminator function, which utilize temporal convolutional\n",
      "networks (TCNs) and thereby achieve to capture long-range dependencies such as\n",
      "the presence of volatility clusters. The generator function is explicitly\n",
      "constructed such that the induced stochastic process allows a transition to its\n",
      "risk-neutral distribution. Our numerical results highlight that distributional\n",
      "properties for small and large lags are in an excellent agreement and\n",
      "dependence properties such as volatility clusters, leverage effects, and serial\n",
      "autocorrelations can be generated by the generator function of Quant GANs,\n",
      "demonstrably in high fidelity.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.05857 \n",
      "Title :Improving the Sample and Communication Complexity for Decentralized\n",
      "  Non-Convex Optimization: A Joint Gradient Estimation and Tracking Approach\n",
      "  Many modern large-scale machine learning problems benefit from decentralized\n",
      "and stochastic optimization. Recent works have shown that utilizing both\n",
      "decentralized computing and local stochastic gradient estimates can outperform\n",
      "state-of-the-art centralized algorithms, in applications involving highly\n",
      "non-convex problems, such as training deep neural networks.\n",
      "  In this work, we propose a decentralized stochastic algorithm to deal with\n",
      "certain smooth non-convex problems where there are $m$ nodes in the system, and\n",
      "each node has a large number of samples (denoted as $n$). Differently from the\n",
      "majority of the existing decentralized learning algorithms for either\n",
      "stochastic or finite-sum problems, our focus is given to both reducing the\n",
      "total communication rounds among the nodes, while accessing the minimum number\n",
      "of local data samples. In particular, we propose an algorithm named D-GET\n",
      "(decentralized gradient estimation and tracking), which jointly performs\n",
      "decentralized gradient estimation (which estimates the local gradient using a\n",
      "subset of local samples) and gradient tracking (which tracks the global full\n",
      "gradient using local estimates). We show that, to achieve certain $\\epsilon$\n",
      "stationary solution of the deterministic finite sum problem, the proposed\n",
      "algorithm achieves an $\\mathcal{O}(mn^{1/2}\\epsilon^{-1})$ sample complexity\n",
      "and an $\\mathcal{O}(\\epsilon^{-1})$ communication complexity. These bounds\n",
      "significantly improve upon the best existing bounds of\n",
      "$\\mathcal{O}(mn\\epsilon^{-1})$ and $\\mathcal{O}(\\epsilon^{-1})$, respectively.\n",
      "Similarly, for online problems, the proposed method achieves an $\\mathcal{O}(m\n",
      "\\epsilon^{-3/2})$ sample complexity and an $\\mathcal{O}(\\epsilon^{-1})$\n",
      "communication complexity, while the best existing bounds are\n",
      "$\\mathcal{O}(m\\epsilon^{-2})$ and $\\mathcal{O}(\\epsilon^{-2})$, respectively.\n",
      "\n",
      "**Paper Id :2002.05056 \n",
      "Title :Quantum Boosting\n",
      "  Suppose we have a weak learning algorithm $\\mathcal{A}$ for a Boolean-valued\n",
      "problem: $\\mathcal{A}$ produces hypotheses whose bias $\\gamma$ is small, only\n",
      "slightly better than random guessing (this could, for instance, be due to\n",
      "implementing $\\mathcal{A}$ on a noisy device), can we boost the performance of\n",
      "$\\mathcal{A}$ so that $\\mathcal{A}$'s output is correct on $2/3$ of the inputs?\n",
      "  Boosting is a technique that converts a weak and inaccurate machine learning\n",
      "algorithm into a strong accurate learning algorithm. The AdaBoost algorithm by\n",
      "Freund and Schapire (for which they were awarded the G\\\"odel prize in 2003) is\n",
      "one of the widely used boosting algorithms, with many applications in theory\n",
      "and practice. Suppose we have a $\\gamma$-weak learner for a Boolean concept\n",
      "class $C$ that takes time $R(C)$, then the time complexity of AdaBoost scales\n",
      "as $VC(C)\\cdot poly(R(C), 1/\\gamma)$, where $VC(C)$ is the $VC$-dimension of\n",
      "$C$. In this paper, we show how quantum techniques can improve the time\n",
      "complexity of classical AdaBoost. To this end, suppose we have a $\\gamma$-weak\n",
      "quantum learner for a Boolean concept class $C$ that takes time $Q(C)$, we\n",
      "introduce a quantum boosting algorithm whose complexity scales as\n",
      "$\\sqrt{VC(C)}\\cdot poly(Q(C),1/\\gamma);$ thereby achieving a quadratic quantum\n",
      "improvement over classical AdaBoost in terms of $VC(C)$.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.05878 \n",
      "Title :Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces\n",
      "  Transfer learning makes use of data or knowledge in one problem to help solve\n",
      "a different, yet related, problem. It is particularly useful in brain-computer\n",
      "interfaces (BCIs), for coping with variations among different subjects and/or\n",
      "tasks. This paper considers offline unsupervised cross-subject\n",
      "electroencephalogram (EEG) classification, i.e., we have labeled EEG trials\n",
      "from one or more source subjects, but only unlabeled EEG trials from the target\n",
      "subject. We propose a novel manifold embedded knowledge transfer (MEKT)\n",
      "approach, which first aligns the covariance matrices of the EEG trials in the\n",
      "Riemannian manifold, extracts features in the tangent space, and then performs\n",
      "domain adaptation by minimizing the joint probability distribution shift\n",
      "between the source and the target domains, while preserving their geometric\n",
      "structures. MEKT can cope with one or multiple source domains, and can be\n",
      "computed efficiently. We also propose a domain transferability estimation (DTE)\n",
      "approach to identify the most beneficial source domains, in case there are a\n",
      "large number of source domains. Experiments on four EEG datasets from two\n",
      "different BCI paradigms demonstrated that MEKT outperformed several\n",
      "state-of-the-art transfer learning approaches, and DTE can reduce more than\n",
      "half of the computational cost when the number of source subjects is large,\n",
      "with little sacrifice of classification accuracy.\n",
      "\n",
      "**Paper Id :1912.01166 \n",
      "Title :Different Set Domain Adaptation for Brain-Computer Interfaces: A Label\n",
      "  Alignment Approach\n",
      "  A brain-computer interface (BCI) system usually needs a long calibration\n",
      "session for each new subject/task to adjust its parameters, which impedes its\n",
      "transition from the laboratory to real-world applications. Domain adaptation,\n",
      "which leverages labeled data from auxiliary subjects/tasks (source domains),\n",
      "has demonstrated its effectiveness in reducing such calibration effort.\n",
      "Currently, most domain adaptation approaches require the source domains to have\n",
      "the same feature space and label space as the target domain, which limits their\n",
      "applications, as the auxiliary data may have different feature spaces and/or\n",
      "different label spaces. This paper considers different set domain adaptation\n",
      "for BCIs, i.e., the source and target domains have different label spaces. We\n",
      "introduce a practical setting of different label sets for BCIs, and propose a\n",
      "novel label alignment (LA) approach to align the source label space with the\n",
      "target label space. It has three desirable properties: 1) LA only needs as few\n",
      "as one labeled sample from each class of the target subject; 2) LA can be used\n",
      "as a preprocessing step before different feature extraction and classification\n",
      "algorithms; and, 3) LA can be integrated with other domain adaptation\n",
      "approaches to achieve even better performance. Experiments on two motor imagery\n",
      "datasets demonstrated the effectiveness of LA.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.06169 \n",
      "Title :The PGM-index: a multicriteria, compressed and learned approach to data\n",
      "  indexing\n",
      "  The recent introduction of learned indexes has shaken the foundations of the\n",
      "decades-old field of indexing data structures. Combining, or even replacing,\n",
      "classic design elements such as B-tree nodes with machine learning models has\n",
      "proven to give outstanding improvements in the space footprint and time\n",
      "efficiency of data systems. However, these novel approaches are based on\n",
      "heuristics, thus they lack any guarantees both in their time and space\n",
      "requirements. We propose the Piecewise Geometric Model index (shortly,\n",
      "PGM-index), which achieves guaranteed I/O-optimality in query operations,\n",
      "learns an optimal number of linear models, and its peculiar recursive\n",
      "construction makes it a purely learned data structure, rather than a hybrid of\n",
      "traditional and learned indexes (such as RMI and FITing-tree). We show that the\n",
      "PGM-index improves the space of the FITing-tree by 63.3% and of the B-tree by\n",
      "more than four orders of magnitude, while achieving their same or even better\n",
      "query time efficiency. We complement this result by proposing three variants of\n",
      "the PGM-index. First, we design a compressed PGM-index that further reduces its\n",
      "space footprint by exploiting the repetitiveness at the level of the learned\n",
      "linear models it is composed of. Second, we design a PGM-index that adapts\n",
      "itself to the distribution of the queries, thus resulting in the first known\n",
      "distribution-aware learned index to date. Finally, given its flexibility in the\n",
      "offered space-time trade-offs, we propose the multicriteria PGM-index that\n",
      "efficiently auto-tune itself in a few seconds over hundreds of millions of keys\n",
      "to the possibly evolving space-time constraints imposed by the application of\n",
      "use.\n",
      "  We remark to the reader that this paper is an extended and improved version\n",
      "of our previous paper titled \"Superseding traditional indexes by orchestrating\n",
      "learning and geometry\" (arXiv:1903.00507).\n",
      "\n",
      "**Paper Id :2002.10306 \n",
      "Title :Adaptive Propagation Graph Convolutional Network\n",
      "  Graph convolutional networks (GCNs) are a family of neural network models\n",
      "that perform inference on graph data by interleaving vertex-wise operations and\n",
      "message-passing exchanges across nodes. Concerning the latter, two key\n",
      "questions arise: (i) how to design a differentiable exchange protocol (e.g., a\n",
      "1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize\n",
      "the trade-off in complexity with respect to the local updates. In this paper,\n",
      "we show that state-of-the-art results can be achieved by adapting the number of\n",
      "communication steps independently at every node. In particular, we endow each\n",
      "node with a halting unit (inspired by Graves' adaptive computation time) that\n",
      "after every exchange decides whether to continue communicating or not. We show\n",
      "that the proposed adaptive propagation GCN (AP-GCN) achieves superior or\n",
      "similar results to the best proposed models so far on a number of benchmarks,\n",
      "while requiring a small overhead in terms of additional parameters. We also\n",
      "investigate a regularization term to enforce an explicit trade-off between\n",
      "communication and accuracy. The code for the AP-GCN experiments is released as\n",
      "an open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.06840 \n",
      "Title :A Hybrid Compact Neural Architecture for Visual Place Recognition\n",
      "  State-of-the-art algorithms for visual place recognition, and related visual\n",
      "navigation systems, can be broadly split into two categories:\n",
      "computer-science-oriented models including deep learning or image\n",
      "retrieval-based techniques with minimal biological plausibility, and\n",
      "neuroscience-oriented dynamical networks that model temporal properties\n",
      "underlying spatial navigation in the brain. In this letter, we propose a new\n",
      "compact and high-performing place recognition model that bridges this divide\n",
      "for the first time. Our approach comprises two key neural models of these\n",
      "categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by\n",
      "brain architectures of fruit flies, Drosophila melanogaster, and (2) a\n",
      "one-dimensional continuous attractor neural network (CANN). The resulting\n",
      "FlyNet+CANN network incorporates the compact pattern recognition capabilities\n",
      "of our FlyNet model with the powerful temporal filtering capabilities of an\n",
      "equally compact CANN, replicating entirely in a hybrid neural implementation\n",
      "the functionality that yields high performance in algorithmic localization\n",
      "approaches like SeqSLAM. We evaluate our model, and compare it to three\n",
      "state-of-the-art methods, on two benchmark real-world datasets with small\n",
      "viewpoint variations and extreme environmental changes - achieving 87% AUC\n",
      "results under day to night transitions compared to 60% for Multi-Process\n",
      "Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times\n",
      "faster, respectively.\n",
      "\n",
      "**Paper Id :1910.09495 \n",
      "Title :S4NN: temporal backpropagation for spiking neural networks with one\n",
      "  spike per neuron\n",
      "  We propose a new supervised learning rule for multilayer spiking neural\n",
      "networks (SNNs) that use a form of temporal coding known as rank-order-coding.\n",
      "With this coding scheme, all neurons fire exactly one spike per stimulus, but\n",
      "the firing order carries information. In particular, in the readout layer, the\n",
      "first neuron to fire determines the class of the stimulus. We derive a new\n",
      "learning rule for this sort of network, named S4NN, akin to traditional error\n",
      "backpropagation, yet based on latencies. We show how approximated error\n",
      "gradients can be computed backward in a feedforward network with any number of\n",
      "layers. This approach reaches state-of-the-art performance with supervised\n",
      "multi fully-connected layer SNNs: test accuracy of 97.4% for the MNIST dataset,\n",
      "and 99.2% for the Caltech Face/Motorbike dataset. Yet, the neuron model that we\n",
      "use, non-leaky integrate-and-fire, is much simpler than the one used in all\n",
      "previous works. The source codes of the proposed S4NN are publicly available at\n",
      "https://github.com/SRKH/S4NN.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.06948 \n",
      "Title :Data-Driven Deep Learning of Partial Differential Equations in Modal\n",
      "  Space\n",
      "  We present a framework for recovering/approximating unknown time-dependent\n",
      "partial differential equation (PDE) using its solution data. Instead of\n",
      "identifying the terms in the underlying PDE, we seek to approximate the\n",
      "evolution operator of the underlying PDE numerically. The evolution operator of\n",
      "the PDE, defined in infinite-dimensional space, maps the solution from a\n",
      "current time to a future time and completely characterizes the solution\n",
      "evolution of the underlying unknown PDE. Our recovery strategy relies on\n",
      "approximation of the evolution operator in a properly defined modal space,\n",
      "i.e., generalized Fourier space, in order to reduce the problem to finite\n",
      "dimensions. The finite dimensional approximation is then accomplished by\n",
      "training a deep neural network structure, which is based on residual network\n",
      "(ResNet), using the given data. Error analysis is provided to illustrate the\n",
      "predictive accuracy of the proposed method. A set of examples of different\n",
      "types of PDEs, including inviscid Burgers' equation that develops discontinuity\n",
      "in its solution, are presented to demonstrate the effectiveness of the proposed\n",
      "method.\n",
      "\n",
      "**Paper Id :1811.01165 \n",
      "Title :Convergence of the Deep BSDE Method for Coupled FBSDEs\n",
      "  The recently proposed numerical algorithm, deep BSDE method, has shown\n",
      "remarkable performance in solving high-dimensional forward-backward stochastic\n",
      "differential equations (FBSDEs) and parabolic partial differential equations\n",
      "(PDEs). This article lays a theoretical foundation for the deep BSDE method in\n",
      "the general case of coupled FBSDEs. In particular, a posteriori error\n",
      "estimation of the solution is provided and it is proved that the error\n",
      "converges to zero given the universal approximation capability of neural\n",
      "networks. Numerical results are presented to demonstrate the accuracy of the\n",
      "analyzed algorithm in solving high-dimensional coupled FBSDEs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.07476 \n",
      "Title :Hidden Unit Specialization in Layered Neural Networks: ReLU vs.\n",
      "  Sigmoidal Activation\n",
      "  We study layered neural networks of rectified linear units (ReLU) in a\n",
      "modelling framework for stochastic training processes. The comparison with\n",
      "sigmoidal activation functions is in the center of interest. We compute typical\n",
      "learning curves for shallow networks with K hidden units in matching student\n",
      "teacher scenarios. The systems exhibit sudden changes of the generalization\n",
      "performance via the process of hidden unit specialization at critical sizes of\n",
      "the training set. Surprisingly, our results show that the training behavior of\n",
      "ReLU networks is qualitatively different from that of networks with sigmoidal\n",
      "activations. In networks with K >= 3 sigmoidal hidden units, the transition is\n",
      "discontinuous: Specialized network configurations co-exist and compete with\n",
      "states of poor performance even for very large training sets. On the contrary,\n",
      "the use of ReLU activations results in continuous transitions for all K: For\n",
      "large enough training sets, two competing, differently specialized states\n",
      "display similar generalization abilities, which coincide exactly for large\n",
      "networks in the limit K to infinity.\n",
      "\n",
      "**Paper Id :1910.09918 \n",
      "Title :From complex to simple : hierarchical free-energy landscape renormalized\n",
      "  in deep neural networks\n",
      "  We develop a statistical mechanical approach based on the replica method to\n",
      "study the design space of deep and wide neural networks constrained to meet a\n",
      "large number of training data. Specifically, we analyze the configuration space\n",
      "of the synaptic weights and neurons in the hidden layers in a simple\n",
      "feed-forward perceptron network for two scenarios: a setting with random\n",
      "inputs/outputs and a teacher-student setting. By increasing the strength of\n",
      "constraints,~i.e. increasing the number of training data, successive 2nd order\n",
      "glass transition (random inputs/outputs) or 2nd order crystalline transition\n",
      "(teacher-student setting) take place layer-by-layer starting next to the\n",
      "inputs/outputs boundaries going deeper into the bulk with the thickness of the\n",
      "solid phase growing logarithmically with the data size. This implies the\n",
      "typical storage capacity of the network grows exponentially fast with the\n",
      "depth. In a deep enough network, the central part remains in the liquid phase.\n",
      "We argue that in systems of finite width N, the weak bias field can remain in\n",
      "the center and plays the role of a symmetry-breaking field that connects the\n",
      "opposite sides of the system. The successive glass transitions bring about a\n",
      "hierarchical free-energy landscape with ultrametricity, which evolves in space:\n",
      "it is most complex close to the boundaries but becomes renormalized into\n",
      "progressively simpler ones in deeper layers. These observations provide clues\n",
      "to understand why deep neural networks operate efficiently. Finally, we present\n",
      "some numerical simulations of learning which reveal spatially heterogeneous\n",
      "glassy dynamics truncated by a finite width $N$ effect.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.07561 \n",
      "Title :A Double Residual Compression Algorithm for Efficient Distributed\n",
      "  Learning\n",
      "  Large-scale machine learning models are often trained by parallel stochastic\n",
      "gradient descent algorithms. However, the communication cost of gradient\n",
      "aggregation and model synchronization between the master and worker nodes\n",
      "becomes the major obstacle for efficient learning as the number of workers and\n",
      "the dimension of the model increase. In this paper, we propose DORE, a DOuble\n",
      "REsidual compression stochastic gradient descent algorithm, to reduce over\n",
      "$95\\%$ of the overall communication such that the obstacle can be immensely\n",
      "mitigated. Our theoretical analyses demonstrate that the proposed strategy has\n",
      "superior convergence properties for both strongly convex and nonconvex\n",
      "objective functions. The experimental results validate that DORE achieves the\n",
      "best communication efficiency while maintaining similar model accuracy and\n",
      "convergence speed in comparison with start-of-the-art baselines.\n",
      "\n",
      "**Paper Id :2010.01342 \n",
      "Title :End-to-End Training of CNN Ensembles for Person Re-Identification\n",
      "  We propose an end-to-end ensemble method for person re-identification (ReID)\n",
      "to address the problem of overfitting in discriminative models. These models\n",
      "are known to converge easily, but they are biased to the training data in\n",
      "general and may produce a high model variance, which is known as overfitting.\n",
      "The ReID task is more prone to this problem due to the large discrepancy\n",
      "between training and test distributions. To address this problem, our proposed\n",
      "ensemble learning framework produces several diverse and accurate base learners\n",
      "in a single DenseNet. Since most of the costly dense blocks are shared, our\n",
      "method is computationally efficient, which makes it favorable compared to the\n",
      "conventional ensemble models. Experiments on several benchmark datasets\n",
      "demonstrate that our method achieves state-of-the-art results. Noticeable\n",
      "performance improvements, especially on relatively small datasets, indicate\n",
      "that the proposed method deals with the overfitting problem effectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.08013 \n",
      "Title :Why bigger is not always better: on finite and infinite neural networks\n",
      "  Recent work has argued that neural networks can be understood theoretically\n",
      "by taking the number of channels to infinity, at which point the outputs become\n",
      "Gaussian process (GP) distributed. However, we note that infinite Bayesian\n",
      "neural networks lack a key facet of the behaviour of real neural networks: the\n",
      "fixed kernel, determined only by network hyperparameters, implies that they\n",
      "cannot do any form of representation learning. The lack of representation or\n",
      "equivalently kernel learning leads to less flexibility and hence worse\n",
      "performance, giving a potential explanation for the inferior performance of\n",
      "infinite networks observed in the literature (e.g. Novak et al. 2019). We give\n",
      "analytic results characterising the prior over representations and\n",
      "representation learning in finite deep linear networks. We show empirically\n",
      "that the representations in SOTA architectures such as ResNets trained with SGD\n",
      "are much closer to those suggested by our deep linear results than by the\n",
      "corresponding infinite network. This motivates the introduction of a new class\n",
      "of network: infinite networks with bottlenecks, which inherit the theoretical\n",
      "tractability of infinite networks while at the same time allowing\n",
      "representation learning.\n",
      "\n",
      "**Paper Id :1808.00408 \n",
      "Title :Geometry of energy landscapes and the optimizability of deep neural\n",
      "  networks\n",
      "  Deep neural networks are workhorse models in machine learning with multiple\n",
      "layers of non-linear functions composed in series. Their loss function is\n",
      "highly non-convex, yet empirically even gradient descent minimisation is\n",
      "sufficient to arrive at accurate and predictive models. It is hitherto unknown\n",
      "why are deep neural networks easily optimizable. We analyze the energy\n",
      "landscape of a spin glass model of deep neural networks using random matrix\n",
      "theory and algebraic geometry. We analytically show that the multilayered\n",
      "structure holds the key to optimizability: Fixing the number of parameters and\n",
      "increasing network depth, the number of stationary points in the loss function\n",
      "decreases, minima become more clustered in parameter space, and the tradeoff\n",
      "between the depth and width of minima becomes less severe. Our analytical\n",
      "results are numerically verified through comparison with neural networks\n",
      "trained on a set of classical benchmark datasets. Our model uncovers generic\n",
      "design principles of machine learning models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.08778 \n",
      "Title :Measurement Dependence Inducing Latent Causal Models\n",
      "  We consider the task of causal structure learning over measurement dependence\n",
      "inducing latent (MeDIL) causal models. We show that this task can be framed in\n",
      "terms of the graph theoretic problem of finding edge clique covers,resulting in\n",
      "an algorithm for returning minimal MeDIL causal models (minMCMs). This\n",
      "algorithm is non-parametric, requiring no assumptions about linearity or\n",
      "Gaussianity. Furthermore, despite rather weak assumptions aboutthe class of\n",
      "MeDIL causal models, we show that minimality in minMCMs implies some rather\n",
      "specific and interesting properties. By establishing MeDIL causal models as a\n",
      "semantics for edge clique covers, we also provide a starting point for future\n",
      "work further connecting causal structure learning to developments in graph\n",
      "theory and network science.\n",
      "\n",
      "**Paper Id :2010.02675 \n",
      "Title :Recovering Causal Structures from Low-Order Conditional Independencies\n",
      "  One of the common obstacles for learning causal models from data is that\n",
      "high-order conditional independence (CI) relationships between random variables\n",
      "are difficult to estimate. Since CI tests with conditioning sets of low order\n",
      "can be performed accurately even for a small number of observations, a\n",
      "reasonable approach to determine casual structures is to base merely on the\n",
      "low-order CIs. Recent research has confirmed that, e.g. in the case of sparse\n",
      "true causal models, structures learned even from zero- and first-order\n",
      "conditional independencies yield good approximations of the models. However, a\n",
      "challenging task here is to provide methods that faithfully explain a given set\n",
      "of low-order CIs. In this paper, we propose an algorithm which, for a given set\n",
      "of conditional independencies of order less or equal to $k$, where $k$ is a\n",
      "small fixed number, computes a faithful graphical representation of the given\n",
      "set. Our results complete and generalize the previous work on learning from\n",
      "pairwise marginal independencies. Moreover, they enable to improve upon the 0-1\n",
      "graph model which, e.g. is heavily used in the estimation of genome networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.08798 \n",
      "Title :Data classification by quantum radial basis function networks\n",
      "  Radial basis function (RBF) network is a third layered neural network that is\n",
      "widely used in function approximation and data classification. Here we propose\n",
      "a quantum model of the RBF network. Similar to the classical case, we still use\n",
      "the radial basis functions as the activation functions. Quantum linear\n",
      "algebraic techniques and coherent states can be applied to implement these\n",
      "functions. Differently, we define the state of the weight as a tensor product\n",
      "of single-qubit states. This gives a simple approach to implement the quantum\n",
      "RBF network in the quantum circuits. Theoretically, we prove that the training\n",
      "is almost quadratic faster than the classical one. Numerically, we demonstrate\n",
      "that the quantum RBF network can solve binary classification problems as good\n",
      "as the classical RBF network. While the time used for training is much shorter.\n",
      "\n",
      "**Paper Id :2011.07929 \n",
      "Title :On the equivalence of molecular graph convolution and molecular wave\n",
      "  function with poor basis set\n",
      "  In this study, we demonstrate that the linear combination of atomic orbitals\n",
      "(LCAO), an approximation of quantum physics introduced by Pauling and\n",
      "Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)\n",
      "for molecules. However, GCNs involve unnecessary nonlinearity and deep\n",
      "architecture. We also verify that molecular GCNs are based on a poor basis\n",
      "function set compared with the standard one used in theoretical calculations or\n",
      "quantum chemical simulations. From these observations, we describe the quantum\n",
      "deep field (QDF), a machine learning (ML) model based on an underlying quantum\n",
      "physics, in particular the density functional theory (DFT). We believe that the\n",
      "QDF model can be easily understood because it can be regarded as a single\n",
      "linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to\n",
      "learn an energy functional and a Hohenberg--Kohn map that have nonlinearities\n",
      "inherent in quantum physics and the DFT. For molecular energy prediction tasks,\n",
      "we demonstrated the viability of an ``extrapolation,'' in which we trained a\n",
      "QDF model with small molecules, tested it with large molecules, and achieved\n",
      "high extrapolation performance. This will lead to reliable and practical\n",
      "applications for discovering effective materials. The implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.08978 \n",
      "Title :Attention Enriched Deep Learning Model for Breast Tumor Segmentation in\n",
      "  Ultrasound Images\n",
      "  Incorporating human domain knowledge for breast tumor diagnosis is\n",
      "challenging, since shape, boundary, curvature, intensity, or other common\n",
      "medical priors vary significantly across patients and cannot be employed. This\n",
      "work proposes a new approach for integrating visual saliency into a deep\n",
      "learning model for breast tumor segmentation in ultrasound images. Visual\n",
      "saliency refers to image maps containing regions that are more likely to\n",
      "attract radiologists visual attention. The proposed approach introduces\n",
      "attention blocks into a U-Net architecture, and learns feature representations\n",
      "that prioritize spatial regions with high saliency levels. The validation\n",
      "results demonstrate increased accuracy for tumor segmentation relative to\n",
      "models without salient attention layers. The approach achieved a Dice\n",
      "similarity coefficient of 90.5 percent on a dataset of 510 images. The salient\n",
      "attention model has potential to enhance accuracy and robustness in processing\n",
      "medical images of other organs, by providing a means to incorporate\n",
      "task-specific knowledge into deep learning architectures.\n",
      "\n",
      "**Paper Id :1905.13209 \n",
      "Title :AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n",
      "  Architectures\n",
      "  Learning to represent videos is a very challenging task both algorithmically\n",
      "and computationally. Standard video CNN architectures have been designed by\n",
      "directly extending architectures devised for image understanding to include the\n",
      "time dimension, using modules such as 3D convolutions, or by using two-stream\n",
      "design to capture both appearance and motion in videos. We interpret a video\n",
      "CNN as a collection of multi-stream convolutional blocks connected to each\n",
      "other, and propose the approach of automatically finding neural architectures\n",
      "with better connectivity and spatio-temporal interactions for video\n",
      "understanding. This is done by evolving a population of overly-connected\n",
      "architectures guided by connection weight learning. Architectures combining\n",
      "representations that abstract different input types (i.e., RGB and optical\n",
      "flow) at multiple temporal resolutions are searched for, allowing different\n",
      "types or sources of information to interact with each other. Our method,\n",
      "referred to as AssembleNet, outperforms prior approaches on public video\n",
      "datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n",
      "34.27% accuracy on Moments-in-Time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09077 \n",
      "Title :Looking Ahead: Anticipating Pedestrians Crossing with Future Frames\n",
      "  Prediction\n",
      "  In this paper, we present an end-to-end future-prediction model that focuses\n",
      "on pedestrian safety. Specifically, our model uses previous video frames,\n",
      "recorded from the perspective of the vehicle, to predict if a pedestrian will\n",
      "cross in front of the vehicle. The long term goal of this work is to design a\n",
      "fully autonomous system that acts and reacts as a defensive human driver would\n",
      "--- predicting future events and reacting to mitigate risk. We focus on\n",
      "pedestrian-vehicle interactions because of the high risk of harm to the\n",
      "pedestrian if their actions are miss-predicted. Our end-to-end model consists\n",
      "of two stages: the first stage is an encoder/decoder network that learns to\n",
      "predict future video frames. The second stage is a deep spatio-temporal network\n",
      "that utilizes the predicted frames of the first stage to predict the\n",
      "pedestrian's future action. Our system achieves state-of-the-art accuracy on\n",
      "pedestrian behavior prediction and future frames prediction on the Joint\n",
      "Attention for Autonomous Driving (JAAD) dataset.\n",
      "\n",
      "**Paper Id :2009.04450 \n",
      "Title :Map-Adaptive Goal-Based Trajectory Prediction\n",
      "  We present a new method for multi-modal, long-term vehicle trajectory\n",
      "prediction. Our approach relies on using lane centerlines captured in rich maps\n",
      "of the environment to generate a set of proposed goal paths for each vehicle.\n",
      "Using these paths -- which are generated at run time and therefore dynamically\n",
      "adapt to the scene -- as spatial anchors, we predict a set of goal-based\n",
      "trajectories along with a categorical distribution over the goals. This\n",
      "approach allows us to directly model the goal-directed behavior of traffic\n",
      "actors, which unlocks the potential for more accurate long-term prediction. Our\n",
      "experimental results on both a large-scale internal driving dataset and on the\n",
      "public nuScenes dataset show that our model outperforms state-of-the-art\n",
      "approaches for vehicle trajectory prediction over a 6-second horizon. We also\n",
      "empirically demonstrate that our model is better able to generalize to road\n",
      "scenes from a completely new city than existing methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09116 \n",
      "Title :Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without\n",
      "  Fully-Sampled Data\n",
      "  Deep learning (DL) has emerged as a tool for improving accelerated MRI\n",
      "reconstruction. A common strategy among DL methods is the physics-based\n",
      "approach, where a regularized iterative algorithm alternating between data\n",
      "consistency and a regularizer is unrolled for a finite number of iterations.\n",
      "This unrolled network is then trained end-to-end in a supervised manner, using\n",
      "fully-sampled data as ground truth for the network output. However, in a number\n",
      "of scenarios, it is difficult to obtain fully-sampled datasets, due to\n",
      "physiological constraints such as organ motion or physical constraints such as\n",
      "signal decay. In this work, we tackle this issue and propose a self-supervised\n",
      "learning strategy that enables physics-based DL reconstruction without\n",
      "fully-sampled data. Our approach is to divide the acquired sub-sampled points\n",
      "for each scan into training and validation subsets. During training, data\n",
      "consistency is enforced over the training subset, while the validation subset\n",
      "is used to define the loss function. Results show that the proposed\n",
      "self-supervised learning method successfully reconstructs images without\n",
      "fully-sampled data, performing similarly to the supervised approach that is\n",
      "trained with fully-sampled references. This has implications for physics-based\n",
      "inverse problem approaches for other settings, where fully-sampled data is not\n",
      "available or possible to acquire.\n",
      "\n",
      "**Paper Id :1907.11374 \n",
      "Title :Deep-learning-based Optimization of the Under-sampling Pattern in MRI\n",
      "  In compressed sensing MRI (CS-MRI), k-space measurements are under-sampled to\n",
      "achieve accelerated scan times. CS-MRI presents two fundamental problems: (1)\n",
      "where to sample and (2) how to reconstruct an under-sampled scan. In this\n",
      "paper, we tackle both problems simultaneously for the specific case of 2D\n",
      "Cartesian sampling, using a novel end-to-end learning framework that we call\n",
      "LOUPE (Learning-based Optimization of the Under-sampling PattErn). Our method\n",
      "trains a neural network model on a set of full-resolution MRI scans, which are\n",
      "retrospectively under-sampled on a 2D Cartesian grid and forwarded to an\n",
      "anti-aliasing (a.k.a. reconstruction) model that computes a reconstruction,\n",
      "which is in turn compared with the input. This formulation enables a\n",
      "data-driven optimized under-sampling pattern at a given sparsity level. In our\n",
      "experiments, we demonstrate that LOUPE-optimized under-sampling masks are\n",
      "data-dependent, varying significantly with the imaged anatomy, and perform well\n",
      "with different reconstruction methods. We present empirical results obtained\n",
      "with a large-scale, publicly available knee MRI dataset, where LOUPE offered\n",
      "superior reconstruction quality across different conditions. Even with an\n",
      "aggressive 8-fold acceleration rate, LOUPE's reconstructions contained much of\n",
      "the anatomical detail that was missed by alternative masks and reconstruction\n",
      "methods. Our experiments also show how LOUPE yielded optimal under-sampling\n",
      "patterns that were significantly different for brain vs knee MRI scans. Our\n",
      "code is made freely available at https://github.com/cagladbahadir/LOUPE/.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09153 \n",
      "Title :Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time\n",
      "  Series Analysis\n",
      "  In this work, we develop a novel framework to measure the similarity between\n",
      "dynamic financial networks, i.e., time-varying financial networks.\n",
      "Particularly, we explore whether the proposed similarity measure can be\n",
      "employed to understand the structural evolution of the financial networks with\n",
      "time. For a set of time-varying financial networks with each vertex\n",
      "representing the individual time series of a different stock and each edge\n",
      "between a pair of time series representing the absolute value of their Pearson\n",
      "correlation, our start point is to compute the commute time matrix associated\n",
      "with the weighted adjacency matrix of the network structures, where each\n",
      "element of the matrix can be seen as the enhanced correlation value between\n",
      "pairwise stocks. For each network, we show how the commute time matrix allows\n",
      "us to identify a reliable set of dominant correlated time series as well as an\n",
      "associated dominant probability distribution of the stock belonging to this\n",
      "set. Furthermore, we represent each original network as a discrete dominant\n",
      "Shannon entropy time series computed from the dominant probability\n",
      "distribution. With the dominant entropy time series for each pair of financial\n",
      "networks to hand, we develop a similarity measure based on the classical\n",
      "dynamic time warping framework, for analyzing the financial time-varying\n",
      "networks. We show that the proposed similarity measure is positive definite and\n",
      "thus corresponds to a kernel measure on graphs. The proposed kernel bridges the\n",
      "gap between graph kernels and the classical dynamic time warping framework for\n",
      "multiple financial time series analysis. Experiments on time-varying networks\n",
      "extracted through New York Stock Exchange (NYSE) database demonstrate the\n",
      "effectiveness of the proposed approach.\n",
      "\n",
      "**Paper Id :2011.02179 \n",
      "Title :Node-Centric Graph Learning from Data for Brain State Identification\n",
      "  Data-driven graph learning models a network by determining the strength of\n",
      "connections between its nodes. The data refers to a graph signal which\n",
      "associates a value with each graph node. Existing graph learning methods either\n",
      "use simplified models for the graph signal, or they are prohibitively expensive\n",
      "in terms of computational and memory requirements. This is particularly true\n",
      "when the number of nodes is high or there are temporal changes in the network.\n",
      "In order to consider richer models with a reasonable computational\n",
      "tractability, we introduce a graph learning method based on representation\n",
      "learning on graphs. Representation learning generates an embedding for each\n",
      "graph node, taking the information from neighbouring nodes into account. Our\n",
      "graph learning method further modifies the embeddings to compute the graph\n",
      "similarity matrix. In this work, graph learning is used to examine brain\n",
      "networks for brain state identification. We infer time-varying brain graphs\n",
      "from an extensive dataset of intracranial electroencephalographic (iEEG)\n",
      "signals from ten patients. We then apply the graphs as input to a classifier to\n",
      "distinguish seizure vs. non-seizure brain states. Using the binary\n",
      "classification metric of area under the receiver operating characteristic curve\n",
      "(AUC), this approach yields an average of 9.13 percent improvement when\n",
      "compared to two widely used brain network modeling methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09200 \n",
      "Title :Deep Reinforcement Learning Control of Quantum Cartpoles\n",
      "  We generalize a standard benchmark of reinforcement learning, the classical\n",
      "cartpole balancing problem, to the quantum regime by stabilizing a particle in\n",
      "an unstable potential through measurement and feedback. We use state-of-the-art\n",
      "deep reinforcement learning to stabilize a quantum cartpole and find that our\n",
      "deep learning approach performs comparably to or better than other strategies\n",
      "in standard control theory. Our approach also applies to measurement-feedback\n",
      "cooling of quantum oscillators, showing the applicability of deep learning to\n",
      "general continuous-space quantum control.\n",
      "\n",
      "**Paper Id :2003.06413 \n",
      "Title :Equivariant flow-based sampling for lattice gauge theory\n",
      "  We define a class of machine-learned flow-based sampling algorithms for\n",
      "lattice gauge theories that are gauge-invariant by construction. We demonstrate\n",
      "the application of this framework to U(1) gauge theory in two spacetime\n",
      "dimensions, and find that near critical points in parameter space the approach\n",
      "is orders of magnitude more efficient at sampling topological quantities than\n",
      "more traditional sampling procedures such as Hybrid Monte Carlo and Heat Bath.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09349 \n",
      "Title :Variational Integrator Networks for Physically Structured Embeddings\n",
      "  Learning workable representations of dynamical systems is becoming an\n",
      "increasingly important problem in a number of application areas. By leveraging\n",
      "recent work connecting deep neural networks to systems of differential\n",
      "equations, we propose \\emph{variational integrator networks}, a class of neural\n",
      "network architectures designed to preserve the geometric structure of physical\n",
      "systems. This class of network architectures facilitates accurate long-term\n",
      "prediction, interpretability, and data-efficient learning, while still\n",
      "remaining highly flexible and capable of modeling complex behavior. We\n",
      "demonstrate that they can accurately learn dynamical systems from both noisy\n",
      "observations in phase space and from image pixels within which the unknown\n",
      "dynamics are embedded.\n",
      "\n",
      "**Paper Id :2002.05909 \n",
      "Title :Deep reconstruction of strange attractors from time series\n",
      "  Experimental measurements of physical systems often have a limited number of\n",
      "independent channels, causing essential dynamical variables to remain\n",
      "unobserved. However, many popular methods for unsupervised inference of latent\n",
      "dynamics from experimental data implicitly assume that the measurements have\n",
      "higher intrinsic dimensionality than the underlying system---making coordinate\n",
      "identification a dimensionality reduction problem. Here, we study the opposite\n",
      "limit, in which hidden governing coordinates must be inferred from only a\n",
      "low-dimensional time series of measurements. Inspired by classical analysis\n",
      "techniques for partial observations of chaotic attractors, we introduce a\n",
      "general embedding technique for univariate and multivariate time series,\n",
      "consisting of an autoencoder trained with a novel latent-space loss function.\n",
      "We show that our technique reconstructs the strange attractors of synthetic and\n",
      "real-world systems better than existing techniques, and that it creates\n",
      "consistent, predictive representations of even stochastic systems. We conclude\n",
      "by using our technique to discover dynamical attractors in diverse systems such\n",
      "as patient electrocardiograms, household electricity usage, neural spiking, and\n",
      "eruptions of the Old Faithful geyser---demonstrating diverse applications of\n",
      "our technique for exploratory data analysis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09358 \n",
      "Title :A Decision-Theoretic Approach for Model Interpretability in Bayesian\n",
      "  Framework\n",
      "  A salient approach to interpretable machine learning is to restrict modeling\n",
      "to simple models. In the Bayesian framework, this can be pursued by restricting\n",
      "the model structure and prior to favor interpretable models. Fundamentally,\n",
      "however, interpretability is about users' preferences, not the data generation\n",
      "mechanism; it is more natural to formulate interpretability as a utility\n",
      "function. In this work, we propose an interpretability utility, which\n",
      "explicates the trade-off between explanation fidelity and interpretability in\n",
      "the Bayesian framework. The method consists of two steps. First, a reference\n",
      "model, possibly a black-box Bayesian predictive model which does not compromise\n",
      "accuracy, is fitted to the training data. Second, a proxy model from an\n",
      "interpretable model family that best mimics the predictive behaviour of the\n",
      "reference model is found by optimizing the interpretability utility function.\n",
      "The approach is model agnostic -- neither the interpretable model nor the\n",
      "reference model are restricted to a certain class of models -- and the\n",
      "optimization problem can be solved using standard tools. Through experiments on\n",
      "real-word data sets, using decision trees as interpretable models and Bayesian\n",
      "additive regression models as reference models, we show that for the same level\n",
      "of interpretability, our approach generates more accurate models than the\n",
      "alternative of restricting the prior. We also propose a systematic way to\n",
      "measure stability of interpretabile models constructed by different\n",
      "interpretability approaches and show that our proposed approach generates more\n",
      "stable models.\n",
      "\n",
      "**Paper Id :2006.01673 \n",
      "Title :Learning Opinion Dynamics From Social Traces\n",
      "  Opinion dynamics - the research field dealing with how people's opinions form\n",
      "and evolve in a social context - traditionally uses agent-based models to\n",
      "validate the implications of sociological theories. These models encode the\n",
      "causal mechanism that drives the opinion formation process, and have the\n",
      "advantage of being easy to interpret. However, as they do not exploit the\n",
      "availability of data, their predictive power is limited. Moreover, parameter\n",
      "calibration and model selection are manual and difficult tasks.\n",
      "  In this work we propose an inference mechanism for fitting a generative,\n",
      "agent-like model of opinion dynamics to real-world social traces. Given a set\n",
      "of observables (e.g., actions and interactions between agents), our model can\n",
      "recover the most-likely latent opinion trajectories that are compatible with\n",
      "the assumptions about the process dynamics. This type of model retains the\n",
      "benefits of agent-based ones (i.e., causal interpretation), while adding the\n",
      "ability to perform model selection and hypothesis testing on real data.\n",
      "  We showcase our proposal by translating a classical agent-based model of\n",
      "opinion dynamics into its generative counterpart. We then design an inference\n",
      "algorithm based on online expectation maximization to learn the latent\n",
      "parameters of the model. Such algorithm can recover the latent opinion\n",
      "trajectories from traces generated by the classical agent-based model. In\n",
      "addition, it can identify the most likely set of macro parameters used to\n",
      "generate a data trace, thus allowing testing of sociological hypotheses.\n",
      "Finally, we apply our model to real-world data from Reddit to explore the\n",
      "long-standing question about the impact of backfire effect. Our results suggest\n",
      "a low prominence of the effect in Reddit's political conversation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "**Paper Id :1911.06147 \n",
      "Title :t-SS3: a text classifier with dynamic n-grams for early risk detection\n",
      "  over text streams\n",
      "  A recently introduced classifier, called SS3, has shown to be well suited to\n",
      "deal with early risk detection (ERD) problems on text streams. It obtained\n",
      "state-of-the-art performance on early depression and anorexia detection on\n",
      "Reddit in the CLEF's eRisk open tasks. SS3 was created to deal with ERD\n",
      "problems naturally since: it supports incremental training and classification\n",
      "over text streams, and it can visually explain its rationale. However, SS3\n",
      "processes the input using a bag-of-word model lacking the ability to recognize\n",
      "important word sequences. This aspect could negatively affect the\n",
      "classification performance and also reduces the descriptiveness of visual\n",
      "explanations. In the standard document classification field, it is very common\n",
      "to use word n-grams to try to overcome some of these limitations.\n",
      "Unfortunately, when working with text streams, using n-grams is not trivial\n",
      "since the system must learn and recognize which n-grams are important \"on the\n",
      "fly\". This paper introduces t-SS3, an extension of SS3 that allows it to\n",
      "recognize useful patterns over text streams dynamically. We evaluated our model\n",
      "in the eRisk 2017 and 2018 tasks on early depression and anorexia detection.\n",
      "Experimental results suggest that t-SS3 is able to improve both current results\n",
      "and the richness of visual explanations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09495 \n",
      "Title :S4NN: temporal backpropagation for spiking neural networks with one\n",
      "  spike per neuron\n",
      "  We propose a new supervised learning rule for multilayer spiking neural\n",
      "networks (SNNs) that use a form of temporal coding known as rank-order-coding.\n",
      "With this coding scheme, all neurons fire exactly one spike per stimulus, but\n",
      "the firing order carries information. In particular, in the readout layer, the\n",
      "first neuron to fire determines the class of the stimulus. We derive a new\n",
      "learning rule for this sort of network, named S4NN, akin to traditional error\n",
      "backpropagation, yet based on latencies. We show how approximated error\n",
      "gradients can be computed backward in a feedforward network with any number of\n",
      "layers. This approach reaches state-of-the-art performance with supervised\n",
      "multi fully-connected layer SNNs: test accuracy of 97.4% for the MNIST dataset,\n",
      "and 99.2% for the Caltech Face/Motorbike dataset. Yet, the neuron model that we\n",
      "use, non-leaky integrate-and-fire, is much simpler than the one used in all\n",
      "previous works. The source codes of the proposed S4NN are publicly available at\n",
      "https://github.com/SRKH/S4NN.\n",
      "\n",
      "**Paper Id :2002.08204 \n",
      "Title :SYMOG: learning symmetric mixture of Gaussian modes for improved\n",
      "  fixed-point quantization\n",
      "  Deep neural networks (DNNs) have been proven to outperform classical methods\n",
      "on several machine learning benchmarks. However, they have high computational\n",
      "complexity and require powerful processing units. Especially when deployed on\n",
      "embedded systems, model size and inference time must be significantly reduced.\n",
      "We propose SYMOG (symmetric mixture of Gaussian modes), which significantly\n",
      "decreases the complexity of DNNs through low-bit fixed-point quantization.\n",
      "SYMOG is a novel soft quantization method such that the learning task and the\n",
      "quantization are solved simultaneously. During training the weight distribution\n",
      "changes from an unimodal Gaussian distribution to a symmetric mixture of\n",
      "Gaussians, where each mean value belongs to a particular fixed-point mode. We\n",
      "evaluate our approach with different architectures (LeNet5, VGG7, VGG11,\n",
      "DenseNet) on common benchmark data sets (MNIST, CIFAR-10, CIFAR-100) and we\n",
      "compare with state-of-the-art quantization approaches. We achieve excellent\n",
      "results and outperform 2-bit state-of-the-art performance with an error rate of\n",
      "only 5.71% on CIFAR-10 and 27.65% on CIFAR-100.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09529 \n",
      "Title :Adaptive Gradient Descent without Descent\n",
      "  We present a strikingly simple proof that two rules are sufficient to\n",
      "automate gradient descent: 1) don't increase the stepsize too fast and 2) don't\n",
      "overstep the local curvature. No need for functional values, no line search, no\n",
      "information about the function except for the gradients. By following these\n",
      "rules, you get a method adaptive to the local geometry, with convergence\n",
      "guarantees depending only on the smoothness in a neighborhood of a solution.\n",
      "Given that the problem is convex, our method converges even if the global\n",
      "smoothness constant is infinity. As an illustration, it can minimize arbitrary\n",
      "continuously twice-differentiable convex function. We examine its performance\n",
      "on a range of convex and nonconvex problems, including logistic regression and\n",
      "matrix factorization.\n",
      "\n",
      "**Paper Id :2006.01759 \n",
      "Title :Sparse Perturbations for Improved Convergence in Stochastic Zeroth-Order\n",
      "  Optimization\n",
      "  Interest in stochastic zeroth-order (SZO) methods has recently been revived\n",
      "in black-box optimization scenarios such as adversarial black-box attacks to\n",
      "deep neural networks. SZO methods only require the ability to evaluate the\n",
      "objective function at random input points, however, their weakness is the\n",
      "dependency of their convergence speed on the dimensionality of the function to\n",
      "be evaluated. We present a sparse SZO optimization method that reduces this\n",
      "factor to the expected dimensionality of the random perturbation during\n",
      "learning. We give a proof that justifies this reduction for sparse SZO\n",
      "optimization for non-convex functions without making any assumptions on\n",
      "sparsity of objective function or gradient. Furthermore, we present\n",
      "experimental results for neural networks on MNIST and CIFAR that show faster\n",
      "convergence in training loss and test accuracy, and a smaller distance of the\n",
      "gradient approximation to the true gradient in sparse SZO compared to dense\n",
      "SZO.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09798 \n",
      "Title :Improving Siamese Networks for One Shot Learning using Kernel Based\n",
      "  Activation functions\n",
      "  The lack of a large amount of training data has always been the constraining\n",
      "factor in solving a lot of problems in machine learning, making One Shot\n",
      "Learning one of the most intriguing ideas in machine learning. It aims to learn\n",
      "information about object categories from one, or only a few training examples.\n",
      "This process of learning in deep learning is usually accomplished by proper\n",
      "objective function, i.e; loss function and embeddings extraction i.e;\n",
      "architecture. In this paper, we discussed about metrics based deep learning\n",
      "architectures for one shot learning such as Siamese neural networks and present\n",
      "a method to improve on their accuracy using Kafnets (kernel-based\n",
      "non-parametric activation functions for neural networks) by learning proper\n",
      "embeddings with relatively less number of epochs. Using kernel activation\n",
      "functions, we are able to achieve strong results which exceed those of ReLU\n",
      "based deep learning models in terms of embeddings structure, loss convergence,\n",
      "and accuracy.\n",
      "\n",
      "**Paper Id :2007.05830 \n",
      "Title :AutoEmbedder: A semi-supervised DNN embedding system for clustering\n",
      "  Clustering is widely used in unsupervised learning method that deals with\n",
      "unlabeled data. Deep clustering has become a popular study area that relates\n",
      "clustering with Deep Neural Network (DNN) architecture. Deep clustering method\n",
      "downsamples high dimensional data, which may also relate clustering loss. Deep\n",
      "clustering is also introduced in semi-supervised learning (SSL). Most SSL\n",
      "methods depend on pairwise constraint information, which is a matrix containing\n",
      "knowledge if data pairs can be in the same cluster or not. This paper\n",
      "introduces a novel embedding system named AutoEmbedder, that downsamples higher\n",
      "dimensional data to clusterable embedding points. To the best of our knowledge,\n",
      "this is the first research endeavor that relates to traditional classifier DNN\n",
      "architecture with a pairwise loss reduction technique. The training process is\n",
      "semi-supervised and uses Siamese network architecture to compute pairwise\n",
      "constraint loss in the feature learning phase. The AutoEmbedder outperforms\n",
      "most of the existing DNN based semi-supervised methods tested on famous\n",
      "datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.09918 \n",
      "Title :From complex to simple : hierarchical free-energy landscape renormalized\n",
      "  in deep neural networks\n",
      "  We develop a statistical mechanical approach based on the replica method to\n",
      "study the design space of deep and wide neural networks constrained to meet a\n",
      "large number of training data. Specifically, we analyze the configuration space\n",
      "of the synaptic weights and neurons in the hidden layers in a simple\n",
      "feed-forward perceptron network for two scenarios: a setting with random\n",
      "inputs/outputs and a teacher-student setting. By increasing the strength of\n",
      "constraints,~i.e. increasing the number of training data, successive 2nd order\n",
      "glass transition (random inputs/outputs) or 2nd order crystalline transition\n",
      "(teacher-student setting) take place layer-by-layer starting next to the\n",
      "inputs/outputs boundaries going deeper into the bulk with the thickness of the\n",
      "solid phase growing logarithmically with the data size. This implies the\n",
      "typical storage capacity of the network grows exponentially fast with the\n",
      "depth. In a deep enough network, the central part remains in the liquid phase.\n",
      "We argue that in systems of finite width N, the weak bias field can remain in\n",
      "the center and plays the role of a symmetry-breaking field that connects the\n",
      "opposite sides of the system. The successive glass transitions bring about a\n",
      "hierarchical free-energy landscape with ultrametricity, which evolves in space:\n",
      "it is most complex close to the boundaries but becomes renormalized into\n",
      "progressively simpler ones in deeper layers. These observations provide clues\n",
      "to understand why deep neural networks operate efficiently. Finally, we present\n",
      "some numerical simulations of learning which reveal spatially heterogeneous\n",
      "glassy dynamics truncated by a finite width $N$ effect.\n",
      "\n",
      "**Paper Id :1910.07476 \n",
      "Title :Hidden Unit Specialization in Layered Neural Networks: ReLU vs.\n",
      "  Sigmoidal Activation\n",
      "  We study layered neural networks of rectified linear units (ReLU) in a\n",
      "modelling framework for stochastic training processes. The comparison with\n",
      "sigmoidal activation functions is in the center of interest. We compute typical\n",
      "learning curves for shallow networks with K hidden units in matching student\n",
      "teacher scenarios. The systems exhibit sudden changes of the generalization\n",
      "performance via the process of hidden unit specialization at critical sizes of\n",
      "the training set. Surprisingly, our results show that the training behavior of\n",
      "ReLU networks is qualitatively different from that of networks with sigmoidal\n",
      "activations. In networks with K >= 3 sigmoidal hidden units, the transition is\n",
      "discontinuous: Specialized network configurations co-exist and compete with\n",
      "states of poor performance even for very large training sets. On the contrary,\n",
      "the use of ReLU activations results in continuous transitions for all K: For\n",
      "large enough training sets, two competing, differently specialized states\n",
      "display similar generalization abilities, which coincide exactly for large\n",
      "networks in the limit K to infinity.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.10397 \n",
      "Title :Efficient Decoupled Neural Architecture Search by Structure and\n",
      "  Operation Sampling\n",
      "  We propose a novel neural architecture search algorithm via reinforcement\n",
      "learning by decoupling structure and operation search processes. Our approach\n",
      "samples candidate models from the multinomial distribution on the policy\n",
      "vectors defined on the two search spaces independently. The proposed technique\n",
      "improves the efficiency of architecture search process significantly compared\n",
      "to the conventional methods based on reinforcement learning with the RNN\n",
      "controllers while achieving competitive accuracy and model size in target\n",
      "tasks. Our policy vectors are easily interpretable throughout the training\n",
      "procedure, which allows to analyze the search progress and the discovered\n",
      "architectures; the black-box characteristics of the RNN controllers hamper\n",
      "understanding training progress in terms of policy parameter updates. Our\n",
      "experiments demonstrate outstanding performance compared to the\n",
      "state-of-the-art methods with a fraction of search cost.\n",
      "\n",
      "**Paper Id :2011.07225 \n",
      "Title :Reinforced Molecular Optimization with Neighborhood-Controlled Grammars\n",
      "  A major challenge in the pharmaceutical industry is to design novel molecules\n",
      "with specific desired properties, especially when the property evaluation is\n",
      "costly. Here, we propose MNCE-RL, a graph convolutional policy network for\n",
      "molecular optimization with molecular neighborhood-controlled embedding\n",
      "grammars through reinforcement learning. We extend the original\n",
      "neighborhood-controlled embedding grammars to make them applicable to molecular\n",
      "graph generation and design an efficient algorithm to infer grammatical\n",
      "production rules from given molecules. The use of grammars guarantees the\n",
      "validity of the generated molecular structures. By transforming molecular\n",
      "graphs to parse trees with the inferred grammars, the molecular structure\n",
      "generation task is modeled as a Markov decision process where a policy gradient\n",
      "strategy is utilized. In a series of experiments, we demonstrate that our\n",
      "approach achieves state-of-the-art performance in a diverse range of molecular\n",
      "optimization tasks and exhibits significant superiority in optimizing molecular\n",
      "properties with a limited number of property evaluations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.10528 \n",
      "Title :ASNM Datasets: A Collection of Network Traffic Features for Testing of\n",
      "  Adversarial Classifiers and Network Intrusion Detectors\n",
      "  In this paper, we present three datasets that have been built from network\n",
      "traffic traces using ASNM features, designed in our previous work. The first\n",
      "dataset was built using a state-of-the-art dataset called CDX 2009, while the\n",
      "remaining two datasets were collected by us in 2015 and 2018, respectively.\n",
      "These two datasets contain several adversarial obfuscation techniques that were\n",
      "applied onto malicious as well as legitimate traffic samples during the\n",
      "execution of particular TCP network connections. Adversarial obfuscation\n",
      "techniques were used for evading machine learning-based network intrusion\n",
      "detection classifiers. Further, we showed that the performance of such\n",
      "classifiers can be improved when partially augmenting their training data by\n",
      "samples obtained from obfuscation techniques. In detail, we utilized tunneling\n",
      "obfuscation in HTTP(S) protocol and non-payload-based obfuscations modifying\n",
      "various properties of network traffic by, e.g., TCP segmentation,\n",
      "re-transmissions, corrupting and reordering of packets, etc. To the best of our\n",
      "knowledge, this is the first collection of network traffic metadata that\n",
      "contains adversarial techniques and is intended for non-payload-based network\n",
      "intrusion detection and adversarial classification. Provided datasets enable\n",
      "testing of the evasion resistance of arbitrary classifier that is using ASNM\n",
      "features.\n",
      "\n",
      "**Paper Id :1904.09433 \n",
      "Title :Can Machine Learning Model with Static Features be Fooled: an\n",
      "  Adversarial Machine Learning Approach\n",
      "  The widespread adoption of smartphones dramatically increases the risk of\n",
      "attacks and the spread of mobile malware, especially on the Android platform.\n",
      "Machine learning-based solutions have been already used as a tool to supersede\n",
      "signature-based anti-malware systems. However, malware authors leverage\n",
      "features from malicious and legitimate samples to estimate statistical\n",
      "difference in-order to create adversarial examples. Hence, to evaluate the\n",
      "vulnerability of machine learning algorithms in malware detection, we propose\n",
      "five different attack scenarios to perturb malicious applications (apps). By\n",
      "doing this, the classification algorithm inappropriately fits the discriminant\n",
      "function on the set of data points, eventually yielding a higher\n",
      "misclassification rate. Further, to distinguish the adversarial examples from\n",
      "benign samples, we propose two defense mechanisms to counter attacks. To\n",
      "validate our attacks and solutions, we test our model on three different\n",
      "benchmark datasets. We also test our methods using various classifier\n",
      "algorithms and compare them with the state-of-the-art data poisoning method\n",
      "using the Jacobian matrix. Promising results show that generated adversarial\n",
      "samples can evade detection with a very high probability. Additionally, evasive\n",
      "variants generated by our attack models when used to harden the developed\n",
      "anti-malware system improves the detection rate up to 50% when using the\n",
      "Generative Adversarial Network (GAN) method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.11516 \n",
      "Title :Leveraging Legacy Data to Accelerate Materials Design via Preference\n",
      "  Learning\n",
      "  Machine learning applications in materials science are often hampered by\n",
      "shortage of experimental data. Integration with legacy data from past\n",
      "experiments is a viable way to solve the problem, but complex calibration is\n",
      "often necessary to use the data obtained under different conditions. In this\n",
      "paper, we present a novel calibration-free strategy to enhance the performance\n",
      "of Bayesian optimization with preference learning. The entire learning process\n",
      "is solely based on pairwise comparison of quantities (i.e., higher or lower) in\n",
      "the same dataset, and experimental design can be done without comparing\n",
      "quantities in different datasets. We demonstrate that Bayesian optimization is\n",
      "significantly enhanced via addition of legacy data for organic molecules and\n",
      "inorganic solid-state materials.\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.11561 \n",
      "Title :Convergence Analysis of Block Coordinate Algorithms with Determinantal\n",
      "  Sampling\n",
      "  We analyze the convergence rate of the randomized Newton-like method\n",
      "introduced by Qu et. al. (2016) for smooth and convex objectives, which uses\n",
      "random coordinate blocks of a Hessian-over-approximation matrix $\\bM$ instead\n",
      "of the true Hessian. The convergence analysis of the algorithm is challenging\n",
      "because of its complex dependence on the structure of $\\bM$. However, we show\n",
      "that when the coordinate blocks are sampled with probability proportional to\n",
      "their determinant, the convergence rate depends solely on the eigenvalue\n",
      "distribution of matrix $\\bM$, and has an analytically tractable form. To do so,\n",
      "we derive a fundamental new expectation formula for determinantal point\n",
      "processes. We show that determinantal sampling allows us to reason about the\n",
      "optimal subset size of blocks in terms of the spectrum of $\\bM$. Additionally,\n",
      "we provide a numerical evaluation of our analysis, demonstrating cases where\n",
      "determinantal sampling is superior or on par with uniform sampling.\n",
      "\n",
      "**Paper Id :2002.04756 \n",
      "Title :Average-case Acceleration Through Spectral Density Estimation\n",
      "  We develop a framework for the average-case analysis of random quadratic\n",
      "problems and derive algorithms that are optimal under this analysis. This\n",
      "yields a new class of methods that achieve acceleration given a model of the\n",
      "Hessian's eigenvalue distribution. We develop explicit algorithms for the\n",
      "uniform, Marchenko-Pastur, and exponential distributions. These methods are\n",
      "momentum-based algorithms, whose hyper-parameters can be estimated without\n",
      "knowledge of the Hessian's smallest singular value, in contrast with classical\n",
      "accelerated methods like Nesterov acceleration and Polyak momentum. Through\n",
      "empirical benchmarks on quadratic and logistic regression problems, we identify\n",
      "regimes in which the the proposed methods improve over classical (worst-case)\n",
      "accelerated methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.11664 \n",
      "Title :SPICE: Self-supervised Pitch Estimation\n",
      "  We propose a model to estimate the fundamental frequency in monophonic audio,\n",
      "often referred to as pitch estimation. We acknowledge the fact that obtaining\n",
      "ground truth annotations at the required temporal and frequency resolution is a\n",
      "particularly daunting task. Therefore, we propose to adopt a self-supervised\n",
      "learning technique, which is able to estimate pitch without any form of\n",
      "supervision. The key observation is that pitch shift maps to a simple\n",
      "translation when the audio signal is analysed through the lens of the\n",
      "constant-Q transform (CQT). We design a self-supervised task by feeding two\n",
      "shifted slices of the CQT to the same convolutional encoder, and require that\n",
      "the difference in the outputs is proportional to the corresponding difference\n",
      "in pitch. In addition, we introduce a small model head on top of the encoder,\n",
      "which is able to determine the confidence of the pitch estimate, so as to\n",
      "distinguish between voiced and unvoiced audio. Our results show that the\n",
      "proposed method is able to estimate pitch at a level of accuracy comparable to\n",
      "fully supervised models, both on clean and noisy audio samples, although it\n",
      "does not require access to large labeled datasets.\n",
      "\n",
      "**Paper Id :2007.13866 \n",
      "Title :se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n",
      "  Residuals in Synthetic Domains\n",
      "  Tracking the 6D pose of objects in video sequences is important for robot\n",
      "manipulation. This task, however, introduces multiple challenges: (i) robot\n",
      "manipulation involves significant occlusions; (ii) data and annotations are\n",
      "troublesome and difficult to collect for 6D poses, which complicates machine\n",
      "learning solutions, and (iii) incremental error drift often accumulates in long\n",
      "term tracking to necessitate re-initialization of the object's pose. This work\n",
      "proposes a data-driven optimization approach for long-term, 6D pose tracking.\n",
      "It aims to identify the optimal relative pose given the current RGB-D\n",
      "observation and a synthetic image conditioned on the previous best estimate and\n",
      "the object's model. The key contribution in this context is a novel neural\n",
      "network architecture, which appropriately disentangles the feature encoding to\n",
      "help reduce domain shift, and an effective 3D orientation representation via\n",
      "Lie Algebra. Consequently, even when the network is trained only with synthetic\n",
      "data can work effectively over real images. Comprehensive experiments over\n",
      "benchmarks - existing ones as well as a new dataset with significant occlusions\n",
      "related to object manipulation - show that the proposed approach achieves\n",
      "consistently robust estimates and outperforms alternatives, even though they\n",
      "have been trained with real images. The approach is also the most\n",
      "computationally efficient among the alternatives and achieves a tracking\n",
      "frequency of 90.9Hz.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.11914 \n",
      "Title :On the convergence of projective-simulation-based reinforcement learning\n",
      "  in Markov decision processes\n",
      "  In recent years, the interest in leveraging quantum effects for enhancing\n",
      "machine learning tasks has significantly increased. Many algorithms speeding up\n",
      "supervised and unsupervised learning were established. The first framework in\n",
      "which ways to exploit quantum resources specifically for the broader context of\n",
      "reinforcement learning were found is projective simulation. Projective\n",
      "simulation presents an agent-based reinforcement learning approach designed in\n",
      "a manner which may support quantum walk-based speed-ups. Although classical\n",
      "variants of projective simulation have been benchmarked against common\n",
      "reinforcement learning algorithms, very few formal theoretical analyses have\n",
      "been provided for its performance in standard learning scenarios. In this\n",
      "paper, we provide a detailed formal discussion of the properties of this model.\n",
      "Specifically, we prove that one version of the projective simulation model,\n",
      "understood as a reinforcement learning approach, converges to optimal behavior\n",
      "in a large class of Markov decision processes. This proof shows that a\n",
      "physically-inspired approach to reinforcement learning can guarantee to\n",
      "converge.\n",
      "\n",
      "**Paper Id :2003.04960 \n",
      "Title :Curriculum Learning for Reinforcement Learning Domains: A Framework and\n",
      "  Survey\n",
      "  Reinforcement learning (RL) is a popular paradigm for addressing sequential\n",
      "decision tasks in which the agent has only limited environmental feedback.\n",
      "Despite many advances over the past three decades, learning in many domains\n",
      "still requires a large amount of interaction with the environment, which can be\n",
      "prohibitively expensive in realistic scenarios. To address this problem,\n",
      "transfer learning has been applied to reinforcement learning such that\n",
      "experience gained in one task can be leveraged when starting to learn the next,\n",
      "harder task. More recently, several lines of research have explored how tasks,\n",
      "or data samples themselves, can be sequenced into a curriculum for the purpose\n",
      "of learning a problem that may otherwise be too difficult to learn from\n",
      "scratch. In this article, we present a framework for curriculum learning (CL)\n",
      "in reinforcement learning, and use it to survey and classify existing CL\n",
      "methods in terms of their assumptions, capabilities, and goals. Finally, we use\n",
      "our framework to find open problems and suggest directions for future RL\n",
      "curriculum learning research.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.12164 \n",
      "Title :Variational Quantum Algorithms for Dimensionality Reduction and\n",
      "  Classification\n",
      "  In this work, we present a quantum neighborhood preserving embedding and a\n",
      "quantum local discriminant embedding for dimensionality reduction and\n",
      "classification. We demonstrate that these two algorithms have an exponential\n",
      "speedup over their respectively classical counterparts. Along the way, we\n",
      "propose a variational quantum generalized eigenvalue solver that finds the\n",
      "generalized eigenvalues and eigenstates of a matrix pencil\n",
      "$(\\mathcal{G},\\mathcal{S})$. As a proof-of-principle, we implement our\n",
      "algorithm to solve $2^5\\times2^5$ generalized eigenvalue problems. Finally, our\n",
      "results offer two optional outputs with quantum or classical form, which can be\n",
      "directly applied in another quantum or classical machine learning process.\n",
      "\n",
      "**Paper Id :2006.10299 \n",
      "Title :A quantum extension of SVM-perf for training nonlinear SVMs in almost\n",
      "  linear time\n",
      "  We propose a quantum algorithm for training nonlinear support vector machines\n",
      "(SVM) for feature space learning where classical input data is encoded in the\n",
      "amplitudes of quantum states. Based on the classical SVM-perf algorithm of\n",
      "Joachims, our algorithm has a running time which scales linearly in the number\n",
      "of training examples $m$ (up to polylogarithmic factors) and applies to the\n",
      "standard soft-margin $\\ell_1$-SVM model. In contrast, while classical SVM-perf\n",
      "has demonstrated impressive performance on both linear and nonlinear SVMs, its\n",
      "efficiency is guaranteed only in certain cases: it achieves linear $m$ scaling\n",
      "only for linear SVMs, where classification is performed in the original input\n",
      "data space, or for the special cases of low-rank or shift-invariant kernels.\n",
      "Similarly, previously proposed quantum algorithms either have super-linear\n",
      "scaling in $m$, or else apply to different SVM models such as the hard-margin\n",
      "or least squares $\\ell_2$-SVM which lack certain desirable properties of the\n",
      "soft-margin $\\ell_1$-SVM model. We classically simulate our algorithm and give\n",
      "evidence that it can perform well in practice, and not only for asymptotically\n",
      "large data sets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.12227 \n",
      "Title :EdgeFool: An Adversarial Image Enhancement Filter\n",
      "  Adversarial examples are intentionally perturbed images that mislead\n",
      "classifiers. These images can, however, be easily detected using denoising\n",
      "algorithms, when high-frequency spatial perturbations are used, or can be\n",
      "noticed by humans, when perturbations are large. In this paper, we propose\n",
      "EdgeFool, an adversarial image enhancement filter that learns structure-aware\n",
      "adversarial perturbations. EdgeFool generates adversarial images with\n",
      "perturbations that enhance image details via training a fully convolutional\n",
      "neural network end-to-end with a multi-task loss function. This loss function\n",
      "accounts for both image detail enhancement and class misleading objectives. We\n",
      "evaluate EdgeFool on three classifiers (ResNet-50, ResNet-18 and AlexNet) using\n",
      "two datasets (ImageNet and Private-Places365) and compare it with six\n",
      "adversarial methods (DeepFool, SparseFool, Carlini-Wagner, SemanticAdv,\n",
      "Non-targeted and Private Fast Gradient Sign Methods). Code is available at\n",
      "https://github.com/smartcameras/EdgeFool.git.\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.12752 \n",
      "Title :Analytical classical density functionals from an equation learning\n",
      "  network\n",
      "  We explore the feasibility of using machine learning methods to obtain an\n",
      "analytic form of the classical free energy functional for two model fluids,\n",
      "hard rods and Lennard--Jones, in one dimension . The Equation Learning Network\n",
      "proposed in Ref. 1 is suitably modified to construct free energy densities\n",
      "which are functions of a set of weighted densities and which are built from a\n",
      "small number of basis functions with flexible combination rules. This setup\n",
      "considerably enlarges the functional space used in the machine learning\n",
      "optimization as compared to previous work 2 where the functional is limited to\n",
      "a simple polynomial form. As a result, we find a good approximation for the\n",
      "exact hard rod functional and its direct correlation function. For the\n",
      "Lennard--Jones fluid, we let the network learn (i) the full excess free energy\n",
      "functional and (ii) the excess free energy functional related to interparticle\n",
      "attractions. Both functionals show a good agreement with simulated density\n",
      "profiles for thermodynamic parameters inside and outside the training region.\n",
      "\n",
      "**Paper Id :2005.00544 \n",
      "Title :Variational Quantum Eigensolver for Frustrated Quantum Systems\n",
      "  Hybrid quantum-classical algorithms have been proposed as a potentially\n",
      "viable application of quantum computers. A particular example - the variational\n",
      "quantum eigensolver, or VQE - is designed to determine a global minimum in an\n",
      "energy landscape specified by a quantum Hamiltonian, which makes it appealing\n",
      "for the needs of quantum chemistry. Experimental realizations have been\n",
      "reported in recent years and theoretical estimates of its efficiency are a\n",
      "subject of intense effort. Here we consider the performance of the VQE\n",
      "technique for a Hubbard-like model describing a one-dimensional chain of\n",
      "fermions with competing nearest- and next-nearest-neighbor interactions. We\n",
      "find that recovering the VQE solution allows one to obtain the correlation\n",
      "function of the ground state consistent with the exact result. We also study\n",
      "the barren plateau phenomenon for the Hamiltonian in question and find that the\n",
      "severity of this effect depends on the encoding of fermions to qubits. Our\n",
      "results are consistent with the current knowledge about the barren plateaus in\n",
      "quantum optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.13062 \n",
      "Title :Disentangling the Spatial Structure and Style in Conditional VAE\n",
      "  This paper aims to disentangle the latent space in cVAE into the spatial\n",
      "structure and the style code, which are complementary to each other, with one\n",
      "of them $z_s$ being label relevant and the other $z_u$ irrelevant. The\n",
      "generator is built by a connected encoder-decoder and a label condition mapping\n",
      "network. Depending on whether the label is related with the spatial structure,\n",
      "the output $z_s$ from the condition mapping network is used either as a style\n",
      "code or a spatial structure code. The encoder provides the label irrelevant\n",
      "posterior from which $z_u$ is sampled. The decoder employs $z_s$ and $z_u$ in\n",
      "each layer by adaptive normalization like SPADE or AdaIN. Extensive experiments\n",
      "on two datasets with different types of labels show the effectiveness of our\n",
      "method.\n",
      "\n",
      "**Paper Id :2001.04029 \n",
      "Title :Tangent-Space Gradient Optimization of Tensor Network for Machine\n",
      "  Learning\n",
      "  The gradient-based optimization method for deep machine learning models\n",
      "suffers from gradient vanishing and exploding problems, particularly when the\n",
      "computational graph becomes deep. In this work, we propose the tangent-space\n",
      "gradient optimization (TSGO) for the probabilistic models to keep the gradients\n",
      "from vanishing or exploding. The central idea is to guarantee the orthogonality\n",
      "between the variational parameters and the gradients. The optimization is then\n",
      "implemented by rotating parameter vector towards the direction of gradient. We\n",
      "explain and testify TSGO in tensor network (TN) machine learning, where the TN\n",
      "describes the joint probability distribution as a normalized state $\\left| \\psi\n",
      "\\right\\rangle $ in Hilbert space. We show that the gradient can be restricted\n",
      "in the tangent space of $\\left\\langle \\psi \\right.\\left| \\psi \\right\\rangle =\n",
      "1$ hyper-sphere. Instead of additional adaptive methods to control the learning\n",
      "rate in deep learning, the learning rate of TSGO is naturally determined by the\n",
      "angle $\\theta $ as $\\eta = \\tan \\theta $. Our numerical results reveal better\n",
      "convergence of TSGO in comparison to the off-the-shelf Adam.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.13321 \n",
      "Title :Semantic Object Accuracy for Generative Text-to-Image Synthesis\n",
      "  Generative adversarial networks conditioned on textual image descriptions are\n",
      "capable of generating realistic-looking images. However, current methods still\n",
      "struggle to generate images based on complex image captions from a\n",
      "heterogeneous domain. Furthermore, quantitatively evaluating these\n",
      "text-to-image models is challenging, as most evaluation metrics only judge\n",
      "image quality but not the conformity between the image and its caption. To\n",
      "address these challenges we introduce a new model that explicitly models\n",
      "individual objects within an image and a new evaluation metric called Semantic\n",
      "Object Accuracy (SOA) that specifically evaluates images given an image\n",
      "caption. The SOA uses a pre-trained object detector to evaluate if a generated\n",
      "image contains objects that are mentioned in the image caption, e.g. whether an\n",
      "image generated from \"a car driving down the street\" contains a car. We perform\n",
      "a user study comparing several text-to-image models and show that our SOA\n",
      "metric ranks the models the same way as humans, whereas other metrics such as\n",
      "the Inception Score do not. Our evaluation also shows that models which\n",
      "explicitly model objects outperform models which only model global image\n",
      "characteristics.\n",
      "\n",
      "**Paper Id :1909.12009 \n",
      "Title :Complex Network based Supervised Keyword Extractor\n",
      "  In this paper, we present a supervised framework for automatic keyword\n",
      "extraction from single document. We model the text as complex network, and\n",
      "construct the feature set by extracting select node properties from it. Several\n",
      "node properties have been exploited by unsupervised, graph-based keyword\n",
      "extraction methods to discriminate keywords from non-keywords. We exploit the\n",
      "complex interplay of node properties to design a supervised keyword extraction\n",
      "method. The training set is created from the feature set by assigning a label\n",
      "to each candidate keyword depending on whether the candidate is listed as a\n",
      "gold-standard keyword or not. Since the number of keywords in a document is\n",
      "much less than non-keywords, the curated training set is naturally imbalanced.\n",
      "We train a binary classifier to predict keywords after balancing the training\n",
      "set. The model is trained using two public datasets from scientific domain and\n",
      "tested using three unseen scientific corpora and one news corpus. Comparative\n",
      "study of the results with several recent keyword and keyphrase extraction\n",
      "methods establishes that the proposed method performs better in most cases.\n",
      "This substantiates our claim that graph-theoretic properties of words are\n",
      "effective discriminators between keywords and non-keywords. We support our\n",
      "argument by showing that the improved performance of the proposed method is\n",
      "statistically significant for all datasets. We also evaluate the effectiveness\n",
      "of the pre-trained model on Hindi and Assamese language documents. We observe\n",
      "that the model performs equally well for the cross-language text even though it\n",
      "was trained only on English language documents. This shows that the proposed\n",
      "method is independent of the domain, collection, and language of the training\n",
      "corpora.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.13496 \n",
      "Title :Asymptotically unbiased estimation of physical observables with neural\n",
      "  samplers\n",
      "  We propose a general framework for the estimation of observables with\n",
      "generative neural samplers focusing on modern deep generative neural networks\n",
      "that provide an exact sampling probability. In this framework, we present\n",
      "asymptotically unbiased estimators for generic observables, including those\n",
      "that explicitly depend on the partition function such as free energy or\n",
      "entropy, and derive corresponding variance estimators. We demonstrate their\n",
      "practical applicability by numerical experiments for the 2d Ising model which\n",
      "highlight the superiority over existing methods. Our approach greatly enhances\n",
      "the applicability of generative neural samplers to real-world physical systems.\n",
      "\n",
      "**Paper Id :2003.04166 \n",
      "Title :Learning entropy production via neural networks\n",
      "  This Letter presents a neural estimator for entropy production, or NEEP, that\n",
      "estimates entropy production (EP) from trajectories of relevant variables\n",
      "without detailed information on the system dynamics. For steady state, we\n",
      "rigorously prove that the estimator, which can be built up from different\n",
      "choices of deep neural networks, provides stochastic EP by optimizing the\n",
      "objective function proposed here. We verify the NEEP with the stochastic\n",
      "processes of the bead-spring and discrete flashing ratchet models, and also\n",
      "demonstrate that our method is applicable to high-dimensional data and can\n",
      "provide coarse-grained EP for Markov systems with unobservable states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.13875 \n",
      "Title :Fault Tolerance of Neural Networks in Adversarial Settings\n",
      "  Artificial Intelligence systems require a through assessment of different\n",
      "pillars of trust, namely, fairness, interpretability, data and model privacy,\n",
      "reliability (safety) and robustness against against adversarial attacks. While\n",
      "these research problems have been extensively studied in isolation, an\n",
      "understanding of the trade-off between different pillars of trust is lacking.\n",
      "To this extent, the trade-off between fault tolerance, privacy and adversarial\n",
      "robustness is evaluated for the specific case of Deep Neural Networks, by\n",
      "considering two adversarial settings under a security and a privacy threat\n",
      "model. Specifically, this work studies the impact of the fault tolerance of the\n",
      "Neural Network on training the model by adding noise to the input (Adversarial\n",
      "Robustness) and noise to the gradients (Differential Privacy). While training\n",
      "models with noise to inputs, gradients or weights enhances fault tolerance, it\n",
      "is observed that adversarial robustness and fault tolerance are at odds with\n",
      "each other. On the other hand, ($\\epsilon,\\delta$)-Differentially Private\n",
      "models enhance the fault tolerance, measured using generalisation error,\n",
      "theoretically has an upper bound of $e^{\\epsilon} - 1 + \\delta$. This novel\n",
      "study of the trade-off between different elements of trust is pivotal for\n",
      "training a model which satisfies the requirements for different pillars of\n",
      "trust simultaneously.\n",
      "\n",
      "**Paper Id :1906.09679 \n",
      "Title :The Value of Collaboration in Convex Machine Learning with Differential\n",
      "  Privacy\n",
      "  In this paper, we apply machine learning to distributed private data owned by\n",
      "multiple data owners, entities with access to non-overlapping training\n",
      "datasets. We use noisy, differentially-private gradients to minimize the\n",
      "fitness cost of the machine learning model using stochastic gradient descent.\n",
      "We quantify the quality of the trained model, using the fitness cost, as a\n",
      "function of privacy budget and size of the distributed datasets to capture the\n",
      "trade-off between privacy and utility in machine learning. This way, we can\n",
      "predict the outcome of collaboration among privacy-aware data owners prior to\n",
      "executing potentially computationally-expensive machine learning algorithms.\n",
      "Particularly, we show that the difference between the fitness of the trained\n",
      "machine learning model using differentially-private gradient queries and the\n",
      "fitness of the trained machine model in the absence of any privacy concerns is\n",
      "inversely proportional to the size of the training datasets squared and the\n",
      "privacy budget squared. We successfully validate the performance prediction\n",
      "with the actual performance of the proposed privacy-aware learning algorithms,\n",
      "applied to: financial datasets for determining interest rates of loans using\n",
      "regression; and detecting credit card frauds using support vector machines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.14464 \n",
      "Title :What Question Answering can Learn from Trivia Nerds\n",
      "  In addition to the traditional task of getting machines to answer questions,\n",
      "a major research question in question answering is to create interesting,\n",
      "challenging questions that can help systems learn how to answer questions and\n",
      "also reveal which systems are the best at answering questions. We argue that\n",
      "creating a question answering dataset -- and the ubiquitous leaderboard that\n",
      "goes with it -- closely resembles running a trivia tournament: you write\n",
      "questions, have agents (either humans or machines) answer the questions, and\n",
      "declare a winner. However, the research community has ignored the decades of\n",
      "hard-learned lessons from decades of the trivia community creating vibrant,\n",
      "fair, and effective question answering competitions. After detailing problems\n",
      "with existing QA datasets, we outline the key lessons -- removing ambiguity,\n",
      "discriminating skill, and adjudicating disputes -- that can transfer to QA\n",
      "research and how they might be implemented for the QA community.\n",
      "\n",
      "**Paper Id :2010.05388 \n",
      "Title :AI Song Contest: Human-AI Co-Creation in Songwriting\n",
      "  Machine learning is challenging the way we make music. Although research in\n",
      "deep generative models has dramatically improved the capability and fluency of\n",
      "music models, recent work has shown that it can be challenging for humans to\n",
      "partner with this new class of algorithms. In this paper, we present findings\n",
      "on what 13 musician/developer teams, a total of 61 users, needed when\n",
      "co-creating a song with AI, the challenges they faced, and how they leveraged\n",
      "and repurposed existing characteristics of AI to overcome some of these\n",
      "challenges. Many teams adopted modular approaches, such as independently\n",
      "running multiple smaller models that align with the musical building blocks of\n",
      "a song, before re-combining their results. As ML models are not easily\n",
      "steerable, teams also generated massive numbers of samples and curated them\n",
      "post-hoc, or used a range of strategies to direct the generation, or\n",
      "algorithmically ranked the samples. Ultimately, teams not only had to manage\n",
      "the \"flare and focus\" aspects of the creative process, but also juggle them\n",
      "with a parallel process of exploring and curating multiple ML models and\n",
      "outputs. These findings reflect a need to design machine learning-powered music\n",
      "interfaces that are more decomposable, steerable, interpretable, and adaptive,\n",
      "which in return will enable artists to more effectively explore how AI can\n",
      "extend their personal expression.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1910.14634 \n",
      "Title :Denoising and Regularization via Exploiting the Structural Bias of\n",
      "  Convolutional Generators\n",
      "  Convolutional Neural Networks (CNNs) have emerged as highly successful tools\n",
      "for image generation, recovery, and restoration. A major contributing factor to\n",
      "this success is that convolutional networks impose strong prior assumptions\n",
      "about natural images. A surprising experiment that highlights this\n",
      "architectural bias towards natural images is that one can remove noise and\n",
      "corruptions from a natural image without using any training data, by simply\n",
      "fitting (via gradient descent) a randomly initialized, over-parameterized\n",
      "convolutional generator to the corrupted image. While this over-parameterized\n",
      "network can fit the corrupted image perfectly, surprisingly after a few\n",
      "iterations of gradient descent it generates an almost uncorrupted image. This\n",
      "intriguing phenomenon enables state-of-the-art CNN-based denoising and\n",
      "regularization of other inverse problems. In this paper, we attribute this\n",
      "effect to a particular architectural choice of convolutional networks, namely\n",
      "convolutions with fixed interpolating filters. We then formally characterize\n",
      "the dynamics of fitting a two-layer convolutional generator to a noisy signal\n",
      "and prove that early-stopped gradient descent denoises/regularizes. Our proof\n",
      "relies on showing that convolutional generators fit the structured part of an\n",
      "image significantly faster than the corrupted portion.\n",
      "\n",
      "**Paper Id :2006.05467 \n",
      "Title :Pruning neural networks without any data by iteratively conserving\n",
      "  synaptic flow\n",
      "  Pruning the parameters of deep neural networks has generated intense interest\n",
      "due to potential savings in time, memory and energy both during training and at\n",
      "test time. Recent works have identified, through an expensive sequence of\n",
      "training and pruning cycles, the existence of winning lottery tickets or sparse\n",
      "trainable subnetworks at initialization. This raises a foundational question:\n",
      "can we identify highly sparse trainable subnetworks at initialization, without\n",
      "ever training, or indeed without ever looking at the data? We provide an\n",
      "affirmative answer to this question through theory driven algorithm design. We\n",
      "first mathematically formulate and experimentally verify a conservation law\n",
      "that explains why existing gradient-based pruning algorithms at initialization\n",
      "suffer from layer-collapse, the premature pruning of an entire layer rendering\n",
      "a network untrainable. This theory also elucidates how layer-collapse can be\n",
      "entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow\n",
      "Pruning (SynFlow). This algorithm can be interpreted as preserving the total\n",
      "flow of synaptic strengths through the network at initialization subject to a\n",
      "sparsity constraint. Notably, this algorithm makes no reference to the training\n",
      "data and consistently competes with or outperforms existing state-of-the-art\n",
      "pruning algorithms at initialization over a range of models (VGG and ResNet),\n",
      "datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to\n",
      "99.99 percent). Thus our data-agnostic pruning algorithm challenges the\n",
      "existing paradigm that, at initialization, data must be used to quantify which\n",
      "synapses are important.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.00397 \n",
      "Title :Generalized Speedy Q-learning\n",
      "  In this paper, we derive a generalization of the Speedy Q-learning (SQL)\n",
      "algorithm that was proposed in the Reinforcement Learning (RL) literature to\n",
      "handle slow convergence of Watkins' Q-learning. In most RL algorithms such as\n",
      "Q-learning, the Bellman equation and the Bellman operator play an important\n",
      "role. It is possible to generalize the Bellman operator using the technique of\n",
      "successive relaxation. We use the generalized Bellman operator to derive a\n",
      "simple and efficient family of algorithms called Generalized Speedy Q-learning\n",
      "(GSQL-w) and analyze its finite time performance. We show that GSQL-w has an\n",
      "improved finite time performance bound compared to SQL for the case when the\n",
      "relaxation parameter w is greater than 1. This improvement is a consequence of\n",
      "the contraction factor of the generalized Bellman operator being less than that\n",
      "of the standard Bellman operator. Numerical experiments are provided to\n",
      "demonstrate the empirical performance of the GSQL-w algorithm.\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.00890 \n",
      "Title :Mean-field inference methods for neural networks\n",
      "  Machine learning algorithms relying on deep neural networks recently allowed\n",
      "a great leap forward in artificial intelligence. Despite the popularity of\n",
      "their applications, the efficiency of these algorithms remains largely\n",
      "unexplained from a theoretical point of view. The mathematical description of\n",
      "learning problems involves very large collections of interacting random\n",
      "variables, difficult to handle analytically as well as numerically. This\n",
      "complexity is precisely the object of study of statistical physics. Its\n",
      "mission, originally pointed towards natural systems, is to understand how\n",
      "macroscopic behaviors arise from microscopic laws. Mean-field methods are one\n",
      "type of approximation strategy developed in this view. We review a selection of\n",
      "classical mean-field methods and recent progress relevant for inference in\n",
      "neural networks. In particular, we remind the principles of derivations of\n",
      "high-temperature expansions, the replica method and message passing algorithms,\n",
      "highlighting their equivalences and complementarities. We also provide\n",
      "references for past and current directions of research on neural networks\n",
      "relying on mean-field methods.\n",
      "\n",
      "**Paper Id :2006.13222 \n",
      "Title :Certified variational quantum algorithms for eigenstate preparation\n",
      "  Solutions to many-body problem instances often involve an intractable number\n",
      "of degrees of freedom and admit no known approximations in general form. In\n",
      "practice, representing quantum-mechanical states of a given Hamiltonian using\n",
      "available numerical methods, in particular those based on variational Monte\n",
      "Carlo simulations, become exponentially more challenging with increasing system\n",
      "size. Recently quantum algorithms implemented as variational models have been\n",
      "proposed to accelerate such simulations. The variational ansatz states are\n",
      "characterized by a polynomial number of parameters devised in a way to minimize\n",
      "the expectation value of a given Hamiltonian, which is emulated by local\n",
      "measurements. In this study, we develop a means to certify the termination of\n",
      "variational algorithms. We demonstrate our approach by applying it to three\n",
      "models: the transverse field Ising model, the model of one-dimensional spinless\n",
      "fermions with competing interactions, and the Schwinger model of quantum\n",
      "electrodynamics. By means of comparison, we observe that our approach shows\n",
      "better performance near critical points in these models. We hence take a\n",
      "further step to improve the applicability and to certify the results of\n",
      "variational quantum simulators.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.00995 \n",
      "Title :Novel semi-metrics for multivariate change point analysis and anomaly\n",
      "  detection\n",
      "  This paper proposes a new method for determining similarity and anomalies\n",
      "between time series, most practically effective in large collections of (likely\n",
      "related) time series, by measuring distances between structural breaks within\n",
      "such a collection. We introduce a class of \\emph{semi-metric} distance\n",
      "measures, which we term \\emph{MJ distances}. These semi-metrics provide an\n",
      "advantage over existing options such as the Hausdorff and Wasserstein metrics.\n",
      "We prove they have desirable properties, including better sensitivity to\n",
      "outliers, while experiments on simulated data demonstrate that they uncover\n",
      "similarity within collections of time series more effectively. Semi-metrics\n",
      "carry a potential disadvantage: without the triangle inequality, they may not\n",
      "satisfy a \"transitivity property of closeness.\" We analyse this failure with\n",
      "proof and introduce an computational method to investigate, in which we\n",
      "demonstrate that our semi-metrics violate transitivity infrequently and mildly.\n",
      "Finally, we apply our methods to cryptocurrency and measles data, introducing a\n",
      "judicious application of eigenvalue analysis.\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.00996 \n",
      "Title :A Study of Data Pre-processing Techniques for Imbalanced Biomedical Data\n",
      "  Classification\n",
      "  Biomedical data are widely accepted in developing prediction models for\n",
      "identifying a specific tumor, drug discovery and classification of human\n",
      "cancers. However, previous studies usually focused on different classifiers,\n",
      "and overlook the class imbalance problem in real-world biomedical datasets.\n",
      "There are a lack of studies on evaluation of data pre-processing techniques,\n",
      "such as resampling and feature selection, on imbalanced biomedical data\n",
      "learning. The relationship between data pre-processing techniques and the data\n",
      "distributions has never been analysed in previous studies. This article mainly\n",
      "focuses on reviewing and evaluating some popular and recently developed\n",
      "resampling and feature selection methods for class imbalance learning. We\n",
      "analyse the effectiveness of each technique from data distribution perspective.\n",
      "Extensive experiments have been done based on five classifiers, four\n",
      "performance measures, eight learning techniques across twenty real-world\n",
      "datasets. Experimental results show that: (1) resampling and feature selection\n",
      "techniques exhibit better performance using support vector machine (SVM)\n",
      "classifier. However, resampling and Feature Selection techniques perform poorly\n",
      "when using C4.5 decision tree and Linear discriminant analysis classifiers; (2)\n",
      "for datasets with different distributions, techniques such as Random\n",
      "undersampling and Feature Selection perform better than other data\n",
      "pre-processing methods with T Location-Scale distribution when using SVM and\n",
      "KNN (K-nearest neighbours) classifiers. Random oversampling outperforms other\n",
      "methods on Negative Binomial distribution using Random Forest classifier with\n",
      "lower level of imbalance ratio; (3) Feature Selection outperforms other data\n",
      "pre-processing methods in most cases, thus, Feature Selection with SVM\n",
      "classifier is the best choice for imbalanced biomedical data learning.\n",
      "\n",
      "**Paper Id :1803.05985 \n",
      "Title :EEG machine learning with Higuchi fractal dimension and Sample Entropy\n",
      "  as features for successful detection of depression\n",
      "  Reliable diagnosis of depressive disorder is essential for both optimal\n",
      "treatment and prevention of fatal outcomes. In this study, we aimed to\n",
      "elucidate the effectiveness of two non-linear measures, Higuchi Fractal\n",
      "Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders\n",
      "when applied on EEG. HFD and SampEn of EEG signals were used as features for\n",
      "seven machine learning algorithms including Multilayer Perceptron, Logistic\n",
      "Regression, Support Vector Machines with the linear and polynomial kernel,\n",
      "Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG\n",
      "between healthy control subjects and patients diagnosed with depression. We\n",
      "confirmed earlier observations that both non-linear measures can discriminate\n",
      "EEG signals of patients from healthy control subjects. The results suggest that\n",
      "good classification is possible even with a small number of principal\n",
      "components. Average accuracy among classifiers ranged from 90.24% to 97.56%.\n",
      "Among the two measures, SampEn had better performance. Using HFD and SampEn and\n",
      "a variety of machine learning techniques we can accurately discriminate\n",
      "patients diagnosed with depression vs controls which can serve as a highly\n",
      "sensitive, clinically relevant marker for the diagnosis of depressive\n",
      "disorders.\n",
      "\n",
      "\n",
      "*********\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Paper Id :1911.01004 \n",
      "Title :Why Non-myopic Bayesian Optimization is Promising and How Far Should We\n",
      "  Look-ahead? A Study via Rollout\n",
      "  Lookahead, also known as non-myopic, Bayesian optimization (BO) aims to find\n",
      "optimal sampling policies through solving a dynamic programming (DP)\n",
      "formulation that maximizes a long-term reward over a rolling horizon. Though\n",
      "promising, lookahead BO faces the risk of error propagation through its\n",
      "increased dependence on a possibly mis-specified model. In this work we focus\n",
      "on the rollout approximation for solving the intractable DP. We first prove the\n",
      "improving nature of rollout in tackling lookahead BO and provide a sufficient\n",
      "condition for the used heuristic to be rollout improving. We then provide both\n",
      "a theoretical and practical guideline to decide on the rolling horizon\n",
      "stagewise. This guideline is built on quantifying the negative effect of a\n",
      "mis-specified model. To illustrate our idea, we provide case studies on both\n",
      "single and multi-information source BO. Empirical results show the advantageous\n",
      "properties of our method over several myopic and non-myopic BO algorithms.\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.01220 \n",
      "Title :Learning-based estimation of dielectric properties and tissue density in\n",
      "  head models for personalized radio-frequency dosimetry\n",
      "  Radio-frequency dosimetry is an important process in human safety and for\n",
      "compliance of related products. Recently, computational human models generated\n",
      "from medical images have often been used for such assessment, especially to\n",
      "consider the inter-variability of subjects. However, the common procedure to\n",
      "develop personalized models is time consuming because it involves excessive\n",
      "segmentation of several components that represent different biological tissues,\n",
      "which limits the inter-variability assessment of radiation safety based on\n",
      "personalized dosimetry. Deep learning methods have been shown to be a powerful\n",
      "approach for pattern recognition and signal analysis. Convolutional neural\n",
      "networks with deep architecture are proven robust for feature extraction and\n",
      "image mapping in several biomedical applications. In this study, we develop a\n",
      "learning-based approach for fast and accurate estimation of the dielectric\n",
      "properties and density of tissues directly from magnetic resonance images in a\n",
      "single shot. The smooth distribution of the dielectric properties in head\n",
      "models, which is realized using a process without tissue segmentation, improves\n",
      "the smoothness of the specific absorption rate (SAR) distribution compared with\n",
      "that in the commonly used procedure. The estimated SAR distributions, as well\n",
      "as that averaged over 10-g of tissue in a cubic shape, are found to be highly\n",
      "consistent with those computed using the conventional methods that employ\n",
      "segmentation.\n",
      "\n",
      "**Paper Id :1910.02420 \n",
      "Title :Deep learning-based development of personalized human head model with\n",
      "  non-uniform conductivity for brain stimulation\n",
      "  Electromagnetic stimulation of the human brain is a key tool for the\n",
      "neurophysiological characterization and diagnosis of several neurological\n",
      "disorders. Transcranial magnetic stimulation (TMS) is one procedure that is\n",
      "commonly used clinically. However, personalized TMS requires a pipeline for\n",
      "accurate head model generation to provide target-specific stimulation. This\n",
      "process includes intensive segmentation of several head tissues based on\n",
      "magnetic resonance imaging (MRI), which has significant potential for\n",
      "segmentation error, especially for low-contrast tissues. Additionally, a\n",
      "uniform electrical conductivity is assigned to each tissue in the model, which\n",
      "is an unrealistic assumption based on conventional volume conductor modeling.\n",
      "This paper proposes a novel approach to the automatic estimation of electric\n",
      "conductivity in the human head for volume conductor models without anatomical\n",
      "segmentation. A convolutional neural network is designed to estimate\n",
      "personalized electrical conductivity values based on anatomical information\n",
      "obtained from T1- and T2-weighted MRI scans. This approach can avoid the\n",
      "time-consuming process of tissue segmentation and maximize the advantages of\n",
      "position-dependent conductivity assignment based on water content values\n",
      "estimated from MRI intensity values. The computational results of the proposed\n",
      "approach provide similar but smoother electric field results for the brain when\n",
      "compared to conventional approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.01225 \n",
      "Title :Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale\n",
      "  Service Environment\n",
      "  Root cause analysis in a large-scale production environment is challenging\n",
      "due to the complexity of services running across global data centers. Due to\n",
      "the distributed nature of a large-scale system, the various hardware, software,\n",
      "and tooling logs are often maintained separately, making it difficult to review\n",
      "the logs jointly for understanding production issues. Another challenge in\n",
      "reviewing the logs for identifying issues is the scale - there could easily be\n",
      "millions of entities, each described by hundreds of features. In this paper we\n",
      "present a fast dimensional analysis framework that automates the root cause\n",
      "analysis on structured logs with improved scalability.\n",
      "  We first explore item-sets, i.e. combinations of feature values, that could\n",
      "identify groups of samples with sufficient support for the target failures\n",
      "using the Apriori algorithm and a subsequent improvement, FP-Growth. These\n",
      "algorithms were designed for frequent item-set mining and association rule\n",
      "learning over transactional databases. After applying them on structured logs,\n",
      "we select the item-sets that are most unique to the target failures based on\n",
      "lift. We propose pre-processing steps with the use of a large-scale real-time\n",
      "database and post-processing techniques and parallelism to further speed up the\n",
      "analysis and improve interpretability, and demonstrate that such optimization\n",
      "is necessary for handling large-scale production datasets. We have successfully\n",
      "rolled out this approach for root cause investigation purposes in a large-scale\n",
      "infrastructure. We also present the setup and results from multiple production\n",
      "use cases in this paper.\n",
      "\n",
      "**Paper Id :2010.06145 \n",
      "Title :Customer Support Ticket Escalation Prediction using Feature Engineering\n",
      "  Understanding and keeping the customer happy is a central tenet of\n",
      "requirements engineering. Strategies to gather, analyze, and negotiate\n",
      "requirements are complemented by efforts to manage customer input after\n",
      "products have been deployed. For the latter, support tickets are key in\n",
      "allowing customers to submit their issues, bug reports, and feature requests.\n",
      "If insufficient attention is given to support issues, however, their escalation\n",
      "to management becomes time-consuming and expensive, especially for large\n",
      "organizations managing hundreds of customers and thousands of support tickets.\n",
      "Our work provides a step towards simplifying the job of support analysts and\n",
      "managers, particularly in predicting the risk of escalating support tickets. In\n",
      "a field study at our large industrial partner, IBM, we used a design science\n",
      "research methodology to characterize the support process and data available to\n",
      "IBM analysts in managing escalations. We then implemented these features into a\n",
      "machine learning model to predict support ticket escalations. We trained and\n",
      "evaluated our machine learning model on over 2.5 million support tickets and\n",
      "10,000 escalations, obtaining a recall of 87.36% and an 88.23% reduction in the\n",
      "workload for support analysts looking to identify support tickets at risk of\n",
      "escalation. Finally, in addition to these research evaluation activities, we\n",
      "compared the performance of our support ticket model with that of a model\n",
      "developed with no feature engineering; the support ticket model features\n",
      "outperformed the non-engineered model. The artifacts created in this research\n",
      "are designed to serve as a starting place for organizations interested in\n",
      "predicting support ticket escalations, and for future researchers to build on\n",
      "to advance research in escalation prediction.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.01366 \n",
      "Title :Framework for Inferring Following Strategies from Time Series of\n",
      "  Movement Data\n",
      "  How do groups of individuals achieve consensus in movement decisions? Do\n",
      "individuals follow their friends, the one predetermined leader, or whomever\n",
      "just happens to be nearby? To address these questions computationally, we\n",
      "formalize \"Coordination Strategy Inference Problem\". In this setting, a group\n",
      "of multiple individuals moves in a coordinated manner towards a target path.\n",
      "Each individual uses a specific strategy to follow others (e.g. nearest\n",
      "neighbors, pre-defined leaders, preferred friends). Given a set of time series\n",
      "that includes coordinated movement and a set of candidate strategies as inputs,\n",
      "we provide the first methodology (to the best of our knowledge) to infer\n",
      "whether each individual uses local-agreement-system or dictatorship-like\n",
      "strategy to achieve movement coordination at the group level. We evaluate and\n",
      "demonstrate the performance of the proposed framework by predicting the\n",
      "direction of movement of an individual in a group in both simulated datasets as\n",
      "well as two real-world datasets: a school of fish and a troop of baboons.\n",
      "Moreover, since there is no prior methodology for inferring individual-level\n",
      "strategies, we compare our framework with the state-of-the-art approach for the\n",
      "task of classification of group-level-coordination models. The results show\n",
      "that our approach is highly accurate in inferring the correct strategy in\n",
      "simulated datasets even in complicated mixed strategy settings, which no\n",
      "existing method can infer. In the task of classification of\n",
      "group-level-coordination models, our framework performs better than the\n",
      "state-of-the-art approach in all datasets. Animal data experiments show that\n",
      "fish, as expected, follow their neighbors, while baboons have a preference to\n",
      "follow specific individuals. Our methodology generalizes to arbitrary time\n",
      "series data of real numbers, beyond movement data.\n",
      "\n",
      "**Paper Id :1911.10120 \n",
      "Title :Multi-Agent Thompson Sampling for Bandit Applications with Sparse\n",
      "  Neighbourhood Structures\n",
      "  Multi-agent coordination is prevalent in many real-world applications.\n",
      "However, such coordination is challenging due to its combinatorial nature. An\n",
      "important observation in this regard is that agents in the real world often\n",
      "only directly affect a limited set of neighbouring agents. Leveraging such\n",
      "loose couplings among agents is key to making coordination in multi-agent\n",
      "systems feasible. In this work, we focus on learning to coordinate.\n",
      "Specifically, we consider the multi-agent multi-armed bandit framework, in\n",
      "which fully cooperative loosely-coupled agents must learn to coordinate their\n",
      "decisions to optimize a common objective. We propose multi-agent Thompson\n",
      "sampling (MATS), a new Bayesian exploration-exploitation algorithm that\n",
      "leverages loose couplings. We provide a regret bound that is sublinear in time\n",
      "and low-order polynomial in the highest number of actions of a single agent for\n",
      "sparse coordination graphs. Additionally, we empirically show that MATS\n",
      "outperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks,\n",
      "and a novel benchmark with Poisson distributions. An example of a\n",
      "loosely-coupled multi-agent system is a wind farm. Coordination within the wind\n",
      "farm is necessary to maximize power production. As upstream wind turbines only\n",
      "affect nearby downstream turbines, we can use MATS to efficiently learn the\n",
      "optimal control mechanism for the farm. To demonstrate the benefits of our\n",
      "method toward applications we apply MATS to a realistic wind farm control task.\n",
      "In this task, wind turbines must coordinate their alignments with respect to\n",
      "the incoming wind vector in order to optimize power production. Our results\n",
      "show that MATS improves significantly upon state-of-the-art coordination\n",
      "methods in terms of performance, demonstrating the value of using MATS in\n",
      "practical applications with sparse neighbourhood structures.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.01546 \n",
      "Title :Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy\n",
      "  While maximizing expected return is the goal in most reinforcement learning\n",
      "approaches, risk-sensitive objectives such as conditional value at risk (CVaR)\n",
      "are more suitable for many high-stakes applications. However, relatively little\n",
      "is known about how to explore to quickly learn policies with good CVaR. In this\n",
      "paper, we present the first algorithm for sample-efficient learning of\n",
      "CVaR-optimal policies in Markov decision processes based on the optimism in the\n",
      "face of uncertainty principle. This method relies on a novel optimistic version\n",
      "of the distributional Bellman operator that moves probability mass from the\n",
      "lower to the upper tail of the return distribution. We prove asymptotic\n",
      "convergence and optimism of this operator for the tabular policy evaluation\n",
      "case. We further demonstrate that our algorithm finds CVaR-optimal policies\n",
      "substantially faster than existing baselines in several simulated environments\n",
      "with discrete and continuous state spaces.\n",
      "\n",
      "**Paper Id :2003.04108 \n",
      "Title :Stable Policy Optimization via Off-Policy Divergence Regularization\n",
      "  Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization\n",
      "(PPO) are among the most successful policy gradient approaches in deep\n",
      "reinforcement learning (RL). While these methods achieve state-of-the-art\n",
      "performance across a wide range of challenging tasks, there is room for\n",
      "improvement in the stabilization of the policy learning and how the off-policy\n",
      "data are used. In this paper we revisit the theoretical foundations of these\n",
      "algorithms and propose a new algorithm which stabilizes the policy improvement\n",
      "through a proximity term that constrains the discounted state-action visitation\n",
      "distribution induced by consecutive policies to be close to one another. This\n",
      "proximity term, expressed in terms of the divergence between the visitation\n",
      "distributions, is learned in an off-policy and adversarial manner. We\n",
      "empirically show that our proposed method can have a beneficial effect on\n",
      "stability and improve final performance in benchmark high-dimensional control\n",
      "tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.02248 \n",
      "Title :MBCAL: Sample Efficient and Variance Reduced Reinforcement Learning for\n",
      "  Recommender Systems\n",
      "  In recommender systems such as news feed stream, it is essential to optimize\n",
      "the long-term utilities in the continuous user-system interaction processes.\n",
      "Previous works have proved the capability of reinforcement learning in this\n",
      "problem. However, there are many practical challenges to implement deep\n",
      "reinforcement learning in online systems, including low sample efficiency,\n",
      "uncontrollable risks, and excessive variances. To address these issues, we\n",
      "propose a novel reinforcement learning method, namely model-based\n",
      "counterfactual advantage learning (MBCAL). The proposed method takes advantage\n",
      "of the characteristics of recommender systems and draws ideas from the\n",
      "model-based reinforcement learning method for higher sample efficiency. It has\n",
      "two components: an environment model that predicts the instant user behavior\n",
      "one-by-one in an auto-regressive form, and a future advantage model that\n",
      "predicts the future utility. To alleviate the impact of excessive variance when\n",
      "learning the future advantage model, we employ counterfactual comparisons\n",
      "derived from the environment model. In consequence, the proposed method\n",
      "possesses high sample efficiency and significantly lower variance; Also, it is\n",
      "able to use existing user logs to avoid the risks of starting from scratch. In\n",
      "contrast to its capability, its implementation cost is relatively low, which\n",
      "fits well with practical systems. Theoretical analysis and elaborate\n",
      "experiments are presented. Results show that the proposed method transcends the\n",
      "other supervised learning and RL-based methods in both sample efficiency and\n",
      "asymptotic performances.\n",
      "\n",
      "**Paper Id :2004.03267 \n",
      "Title :Guided Dialog Policy Learning without Adversarial Learning in the Loop\n",
      "  Reinforcement Learning (RL) methods have emerged as a popular choice for\n",
      "training an efficient and effective dialogue policy. However, these methods\n",
      "suffer from sparse and unstable reward signals returned by a user simulator\n",
      "only when a dialogue finishes. Besides, the reward signal is manually designed\n",
      "by human experts, which requires domain knowledge. Recently, a number of\n",
      "adversarial learning methods have been proposed to learn the reward function\n",
      "together with the dialogue policy. However, to alternatively update the\n",
      "dialogue policy and the reward model on the fly, we are limited to\n",
      "policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the\n",
      "alternating training of a dialogue agent and the reward model can easily get\n",
      "stuck in local optima or result in mode collapse. To overcome the listed\n",
      "issues, we propose to decompose the adversarial training into two steps. First,\n",
      "we train the discriminator with an auxiliary dialogue generator and then\n",
      "incorporate a derived reward model into a common RL method to guide the\n",
      "dialogue policy learning. This approach is applicable to both on-policy and\n",
      "off-policy RL methods. Based on our extensive experimentation, we can conclude\n",
      "the proposed method: (1) achieves a remarkable task success rate using both\n",
      "on-policy and off-policy RL methods; and (2) has the potential to transfer\n",
      "knowledge from existing domains to a new domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.02344 \n",
      "Title :Statistical physics of unsupervised learning with prior knowledge in\n",
      "  neural networks\n",
      "  Integrating sensory inputs with prior beliefs from past experiences in\n",
      "unsupervised learning is a common and fundamental characteristic of brain or\n",
      "artificial neural computation. However, a quantitative role of prior knowledge\n",
      "in unsupervised learning remains unclear, prohibiting a scientific\n",
      "understanding of unsupervised learning. Here, we propose a statistical physics\n",
      "model of unsupervised learning with prior knowledge, revealing that the sensory\n",
      "inputs drive a series of continuous phase transitions related to spontaneous\n",
      "intrinsic-symmetry breaking. The intrinsic symmetry includes both reverse\n",
      "symmetry and permutation symmetry, commonly observed in most artificial neural\n",
      "networks. Compared to the prior-free scenario, the prior reduces more strongly\n",
      "the minimal data size triggering the reverse symmetry breaking transition, and\n",
      "moreover, the prior merges, rather than separates, permutation symmetry\n",
      "breaking phases. We claim that the prior can be learned from data samples,\n",
      "which in physics corresponds to a two-parameter Nishimori constraint. This work\n",
      "thus reveals mechanisms about the influence of the prior on unsupervised\n",
      "learning.\n",
      "\n",
      "**Paper Id :1911.07662 \n",
      "Title :Variational mean-field theory for training restricted Boltzmann machines\n",
      "  with binary synapses\n",
      "  Unsupervised learning requiring only raw data is not only a fundamental\n",
      "function of the cerebral cortex, but also a foundation for a next generation of\n",
      "artificial neural networks. However, a unified theoretical framework to treat\n",
      "sensory inputs, synapses and neural activity together is still lacking. The\n",
      "computational obstacle originates from the discrete nature of synapses, and\n",
      "complex interactions among these three essential elements of learning. Here, we\n",
      "propose a variational mean-field theory in which the distribution of synaptic\n",
      "weights is considered. The unsupervised learning can then be decomposed into\n",
      "two intertwined steps: a maximization step is carried out as a gradient ascent\n",
      "of the lower-bound on the data log-likelihood, in which the synaptic weight\n",
      "distribution is determined by updating variational parameters, and an\n",
      "expectation step is carried out as a message passing procedure on an equivalent\n",
      "or dual neural network whose parameter is specified by the variational\n",
      "parameters of the weight distribution. Therefore, our framework provides\n",
      "insights on how data (or sensory inputs), synapses and neural activities\n",
      "interact with each other to achieve the goal of extracting statistical\n",
      "regularities in sensory inputs. This variational framework is verified in\n",
      "restricted Boltzmann machines with planted synaptic weights and\n",
      "handwritten-digits learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.02692 \n",
      "Title :Multi-Domain Neural Machine Translation with Word-Level Adaptive\n",
      "  Layer-wise Domain Mixing\n",
      "  Many multi-domain neural machine translation (NMT) models achieve knowledge\n",
      "transfer by enforcing one encoder to learn shared embedding across domains.\n",
      "However, this design lacks adaptation to individual domains. To overcome this\n",
      "limitation, we propose a novel multi-domain NMT model using individual modules\n",
      "for each domain, on which we apply word-level, adaptive and layer-wise domain\n",
      "mixing. We first observe that words in a sentence are often related to multiple\n",
      "domains. Hence, we assume each word has a domain proportion, which indicates\n",
      "its domain preference. Then word representations are obtained by mixing their\n",
      "embedding in individual domains based on their domain proportions. We show this\n",
      "can be achieved by carefully designing multi-head dot-product attention modules\n",
      "for different domains, and eventually taking weighted averages of their\n",
      "parameters by word-level layer-wise domain proportions. Through this, we can\n",
      "achieve effective domain knowledge sharing, and capture fine-grained\n",
      "domain-specific knowledge as well. Our experiments show that our proposed model\n",
      "outperforms existing ones in several NMT tasks.\n",
      "\n",
      "**Paper Id :2003.09831 \n",
      "Title :Prior Knowledge Driven Label Embedding for Slot Filling in Natural\n",
      "  Language Understanding\n",
      "  Traditional slot filling in natural language understanding (NLU) predicts a\n",
      "one-hot vector for each word. This form of label representation lacks semantic\n",
      "correlation modelling, which leads to severe data sparsity problem, especially\n",
      "when adapting an NLU model to a new domain. To address this issue, a novel\n",
      "label embedding based slot filling framework is proposed in this paper. Here,\n",
      "distributed label embedding is constructed for each slot using prior knowledge.\n",
      "Three encoding methods are investigated to incorporate different kinds of prior\n",
      "knowledge about slots: atomic concepts, slot descriptions, and slot exemplars.\n",
      "The proposed label embeddings tend to share text patterns and reuses data with\n",
      "different slot labels. This makes it useful for adaptive NLU with limited data.\n",
      "Also, since label embedding is independent of NLU model, it is compatible with\n",
      "almost all deep learning based slot filling models. The proposed approaches are\n",
      "evaluated on three datasets. Experiments on single domain and domain adaptation\n",
      "tasks show that label embedding achieves significant performance improvement\n",
      "over traditional one-hot label representation as well as advanced zero-shot\n",
      "approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.02700 \n",
      "Title :Uncertainty relations and fluctuation theorems for Bayes nets\n",
      "  Recent research has considered the stochastic thermodynamics of multiple\n",
      "interacting systems, representing the overall system as a Bayes net. I derive\n",
      "fluctuation theorems governing the entropy production (EP)of arbitrary sets of\n",
      "the systems in such a Bayes net. I also derive ``conditional'' fluctuation\n",
      "theorems, governing the distribution of EP in one set of systems conditioned on\n",
      "the EP of a different set of systems. I then derive thermodynamic uncertainty\n",
      "relations relating the EP of the overall system to the precisions of\n",
      "probability currents within the individual systems.\n",
      "\n",
      "**Paper Id :1909.04305 \n",
      "Title :Inverse Ising inference from high-temperature re-weighting of\n",
      "  observations\n",
      "  Maximum Likelihood Estimation (MLE) is the bread and butter of system\n",
      "inference for stochastic systems. In some generality, MLE will converge to the\n",
      "correct model in the infinite data limit. In the context of physical approaches\n",
      "to system inference, such as Boltzmann machines, MLE requires the arduous\n",
      "computation of partition functions summing over all configurations, both\n",
      "observed and unobserved. We present here a conceptually and computationally\n",
      "transparent data-driven approach to system inference that is based on the\n",
      "simple question: How should the Boltzmann weights of observed configurations be\n",
      "modified to make the probability distribution of observed configurations close\n",
      "to a flat distribution? This algorithm gives accurate inference by using only\n",
      "observed configurations for systems with a large number of degrees of freedom\n",
      "where other approaches are intractable.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.02924 \n",
      "Title :Aerodynamic Data Fusion Towards the Digital Twin Paradigm\n",
      "  We consider the fusion of two aerodynamic data sets originating from\n",
      "differing fidelity physical or computer experiments. We specifically address\n",
      "the fusion of: 1) noisy and in-complete fields from wind tunnel measurements\n",
      "and 2) deterministic but biased fields from numerical simulations. These two\n",
      "data sources are fused in order to estimate the \\emph{true} field that best\n",
      "matches measured quantities that serves as the ground truth. For example, two\n",
      "sources of pressure fields about an aircraft are fused based on measured forces\n",
      "and moments from a wind-tunnel experiment. A fundamental challenge in this\n",
      "problem is that the true field is unknown and can not be estimated with 100\\%\n",
      "certainty. We employ a Bayesian framework to infer the true fields conditioned\n",
      "on measured quantities of interest; essentially we perform a \\emph{statistical\n",
      "correction} to the data. The fused data may then be used to construct more\n",
      "accurate surrogate models suitable for early stages of aerospace design. We\n",
      "also introduce an extension of the Proper Orthogonal Decomposition with\n",
      "constraints to solve the same problem. Both methods are demonstrated on fusing\n",
      "the pressure distributions for flow past the RAE2822 airfoil and the Common\n",
      "Research Model wing at transonic conditions. Comparison of both methods reveal\n",
      "that the Bayesian method is more robust when data is scarce while capable of\n",
      "also accounting for uncertainties in the data. Furthermore, given adequate\n",
      "data, the POD based and Bayesian approaches lead to \\emph{similar} results.\n",
      "\n",
      "**Paper Id :2001.06270 \n",
      "Title :Bayesian inference of chaotic dynamics by merging data assimilation,\n",
      "  machine learning and expectation-maximization\n",
      "  The reconstruction from observations of high-dimensional chaotic dynamics\n",
      "such as geophysical flows is hampered by (i) the partial and noisy observations\n",
      "that can realistically be obtained, (ii) the need to learn from long time\n",
      "series of data, and (iii) the unstable nature of the dynamics. To achieve such\n",
      "inference from the observations over long time series, it has been suggested to\n",
      "combine data assimilation and machine learning in several ways. We show how to\n",
      "unify these approaches from a Bayesian perspective using\n",
      "expectation-maximization and coordinate descents. In doing so, the model, the\n",
      "state trajectory and model error statistics are estimated all together.\n",
      "Implementations and approximations of these methods are discussed. Finally, we\n",
      "numerically and successfully test the approach on two relevant low-order\n",
      "chaotic models with distinct identifiability.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.02945 \n",
      "Title :J-MoDL: Joint Model-Based Deep Learning for Optimized Sampling and\n",
      "  Reconstruction\n",
      "  Modern MRI schemes, which rely on compressed sensing or deep learning\n",
      "algorithms to recover MRI data from undersampled multichannel Fourier\n",
      "measurements, are widely used to reduce scan time. The image quality of these\n",
      "approaches is heavily dependent on the sampling pattern. We introduce a\n",
      "continuous strategy to jointly optimize the sampling pattern and network\n",
      "parameters. We use a multichannel forward model, consisting of a non-uniform\n",
      "Fourier transform with continuously defined sampling locations, to realize the\n",
      "data consistency block within a model-based deep learning image reconstruction\n",
      "scheme. This approach facilitates the joint and continuous optimization of the\n",
      "sampling pattern and the CNN parameters to improve image quality. We observe\n",
      "that the joint optimization of the sampling patterns and the reconstruction\n",
      "module significantly improves the performance of most deep learning\n",
      "reconstruction algorithms. The source code of the proposed joint learning\n",
      "framework is available at https://github.com/hkaggarwal/J-MoDL.\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.03437 \n",
      "Title :SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language\n",
      "  Models through Principled Regularized Optimization\n",
      "  Transfer learning has fundamentally changed the landscape of natural language\n",
      "processing (NLP) research. Many existing state-of-the-art models are first\n",
      "pre-trained on a large text corpus and then fine-tuned on downstream tasks.\n",
      "However, due to limited data resources from downstream tasks and the extremely\n",
      "large capacity of pre-trained models, aggressive fine-tuning often causes the\n",
      "adapted model to overfit the data of downstream tasks and forget the knowledge\n",
      "of the pre-trained model. To address the above issue in a more principled\n",
      "manner, we propose a new computational framework for robust and efficient\n",
      "fine-tuning for pre-trained language models. Specifically, our proposed\n",
      "framework contains two important ingredients: 1. Smoothness-inducing\n",
      "regularization, which effectively manages the capacity of the model; 2. Bregman\n",
      "proximal point optimization, which is a class of trust-region methods and can\n",
      "prevent knowledge forgetting. Our experiments demonstrate that our proposed\n",
      "method achieves the state-of-the-art performance on multiple NLP benchmarks.\n",
      "\n",
      "**Paper Id :2010.01342 \n",
      "Title :End-to-End Training of CNN Ensembles for Person Re-Identification\n",
      "  We propose an end-to-end ensemble method for person re-identification (ReID)\n",
      "to address the problem of overfitting in discriminative models. These models\n",
      "are known to converge easily, but they are biased to the training data in\n",
      "general and may produce a high model variance, which is known as overfitting.\n",
      "The ReID task is more prone to this problem due to the large discrepancy\n",
      "between training and test distributions. To address this problem, our proposed\n",
      "ensemble learning framework produces several diverse and accurate base learners\n",
      "in a single DenseNet. Since most of the costly dense blocks are shared, our\n",
      "method is computationally efficient, which makes it favorable compared to the\n",
      "conventional ensemble models. Experiments on several benchmark datasets\n",
      "demonstrate that our method achieves state-of-the-art results. Noticeable\n",
      "performance improvements, especially on relatively small datasets, indicate\n",
      "that the proposed method deals with the overfitting problem effectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.04357 \n",
      "Title :Limited View and Sparse Photoacoustic Tomography for Neuroimaging with\n",
      "  Deep Learning\n",
      "  Photoacoustic tomography (PAT) is a nonionizing imaging modality capable of\n",
      "acquiring high contrast and resolution images of optical absorption at depths\n",
      "greater than traditional optical imaging techniques. Practical considerations\n",
      "with instrumentation and geometry limit the number of available acoustic\n",
      "sensors and their view of the imaging target, which result in significant image\n",
      "reconstruction artifacts degrading image quality. Iterative reconstruction\n",
      "methods can be used to reduce artifacts but are computationally expensive. In\n",
      "this work, we propose a novel deep learning approach termed pixelwise deep\n",
      "learning (PixelDL) that first employs pixelwise interpolation governed by the\n",
      "physics of photoacoustic wave propagation and then uses a convolution neural\n",
      "network to directly reconstruct an image. Simulated photoacoustic data from\n",
      "synthetic vasculature phantom and mouse-brain vasculature were used for\n",
      "training and testing, respectively. Results demonstrated that PixelDL achieved\n",
      "comparable performance to iterative methods and outperformed other CNN-based\n",
      "approaches for correcting artifacts. PixelDL is a computationally efficient\n",
      "approach that enables for realtime PAT rendering and for improved image\n",
      "quality, quantification, and interpretation.\n",
      "\n",
      "**Paper Id :2004.09610 \n",
      "Title :Deep variational network for rapid 4D flow MRI reconstruction\n",
      "  Phase-contrast magnetic resonance imaging (MRI) provides time-resolved\n",
      "quantification of blood flow dynamics that can aid clinical diagnosis. Long in\n",
      "vivo scan times due to repeated three-dimensional (3D) volume sampling over\n",
      "cardiac phases and breathing cycles necessitate accelerated imaging techniques\n",
      "that leverage data correlations. Standard compressed sensing reconstruction\n",
      "methods require tuning of hyperparameters and are computationally expensive,\n",
      "which diminishes the potential reduction of examination times. We propose an\n",
      "efficient model-based deep neural reconstruction network and evaluate its\n",
      "performance on clinical aortic flow data. The network is shown to reconstruct\n",
      "undersampled 4D flow MRI data in under a minute on standard consumer hardware.\n",
      "Remarkably, the relatively low amounts of tunable parameters allowed the\n",
      "network to be trained on images from 11 reference scans while generalizing well\n",
      "to retrospective and prospective undersampled data for various acceleration\n",
      "factors and anatomies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.04975 \n",
      "Title :word2ket: Space-efficient Word Embeddings inspired by Quantum\n",
      "  Entanglement\n",
      "  Deep learning natural language processing models often use vector word\n",
      "embeddings, such as word2vec or GloVe, to represent words. A discrete sequence\n",
      "of words can be much more easily integrated with downstream neural layers if it\n",
      "is represented as a sequence of continuous vectors. Also, semantic\n",
      "relationships between words, learned from a text corpus, can be encoded in the\n",
      "relative configurations of the embedding vectors. However, storing and\n",
      "accessing embedding vectors for all words in a dictionary requires large amount\n",
      "of space, and may stain systems with limited GPU memory. Here, we used\n",
      "approaches inspired by quantum computing to propose two related methods, {\\em\n",
      "word2ket} and {\\em word2ketXS}, for storing word embedding matrix during\n",
      "training and inference in a highly efficient way. Our approach achieves a\n",
      "hundred-fold or more reduction in the space required to store the embeddings\n",
      "with almost no relative drop in accuracy in practical natural language\n",
      "processing tasks.\n",
      "\n",
      "**Paper Id :2011.03479 \n",
      "Title :Massively Parallel Graph Drawing and Representation Learning\n",
      "  To fully exploit the performance potential of modern multi-core processors,\n",
      "machine learning and data mining algorithms for big data must be parallelized\n",
      "in multiple ways. Today's CPUs consist of multiple cores, each following an\n",
      "independent thread of control, and each equipped with multiple arithmetic units\n",
      "which can perform the same operation on a vector of multiple data objects.\n",
      "Graph embedding, i.e. converting the vertices of a graph into numerical vectors\n",
      "is a data mining task of high importance and is useful for graph drawing\n",
      "(low-dimensional vectors) and graph representation learning (high-dimensional\n",
      "vectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by\n",
      "Minimizing the Predictive Entropy), an information-theoretic method which can\n",
      "generate low and high-dimensional vectors. MulticoreGEMPE applies MIMD\n",
      "(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single\n",
      "Instructions Multiple Data, using AVX-512) parallelism. We propose general\n",
      "ideas applicable in other graph-based algorithms like \\emph{vectorized hashing}\n",
      "and \\emph{vectorized reduction}. Our experimental evaluation demonstrates the\n",
      "superiority of our approach.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.05020 \n",
      "Title :Generative adversarial networks (GAN) based efficient sampling of\n",
      "  chemical space for inverse design of inorganic materials\n",
      "  A major challenge in materials design is how to efficiently search the vast\n",
      "chemical design space to find the materials with desired properties. One\n",
      "effective strategy is to develop sampling algorithms that can exploit both\n",
      "explicit chemical knowledge and implicit composition rules embodied in the\n",
      "large materials database. Here, we propose a generative machine learning model\n",
      "(MatGAN) based on a generative adversarial network (GAN) for efficient\n",
      "generation of new hypothetical inorganic materials. Trained with materials from\n",
      "the ICSD database, our GAN model can generate hypothetical materials not\n",
      "existing in the training dataset, reaching a novelty of 92.53% when generating\n",
      "2 million samples. The percentage of chemically valid (charge neutral and\n",
      "electronegativity balanced) samples out of all generated ones reaches 84.5% by\n",
      "our GAN when trained with materials from ICSD even though no such chemical\n",
      "rules are explicitly enforced in our GAN model, indicating its capability to\n",
      "learn implicit chemical composition rules. Our algorithm could be used to speed\n",
      "up inverse design or computational screening of inorganic materials.\n",
      "\n",
      "**Paper Id :2003.13379 \n",
      "Title :Global Attention based Graph Convolutional Neural Networks for Improved\n",
      "  Materials Property Prediction\n",
      "  Machine learning (ML) methods have gained increasing popularity in exploring\n",
      "and developing new materials. More specifically, graph neural network (GNN) has\n",
      "been applied in predicting material properties. In this work, we develop a\n",
      "novel model, GATGNN, for predicting inorganic material properties based on\n",
      "graph neural networks composed of multiple graph-attention layers (GAT) and a\n",
      "global attention layer. Through the application of the GAT layers, our model\n",
      "can efficiently learn the complex bonds shared among the atoms within each\n",
      "atom's local neighborhood. Subsequently, the global attention layer provides\n",
      "the weight coefficients of each atom in the inorganic crystal material which\n",
      "are used to considerably improve our model's performance. Notably, with the\n",
      "development of our GATGNN model, we show that our method is able to both\n",
      "outperform the previous models' predictions and provide insight into the\n",
      "crystallization of the material.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.05797 \n",
      "Title :AI-optimized detector design for the future Electron-Ion Collider: the\n",
      "  dual-radiator RICH case\n",
      "  Advanced detector R&D requires performing computationally intensive and\n",
      "detailed simulations as part of the detector-design optimization process. We\n",
      "propose a general approach to this process based on Bayesian optimization and\n",
      "machine learning that encodes detector requirements. As a case study, we focus\n",
      "on the design of the dual-radiator Ring Imaging Cherenkov (dRICH) detector\n",
      "under development as part of the particle-identification system at the future\n",
      "Electron-Ion Collider (EIC). The EIC is a US-led frontier accelerator project\n",
      "for nuclear physics, which has been proposed to further explore the structure\n",
      "and interactions of nuclear matter at the scale of sea quarks and gluons. We\n",
      "show that the detector design obtained with our automated and highly\n",
      "parallelized framework outperforms the baseline dRICH design within the\n",
      "assumptions of the current model. Our approach can be applied to any detector\n",
      "R&D, provided that realistic simulations are available.\n",
      "\n",
      "**Paper Id :1905.11825 \n",
      "Title :Fast Data-Driven Simulation of Cherenkov Detectors Using Generative\n",
      "  Adversarial Networks\n",
      "  The increasing luminosities of future Large Hadron Collider runs and next\n",
      "generation of collider experiments will require an unprecedented amount of\n",
      "simulated events to be produced. Such large scale productions are extremely\n",
      "demanding in terms of computing resources. Thus new approaches to event\n",
      "generation and simulation of detector responses are needed. In LHCb, the\n",
      "accurate simulation of Cherenkov detectors takes a sizeable fraction of CPU\n",
      "time. An alternative approach is described here, when one generates high-level\n",
      "reconstructed observables using a generative neural network to bypass low level\n",
      "details. This network is trained to reproduce the particle species likelihood\n",
      "function values based on the track kinematic parameters and detector occupancy.\n",
      "The fast simulation is trained using real data samples collected by LHCb during\n",
      "run 2. We demonstrate that this approach provides high-fidelity results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.06147 \n",
      "Title :t-SS3: a text classifier with dynamic n-grams for early risk detection\n",
      "  over text streams\n",
      "  A recently introduced classifier, called SS3, has shown to be well suited to\n",
      "deal with early risk detection (ERD) problems on text streams. It obtained\n",
      "state-of-the-art performance on early depression and anorexia detection on\n",
      "Reddit in the CLEF's eRisk open tasks. SS3 was created to deal with ERD\n",
      "problems naturally since: it supports incremental training and classification\n",
      "over text streams, and it can visually explain its rationale. However, SS3\n",
      "processes the input using a bag-of-word model lacking the ability to recognize\n",
      "important word sequences. This aspect could negatively affect the\n",
      "classification performance and also reduces the descriptiveness of visual\n",
      "explanations. In the standard document classification field, it is very common\n",
      "to use word n-grams to try to overcome some of these limitations.\n",
      "Unfortunately, when working with text streams, using n-grams is not trivial\n",
      "since the system must learn and recognize which n-grams are important \"on the\n",
      "fly\". This paper introduces t-SS3, an extension of SS3 that allows it to\n",
      "recognize useful patterns over text streams dynamically. We evaluated our model\n",
      "in the eRisk 2017 and 2018 tasks on early depression and anorexia detection.\n",
      "Experimental results suggest that t-SS3 is able to improve both current results\n",
      "and the richness of visual explanations.\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.06478 \n",
      "Title :Sequential Recommendation with Relation-Aware Kernelized Self-Attention\n",
      "  Recent studies identified that sequential Recommendation is improved by the\n",
      "attention mechanism. By following this development, we propose Relation-Aware\n",
      "Kernelized Self-Attention (RKSA) adopting a self-attention mechanism of the\n",
      "Transformer with augmentation of a probabilistic model. The original\n",
      "self-attention of Transformer is a deterministic measure without\n",
      "relation-awareness. Therefore, we introduce a latent space to the\n",
      "self-attention, and the latent space models the recommendation context from\n",
      "relation as a multivariate skew-normal distribution with a kernelized\n",
      "covariance matrix from co-occurrences, item characteristics, and user\n",
      "information. This work merges the self-attention of the Transformer and the\n",
      "sequential recommendation by adding a probabilistic model of the recommendation\n",
      "task specifics. We experimented RKSA over the benchmark datasets, and RKSA\n",
      "shows significant improvements compared to the recent baseline models. Also,\n",
      "RKSA were able to produce a latent space model that answers the reasons for\n",
      "recommendation.\n",
      "\n",
      "**Paper Id :2005.01690 \n",
      "Title :Learning Geo-Contextual Embeddings for Commuting Flow Prediction\n",
      "  Predicting commuting flows based on infrastructure and land-use information\n",
      "is critical for urban planning and public policy development. However, it is a\n",
      "challenging task given the complex patterns of commuting flows. Conventional\n",
      "models, such as gravity model, are mainly derived from physics principles and\n",
      "limited by their predictive power in real-world scenarios where many factors\n",
      "need to be considered. Meanwhile, most existing machine learning-based methods\n",
      "ignore the spatial correlations and fail to model the influence of nearby\n",
      "regions. To address these issues, we propose Geo-contextual Multitask Embedding\n",
      "Learner (GMEL), a model that captures the spatial correlations from geographic\n",
      "contextual information for commuting flow prediction. Specifically, we first\n",
      "construct a geo-adjacency network containing the geographic contextual\n",
      "information. Then, an attention mechanism is proposed based on the framework of\n",
      "graph attention network (GAT) to capture the spatial correlations and encode\n",
      "geographic contextual information to embedding space. Two separate GATs are\n",
      "used to model supply and demand characteristics. A multitask learning framework\n",
      "is used to introduce stronger restrictions and enhance the effectiveness of the\n",
      "embedding representation. Finally, a gradient boosting machine is trained based\n",
      "on the learned embeddings to predict commuting flows. We evaluate our model\n",
      "using real-world datasets from New York City and the experimental results\n",
      "demonstrate the effectiveness of our proposal against the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.06502 \n",
      "Title :Simple iterative method for generating targeted universal adversarial\n",
      "  perturbations\n",
      "  Deep neural networks (DNNs) are vulnerable to adversarial attacks. In\n",
      "particular, a single perturbation known as the universal adversarial\n",
      "perturbation (UAP) can foil most classification tasks conducted by DNNs. Thus,\n",
      "different methods for generating UAPs are required to fully evaluate the\n",
      "vulnerability of DNNs. A realistic evaluation would be with cases that consider\n",
      "targeted attacks; wherein the generated UAP causes DNN to classify an input\n",
      "into a specific class. However, the development of UAPs for targeted attacks\n",
      "has largely fallen behind that of UAPs for non-targeted attacks. Therefore, we\n",
      "propose a simple iterative method to generate UAPs for targeted attacks. Our\n",
      "method combines the simple iterative method for generating non-targeted UAPs\n",
      "and the fast gradient sign method for generating a targeted adversarial\n",
      "perturbation for an input. We applied the proposed method to state-of-the-art\n",
      "DNN models for image classification and proved the existence of almost\n",
      "imperceptible UAPs for targeted attacks; further, we demonstrated that such\n",
      "UAPs are easily generatable.\n",
      "\n",
      "**Paper Id :2005.02552 \n",
      "Title :Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder\n",
      "  Whereas adversarial training is employed as the main defence strategy against\n",
      "specific adversarial samples, it has limited generalization capability and\n",
      "incurs excessive time complexity. In this paper, we propose an attack-agnostic\n",
      "defence framework to enhance the intrinsic robustness of neural networks,\n",
      "without jeopardizing the ability of generalizing clean samples. Our Feature\n",
      "Pyramid Decoder (FPD) framework applies to all block-based convolutional neural\n",
      "networks (CNNs). It implants denoising and image restoration modules into a\n",
      "targeted CNN, and it also constraints the Lipschitz constant of the\n",
      "classification layer. Moreover, we propose a two-phase strategy to train the\n",
      "FPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\n",
      "multi-task and self-supervised learning. Evaluated against a variety of\n",
      "white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\n",
      "sufficient robustness against general adversarial samples on MNIST, SVHN and\n",
      "CALTECH. In addition, if we further conduct adversarial training, the\n",
      "FPD-enhanced CNNs perform better than their non-enhanced versions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.07571 \n",
      "Title :Casimir effect with machine learning\n",
      "  Vacuum fluctuations of quantum fields between physical objects depend on the\n",
      "shapes, positions, and internal composition of the latter. For objects of\n",
      "arbitrary shapes, even made from idealized materials, the calculation of the\n",
      "associated zero-point (Casimir) energy is an analytically intractable\n",
      "challenge. We propose a new numerical approach to this problem based on\n",
      "machine-learning techniques and illustrate the effectiveness of the method in a\n",
      "(2+1) dimensional scalar field theory. The Casimir energy is first calculated\n",
      "numerically using a Monte-Carlo algorithm for a set of the Dirichlet boundaries\n",
      "of various shapes. Then, a neural network is trained to compute this energy\n",
      "given the Dirichlet domain, treating the latter as black-and-white pixelated\n",
      "images. We show that after the learning phase, the neural network is able to\n",
      "quickly predict the Casimir energy for new boundaries of general shapes with\n",
      "reasonable accuracy.\n",
      "\n",
      "**Paper Id :2006.09113 \n",
      "Title :Topological defects and confinement with machine learning: the case of\n",
      "  monopoles in compact electrodynamics\n",
      "  We investigate the advantages of machine learning techniques to recognize the\n",
      "dynamics of topological objects in quantum field theories. We consider the\n",
      "compact U(1) gauge theory in three spacetime dimensions as the simplest example\n",
      "of a theory that exhibits confinement and mass gap phenomena generated by\n",
      "monopoles. We train a neural network with a generated set of monopole\n",
      "configurations to distinguish between confinement and deconfinement phases,\n",
      "from which it is possible to determine the deconfinement transition point and\n",
      "to predict several observables. The model uses a supervised learning approach\n",
      "and treats the monopole configurations as three-dimensional images (holograms).\n",
      "We show that the model can determine the transition temperature with accuracy,\n",
      "which depends on the criteria implemented in the algorithm. More importantly,\n",
      "we train the neural network with configurations from a single lattice size\n",
      "before making predictions for configurations from other lattice sizes, from\n",
      "which a reliable estimation of the critical temperatures are obtained.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.07662 \n",
      "Title :Variational mean-field theory for training restricted Boltzmann machines\n",
      "  with binary synapses\n",
      "  Unsupervised learning requiring only raw data is not only a fundamental\n",
      "function of the cerebral cortex, but also a foundation for a next generation of\n",
      "artificial neural networks. However, a unified theoretical framework to treat\n",
      "sensory inputs, synapses and neural activity together is still lacking. The\n",
      "computational obstacle originates from the discrete nature of synapses, and\n",
      "complex interactions among these three essential elements of learning. Here, we\n",
      "propose a variational mean-field theory in which the distribution of synaptic\n",
      "weights is considered. The unsupervised learning can then be decomposed into\n",
      "two intertwined steps: a maximization step is carried out as a gradient ascent\n",
      "of the lower-bound on the data log-likelihood, in which the synaptic weight\n",
      "distribution is determined by updating variational parameters, and an\n",
      "expectation step is carried out as a message passing procedure on an equivalent\n",
      "or dual neural network whose parameter is specified by the variational\n",
      "parameters of the weight distribution. Therefore, our framework provides\n",
      "insights on how data (or sensory inputs), synapses and neural activities\n",
      "interact with each other to achieve the goal of extracting statistical\n",
      "regularities in sensory inputs. This variational framework is verified in\n",
      "restricted Boltzmann machines with planted synaptic weights and\n",
      "handwritten-digits learning.\n",
      "\n",
      "**Paper Id :2001.03354 \n",
      "Title :Learning credit assignment\n",
      "  Deep learning has achieved impressive prediction accuracies in a variety of\n",
      "scientific and industrial domains. However, the nested non-linear feature of\n",
      "deep learning makes the learning highly non-transparent, i.e., it is still\n",
      "unknown how the learning coordinates a huge number of parameters to achieve a\n",
      "decision making. To explain this hierarchical credit assignment, we propose a\n",
      "mean-field learning model by assuming that an ensemble of sub-networks, rather\n",
      "than a single network, are trained for a classification task. Surprisingly, our\n",
      "model reveals that apart from some deterministic synaptic weights connecting\n",
      "two neurons at neighboring layers, there exist a large number of connections\n",
      "that can be absent, and other connections can allow for a broad distribution of\n",
      "their weight values. Therefore, synaptic connections can be classified into\n",
      "three categories: very important ones, unimportant ones, and those of\n",
      "variability that may partially encode nuisance factors. Therefore, our model\n",
      "learns the credit assignment leading to the decision, and predicts an ensemble\n",
      "of sub-networks that can accomplish the same task, thereby providing insights\n",
      "toward understanding the macroscopic behavior of deep learning through the lens\n",
      "of distinct roles of synaptic weights.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.07849 \n",
      "Title :Co-Attentive Equivariant Neural Networks: Focusing Equivariance On\n",
      "  Transformations Co-Occurring In Data\n",
      "  Equivariance is a nice property to have as it produces much more parameter\n",
      "efficient neural architectures and preserves the structure of the input through\n",
      "the feature mapping. Even though some combinations of transformations might\n",
      "never appear (e.g. an upright face with a horizontal nose), current equivariant\n",
      "architectures consider the set of all possible transformations in a\n",
      "transformation group when learning feature representations. Contrarily, the\n",
      "human visual system is able to attend to the set of relevant transformations\n",
      "occurring in the environment and utilizes this information to assist and\n",
      "improve object recognition. Based on this observation, we modify conventional\n",
      "equivariant feature mappings such that they are able to attend to the set of\n",
      "co-occurring transformations in data and generalize this notion to act on\n",
      "groups consisting of multiple symmetries. We show that our proposed\n",
      "co-attentive equivariant neural networks consistently outperform conventional\n",
      "rotation equivariant and rotation & reflection equivariant neural networks on\n",
      "rotated MNIST and CIFAR-10.\n",
      "\n",
      "**Paper Id :1905.13209 \n",
      "Title :AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n",
      "  Architectures\n",
      "  Learning to represent videos is a very challenging task both algorithmically\n",
      "and computationally. Standard video CNN architectures have been designed by\n",
      "directly extending architectures devised for image understanding to include the\n",
      "time dimension, using modules such as 3D convolutions, or by using two-stream\n",
      "design to capture both appearance and motion in videos. We interpret a video\n",
      "CNN as a collection of multi-stream convolutional blocks connected to each\n",
      "other, and propose the approach of automatically finding neural architectures\n",
      "with better connectivity and spatio-temporal interactions for video\n",
      "understanding. This is done by evolving a population of overly-connected\n",
      "architectures guided by connection weight learning. Architectures combining\n",
      "representations that abstract different input types (i.e., RGB and optical\n",
      "flow) at multiple temporal resolutions are searched for, allowing different\n",
      "types or sources of information to interact with each other. Our method,\n",
      "referred to as AssembleNet, outperforms prior approaches on public video\n",
      "datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n",
      "34.27% accuracy on Moments-in-Time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08105 \n",
      "Title :Three-dimensional Generative Adversarial Nets for Unsupervised Metal\n",
      "  Artifact Reduction\n",
      "  The reduction of metal artifacts in computed tomography (CT) images,\n",
      "specifically for strong artifacts generated from multiple metal objects, is a\n",
      "challenging issue in medical imaging research. Although there have been some\n",
      "studies on supervised metal artifact reduction through the learning of\n",
      "synthesized artifacts, it is difficult for simulated artifacts to cover the\n",
      "complexity of the real physical phenomena that may be observed in X-ray\n",
      "propagation. In this paper, we introduce metal artifact reduction methods based\n",
      "on an unsupervised volume-to-volume translation learned from clinical CT\n",
      "images. We construct three-dimensional adversarial nets with a regularized loss\n",
      "function designed for metal artifacts from multiple dental fillings. The\n",
      "results of experiments using 915 CT volumes from real patients demonstrate that\n",
      "the proposed framework has an outstanding capacity to reduce strong artifacts\n",
      "and to recover underlying missing voxels, while preserving the anatomical\n",
      "features of soft tissues and tooth structures from the original images.\n",
      "\n",
      "**Paper Id :2002.00011 \n",
      "Title :Age-Conditioned Synthesis of Pediatric Computed Tomography with\n",
      "  Auxiliary Classifier Generative Adversarial Networks\n",
      "  Deep learning is a popular and powerful tool in computed tomography (CT)\n",
      "image processing such as organ segmentation, but its requirement of large\n",
      "training datasets remains a challenge. Even though there is a large anatomical\n",
      "variability for children during their growth, the training datasets for\n",
      "pediatric CT scans are especially hard to obtain due to risks of radiation to\n",
      "children. In this paper, we propose a method to conditionally synthesize\n",
      "realistic pediatric CT images using a new auxiliary classifier generative\n",
      "adversarial network (ACGAN) architecture by taking age information into\n",
      "account. The proposed network generated age-conditioned high-resolution CT\n",
      "images to enrich pediatric training datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08250 \n",
      "Title :On the Discrepancy between the Theoretical Analysis and Practical\n",
      "  Implementations of Compressed Communication for Distributed Deep Learning\n",
      "  Compressed communication, in the form of sparsification or quantization of\n",
      "stochastic gradients, is employed to reduce communication costs in distributed\n",
      "data-parallel training of deep neural networks. However, there exists a\n",
      "discrepancy between theory and practice: while theoretical analysis of most\n",
      "existing compression methods assumes compression is applied to the gradients of\n",
      "the entire model, many practical implementations operate individually on the\n",
      "gradients of each layer of the model. In this paper, we prove that layer-wise\n",
      "compression is, in theory, better, because the convergence rate is upper\n",
      "bounded by that of entire-model compression for a wide range of biased and\n",
      "unbiased compression methods. However, despite the theoretical bound, our\n",
      "experimental study of six well-known methods shows that convergence, in\n",
      "practice, may or may not be better, depending on the actual trained model and\n",
      "compression ratio. Our findings suggest that it would be advantageous for deep\n",
      "learning frameworks to include support for both layer-wise and entire-model\n",
      "compression.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08411 \n",
      "Title :Mixed-curvature Variational Autoencoders\n",
      "  Euclidean geometry has historically been the typical \"workhorse\" for machine\n",
      "learning applications due to its power and simplicity. However, it has recently\n",
      "been shown that geometric spaces with constant non-zero curvature improve\n",
      "representations and performance on a variety of data types and downstream\n",
      "tasks. Consequently, generative models like Variational Autoencoders (VAEs)\n",
      "have been successfully generalized to elliptical and hyperbolic latent spaces.\n",
      "While these approaches work well on data with particular kinds of biases e.g.\n",
      "tree-like data for a hyperbolic VAE, there exists no generic approach unifying\n",
      "and leveraging all three models. We develop a Mixed-curvature Variational\n",
      "Autoencoder, an efficient way to train a VAE whose latent space is a product of\n",
      "constant curvature Riemannian manifolds, where the per-component curvature is\n",
      "fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces\n",
      "and recovers it when curvatures of all latent space components go to 0.\n",
      "\n",
      "**Paper Id :2001.05559 \n",
      "Title :Mode-Assisted Unsupervised Learning of Restricted Boltzmann Machines\n",
      "  Restricted Boltzmann machines (RBMs) are a powerful class of generative\n",
      "models, but their training requires computing a gradient that, unlike\n",
      "supervised backpropagation on typical loss functions, is notoriously difficult\n",
      "even to approximate. Here, we show that properly combining standard gradient\n",
      "updates with an off-gradient direction, constructed from samples of the RBM\n",
      "ground state (mode), improves their training dramatically over traditional\n",
      "gradient methods. This approach, which we call mode training, promotes faster\n",
      "training and stability, in addition to lower converged relative entropy (KL\n",
      "divergence). Along with the proofs of stability and convergence of this method,\n",
      "we also demonstrate its efficacy on synthetic datasets where we can compute KL\n",
      "divergences exactly, as well as on a larger machine learning standard, MNIST.\n",
      "The mode training we suggest is quite versatile, as it can be applied in\n",
      "conjunction with any given gradient method, and is easily extended to more\n",
      "general energy-based neural network structures such as deep, convolutional and\n",
      "unrestricted Boltzmann machines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08508 \n",
      "Title :Parameters Estimation for the Cosmic Microwave Background with Bayesian\n",
      "  Neural Networks\n",
      "  In this paper, we present the first study that compares different models of\n",
      "Bayesian Neural Networks (BNNs) to predict the posterior distribution of the\n",
      "cosmological parameters directly from the Cosmic Microwave Background\n",
      "temperature and polarization maps. We focus our analysis on four different\n",
      "methods to sample the weights of the network during training: Dropout,\n",
      "DropConnect, Reparameterization Trick (RT), and Flipout. We find out that\n",
      "Flipout outperforms all other methods regardless of the architecture used, and\n",
      "provides tighter constraints for the cosmological parameters. Moreover we\n",
      "compare with MCMC posterior analysis obtaining comparable error correlation\n",
      "among parameters, with BNNs being orders of magnitude faster in inference,\n",
      "although less accurate. Thanks to the speed of the inference process with BNNs,\n",
      "the posterior distribution, outcome of the neural network, can be used as the\n",
      "initial proposal for the Markov Chain. We show that this combined approach\n",
      "increases the acceptance rate in the Metropolis-Hasting algorithm and\n",
      "accelerates the convergence of the MCMC, while reaching the same final\n",
      "accuracy. In the second part of the paper, we present a guide to the training\n",
      "and calibration of a successful multi-channel BNN for the CMB temperature and\n",
      "polarization map. We show how tuning the regularization parameter for the\n",
      "standard deviation of the approximate posterior on the weights in Flipout and\n",
      "RT we can produce unbiased and reliable uncertainty estimates, i.e., the\n",
      "regularizer acts like a hyperparameter analogous to the dropout rate in\n",
      "Dropout. Finally, we show how polarization, when combined with the temperature\n",
      "in a unique multi-channel tensor fed to a single BNN, helps to break\n",
      "degeneracies among parameters and provides stringent constraints.\n",
      "\n",
      "**Paper Id :2005.07694 \n",
      "Title :Constraining the Reionization History using Bayesian Normalizing Flows\n",
      "  The next generation 21 cm surveys open a new window onto the early stages of\n",
      "cosmic structure formation and provide new insights about the Epoch of\n",
      "Reionization (EoR). However, the non-Gaussian nature of the 21 cm signal along\n",
      "with the huge amount of data generated from these surveys will require more\n",
      "advanced techniques capable to efficiently extract the necessary information to\n",
      "constrain the Reionization History of the Universe. In this paper we present\n",
      "the use of Bayesian Neural Networks (BNNs) to predict the posterior\n",
      "distribution for four astrophysical and cosmological parameters. Besides\n",
      "achieving state-of-the-art prediction performances, the proposed methods\n",
      "provide accurate estimation of parameters uncertainties and infer correlations\n",
      "among them. Additionally, we demonstrate the advantages of Normalizing Flows\n",
      "(NF) combined with BNNs, being able to model more complex output distributions\n",
      "and thus capture key information as non-Gaussianities in the parameter\n",
      "conditional density distribution for astrophysical and cosmological dataset.\n",
      "Finally, we propose novel calibration methods employing Normalizing Flows after\n",
      "training, to produce reliable predictions, and we demonstrate the advantages of\n",
      "this approach both in terms of computational cost and prediction performances.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08670 \n",
      "Title :MMTM: Multimodal Transfer Module for CNN Fusion\n",
      "  In late fusion, each modality is processed in a separate unimodal\n",
      "Convolutional Neural Network (CNN) stream and the scores of each modality are\n",
      "fused at the end. Due to its simplicity late fusion is still the predominant\n",
      "approach in many state-of-the-art multimodal applications. In this paper, we\n",
      "present a simple neural network module for leveraging the knowledge from\n",
      "multiple modalities in convolutional neural networks. The propose unit, named\n",
      "Multimodal Transfer Module (MMTM), can be added at different levels of the\n",
      "feature hierarchy, enabling slow modality fusion. Using squeeze and excitation\n",
      "operations, MMTM utilizes the knowledge of multiple modalities to recalibrate\n",
      "the channel-wise features in each CNN stream. Despite other intermediate fusion\n",
      "methods, the proposed module could be used for feature modality fusion in\n",
      "convolution layers with different spatial dimensions. Another advantage of the\n",
      "proposed method is that it could be added among unimodal branches with minimum\n",
      "changes in the their network architectures, allowing each branch to be\n",
      "initialized with existing pretrained weights. Experimental results show that\n",
      "our framework improves the recognition accuracy of well-known multimodal\n",
      "networks. We demonstrate state-of-the-art or competitive performance on four\n",
      "datasets that span the task domains of dynamic hand gesture recognition, speech\n",
      "enhancement, and action recognition with RGB and body joints.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08817 \n",
      "Title :Black-box Combinatorial Optimization using Models with Integer-valued\n",
      "  Minima\n",
      "  When a black-box optimization objective can only be evaluated with costly or\n",
      "noisy measurements, most standard optimization algorithms are unsuited to find\n",
      "the optimal solution. Specialized algorithms that deal with exactly this\n",
      "situation make use of surrogate models. These models are usually continuous and\n",
      "smooth, which is beneficial for continuous optimization problems, but not\n",
      "necessarily for combinatorial problems. However, by choosing the basis\n",
      "functions of the surrogate model in a certain way, we show that it can be\n",
      "guaranteed that the optimal solution of the surrogate model is integer. This\n",
      "approach outperforms random search, simulated annealing and one Bayesian\n",
      "optimization algorithm on the problem of finding robust routes for a\n",
      "noise-perturbed traveling salesman benchmark problem, with similar performance\n",
      "as another Bayesian optimization algorithm, and outperforms all compared\n",
      "algorithms on a convex binary optimization problem with a large number of\n",
      "variables.\n",
      "\n",
      "**Paper Id :1905.00820 \n",
      "Title :On the smoothness of nonlinear system identification\n",
      "  We shed new light on the \\textit{smoothness} of optimization problems arising\n",
      "in prediction error parameter estimation of linear and nonlinear systems. We\n",
      "show that for regions of the parameter space where the model is not\n",
      "contractive, the Lipschitz constant and $\\beta$-smoothness of the objective\n",
      "function might blow up exponentially with the simulation length, making it hard\n",
      "to numerically find minima within those regions or, even, to escape from them.\n",
      "In addition to providing theoretical understanding of this problem, this paper\n",
      "also proposes the use of multiple shooting as a viable solution. The proposed\n",
      "method minimizes the error between a prediction model and the observed values.\n",
      "Rather than running the prediction model over the entire dataset, multiple\n",
      "shooting splits the data into smaller subsets and runs the prediction model\n",
      "over each subset, making the simulation length a design parameter and making it\n",
      "possible to solve problems that would be infeasible using a standard approach.\n",
      "The equivalence to the original problem is obtained by including constraints in\n",
      "the optimization. The new method is illustrated by estimating the parameters of\n",
      "nonlinear systems with chaotic or unstable behavior, as well as neural\n",
      "networks. We also present a comparative analysis of the proposed method with\n",
      "multi-step-ahead prediction error minimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08871 \n",
      "Title :CNAK : Cluster Number Assisted K-means\n",
      "  Determining the number of clusters present in a dataset is an important\n",
      "problem in cluster analysis. Conventional clustering techniques generally\n",
      "assume this parameter to be provided up front. %user supplied. %Recently,\n",
      "robustness of any given clustering algorithm is analyzed to measure cluster\n",
      "stability/instability which in turn determines the cluster number. In this\n",
      "paper, we propose a method which analyzes cluster stability for predicting the\n",
      "cluster number. Under the same computational framework, the technique also\n",
      "finds representatives of the clusters. The method is apt for handling big data,\n",
      "as we design the algorithm using \\emph{Monte-Carlo} simulation. Also, we\n",
      "explore a few pertinent issues found to be of also clustering. Experiments\n",
      "reveal that the proposed method is capable of identifying a single cluster. It\n",
      "is robust in handling high dimensional dataset and performs reasonably well\n",
      "over datasets having cluster imbalance. Moreover, it can indicate cluster\n",
      "hierarchy, if present. Overall we have observed significant improvement in\n",
      "speed and quality for predicting cluster numbers as well as the composition of\n",
      "clusters in a large dataset.\n",
      "\n",
      "**Paper Id :1804.10168 \n",
      "Title :BEST : A decision tree algorithm that handles missing values\n",
      "  The main contribution of this paper is the development of a new decision tree\n",
      "algorithm. The proposed approach allows users to guide the algorithm through\n",
      "the data partitioning process. We believe this feature has many applications\n",
      "but in this paper we demonstrate how to utilize this algorithm to analyse data\n",
      "sets containing missing values. We tested our algorithm against simulated data\n",
      "sets with various missing data structures and a real data set. The results\n",
      "demonstrate that this new classification procedure efficiently handles missing\n",
      "values and produces results that are slightly more accurate and more\n",
      "interpretable than most common procedures without any imputations or\n",
      "pre-processing.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.08934 \n",
      "Title :Joint NN-Supported Multichannel Reduction of Acoustic Echo,\n",
      "  Reverberation and Noise\n",
      "  We consider the problem of simultaneous reduction of acoustic echo,\n",
      "reverberation and noise. In real scenarios, these distortion sources may occur\n",
      "simultaneously and reducing them implies combining the corresponding\n",
      "distortion-specific filters. As these filters interact with each other, they\n",
      "must be jointly optimized. We propose to model the target and residual signals\n",
      "after linear echo cancellation and dereverberation using a multichannel\n",
      "Gaussian modeling framework and to jointly represent their spectra by means of\n",
      "a neural network. We develop an iterative block-coordinate ascent algorithm to\n",
      "update all the filters. We evaluate our system on real recordings of acoustic\n",
      "echo, reverberation and noise acquired with a smart speaker in various\n",
      "situations. The proposed approach outperforms in terms of overall distortion a\n",
      "cascade of the individual approaches and a joint reduction approach which does\n",
      "not rely on a spectral model of the target and residual signals.\n",
      "\n",
      "**Paper Id :2005.08128 \n",
      "Title :Sparse Mixture of Local Experts for Efficient Speech Enhancement\n",
      "  In this paper, we investigate a deep learning approach for speech denoising\n",
      "through an efficient ensemble of specialist neural networks. By splitting up\n",
      "the speech denoising task into non-overlapping subproblems and introducing a\n",
      "classifier, we are able to improve denoising performance while also reducing\n",
      "computational complexity. More specifically, the proposed model incorporates a\n",
      "gating network which assigns noisy speech signals to an appropriate specialist\n",
      "network based on either speech degradation level or speaker gender. In our\n",
      "experiments, a baseline recurrent network is compared against an ensemble of\n",
      "similarly-designed smaller recurrent networks regulated by the auxiliary gating\n",
      "network. Using stochastically generated batches from a large noisy speech\n",
      "corpus, the proposed model learns to estimate a time-frequency masking matrix\n",
      "based on the magnitude spectrogram of an input mixture signal. Both baseline\n",
      "and specialist networks are trained to estimate the ideal ratio mask, while the\n",
      "gating network is trained to perform subproblem classification. Our findings\n",
      "demonstrate that a fine-tuned ensemble network is able to exceed the speech\n",
      "denoising capabilities of a generalist network, doing so with fewer model\n",
      "parameters.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09070 \n",
      "Title :EfficientDet: Scalable and Efficient Object Detection\n",
      "  Model efficiency has become increasingly important in computer vision. In\n",
      "this paper, we systematically study neural network architecture design choices\n",
      "for object detection and propose several key optimizations to improve\n",
      "efficiency. First, we propose a weighted bi-directional feature pyramid network\n",
      "(BiFPN), which allows easy and fast multiscale feature fusion; Second, we\n",
      "propose a compound scaling method that uniformly scales the resolution, depth,\n",
      "and width for all backbone, feature network, and box/class prediction networks\n",
      "at the same time. Based on these optimizations and better backbones, we have\n",
      "developed a new family of object detectors, called EfficientDet, which\n",
      "consistently achieve much better efficiency than prior art across a wide\n",
      "spectrum of resource constraints. In particular, with single model and\n",
      "single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO\n",
      "test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using\n",
      "13x - 42x fewer FLOPs than previous detectors. Code is available at\n",
      "https://github.com/google/automl/tree/master/efficientdet.\n",
      "\n",
      "**Paper Id :2010.12438 \n",
      "Title :Transferable Graph Optimizers for ML Compilers\n",
      "  Most compilers for machine learning (ML) frameworks need to solve many\n",
      "correlated optimization problems to generate efficient machine code. Current ML\n",
      "compilers rely on heuristics based algorithms to solve these optimization\n",
      "problems one at a time. However, this approach is not only hard to maintain but\n",
      "often leads to sub-optimal solutions especially for newer model architectures.\n",
      "Existing learning based approaches in the literature are sample inefficient,\n",
      "tackle a single optimization problem, and do not generalize to unseen graphs\n",
      "making them infeasible to be deployed in practice. To address these\n",
      "limitations, we propose an end-to-end, transferable deep reinforcement learning\n",
      "method for computational graph optimization (GO), based on a scalable\n",
      "sequential attention mechanism over an inductive graph neural network. GO\n",
      "generates decisions on the entire graph rather than on each individual node\n",
      "autoregressively, drastically speeding up the search compared to prior methods.\n",
      "Moreover, we propose recurrent attention layers to jointly optimize dependent\n",
      "graph optimization tasks and demonstrate 33%-60% speedup on three graph\n",
      "optimization tasks compared to TensorFlow default optimization. On a diverse\n",
      "set of representative graphs consisting of up to 80,000 nodes, including\n",
      "Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21%\n",
      "improvement over human experts and 18% improvement over the prior state of the\n",
      "art with 15x faster convergence, on a device placement task evaluated in real\n",
      "systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09083 \n",
      "Title :Machine-learning non-stationary noise out of gravitational wave\n",
      "  detectors\n",
      "  Signal extraction out of background noise is a common challenge in high\n",
      "precision physics experiments, where the measurement output is often a\n",
      "continuous data stream. To improve the signal to noise ratio of the detection,\n",
      "witness sensors are often used to independently measure background noises and\n",
      "subtract them from the main signal. If the noise coupling is linear and\n",
      "stationary, optimal techniques already exist and are routinely implemented in\n",
      "many experiments. However, when the noise coupling is non-stationary, linear\n",
      "techniques often fail or are sub-optimal. Inspired by the properties of the\n",
      "background noise in gravitational wave detectors, this work develops a novel\n",
      "algorithm to efficiently characterize and remove non-stationary noise\n",
      "couplings, provided there exist witnesses of the noise source and of the\n",
      "modulation. In this work, the algorithm is described in its most general\n",
      "formulation, and its efficiency is demonstrated with examples from the data of\n",
      "the Advanced LIGO gravitational wave observatory, where we could obtain an\n",
      "improvement of the detector gravitational wave reach without introducing any\n",
      "bias on the source parameter estimation.\n",
      "\n",
      "**Paper Id :2003.01643 \n",
      "Title :Single-exposure absorption imaging of ultracold atoms using deep\n",
      "  learning\n",
      "  Absorption imaging is the most common probing technique in experiments with\n",
      "ultracold atoms. The standard procedure involves the division of two frames\n",
      "acquired at successive exposures, one with the atomic absorption signal and one\n",
      "without. A well-known problem is the presence of residual structured noise in\n",
      "the final image, due to small differences between the imaging light in the two\n",
      "exposures. Here we solve this problem by performing absorption imaging with\n",
      "only a single exposure, where instead of a second exposure the reference frame\n",
      "is generated by an unsupervised image-completion autoencoder neural network.\n",
      "The network is trained on images without absorption signal such that it can\n",
      "infer the noise overlaying the atomic signal based only on the information in\n",
      "the region encircling the signal. We demonstrate our approach on data captured\n",
      "with a quantum degenerate Fermi gas. The average residual noise in the\n",
      "resulting images is below that of the standard double-shot technique. Our\n",
      "method simplifies the experimental sequence, reduces the hardware requirements,\n",
      "and can improve the accuracy of extracted physical observables. The trained\n",
      "network and its generating scripts are available as an open-source repository\n",
      "(http://absDL.github.io/).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09447 \n",
      "Title :S-RASTER: Contraction Clustering for Evolving Data Streams\n",
      "  Contraction Clustering (RASTER) is a single-pass algorithm for density-based\n",
      "clustering of 2D data. It can process arbitrary amounts of data in linear time\n",
      "and in constant memory, quickly identifying approximate clusters. It also\n",
      "exhibits good scalability in the presence of multiple CPU cores. RASTER\n",
      "exhibits very competitive performance compared to standard clustering\n",
      "algorithms, but at the cost of decreased precision. Yet, RASTER is limited to\n",
      "batch processing and unable to identify clusters that only exist temporarily.\n",
      "In contrast, S-RASTER is an adaptation of RASTER to the stream processing\n",
      "paradigm that is able to identify clusters in evolving data streams. This\n",
      "algorithm retains the main benefits of its parent algorithm, i.e. single-pass\n",
      "linear time cost and constant memory requirements for each discrete time step\n",
      "within a sliding window. The sliding window is efficiently pruned, and\n",
      "clustering is still performed in linear time. Like RASTER, S-RASTER trades off\n",
      "an often negligible amount of precision for speed. Our evaluation shows that\n",
      "competing algorithms are at least 50% slower. Furthermore, S-RASTER shows good\n",
      "qualitative results, based on standard metrics. It is very well suited to\n",
      "real-world scenarios where clustering does not happen continually but only\n",
      "periodically.\n",
      "\n",
      "**Paper Id :2010.00730 \n",
      "Title :Modifying the Symbolic Aggregate Approximation Method to Capture Segment\n",
      "  Trend Information\n",
      "  The Symbolic Aggregate approXimation (SAX) is a very popular symbolic\n",
      "dimensionality reduction technique of time series data, as it has several\n",
      "advantages over other dimensionality reduction techniques. One of its major\n",
      "advantages is its efficiency, as it uses precomputed distances. The other main\n",
      "advantage is that in SAX the distance measure defined on the reduced space\n",
      "lower bounds the distance measure defined on the original space. This enables\n",
      "SAX to return exact results in query-by-content tasks. Yet SAX has an inherent\n",
      "drawback, which is its inability to capture segment trend information. Several\n",
      "researchers have attempted to enhance SAX by proposing modifications to include\n",
      "trend information. However, this comes at the expense of giving up on one or\n",
      "more of the advantages of SAX. In this paper we investigate three modifications\n",
      "of SAX to add trend capturing ability to it. These modifications retain the\n",
      "same features of SAX in terms of simplicity, efficiency, as well as the exact\n",
      "results it returns. They are simple procedures based on a different\n",
      "segmentation of the time series than that used in classic-SAX. We test the\n",
      "performance of these three modifications on 45 time series datasets of\n",
      "different sizes, dimensions, and nature, on a classification task and we\n",
      "compare it to that of classic-SAX. The results we obtained show that one of\n",
      "these modifications manages to outperform classic-SAX and that another one\n",
      "slightly gives better results than classic-SAX.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09450 \n",
      "Title :Few Shot Network Compression via Cross Distillation\n",
      "  Model compression has been widely adopted to obtain light-weighted deep\n",
      "neural networks. Most prevalent methods, however, require fine-tuning with\n",
      "sufficient training data to ensure accuracy, which could be challenged by\n",
      "privacy and security issues. As a compromise between privacy and performance,\n",
      "in this paper we investigate few shot network compression: given few samples\n",
      "per class, how can we effectively compress the network with negligible\n",
      "performance drop? The core challenge of few shot network compression lies in\n",
      "high estimation errors from the original network during inference, since the\n",
      "compressed network can easily over-fits on the few training instances. The\n",
      "estimation errors could propagate and accumulate layer-wisely and finally\n",
      "deteriorate the network output. To address the problem, we propose cross\n",
      "distillation, a novel layer-wise knowledge distillation approach. By\n",
      "interweaving hidden layers of teacher and student network, layer-wisely\n",
      "accumulated estimation errors can be effectively reduced.The proposed method\n",
      "offers a general framework compatible with prevalent network compression\n",
      "techniques such as pruning. Extensive experiments on benchmark datasets\n",
      "demonstrate that cross distillation can significantly improve the student\n",
      "network's accuracy when only a few training instances are available.\n",
      "\n",
      "**Paper Id :1907.06870 \n",
      "Title :Light Multi-segment Activation for Model Compression\n",
      "  Model compression has become necessary when applying neural networks (NN)\n",
      "into many real application tasks that can accept slightly-reduced model\n",
      "accuracy with strict tolerance to model complexity. Recently, Knowledge\n",
      "Distillation, which distills the knowledge from well-trained and highly complex\n",
      "teacher model into a compact student model, has been widely used for model\n",
      "compression. However, under the strict requirement on the resource cost, it is\n",
      "quite challenging to achieve comparable performance with the teacher model,\n",
      "essentially due to the drastically-reduced expressiveness ability of the\n",
      "compact student model. Inspired by the nature of the expressiveness ability in\n",
      "Neural Networks, we propose to use multi-segment activation, which can\n",
      "significantly improve the expressiveness ability with very little cost, in the\n",
      "compact student model. Specifically, we propose a highly efficient\n",
      "multi-segment activation, called Light Multi-segment Activation (LMA), which\n",
      "can rapidly produce multiple linear regions with very few parameters by\n",
      "leveraging the statistical information. With using LMA, the compact student\n",
      "model is capable of achieving much better performance effectively and\n",
      "efficiently, than the ReLU-equipped one with same model scale. Furthermore, the\n",
      "proposed method is compatible with other model compression techniques, such as\n",
      "quantization, which means they can be used jointly for better compression\n",
      "performance. Experiments on state-of-the-art NN architectures over the\n",
      "real-world tasks demonstrate the effectiveness and extensibility of the LMA.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09776 \n",
      "Title :TMI: Thermodynamic inference of data manifolds\n",
      "  The Gibbs-Boltzmann distribution offers a physically interpretable way to\n",
      "massively reduce the dimensionality of high dimensional probability\n",
      "distributions where the extensive variables are `features' and the intensive\n",
      "variables are `descriptors'. However, not all probability distributions can be\n",
      "modeled using the Gibbs-Boltzmann form. Here, we present TMI: TMI, {\\bf\n",
      "T}hermodynamic {\\bf M}anifold {\\bf I}nference; a thermodynamic approach to\n",
      "approximate a collection of arbitrary distributions. TMI simultaneously learns\n",
      "from data intensive and extensive variables and achieves dimensionality\n",
      "reduction through a multiplicative, positive valued, and interpretable\n",
      "decomposition of the data. Importantly, the reduced dimensional space of\n",
      "intensive parameters is not homogeneous. The Gibbs-Boltzmann distribution\n",
      "defines an analytically tractable Riemannian metric on the space of intensive\n",
      "variables allowing us to calculate geodesics and volume elements. We discuss\n",
      "the applications of TMI with multiple real and artificial data sets. Possible\n",
      "extensions are discussed as well.\n",
      "\n",
      "**Paper Id :1808.07452 \n",
      "Title :Generalized Canonical Polyadic Tensor Decomposition\n",
      "  Tensor decomposition is a fundamental unsupervised machine learning method in\n",
      "data science, with applications including network analysis and sensor data\n",
      "processing. This work develops a generalized canonical polyadic (GCP) low-rank\n",
      "tensor decomposition that allows other loss functions besides squared error.\n",
      "For instance, we can use logistic loss or Kullback-Leibler divergence, enabling\n",
      "tensor decomposition for binary or count data. We present a variety\n",
      "statistically-motivated loss functions for various scenarios. We provide a\n",
      "generalized framework for computing gradients and handling missing data that\n",
      "enables the use of standard optimization methods for fitting the model. We\n",
      "demonstrate the flexibility of GCP on several real-world examples including\n",
      "interactions in a social network, neural activity in a mouse, and monthly\n",
      "rainfall measurements in India.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09812 \n",
      "Title :Zero-Resource Cross-Lingual Named Entity Recognition\n",
      "  Recently, neural methods have achieved state-of-the-art (SOTA) results in\n",
      "Named Entity Recognition (NER) tasks for many languages without the need for\n",
      "manually crafted features. However, these models still require manually\n",
      "annotated training data, which is not available for many languages. In this\n",
      "paper, we propose an unsupervised cross-lingual NER model that can transfer NER\n",
      "knowledge from one language to another in a completely unsupervised way without\n",
      "relying on any bilingual dictionary or parallel data. Our model achieves this\n",
      "through word-level adversarial learning and augmented fine-tuning with\n",
      "parameter sharing and feature augmentation. Experiments on five different\n",
      "languages demonstrate the effectiveness of our approach, outperforming existing\n",
      "models by a good margin and setting a new SOTA for each language pair.\n",
      "\n",
      "**Paper Id :2005.04816 \n",
      "Title :Leveraging Monolingual Data with Self-Supervision for Multilingual\n",
      "  Neural Machine Translation\n",
      "  Over the last few years two promising research directions in low-resource\n",
      "neural machine translation (NMT) have emerged. The first focuses on utilizing\n",
      "high-resource languages to improve the quality of low-resource languages via\n",
      "multilingual NMT. The second direction employs monolingual data with\n",
      "self-supervision to pre-train translation models, followed by fine-tuning on\n",
      "small amounts of supervised data. In this work, we join these two lines of\n",
      "research and demonstrate the efficacy of monolingual data with self-supervision\n",
      "in multilingual NMT. We offer three major results: (i) Using monolingual data\n",
      "significantly boosts the translation quality of low-resource languages in\n",
      "multilingual models. (ii) Self-supervision improves zero-shot translation\n",
      "quality in multilingual models. (iii) Leveraging monolingual data with\n",
      "self-supervision provides a viable path towards adding new languages to\n",
      "multilingual models, getting up to 33 BLEU on ro-en translation without any\n",
      "parallel data or back-translation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09840 \n",
      "Title :Real-time Ultrasound-enhanced Multimodal Imaging of Tongue using 3D\n",
      "  Printable Stabilizer System: A Deep Learning Approach\n",
      "  Despite renewed awareness of the importance of articulation, it remains a\n",
      "challenge for instructors to handle the pronunciation needs of language\n",
      "learners. There are relatively scarce pedagogical tools for pronunciation\n",
      "teaching and learning. Unlike inefficient, traditional pronunciation\n",
      "instructions like listening and repeating, electronic visual feedback (EVF)\n",
      "systems such as ultrasound technology have been employed in new approaches.\n",
      "Recently, an ultrasound-enhanced multimodal method has been developed for\n",
      "visualizing tongue movements of a language learner overlaid on the face-side of\n",
      "the speaker's head. That system was evaluated for several language courses via\n",
      "a blended learning paradigm at the university level. The result was asserted\n",
      "that visualizing the articulator's system as biofeedback to language learners\n",
      "will significantly improve articulation learning efficiency. In spite of the\n",
      "successful usage of multimodal techniques for pronunciation training, it still\n",
      "requires manual works and human manipulation. In this article, we aim to\n",
      "contribute to this growing body of research by addressing difficulties of the\n",
      "previous approaches by proposing a new comprehensive, automatic, real-time\n",
      "multimodal pronunciation training system, benefits from powerful artificial\n",
      "intelligence techniques. The main objective of this research was to combine the\n",
      "advantages of ultrasound technology, three-dimensional printing, and deep\n",
      "learning algorithms to enhance the performance of previous systems. Our\n",
      "preliminary pedagogical evaluation of the proposed system revealed a\n",
      "significant improvement in flexibility, control, robustness, and autonomy.\n",
      "\n",
      "**Paper Id :2003.08767 \n",
      "Title :A Review of Computational Approaches for Evaluation of Rehabilitation\n",
      "  Exercises\n",
      "  Recent advances in data analytics and computer-aided diagnostics stimulate\n",
      "the vision of patient-centric precision healthcare, where treatment plans are\n",
      "customized based on the health records and needs of every patient. In physical\n",
      "rehabilitation, the progress in machine learning and the advent of affordable\n",
      "and reliable motion capture sensors have been conducive to the development of\n",
      "approaches for automated assessment of patient performance and progress toward\n",
      "functional recovery. The presented study reviews computational approaches for\n",
      "evaluating patient performance in rehabilitation programs using motion capture\n",
      "systems. Such approaches will play an important role in supplementing\n",
      "traditional rehabilitation assessment performed by trained clinicians, and in\n",
      "assisting patients participating in home-based rehabilitation. The reviewed\n",
      "computational methods for exercise evaluation are grouped into three main\n",
      "categories: discrete movement score, rule-based, and template-based approaches.\n",
      "The review places an emphasis on the application of machine learning methods\n",
      "for movement evaluation in rehabilitation. Related work in the literature on\n",
      "data representation, feature engineering, movement segmentation, and scoring\n",
      "functions is presented. The study also reviews existing sensors for capturing\n",
      "rehabilitation movements and provides an informative listing of pertinent\n",
      "benchmark datasets. The significance of this paper is in being the first to\n",
      "provide a comprehensive review of computational methods for evaluation of\n",
      "patient performance in rehabilitation programs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.09983 \n",
      "Title :TreeGen: A Tree-Based Transformer Architecture for Code Generation\n",
      "  A code generation system generates programming language code based on an\n",
      "input natural language description. State-of-the-art approaches rely on neural\n",
      "networks for code generation. However, these code generators suffer from two\n",
      "problems. One is the long dependency problem, where a code element often\n",
      "depends on another far-away code element. A variable reference, for example,\n",
      "depends on its definition, which may appear quite a few lines before. The other\n",
      "problem is structure modeling, as programs contain rich structural information.\n",
      "In this paper, we propose a novel tree-based neural architecture, TreeGen, for\n",
      "code generation. TreeGen uses the attention mechanism of Transformers to\n",
      "alleviate the long-dependency problem, and introduces a novel AST reader\n",
      "(encoder) to incorporate grammar rules and AST structures into the network. We\n",
      "evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing\n",
      "benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art\n",
      "approach by 4.5 percentage points on HearthStone, and achieved the best\n",
      "accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%).\n",
      "We also conducted an ablation test to better understand each component of our\n",
      "model.\n",
      "\n",
      "**Paper Id :1910.05493 \n",
      "Title :Deep Transfer Learning for Source Code Modeling\n",
      "  In recent years, deep learning models have shown great potential in source\n",
      "code modeling and analysis. Generally, deep learning-based approaches are\n",
      "problem-specific and data-hungry. A challenging issue of these approaches is\n",
      "that they require training from starch for a different related problem. In this\n",
      "work, we propose a transfer learning-based approach that significantly improves\n",
      "the performance of deep learning-based source code models. In contrast to\n",
      "traditional learning paradigms, transfer learning can transfer the knowledge\n",
      "learned in solving one problem into another related problem. First, we present\n",
      "two recurrent neural network-based models RNN and GRU for the purpose of\n",
      "transfer learning in the domain of source code modeling. Next, via transfer\n",
      "learning, these pre-trained (RNN and GRU) models are used as feature\n",
      "extractors. Then, these extracted features are combined into attention learner\n",
      "for different downstream tasks. The attention learner leverages from the\n",
      "learned knowledge of pre-trained models and fine-tunes them for a specific\n",
      "downstream task. We evaluate the performance of the proposed approach with\n",
      "extensive experiments with the source code suggestion task. The results\n",
      "indicate that the proposed approach outperforms the state-of-the-art models in\n",
      "terms of accuracy, precision, recall, and F-measure without training the models\n",
      "from scratch.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.10081 \n",
      "Title :ptype: Probabilistic Type Inference\n",
      "  Type inference refers to the task of inferring the data type of a given\n",
      "column of data. Current approaches often fail when data contains missing data\n",
      "and anomalies, which are found commonly in real-world data sets. In this paper,\n",
      "we propose ptype, a probabilistic robust type inference method that allows us\n",
      "to detect such entries, and infer data types. We further show that the proposed\n",
      "method outperforms the existing methods.\n",
      "\n",
      "**Paper Id :2006.14755 \n",
      "Title :DeltaGrad: Rapid retraining of machine learning models\n",
      "  Machine learning models are not static and may need to be retrained on\n",
      "slightly changed datasets, for instance, with the addition or deletion of a set\n",
      "of data points. This has many applications, including privacy, robustness, bias\n",
      "reduction, and uncertainty quantifcation. However, it is expensive to retrain\n",
      "models from scratch. To address this problem, we propose the DeltaGrad\n",
      "algorithm for rapid retraining machine learning models based on information\n",
      "cached during the training phase. We provide both theoretical and empirical\n",
      "support for the effectiveness of DeltaGrad, and show that it compares favorably\n",
      "to the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.10120 \n",
      "Title :Multi-Agent Thompson Sampling for Bandit Applications with Sparse\n",
      "  Neighbourhood Structures\n",
      "  Multi-agent coordination is prevalent in many real-world applications.\n",
      "However, such coordination is challenging due to its combinatorial nature. An\n",
      "important observation in this regard is that agents in the real world often\n",
      "only directly affect a limited set of neighbouring agents. Leveraging such\n",
      "loose couplings among agents is key to making coordination in multi-agent\n",
      "systems feasible. In this work, we focus on learning to coordinate.\n",
      "Specifically, we consider the multi-agent multi-armed bandit framework, in\n",
      "which fully cooperative loosely-coupled agents must learn to coordinate their\n",
      "decisions to optimize a common objective. We propose multi-agent Thompson\n",
      "sampling (MATS), a new Bayesian exploration-exploitation algorithm that\n",
      "leverages loose couplings. We provide a regret bound that is sublinear in time\n",
      "and low-order polynomial in the highest number of actions of a single agent for\n",
      "sparse coordination graphs. Additionally, we empirically show that MATS\n",
      "outperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks,\n",
      "and a novel benchmark with Poisson distributions. An example of a\n",
      "loosely-coupled multi-agent system is a wind farm. Coordination within the wind\n",
      "farm is necessary to maximize power production. As upstream wind turbines only\n",
      "affect nearby downstream turbines, we can use MATS to efficiently learn the\n",
      "optimal control mechanism for the farm. To demonstrate the benefits of our\n",
      "method toward applications we apply MATS to a realistic wind farm control task.\n",
      "In this task, wind turbines must coordinate their alignments with respect to\n",
      "the incoming wind vector in order to optimize power production. Our results\n",
      "show that MATS improves significantly upon state-of-the-art coordination\n",
      "methods in terms of performance, demonstrating the value of using MATS in\n",
      "practical applications with sparse neighbourhood structures.\n",
      "\n",
      "**Paper Id :2010.13032 \n",
      "Title :Byzantine Resilient Distributed Multi-Task Learning\n",
      "  Distributed multi-task learning provides significant advantages in\n",
      "multi-agent networks with heterogeneous data sources where agents aim to learn\n",
      "distinct but correlated models simultaneously. However, distributed algorithms\n",
      "for learning relatedness among tasks are not resilient in the presence of\n",
      "Byzantine agents. In this paper, we present an approach for Byzantine resilient\n",
      "distributed multi-task learning. We propose an efficient online weight\n",
      "assignment rule by measuring the accumulated loss using an agent's data and its\n",
      "neighbors' models. A small accumulated loss indicates a large similarity\n",
      "between the two tasks. In order to ensure the Byzantine resilience of the\n",
      "aggregation at a normal agent, we introduce a step for filtering out larger\n",
      "losses. We analyze the approach for convex models and show that normal agents\n",
      "converge resiliently towards their true targets. Further, an agent's learning\n",
      "performance using the proposed weight assignment rule is guaranteed to be at\n",
      "least as good as in the non-cooperative case as measured by the expected\n",
      "regret. Finally, we demonstrate the approach using three case studies,\n",
      "including regression and classification problems, and show that our method\n",
      "exhibits good empirical performance for non-convex models, such as\n",
      "convolutional neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.10143 \n",
      "Title :Adversarial Learning of Privacy-Preserving and Task-Oriented\n",
      "  Representations\n",
      "  Data privacy has emerged as an important issue as data-driven deep learning\n",
      "has been an essential component of modern machine learning systems. For\n",
      "instance, there could be a potential privacy risk of machine learning systems\n",
      "via the model inversion attack, whose goal is to reconstruct the input data\n",
      "from the latent representation of deep networks. Our work aims at learning a\n",
      "privacy-preserving and task-oriented representation to defend against such\n",
      "model inversion attacks. Specifically, we propose an adversarial reconstruction\n",
      "learning framework that prevents the latent representations decoded into\n",
      "original input data. By simulating the expected behavior of adversary, our\n",
      "framework is realized by minimizing the negative pixel reconstruction loss or\n",
      "the negative feature reconstruction (i.e., perceptual distance) loss. We\n",
      "validate the proposed method on face attribute prediction, showing that our\n",
      "method allows protecting visual privacy with a small decrease in utility\n",
      "performance. In addition, we show the utility-privacy trade-off with different\n",
      "choices of hyperparameter for negative perceptual distance loss at training,\n",
      "allowing service providers to determine the right level of privacy-protection\n",
      "with a certain utility performance. Moreover, we provide an extensive study\n",
      "with different selections of features, tasks, and the data to further analyze\n",
      "their influence on privacy protection.\n",
      "\n",
      "**Paper Id :1912.01899 \n",
      "Title :Distribution-induced Bidirectional Generative Adversarial Network for\n",
      "  Graph Representation Learning\n",
      "  Graph representation learning aims to encode all nodes of a graph into\n",
      "low-dimensional vectors that will serve as input of many compute vision tasks.\n",
      "However, most existing algorithms ignore the existence of inherent data\n",
      "distribution and even noises. This may significantly increase the phenomenon of\n",
      "over-fitting and deteriorate the testing accuracy. In this paper, we propose a\n",
      "Distribution-induced Bidirectional Generative Adversarial Network (named DBGAN)\n",
      "for graph representation learning. Instead of the widely used normal\n",
      "distribution assumption, the prior distribution of latent representation in our\n",
      "DBGAN is estimated in a structure-aware way, which implicitly bridges the graph\n",
      "and feature spaces by prototype learning. Thus discriminative and robust\n",
      "representations are generated for all nodes. Furthermore, to improve their\n",
      "generalization ability while preserving representation ability, the\n",
      "sample-level and distribution-level consistency is well balanced via a\n",
      "bidirectional adversarial learning framework. An extensive group of experiments\n",
      "are then carefully designed and presented, demonstrating that our DBGAN obtains\n",
      "remarkably more favorable trade-off between representation and robustness, and\n",
      "meanwhile is dimension-efficient, over currently available alternatives in\n",
      "various tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.11237 \n",
      "Title :Learning to Learn Words from Visual Scenes\n",
      "  Language acquisition is the process of learning words from the surrounding\n",
      "scene. We introduce a meta-learning framework that learns how to learn word\n",
      "representations from unconstrained scenes. We leverage the natural\n",
      "compositional structure of language to create training episodes that cause a\n",
      "meta-learner to learn strong policies for language acquisition. Experiments on\n",
      "two datasets show that our approach is able to more rapidly acquire novel words\n",
      "as well as more robustly generalize to unseen compositions, significantly\n",
      "outperforming established baselines. A key advantage of our approach is that it\n",
      "is data efficient, allowing representations to be learned from scratch without\n",
      "language pre-training. Visualizations and analysis suggest visual information\n",
      "helps our approach learn a rich cross-modal representation from minimal\n",
      "examples. Project webpage is available at https://expert.cs.columbia.edu/\n",
      "\n",
      "**Paper Id :2003.13834 \n",
      "Title :Label-Efficient Learning on Point Clouds using Approximate Convex\n",
      "  Decompositions\n",
      "  The problems of shape classification and part segmentation from 3D point\n",
      "clouds have garnered increasing attention in the last few years. Both of these\n",
      "problems, however, suffer from relatively small training sets, creating the\n",
      "need for statistically efficient methods to learn 3D shape representations. In\n",
      "this paper, we investigate the use of Approximate Convex Decompositions (ACD)\n",
      "as a self-supervisory signal for label-efficient learning of point cloud\n",
      "representations. We show that using ACD to approximate ground truth\n",
      "segmentation provides excellent self-supervision for learning 3D point cloud\n",
      "representations that are highly effective on downstream tasks. We report\n",
      "improvements over the state-of-the-art for unsupervised representation learning\n",
      "on the ModelNet40 shape classification dataset and significant gains in\n",
      "few-shot part segmentation on the ShapeNetPart dataset.Code available at\n",
      "https://github.com/matheusgadelha/PointCloudLearningACD\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.11374 \n",
      "Title :Representation Learning: A Statistical Perspective\n",
      "  Learning representations of data is an important problem in statistics and\n",
      "machine learning. While the origin of learning representations can be traced\n",
      "back to factor analysis and multidimensional scaling in statistics, it has\n",
      "become a central theme in deep learning with important applications in computer\n",
      "vision and computational neuroscience. In this article, we review recent\n",
      "advances in learning representations from a statistical perspective. In\n",
      "particular, we review the following two themes: (a) unsupervised learning of\n",
      "vector representations and (b) learning of both vector and matrix\n",
      "representations.\n",
      "\n",
      "**Paper Id :1901.05331 \n",
      "Title :Optimization Models for Machine Learning: A Survey\n",
      "  This paper surveys the machine learning literature and presents in an\n",
      "optimization framework several commonly used machine learning approaches.\n",
      "Particularly, mathematical optimization models are presented for regression,\n",
      "classification, clustering, deep learning, and adversarial learning, as well as\n",
      "new emerging applications in machine teaching, empirical model learning, and\n",
      "Bayesian network structure learning. Such models can benefit from the\n",
      "advancement of numerical optimization techniques which have already played a\n",
      "distinctive role in several machine learning settings. The strengths and the\n",
      "shortcomings of these models are discussed and potential research directions\n",
      "and open problems are highlighted.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.11622 \n",
      "Title :A discriminative condition-aware backend for speaker verification\n",
      "  We present a scoring approach for speaker verification that mimics the\n",
      "standard PLDA-based backend process used in most current speaker verification\n",
      "systems. However, unlike the standard backends, all parameters of the model are\n",
      "jointly trained to optimize the binary cross-entropy for the speaker\n",
      "verification task. We further integrate the calibration stage inside the model,\n",
      "making the parameters of this stage depend on metadata vectors that represent\n",
      "the conditions of the signals. We show that the proposed backend has excellent\n",
      "out-of-the-box calibration performance on most of our test sets, making it an\n",
      "ideal approach for cases in which the test conditions are not known and\n",
      "development data is not available for training a domain-specific calibration\n",
      "model.\n",
      "\n",
      "**Paper Id :2007.03511 \n",
      "Title :Estimating Generalization under Distribution Shifts via Domain-Invariant\n",
      "  Representations\n",
      "  When machine learning models are deployed on a test distribution different\n",
      "from the training distribution, they can perform poorly, but overestimate their\n",
      "performance. In this work, we aim to better estimate a model's performance\n",
      "under distribution shift, without supervision. To do so, we use a set of\n",
      "domain-invariant predictors as a proxy for the unknown, true target labels.\n",
      "Since the error of the resulting risk estimate depends on the target risk of\n",
      "the proxy model, we study generalization of domain-invariant representations\n",
      "and show that the complexity of the latent representation has a significant\n",
      "influence on the target risk. Empirically, our approach (1) enables self-tuning\n",
      "of domain adaptation models, and (2) accurately estimates the target error of\n",
      "given models under distribution shift. Other applications include model\n",
      "selection, deciding early stopping and error detection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.11680 \n",
      "Title :FAN: Feature Adaptation Network for Surveillance Face Recognition and\n",
      "  Normalization\n",
      "  This paper studies face recognition (FR) and normalization in surveillance\n",
      "imagery. Surveillance FR is a challenging problem that has great values in law\n",
      "enforcement. Despite recent progress in conventional FR, less effort has been\n",
      "devoted to surveillance FR. To bridge this gap, we propose a Feature Adaptation\n",
      "Network (FAN) to jointly perform surveillance FR and normalization. Our face\n",
      "normalization mainly acts on the aspect of image resolution, closely related to\n",
      "face super-resolution. However, previous face super-resolution methods require\n",
      "paired training data with pixel-to-pixel correspondence, which is typically\n",
      "unavailable between real-world low-resolution and high-resolution faces. FAN\n",
      "can leverage both paired and unpaired data as we disentangle the features into\n",
      "identity and non-identity components and adapt the distribution of the identity\n",
      "features, which breaks the limit of current face super-resolution methods. We\n",
      "further propose a random scale augmentation scheme to learn resolution robust\n",
      "identity features, with advantages over previous fixed scale augmentation.\n",
      "Extensive experiments on LFW, WIDER FACE, QUML-SurvFace and SCface datasets\n",
      "have shown the effectiveness of our method on surveillance FR and\n",
      "normalization.\n",
      "\n",
      "**Paper Id :2004.12700 \n",
      "Title :In-Vehicle Object Detection in the Wild for Driverless Vehicles\n",
      "  In-vehicle human object identification plays an important role in\n",
      "vision-based automated vehicle driving systems while objects such as\n",
      "pedestrians and vehicles on roads or streets are the primary targets to protect\n",
      "from driverless vehicles. A challenge is the difficulty to detect objects in\n",
      "moving under the wild conditions, while illumination and image quality could\n",
      "drastically vary. In this work, to address this challenge, we exploit Deep\n",
      "Convolutional Generative Adversarial Networks (DCGANs) with Single Shot\n",
      "Detector (SSD) to handle with the wild conditions. In our work, a GAN was\n",
      "trained with low-quality images to handle with the challenges arising from the\n",
      "wild conditions in smart cities, while a cascaded SSD is employed as the object\n",
      "detector to perform with the GAN. We used tested our approach under wild\n",
      "conditions using taxi driver videos on London street in both daylight and night\n",
      "times, and the tests from in-vehicle videos demonstrate that this strategy can\n",
      "drastically achieve a better detection rate under the wild conditions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.11758 \n",
      "Title :MixNMatch: Multifactor Disentanglement and Encoding for Conditional\n",
      "  Image Generation\n",
      "  We present MixNMatch, a conditional generative model that learns to\n",
      "disentangle and encode background, object pose, shape, and texture from real\n",
      "images with minimal supervision, for mix-and-match image generation. We build\n",
      "upon FineGAN, an unconditional generative model, to learn the desired\n",
      "disentanglement and image generator, and leverage adversarial joint image-code\n",
      "distribution matching to learn the latent factor encoders. MixNMatch requires\n",
      "bounding boxes during training to model background, but requires no other\n",
      "supervision. Through extensive experiments, we demonstrate MixNMatch's ability\n",
      "to accurately disentangle, encode, and combine multiple factors for\n",
      "mix-and-match image generation, including sketch2color, cartoon2img, and\n",
      "img2gif applications. Our code/models/demo can be found at\n",
      "https://github.com/Yuheng-Li/MixNMatch\n",
      "\n",
      "**Paper Id :2011.00359 \n",
      "Title :TartanVO: A Generalizable Learning-based VO\n",
      "  We present the first learning-based visual odometry (VO) model, which\n",
      "generalizes to multiple datasets and real-world scenarios and outperforms\n",
      "geometry-based methods in challenging scenes. We achieve this by leveraging the\n",
      "SLAM dataset TartanAir, which provides a large amount of diverse synthetic data\n",
      "in challenging environments. Furthermore, to make our VO model generalize\n",
      "across datasets, we propose an up-to-scale loss function and incorporate the\n",
      "camera intrinsic parameters into the model. Experiments show that a single\n",
      "model, TartanVO, trained only on synthetic data, without any finetuning, can be\n",
      "generalized to real-world datasets such as KITTI and EuRoC, demonstrating\n",
      "significant advantages over the geometry-based methods on challenging\n",
      "trajectories. Our code is available at https://github.com/castacks/tartanvo.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.11908 \n",
      "Title :Lifelong Spectral Clustering\n",
      "  In the past decades, spectral clustering (SC) has become one of the most\n",
      "effective clustering algorithms. However, most previous studies focus on\n",
      "spectral clustering tasks with a fixed task set, which cannot incorporate with\n",
      "a new spectral clustering task without accessing to previously learned tasks.\n",
      "In this paper, we aim to explore the problem of spectral clustering in a\n",
      "lifelong machine learning framework, i.e., Lifelong Spectral Clustering (L2SC).\n",
      "Its goal is to efficiently learn a model for a new spectral clustering task by\n",
      "selectively transferring previously accumulated experience from knowledge\n",
      "library. Specifically, the knowledge library of L2SC contains two components:\n",
      "1) orthogonal basis library: capturing latent cluster centers among the\n",
      "clusters in each pair of tasks; 2) feature embedding library: embedding the\n",
      "feature manifold information shared among multiple related tasks. As a new\n",
      "spectral clustering task arrives, L2SC firstly transfers knowledge from both\n",
      "basis library and feature library to obtain encoding matrix, and further\n",
      "redefines the library base over time to maximize performance across all the\n",
      "clustering tasks. Meanwhile, a general online update formulation is derived to\n",
      "alternatively update the basis library and feature library. Finally, the\n",
      "empirical experiments on several real-world benchmark datasets demonstrate that\n",
      "our L2SC model can effectively improve the clustering performance when\n",
      "comparing with other state-of-the-art spectral clustering algorithms.\n",
      "\n",
      "**Paper Id :2006.06922 \n",
      "Title :Incorporating User Micro-behaviors and Item Knowledge into Multi-task\n",
      "  Learning for Session-based Recommendation\n",
      "  Session-based recommendation (SR) has become an important and popular\n",
      "component of various e-commerce platforms, which aims to predict the next\n",
      "interacted item based on a given session. Most of existing SR models only focus\n",
      "on exploiting the consecutive items in a session interacted by a certain user,\n",
      "to capture the transition pattern among the items. Although some of them have\n",
      "been proven effective, the following two insights are often neglected. First, a\n",
      "user's micro-behaviors, such as the manner in which the user locates an item,\n",
      "the activities that the user commits on an item (e.g., reading comments, adding\n",
      "to cart), offer fine-grained and deep understanding of the user's preference.\n",
      "Second, the item attributes, also known as item knowledge, provide side\n",
      "information to model the transition pattern among interacted items and\n",
      "alleviate the data sparsity problem. These insights motivate us to propose a\n",
      "novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors\n",
      "and item Knowledge into Multi-task learning for Session-based Recommendation.\n",
      "Specifically, a given session is modeled on micro-behavior level in MKM-SR,\n",
      "i.e., with a sequence of item-operation pairs rather than a sequence of items,\n",
      "to capture the transition pattern in the session sufficiently. Furthermore, we\n",
      "propose a multi-task learning paradigm to involve learning knowledge embeddings\n",
      "which plays a role as an auxiliary task to promote the major task of SR. It\n",
      "enables our model to obtain better session representations, resulting in more\n",
      "precise SR recommendation results. The extensive evaluations on two benchmark\n",
      "datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models,\n",
      "justifying the strategy of incorporating knowledge learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.12252 \n",
      "Title :Neural Network Based in Silico Simulation of Combustion Reactions\n",
      "  Understanding and prediction of the chemical reactions are fundamental\n",
      "demanding in the study of many complex chemical systems. Reactive molecular\n",
      "dynamics (MD) simulation has been widely used for this purpose as it can offer\n",
      "atomic details and can help us better interpret chemical reaction mechanisms.\n",
      "In this study, two reference datasets were constructed and corresponding neural\n",
      "network (NN) potentials were trained based on them. For given large-scale\n",
      "reaction systems, the NN potentials can predict the potential energy and atomic\n",
      "forces of DFT precision, while it is orders of magnitude faster than the\n",
      "conventional DFT calculation. With these two models, reactive MD simulations\n",
      "were performed to explore the combustion mechanisms of hydrogen and methane.\n",
      "Benefit from the high efficiency of the NN model, nanosecond MD trajectories\n",
      "for large-scale systems containing hundreds of atoms were produced and detailed\n",
      "combustion mechanism was obtained. Through further development, the algorithms\n",
      "in this study can be used to explore and discovery reaction mechanisms of many\n",
      "complex reaction systems, such as combustion, synthesis, and heterogeneous\n",
      "catalysis without any predefined reaction coordinates and elementary reaction\n",
      "steps.\n",
      "\n",
      "**Paper Id :2003.13425 \n",
      "Title :Predicting Elastic Properties of Materials from Electronic Charge\n",
      "  Density Using 3D Deep Convolutional Neural Networks\n",
      "  Materials representation plays a key role in machine learning based\n",
      "prediction of materials properties and new materials discovery. Currently both\n",
      "graph and 3D voxel representation methods are based on the heterogeneous\n",
      "elements of the crystal structures. Here, we propose to use electronic charge\n",
      "density (ECD) as a generic unified 3D descriptor for materials property\n",
      "prediction with the advantage of possessing close relation with the physical\n",
      "and chemical properties of materials. We developed an ECD based 3D\n",
      "convolutional neural networks (CNNs) for predicting elastic properties of\n",
      "materials, in which CNNs can learn effective hierarchical features with\n",
      "multiple convolving and pooling operations. Extensive benchmark experiments\n",
      "over 2,170 Fm-3m face-centered-cubic (FCC) materials show that our ECD based\n",
      "CNNs can achieve good performance for elasticity prediction. Especially, our\n",
      "CNN models based on the fusion of elemental Magpie features and ECD descriptors\n",
      "achieved the best 5-fold cross-validation performance. More importantly, we\n",
      "showed that our ECD based CNN models can achieve significantly better\n",
      "extrapolation performance when evaluated over non-redundant datasets where\n",
      "there are few neighbor training samples around test samples. As additional\n",
      "validation, we evaluated the predictive performance of our models on 329\n",
      "materials of space group Fm-3m by comparing to DFT calculated values, which\n",
      "shows better prediction power of our model for bulk modulus than shear modulus.\n",
      "Due to the unified representation power of ECD, it is expected that our ECD\n",
      "based CNN approach can also be applied to predict other physical and chemical\n",
      "properties of crystalline materials.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.12562 \n",
      "Title :Towards Security Threats of Deep Learning Systems: A Survey\n",
      "  Deep learning has gained tremendous success and great popularity in the past\n",
      "few years. However, deep learning systems are suffering several inherent\n",
      "weaknesses, which can threaten the security of learning models. Deep learning's\n",
      "wide use further magnifies the impact and consequences. To this end, lots of\n",
      "research has been conducted with the purpose of exhaustively identifying\n",
      "intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few\n",
      "are clear about how these weaknesses are incurred and how effective these\n",
      "attack approaches are in assaulting deep learning. In order to unveil the\n",
      "security weaknesses and aid in the development of a robust deep learning\n",
      "system, we undertake an investigation on attacks towards deep learning, and\n",
      "analyze these attacks to conclude some findings in multiple views. In\n",
      "particular, we focus on four types of attacks associated with security threats\n",
      "of deep learning: model extraction attack, model inversion attack, poisoning\n",
      "attack and adversarial attack. For each type of attack, we construct its\n",
      "essential workflow as well as adversary capabilities and attack goals. Pivot\n",
      "metrics are devised for comparing the attack approaches, by which we perform\n",
      "quantitative and qualitative analyses. From the analysis, we have identified\n",
      "significant and indispensable factors in an attack vector, e.g., how to reduce\n",
      "queries to target models, what distance should be used for measuring\n",
      "perturbation. We shed light on 18 findings covering these approaches' merits\n",
      "and demerits, success probability, deployment complexity and prospects.\n",
      "Moreover, we discuss other potential security weaknesses and possible\n",
      "mitigation which can inspire relevant research in this area.\n",
      "\n",
      "**Paper Id :2007.14622 \n",
      "Title :Approaches to Fraud Detection on Credit Card Transactions Using\n",
      "  Artificial Intelligence Methods\n",
      "  Credit card fraud is an ongoing problem for almost all industries in the\n",
      "world, and it raises millions of dollars to the global economy each year.\n",
      "Therefore, there is a number of research either completed or proceeding in\n",
      "order to detect these kinds of frauds in the industry. These researches\n",
      "generally use rule-based or novel artificial intelligence approaches to find\n",
      "eligible solutions. The ultimate goal of this paper is to summarize\n",
      "state-of-the-art approaches to fraud detection using artificial intelligence\n",
      "and machine learning techniques. While summarizing, we will categorize the\n",
      "common problems such as imbalanced dataset, real time working scenarios, and\n",
      "feature engineering challenges that almost all research works encounter, and\n",
      "identify general approaches to solve them. The imbalanced dataset problem\n",
      "occurs because the number of legitimate transactions is much higher than the\n",
      "fraudulent ones whereas applying the right feature engineering is substantial\n",
      "as the features obtained from the industries are limited, and applying feature\n",
      "engineering methods and reforming the dataset is crucial. Also, adapting the\n",
      "detection system to real time scenarios is a challenge since the number of\n",
      "credit card transactions in a limited time period is very high. In addition, we\n",
      "will discuss how evaluation metrics and machine learning methods differentiate\n",
      "among each research.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1911.12672 \n",
      "Title :Improved cross-validation for classifiers that make algorithmic choices\n",
      "  to minimise runtime without compromising output correctness\n",
      "  Our topic is the use of machine learning to improve software by making\n",
      "choices which do not compromise the correctness of the output, but do affect\n",
      "the time taken to produce such output. We are particularly concerned with\n",
      "computer algebra systems (CASs), and in particular, our experiments are for\n",
      "selecting the variable ordering to use when performing a cylindrical algebraic\n",
      "decomposition of $n$-dimensional real space with respect to the signs of a set\n",
      "of polynomials.\n",
      "  In our prior work we explored the different ML models that could be used, and\n",
      "how to identify suitable features of the input polynomials. In the present\n",
      "paper we both repeat our prior experiments on problems which have more\n",
      "variables (and thus exponentially more possible orderings), and examine the\n",
      "metric which our ML classifiers targets. The natural metric is computational\n",
      "runtime, with classifiers trained to pick the ordering which minimises this.\n",
      "However, this leads to the situation were models do not distinguish between any\n",
      "of the non-optimal orderings, whose runtimes may still vary dramatically. In\n",
      "this paper we investigate a modification to the cross-validation algorithms of\n",
      "the classifiers so that they do distinguish these cases, leading to improved\n",
      "results.\n",
      "\n",
      "**Paper Id :2005.11251 \n",
      "Title :A machine learning based software pipeline to pick the variable ordering\n",
      "  for algorithms with polynomial inputs\n",
      "  We are interested in the application of Machine Learning (ML) technology to\n",
      "improve mathematical software. It may seem that the probabilistic nature of ML\n",
      "tools would invalidate the exact results prized by such software, however, the\n",
      "algorithms which underpin the software often come with a range of choices which\n",
      "are good candidates for ML application. We refer to choices which have no\n",
      "effect on the mathematical correctness of the software, but do impact its\n",
      "performance.\n",
      "  In the past we experimented with one such choice: the variable ordering to\n",
      "use when building a Cylindrical Algebraic Decomposition (CAD). We used the\n",
      "Python library Scikit-Learn (sklearn) to experiment with different ML models,\n",
      "and developed new techniques for feature generation and hyper-parameter\n",
      "selection.\n",
      "  These techniques could easily be adapted for making decisions other than our\n",
      "immediate application of CAD variable ordering. Hence in this paper we present\n",
      "a software pipeline to use sklearn to pick the variable ordering for an\n",
      "algorithm that acts on a polynomial system. The code described is freely\n",
      "available online.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.00283 \n",
      "Title :Interpreting Deep Learning Features for Myoelectric Control: A\n",
      "  Comparison with Handcrafted Features\n",
      "  The research in myoelectric control systems primarily focuses on extracting\n",
      "discriminative representations from the electromyographic (EMG) signal by\n",
      "designing handcrafted features. Recently, deep learning techniques have been\n",
      "applied to the challenging task of EMG-based gesture recognition. The adoption\n",
      "of these techniques slowly shifts the focus from feature engineering to feature\n",
      "learning. However, the black-box nature of deep learning makes it hard to\n",
      "understand the type of information learned by the network and how it relates to\n",
      "handcrafted features. Additionally, due to the high variability in EMG\n",
      "recordings between participants, deep features tend to generalize poorly across\n",
      "subjects using standard training methods. Consequently, this work introduces a\n",
      "new multi-domain learning algorithm, named ADANN, which significantly enhances\n",
      "(p=0.00004) inter-subject classification accuracy by an average of 19.40%\n",
      "compared to standard training. Using ADANN-generated features, the main\n",
      "contribution of this work is to provide the first topological data analysis of\n",
      "EMG-based gesture recognition for the characterisation of the information\n",
      "encoded within a deep network, using handcrafted features as landmarks. This\n",
      "analysis reveals that handcrafted features and the learned features (in the\n",
      "earlier layers) both try to discriminate between all gestures, but do not\n",
      "encode the same information to do so. Furthermore, using convolutional network\n",
      "visualization techniques reveal that learned features tend to ignore the most\n",
      "activated channel during gesture contraction, which is in stark contrast with\n",
      "the prevalence of handcrafted features designed to capture amplitude\n",
      "information. Overall, this work paves the way for hybrid feature sets by\n",
      "providing a clear guideline of complementary information encoded within learned\n",
      "and handcrafted features.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.01137 \n",
      "Title :Mixing autoencoder with classifier: conceptual data visualization\n",
      "  In this short paper, a neural network that is able to form a low dimensional\n",
      "topological hidden representation is explained. The neural network can be\n",
      "trained as an autoencoder, a classifier or mix of both, and produces different\n",
      "low dimensional topological map for each of them. When it is trained as an\n",
      "autoencoder, the inherent topological structure of the data can be visualized,\n",
      "while when it is trained as a classifier, the topological structure is further\n",
      "constrained by the concept, for example the labels the data, hence the\n",
      "visualization is not only structural but also conceptual. The proposed neural\n",
      "network significantly differ from many dimensional reduction models, primarily\n",
      "in its ability to execute both supervised and unsupervised dimensional\n",
      "reduction. The neural network allows multi perspective visualization of the\n",
      "data, and thus giving more flexibility in data analysis. This paper is\n",
      "supported by preliminary but intuitive visualization experiments.\n",
      "\n",
      "**Paper Id :2007.00936 \n",
      "Title :Deep Neural Networks for Nonlinear Model Order Reduction of Unsteady\n",
      "  Flows\n",
      "  Unsteady fluid systems are nonlinear high-dimensional dynamical systems that\n",
      "may exhibit multiple complex phenomena both in time and space. Reduced Order\n",
      "Modeling (ROM) of fluid flows has been an active research topic in the recent\n",
      "decade with the primary goal to decompose complex flows to a set of features\n",
      "most important for future state prediction and control, typically using a\n",
      "dimensionality reduction technique. In this work, a novel data-driven technique\n",
      "based on the power of deep neural networks for reduced order modeling of the\n",
      "unsteady fluid flows is introduced. An autoencoder network is used for\n",
      "nonlinear dimension reduction and feature extraction as an alternative for\n",
      "singular value decomposition (SVD). Then, the extracted features are used as an\n",
      "input for long short-term memory network (LSTM) to predict the velocity field\n",
      "at future time instances. The proposed autoencoder-LSTM method is compared with\n",
      "non-intrusive reduced order models based on dynamic mode decomposition (DMD)\n",
      "and proper orthogonal decomposition (POD). Moreover, an autoencoder-DMD\n",
      "algorithm is introduced for reduced order modeling, which uses the autoencoder\n",
      "network for dimensionality reduction rather than SVD rank truncation. Results\n",
      "show that the autoencoder-LSTM method is considerably capable of predicting\n",
      "fluid flow evolution, where higher values for coefficient of determination\n",
      "$R^{2}$ are obtained using autoencoder-LSTM compared to other models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.01166 \n",
      "Title :Different Set Domain Adaptation for Brain-Computer Interfaces: A Label\n",
      "  Alignment Approach\n",
      "  A brain-computer interface (BCI) system usually needs a long calibration\n",
      "session for each new subject/task to adjust its parameters, which impedes its\n",
      "transition from the laboratory to real-world applications. Domain adaptation,\n",
      "which leverages labeled data from auxiliary subjects/tasks (source domains),\n",
      "has demonstrated its effectiveness in reducing such calibration effort.\n",
      "Currently, most domain adaptation approaches require the source domains to have\n",
      "the same feature space and label space as the target domain, which limits their\n",
      "applications, as the auxiliary data may have different feature spaces and/or\n",
      "different label spaces. This paper considers different set domain adaptation\n",
      "for BCIs, i.e., the source and target domains have different label spaces. We\n",
      "introduce a practical setting of different label sets for BCIs, and propose a\n",
      "novel label alignment (LA) approach to align the source label space with the\n",
      "target label space. It has three desirable properties: 1) LA only needs as few\n",
      "as one labeled sample from each class of the target subject; 2) LA can be used\n",
      "as a preprocessing step before different feature extraction and classification\n",
      "algorithms; and, 3) LA can be integrated with other domain adaptation\n",
      "approaches to achieve even better performance. Experiments on two motor imagery\n",
      "datasets demonstrated the effectiveness of LA.\n",
      "\n",
      "**Paper Id :1910.05878 \n",
      "Title :Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces\n",
      "  Transfer learning makes use of data or knowledge in one problem to help solve\n",
      "a different, yet related, problem. It is particularly useful in brain-computer\n",
      "interfaces (BCIs), for coping with variations among different subjects and/or\n",
      "tasks. This paper considers offline unsupervised cross-subject\n",
      "electroencephalogram (EEG) classification, i.e., we have labeled EEG trials\n",
      "from one or more source subjects, but only unlabeled EEG trials from the target\n",
      "subject. We propose a novel manifold embedded knowledge transfer (MEKT)\n",
      "approach, which first aligns the covariance matrices of the EEG trials in the\n",
      "Riemannian manifold, extracts features in the tangent space, and then performs\n",
      "domain adaptation by minimizing the joint probability distribution shift\n",
      "between the source and the target domains, while preserving their geometric\n",
      "structures. MEKT can cope with one or multiple source domains, and can be\n",
      "computed efficiently. We also propose a domain transferability estimation (DTE)\n",
      "approach to identify the most beneficial source domains, in case there are a\n",
      "large number of source domains. Experiments on four EEG datasets from two\n",
      "different BCI paradigms demonstrated that MEKT outperformed several\n",
      "state-of-the-art transfer learning approaches, and DTE can reduce more than\n",
      "half of the computational cost when the number of source subjects is large,\n",
      "with little sacrifice of classification accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.01834 \n",
      "Title :PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting\n",
      "  The latest methods based on deep learning have achieved amazing results\n",
      "regarding the complex work of inpainting large missing areas in an image. But\n",
      "this type of method generally attempts to generate one single \"optimal\" result,\n",
      "ignoring many other plausible results. Considering the uncertainty of the\n",
      "inpainting task, one sole result can hardly be regarded as a desired\n",
      "regeneration of the missing area. In view of this weakness, which is related to\n",
      "the design of the previous algorithms, we propose a novel deep generative model\n",
      "equipped with a brand new style extractor which can extract the style feature\n",
      "(latent vector) from the ground truth. Once obtained, the extracted style\n",
      "feature and the ground truth are both input into the generator. We also craft a\n",
      "consistency loss that guides the generated image to approximate the ground\n",
      "truth. After iterations, our generator is able to learn the mapping of styles\n",
      "corresponding to multiple sets of vectors. The proposed model can generate a\n",
      "large number of results consistent with the context semantics of the image.\n",
      "Moreover, we evaluated the effectiveness of our model on three datasets, i.e.,\n",
      "CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting\n",
      "methods, this model is able to offer desirable inpainting results with both\n",
      "better quality and higher diversity. The code and model will be made available\n",
      "on https://github.com/vivitsai/PiiGAN.\n",
      "\n",
      "**Paper Id :2002.05878 \n",
      "Title :An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset\n",
      "  The Waymo Open Dataset has been released recently, providing a platform to\n",
      "crowdsource some fundamental challenges for automated vehicles (AVs), such as\n",
      "3D detection and tracking. While~the dataset provides a large amount of\n",
      "high-quality and multi-source driving information, people in academia are more\n",
      "interested in the underlying driving policy programmed in Waymo self-driving\n",
      "cars, which is inaccessible due to AV manufacturers' proprietary protection.\n",
      "Accordingly, academic researchers have to make various assumptions to implement\n",
      "AV components in their models or simulations, which may not represent the\n",
      "realistic interactions in real-world traffic. Thus, this paper introduces an\n",
      "approach to learn a long short-term memory (LSTM)-based model for imitating the\n",
      "behavior of Waymo's self-driving model. The proposed model has been evaluated\n",
      "based on Mean Absolute Error (MAE). The experimental results show that our\n",
      "model outperforms several baseline models in driving action prediction. In\n",
      "addition, a visualization tool is presented for verifying the performance of\n",
      "the model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.01899 \n",
      "Title :Distribution-induced Bidirectional Generative Adversarial Network for\n",
      "  Graph Representation Learning\n",
      "  Graph representation learning aims to encode all nodes of a graph into\n",
      "low-dimensional vectors that will serve as input of many compute vision tasks.\n",
      "However, most existing algorithms ignore the existence of inherent data\n",
      "distribution and even noises. This may significantly increase the phenomenon of\n",
      "over-fitting and deteriorate the testing accuracy. In this paper, we propose a\n",
      "Distribution-induced Bidirectional Generative Adversarial Network (named DBGAN)\n",
      "for graph representation learning. Instead of the widely used normal\n",
      "distribution assumption, the prior distribution of latent representation in our\n",
      "DBGAN is estimated in a structure-aware way, which implicitly bridges the graph\n",
      "and feature spaces by prototype learning. Thus discriminative and robust\n",
      "representations are generated for all nodes. Furthermore, to improve their\n",
      "generalization ability while preserving representation ability, the\n",
      "sample-level and distribution-level consistency is well balanced via a\n",
      "bidirectional adversarial learning framework. An extensive group of experiments\n",
      "are then carefully designed and presented, demonstrating that our DBGAN obtains\n",
      "remarkably more favorable trade-off between representation and robustness, and\n",
      "meanwhile is dimension-efficient, over currently available alternatives in\n",
      "various tasks.\n",
      "\n",
      "**Paper Id :1911.10143 \n",
      "Title :Adversarial Learning of Privacy-Preserving and Task-Oriented\n",
      "  Representations\n",
      "  Data privacy has emerged as an important issue as data-driven deep learning\n",
      "has been an essential component of modern machine learning systems. For\n",
      "instance, there could be a potential privacy risk of machine learning systems\n",
      "via the model inversion attack, whose goal is to reconstruct the input data\n",
      "from the latent representation of deep networks. Our work aims at learning a\n",
      "privacy-preserving and task-oriented representation to defend against such\n",
      "model inversion attacks. Specifically, we propose an adversarial reconstruction\n",
      "learning framework that prevents the latent representations decoded into\n",
      "original input data. By simulating the expected behavior of adversary, our\n",
      "framework is realized by minimizing the negative pixel reconstruction loss or\n",
      "the negative feature reconstruction (i.e., perceptual distance) loss. We\n",
      "validate the proposed method on face attribute prediction, showing that our\n",
      "method allows protecting visual privacy with a small decrease in utility\n",
      "performance. In addition, we show the utility-privacy trade-off with different\n",
      "choices of hyperparameter for negative perceptual distance loss at training,\n",
      "allowing service providers to determine the right level of privacy-protection\n",
      "with a certain utility performance. Moreover, we provide an extensive study\n",
      "with different selections of features, tasks, and the data to further analyze\n",
      "their influence on privacy protection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.02008 \n",
      "Title :Exact asymptotics for phase retrieval and compressed sensing with random\n",
      "  generative priors\n",
      "  We consider the problem of compressed sensing and of (real-valued) phase\n",
      "retrieval with random measurement matrix. We derive sharp asymptotics for the\n",
      "information-theoretically optimal performance and for the best known polynomial\n",
      "algorithm for an ensemble of generative priors consisting of fully connected\n",
      "deep neural networks with random weight matrices and arbitrary activations. We\n",
      "compare the performance to sparse separable priors and conclude that generative\n",
      "priors might be advantageous in terms of algorithmic performance. In\n",
      "particular, while sparsity does not allow to perform compressive phase\n",
      "retrieval efficiently close to its information-theoretic limit, it is found\n",
      "that under the random generative prior compressed phase retrieval becomes\n",
      "tractable.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.02143 \n",
      "Title :Landscape Complexity for the Empirical Risk of Generalized Linear Models\n",
      "  We present a method to obtain the average and the typical value of the number\n",
      "of critical points of the empirical risk landscape for generalized linear\n",
      "estimation problems and variants. This represents a substantial extension of\n",
      "previous applications of the Kac-Rice method since it allows to analyze the\n",
      "critical points of high dimensional non-Gaussian random functions. We obtain a\n",
      "rigorous explicit variational formula for the annealed complexity, which is the\n",
      "logarithm of the average number of critical points at fixed value of the\n",
      "empirical risk. This result is simplified, and extended, using the non-rigorous\n",
      "Kac-Rice replicated method from theoretical physics. In this way we find an\n",
      "explicit variational formula for the quenched complexity, which is generally\n",
      "different from its annealed counterpart, and allows to obtain the number of\n",
      "critical points for typical instances up to exponential accuracy.\n",
      "\n",
      "**Paper Id :1907.11985 \n",
      "Title :The Wang-Landau Algorithm as Stochastic Optimization and Its\n",
      "  Acceleration\n",
      "  We show that the Wang-Landau algorithm can be formulated as a stochastic\n",
      "gradient descent algorithm minimizing a smooth and convex objective function,\n",
      "of which the gradient is estimated using Markov chain Monte Carlo iterations.\n",
      "The optimization formulation provides us a new way to establish the convergence\n",
      "rate of the Wang-Landau algorithm, by exploiting the fact that almost surely,\n",
      "the density estimates (on the logarithmic scale) remain in a compact set, upon\n",
      "which the objective function is strongly convex. The optimization viewpoint\n",
      "motivates us to improve the efficiency of the Wang-Landau algorithm using\n",
      "popular tools including the momentum method and the adaptive learning rate\n",
      "method. We demonstrate the accelerated Wang-Landau algorithm on a\n",
      "two-dimensional Ising model and a two-dimensional ten-state Potts model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.02182 \n",
      "Title :Towards better social crisis data with HERMES: Hybrid sensing for\n",
      "  EmeRgency ManagEment System\n",
      "  People involved in mass emergencies increasingly publish information-rich\n",
      "contents in online social networks (OSNs), thus acting as a distributed and\n",
      "resilient network of human sensors. In this work, we present HERMES, a system\n",
      "designed to enrich the information spontaneously disclosed by OSN users in the\n",
      "aftermath of disasters. HERMES leverages a mixed data collection strategy,\n",
      "called hybrid crowdsensing, and state-of-the-art AI techniques. Evaluated in\n",
      "real-world emergencies, HERMES proved to increase: (i) the amount of the\n",
      "available damage information; (ii) the density (up to 7x) and the variety (up\n",
      "to 18x) of the retrieved geographic information; (iii) the geographic coverage\n",
      "(up to 30%) and granularity.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.02605 \n",
      "Title :Towards Understanding Residual and Dilated Dense Neural Networks via\n",
      "  Convolutional Sparse Coding\n",
      "  Convolutional neural network (CNN) and its variants have led to many\n",
      "state-of-art results in various fields. However, a clear theoretical\n",
      "understanding about them is still lacking. Recently, multi-layer convolutional\n",
      "sparse coding (ML-CSC) has been proposed and proved to equal such simply\n",
      "stacked networks (plain networks). Here, we think three factors in each layer\n",
      "of it including the initialization, the dictionary design and the number of\n",
      "iterations greatly affect the performance of ML-CSC. Inspired by these\n",
      "considerations, we propose two novel multi-layer models--residual convolutional\n",
      "sparse coding model (Res-CSC) and mixed-scale dense convolutional sparse coding\n",
      "model (MSD-CSC), which have close relationship with the residual neural network\n",
      "(ResNet) and mixed-scale (dilated) dense neural network (MSDNet), respectively.\n",
      "Mathematically, we derive the shortcut connection in ResNet as a special case\n",
      "of a new forward propagation rule on ML-CSC. We find a theoretical\n",
      "interpretation of the dilated convolution and dense connection in MSDNet by\n",
      "analyzing MSD-CSC, which gives a clear mathematical understanding about them.\n",
      "We implement the iterative soft thresholding algorithm (ISTA) and its fast\n",
      "version to solve Res-CSC and MSD-CSC, which can employ the unfolding operation\n",
      "for further improvements. At last, extensive numerical experiments and\n",
      "comparison with competing methods demonstrate their effectiveness using three\n",
      "typical datasets.\n",
      "\n",
      "**Paper Id :1904.04238 \n",
      "Title :Learning Backtrackless Aligned-Spatial Graph Convolutional Networks for\n",
      "  Graph Classification\n",
      "  In this paper, we develop a novel Backtrackless Aligned-Spatial Graph\n",
      "Convolutional Network (BASGCN) model to learn effective features for graph\n",
      "classification. Our idea is to transform arbitrary-sized graphs into\n",
      "fixed-sized backtrackless aligned grid structures and define a new spatial\n",
      "graph convolution operation associated with the grid structures. We show that\n",
      "the proposed BASGCN model not only reduces the problems of information loss and\n",
      "imprecise information representation arising in existing spatially-based Graph\n",
      "Convolutional Network (GCN) models, but also bridges the theoretical gap\n",
      "between traditional Convolutional Neural Network (CNN) models and\n",
      "spatially-based GCN models. Furthermore, the proposed BASGCN model can both\n",
      "adaptively discriminate the importance between specified vertices during the\n",
      "convolution process and reduce the notorious tottering problem of existing\n",
      "spatially-based GCNs related to the Weisfeiler-Lehman algorithm, explaining the\n",
      "effectiveness of the proposed model. Experiments on standard graph datasets\n",
      "demonstrate the effectiveness of the proposed model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.02729 \n",
      "Title :Rademacher complexity and spin glasses: A link between the replica and\n",
      "  statistical theories of learning\n",
      "  Statistical learning theory provides bounds of the generalization gap, using\n",
      "in particular the Vapnik-Chervonenkis dimension and the Rademacher complexity.\n",
      "An alternative approach, mainly studied in the statistical physics literature,\n",
      "is the study of generalization in simple synthetic-data models. Here we discuss\n",
      "the connections between these approaches and focus on the link between the\n",
      "Rademacher complexity in statistical learning and the theories of\n",
      "generalization for typical-case synthetic models from statistical physics,\n",
      "involving quantities known as Gardner capacity and ground state energy. We show\n",
      "that in these models the Rademacher complexity is closely related to the ground\n",
      "state energy computed by replica theories. Using this connection, one may\n",
      "reinterpret many results of the literature as rigorous Rademacher bounds in a\n",
      "variety of models in the high-dimensional statistics limit. Somewhat\n",
      "surprisingly, we also show that statistical learning theory provides\n",
      "predictions for the behavior of the ground-state energies in some full replica\n",
      "symmetry breaking models.\n",
      "\n",
      "**Paper Id :2003.01695 \n",
      "Title :Robust data encodings for quantum classifiers\n",
      "  Data representation is crucial for the success of machine learning models. In\n",
      "the context of quantum machine learning with near-term quantum computers,\n",
      "equally important considerations of how to efficiently input (encode) data and\n",
      "effectively deal with noise arise. In this work, we study data encodings for\n",
      "binary quantum classification and investigate their properties both with and\n",
      "without noise. For the common classifier we consider, we show that encodings\n",
      "determine the classes of learnable decision boundaries as well as the set of\n",
      "points which retain the same classification in the presence of noise. After\n",
      "defining the notion of a robust data encoding, we prove several results on\n",
      "robustness for different channels, discuss the existence of robust encodings,\n",
      "and prove an upper bound on the number of robust points in terms of fidelities\n",
      "between noisy and noiseless states. Numerical results for several example\n",
      "implementations are provided to reinforce our findings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.02984 \n",
      "Title :Grid-GCN for Fast and Scalable Point Cloud Learning\n",
      "  Due to the sparsity and irregularity of the point cloud data, methods that\n",
      "directly consume points have become popular. Among all point-based models,\n",
      "graph convolutional networks (GCN) lead to notable performance by fully\n",
      "preserving the data granularity and exploiting point interrelation. However,\n",
      "point-based networks spend a significant amount of time on data structuring\n",
      "(e.g., Farthest Point Sampling (FPS) and neighbor points querying), which limit\n",
      "the speed and scalability. In this paper, we present a method, named Grid-GCN,\n",
      "for fast and scalable point cloud learning. Grid-GCN uses a novel data\n",
      "structuring strategy, Coverage-Aware Grid Query (CAGQ). By leveraging the\n",
      "efficiency of grid space, CAGQ improves spatial coverage while reducing the\n",
      "theoretical time complexity. Compared with popular sampling methods such as\n",
      "Farthest Point Sampling (FPS) and Ball Query, CAGQ achieves up to 50X speed-up.\n",
      "With a Grid Context Aggregation (GCA) module, Grid-GCN achieves\n",
      "state-of-the-art performance on major point cloud classification and\n",
      "segmentation benchmarks with significantly faster runtime than previous\n",
      "studies. Remarkably, Grid-GCN achieves the inference speed of 50fps on ScanNet\n",
      "using 81920 points per scene as input.\n",
      "\n",
      "**Paper Id :2001.09382 \n",
      "Title :GraphAF: a Flow-based Autoregressive Model for Molecular Graph\n",
      "  Generation\n",
      "  Molecular graph generation is a fundamental problem for drug discovery and\n",
      "has been attracting growing attention. The problem is challenging since it\n",
      "requires not only generating chemically valid molecular structures but also\n",
      "optimizing their chemical properties in the meantime. Inspired by the recent\n",
      "progress in deep generative models, in this paper we propose a flow-based\n",
      "autoregressive model for graph generation called GraphAF. GraphAF combines the\n",
      "advantages of both autoregressive and flow-based approaches and enjoys: (1)\n",
      "high model flexibility for data density estimation; (2) efficient parallel\n",
      "computation for training; (3) an iterative sampling process, which allows\n",
      "leveraging chemical domain knowledge for valency checking. Experimental results\n",
      "show that GraphAF is able to generate 68% chemically valid molecules even\n",
      "without chemical knowledge rules and 100% valid molecules with chemical rules.\n",
      "The training process of GraphAF is two times faster than the existing\n",
      "state-of-the-art approach GCPN. After fine-tuning the model for goal-directed\n",
      "property optimization with reinforcement learning, GraphAF achieves\n",
      "state-of-the-art performance on both chemical property optimization and\n",
      "constrained property optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.03283 \n",
      "Title :A quantum active learning algorithm for sampling against adversarial\n",
      "  attacks\n",
      "  Adversarial attacks represent a serious menace for learning algorithms and\n",
      "may compromise the security of future autonomous systems. A theorem by Khoury\n",
      "and Hadfield-Menell (KH), provides sufficient conditions to guarantee the\n",
      "robustness of machine learning algorithms, but comes with a caveat: it is\n",
      "crucial to know the smallest distance among the classes of the corresponding\n",
      "classification problem. We propose a theoretical framework that allows us to\n",
      "think of active learning as sampling the most promising new points to be\n",
      "classified, so that the minimum distance between classes can be found and the\n",
      "theorem KH used. Additionally, we introduce a quantum active learning algorithm\n",
      "that makes use of such framework and whose complexity is polylogarithmic in the\n",
      "dimension of the space, $m$, and the size of the initial training data $n$,\n",
      "provided the use of qRAMs; and polynomial in the precision, achieving an\n",
      "exponential speedup over the equivalent classical algorithm in $n$ and $m$.\n",
      "This algorithm may be nevertheless `dequantized' reducing the advantage to\n",
      "polynomial.\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.03366 \n",
      "Title :Med2Meta: Learning Representations of Medical Concepts with\n",
      "  Meta-Embeddings\n",
      "  Distributed representations of medical concepts have been used to support\n",
      "downstream clinical tasks recently. Electronic Health Records (EHR) capture\n",
      "different aspects of patients' hospital encounters and serve as a rich source\n",
      "for augmenting clinical decision making by learning robust medical concept\n",
      "embeddings. However, the same medical concept can be recorded in different\n",
      "modalities (e.g., clinical notes, lab results)-with each capturing salient\n",
      "information unique to that modality-and a holistic representation calls for\n",
      "relevant feature ensemble from all information sources. We hypothesize that\n",
      "representations learned from heterogeneous data types would lead to performance\n",
      "enhancement on various clinical informatics and predictive modeling tasks. To\n",
      "this end, our proposed approach makes use of meta-embeddings, embeddings\n",
      "aggregated from learned embeddings. Firstly, modality-specific embeddings for\n",
      "each medical concept is learned with graph autoencoders. The ensemble of all\n",
      "the embeddings is then modeled as a meta-embedding learning problem to\n",
      "incorporate their correlating and complementary information through a joint\n",
      "reconstruction. Empirical results of our model on both quantitative and\n",
      "qualitative clinical evaluations have shown improvements over state-of-the-art\n",
      "embedding models, thus validating our hypothesis.\n",
      "\n",
      "**Paper Id :2003.06516 \n",
      "Title :Deep Representation Learning of Electronic Health Records to Unlock\n",
      "  Patient Stratification at Scale\n",
      "  Deriving disease subtypes from electronic health records (EHRs) can guide\n",
      "next-generation personalized medicine. However, challenges in summarizing and\n",
      "representing patient data prevent widespread practice of scalable EHR-based\n",
      "stratification analysis. Here we present an unsupervised framework based on\n",
      "deep learning to process heterogeneous EHRs and derive patient representations\n",
      "that can efficiently and effectively enable patient stratification at scale. We\n",
      "considered EHRs of 1,608,741 patients from a diverse hospital cohort comprising\n",
      "of a total of 57,464 clinical concepts. We introduce a representation learning\n",
      "model based on word embeddings, convolutional neural networks, and autoencoders\n",
      "(i.e., ConvAE) to transform patient trajectories into low-dimensional latent\n",
      "vectors. We evaluated these representations as broadly enabling patient\n",
      "stratification by applying hierarchical clustering to different multi-disease\n",
      "and disease-specific patient cohorts. ConvAE significantly outperformed several\n",
      "baselines in a clustering task to identify patients with different complex\n",
      "conditions, with 2.61 entropy and 0.31 purity average scores. When applied to\n",
      "stratify patients within a certain condition, ConvAE led to various clinically\n",
      "relevant subtypes for different disorders, including type 2 diabetes,\n",
      "Parkinson's disease and Alzheimer's disease, largely related to comorbidities,\n",
      "disease progression, and symptom severity. With these results, we demonstrate\n",
      "that ConvAE can generate patient representations that lead to clinically\n",
      "meaningful insights. This scalable framework can help better understand varying\n",
      "etiologies in heterogeneous sub-populations and unlock patterns for EHR-based\n",
      "research in the realm of personalized medicine.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.03517 \n",
      "Title :No-Regret Exploration in Goal-Oriented Reinforcement Learning\n",
      "  Many popular reinforcement learning problems (e.g., navigation in a maze,\n",
      "some Atari games, mountain car) are instances of the episodic setting under its\n",
      "stochastic shortest path (SSP) formulation, where an agent has to achieve a\n",
      "goal state while minimizing the cumulative cost. Despite the popularity of this\n",
      "setting, the exploration-exploitation dilemma has been sparsely studied in\n",
      "general SSP problems, with most of the theoretical literature focusing on\n",
      "different problems (i.e., fixed-horizon and infinite-horizon) or making the\n",
      "restrictive loop-free SSP assumption (i.e., no state can be visited twice\n",
      "during an episode). In this paper, we study the general SSP problem with no\n",
      "assumption on its dynamics (some policies may actually never reach the goal).\n",
      "We introduce UC-SSP, the first no-regret algorithm in this setting, and prove a\n",
      "regret bound scaling as $\\displaystyle \\widetilde{\\mathcal{O}}( D S \\sqrt{ A D\n",
      "K})$ after $K$ episodes for any unknown SSP with $S$ states, $A$ actions,\n",
      "positive costs and SSP-diameter $D$, defined as the smallest expected hitting\n",
      "time from any starting state to the goal. We achieve this result by crafting a\n",
      "novel stopping rule, such that UC-SSP may interrupt the current policy if it is\n",
      "taking too long to achieve the goal and switch to alternative policies that are\n",
      "designed to rapidly terminate the episode.\n",
      "\n",
      "**Paper Id :2010.00081 \n",
      "Title :Stage-wise Conservative Linear Bandits\n",
      "  We study stage-wise conservative linear stochastic bandits: an instance of\n",
      "bandit optimization, which accounts for (unknown) safety constraints that\n",
      "appear in applications such as online advertising and medical trials. At each\n",
      "stage, the learner must choose actions that not only maximize cumulative reward\n",
      "across the entire time horizon but further satisfy a linear baseline constraint\n",
      "that takes the form of a lower bound on the instantaneous reward. For this\n",
      "problem, we present two novel algorithms, stage-wise conservative linear\n",
      "Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that\n",
      "respect the baseline constraints and enjoy probabilistic regret bounds of order\n",
      "O(\\sqrt{T} \\log^{3/2}T) and O(\\sqrt{T} \\log T), respectively. Notably, the\n",
      "proposed algorithms can be adjusted with only minor modifications to tackle\n",
      "different problem variations, such as constraints with bandit-feedback, or an\n",
      "unknown sequence of baseline actions. We discuss these and other improvements\n",
      "over the state-of-the-art. For instance, compared to existing solutions, we\n",
      "show that SCLTS plays the (non-optimal) baseline action at most O(\\log{T})\n",
      "times (compared to O(\\sqrt{T})). Finally, we make connections to another\n",
      "studied form of safety constraints that takes the form of an upper bound on the\n",
      "instantaneous reward. While this incurs additional complexity to the learning\n",
      "process as the optimal action is not guaranteed to belong to the safe set at\n",
      "each round, we show that SCLUCB can properly adjust in this setting via a\n",
      "simple modification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.05845 \n",
      "Title :Local Context Normalization: Revisiting Local Normalization\n",
      "  Normalization layers have been shown to improve convergence in deep neural\n",
      "networks, and even add useful inductive biases. In many vision applications the\n",
      "local spatial context of the features is important, but most common\n",
      "normalization schemes including Group Normalization (GN), Instance\n",
      "Normalization (IN), and Layer Normalization (LN) normalize over the entire\n",
      "spatial dimension of a feature. This can wash out important signals and degrade\n",
      "performance. For example, in applications that use satellite imagery, input\n",
      "images can be arbitrarily large; consequently, it is nonsensical to normalize\n",
      "over the entire area. Positional Normalization (PN), on the other hand, only\n",
      "normalizes over a single spatial position at a time. A natural compromise is to\n",
      "normalize features by local context, while also taking into account group level\n",
      "information. In this paper, we propose Local Context Normalization (LCN): a\n",
      "normalization layer where every feature is normalized based on a window around\n",
      "it and the filters in its group. We propose an algorithmic solution to make LCN\n",
      "efficient for arbitrary window sizes, even if every point in the image has a\n",
      "unique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN\n",
      "counterparts for object detection, semantic segmentation, and instance\n",
      "segmentation applications in several benchmark datasets, while keeping\n",
      "performance independent of the batch size and facilitating transfer learning.\n",
      "\n",
      "**Paper Id :2002.02959 \n",
      "Title :Revisiting Spatial Invariance with Low-Rank Local Connectivity\n",
      "  Convolutional neural networks are among the most successful architectures in\n",
      "deep learning with this success at least partially attributable to the efficacy\n",
      "of spatial invariance as an inductive bias. Locally connected layers, which\n",
      "differ from convolutional layers only in their lack of spatial invariance,\n",
      "usually perform poorly in practice. However, these observations still leave\n",
      "open the possibility that some degree of relaxation of spatial invariance may\n",
      "yield a better inductive bias than either convolution or local connectivity. To\n",
      "test this hypothesis, we design a method to relax the spatial invariance of a\n",
      "network layer in a controlled manner; we create a \\textit{low-rank} locally\n",
      "connected layer, where the filter bank applied at each position is constructed\n",
      "as a linear combination of basis set of filter banks with spatially varying\n",
      "combining weights. By varying the number of basis filter banks, we can control\n",
      "the degree of relaxation of spatial invariance. In experiments with small\n",
      "convolutional networks, we find that relaxing spatial invariance improves\n",
      "classification accuracy over both convolution and locally connected layers\n",
      "across MNIST, CIFAR-10, and CelebA datasets, thus suggesting that spatial\n",
      "invariance may be an overly restrictive prior.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.06015 \n",
      "Title :Efficient Per-Example Gradient Computations in Convolutional Neural\n",
      "  Networks\n",
      "  Deep learning frameworks leverage GPUs to perform massively-parallel\n",
      "computations over batches of many training examples efficiently. However, for\n",
      "certain tasks, one may be interested in performing per-example computations,\n",
      "for instance using per-example gradients to evaluate a quantity of interest\n",
      "unique to each example. One notable application comes from the field of\n",
      "differential privacy, where per-example gradients must be norm-bounded in order\n",
      "to limit the impact of each example on the aggregated batch gradient. In this\n",
      "work, we discuss how per-example gradients can be efficiently computed in\n",
      "convolutional neural networks (CNNs). We compare existing strategies by\n",
      "performing a few steps of differentially-private training on CNNs of varying\n",
      "sizes. We also introduce a new strategy for per-example gradient calculation,\n",
      "which is shown to be advantageous depending on the model architecture and how\n",
      "the model is trained. This is a first step in making differentially-private\n",
      "training of CNNs practical.\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.06073 \n",
      "Title :Normalizing Constant Estimation with Gaussianized Bridge Sampling\n",
      "  Normalizing constant (also called partition function, Bayesian evidence, or\n",
      "marginal likelihood) is one of the central goals of Bayesian inference, yet\n",
      "most of the existing methods are both expensive and inaccurate. Here we develop\n",
      "a new approach, starting from posterior samples obtained with a standard Markov\n",
      "Chain Monte Carlo (MCMC). We apply a novel Normalizing Flow (NF) approach to\n",
      "obtain an analytic density estimator from these samples, followed by Optimal\n",
      "Bridge Sampling (OBS) to obtain the normalizing constant. We compare our method\n",
      "which we call Gaussianized Bridge Sampling (GBS) to existing methods such as\n",
      "Nested Sampling (NS) and Annealed Importance Sampling (AIS) on several\n",
      "examples, showing our method is both significantly faster and substantially\n",
      "more accurate than these methods, and comes with a reliable error estimation.\n",
      "\n",
      "**Paper Id :2003.07070 \n",
      "Title :Merge-split Markov chain Monte Carlo for community detection\n",
      "  We present a Markov chain Monte Carlo scheme based on merges and splits of\n",
      "groups that is capable of efficiently sampling from the posterior distribution\n",
      "of network partitions, defined according to the stochastic block model (SBM).\n",
      "We demonstrate how schemes based on the move of single nodes between groups\n",
      "systematically fail at correctly sampling from the posterior distribution even\n",
      "on small networks, and how our merge-split approach behaves significantly\n",
      "better, and improves the mixing time of the Markov chain by several orders of\n",
      "magnitude in typical cases. We also show how the scheme can be\n",
      "straightforwardly extended to nested versions of the SBM, yielding\n",
      "asymptotically exact samples of hierarchical network partitions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.06137 \n",
      "Title :Calibrated model-based evidential clustering using bootstrapping\n",
      "  Evidential clustering is an approach to clustering in which\n",
      "cluster-membership uncertainty is represented by a collection of\n",
      "Dempster-Shafer mass functions forming an evidential partition. In this paper,\n",
      "we propose to construct these mass functions by bootstrapping finite mixture\n",
      "models. In the first step, we compute bootstrap percentile confidence intervals\n",
      "for all pairwise probabilities (the probabilities for any two objects to belong\n",
      "to the same class). We then construct an evidential partition such that the\n",
      "pairwise belief and plausibility degrees approximate the bounds of the\n",
      "confidence intervals. This evidential partition is calibrated, in the sense\n",
      "that the pairwise belief-plausibility intervals contain the true probabilities\n",
      "\"most of the time\", i.e., with a probability close to the defined confidence\n",
      "level. This frequentist property is verified by simulation, and the practical\n",
      "applicability of the method is demonstrated using several real datasets.\n",
      "\n",
      "**Paper Id :2002.10399 \n",
      "Title :Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference\n",
      "  Setting\n",
      "  Parameter estimation, statistical tests and confidence sets are the\n",
      "cornerstones of classical statistics that allow scientists to make inferences\n",
      "about the underlying process that generated the observed data. A key question\n",
      "is whether one can still construct hypothesis tests and confidence sets with\n",
      "proper coverage and high power in a so-called likelihood-free inference (LFI)\n",
      "setting; that is, a setting where the likelihood is not explicitly known but\n",
      "one can forward-simulate observable data according to a stochastic model. In\n",
      "this paper, we present $\\texttt{ACORE}$ (Approximate Computation via Odds Ratio\n",
      "Estimation), a frequentist approach to LFI that first formulates the classical\n",
      "likelihood ratio test (LRT) as a parametrized classification problem, and then\n",
      "uses the equivalence of tests and confidence sets to build confidence regions\n",
      "for parameters of interest. We also present a goodness-of-fit procedure for\n",
      "checking whether the constructed tests and confidence regions are valid.\n",
      "$\\texttt{ACORE}$ is based on the key observation that the LRT statistic, the\n",
      "rejection probability of the test, and the coverage of the confidence set are\n",
      "conditional distribution functions which often vary smoothly as a function of\n",
      "the parameters of interest. Hence, instead of relying solely on samples\n",
      "simulated at fixed parameter settings (as is the convention in standard Monte\n",
      "Carlo solutions), one can leverage machine learning tools and data simulated in\n",
      "the neighborhood of a parameter to improve estimates of quantities of interest.\n",
      "We demonstrate the efficacy of $\\texttt{ACORE}$ with both theoretical and\n",
      "empirical results. Our implementation is available on Github.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.06321 \n",
      "Title :Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World\n",
      "  Performance?\n",
      "  Does progress in simulation translate to progress on robots? If one method\n",
      "outperforms another in simulation, how likely is that trend to hold in reality\n",
      "on a robot? We examine this question for embodied PointGoal navigation,\n",
      "developing engineering tools and a research paradigm for evaluating a simulator\n",
      "by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),\n",
      "a library for seamless execution of identical code on simulated agents and\n",
      "robots, transferring simulation-trained agents to a LoCoBot platform with a\n",
      "one-line code change. Second, we investigate the sim2real predictivity of\n",
      "Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create\n",
      "a virtualized replica, and run parallel tests of 9 different models in reality\n",
      "and simulation. We present a new metric called Sim-vs-Real Correlation\n",
      "Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as\n",
      "used for the CVPR19 challenge is low (0.18 for the success metric), suggesting\n",
      "that performance differences in this simulator-based challenge do not persist\n",
      "after physical deployment. This gap is largely due to AI agents learning to\n",
      "exploit simulator imperfections, abusing collision dynamics to 'slide' along\n",
      "walls, leading to shortcuts through otherwise non-navigable space. Naturally,\n",
      "such exploits do not work in the real world. Our experiments show that it is\n",
      "possible to tune simulation parameters to improve sim2real predictivity (e.g.\n",
      "improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that\n",
      "in-simulation comparisons will translate to deployed systems in reality.\n",
      "\n",
      "**Paper Id :1905.13402 \n",
      "Title :Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep\n",
      "  Model-Based RL for Sparse Cost Robotic Tasks\n",
      "  Reinforcement learning (RL) for robotics is challenging due to the difficulty\n",
      "in hand-engineering a dense cost function, which can lead to unintended\n",
      "behavior, and dynamical uncertainty, which makes exploration and constraint\n",
      "satisfaction challenging. We address these issues with a new model-based\n",
      "reinforcement learning algorithm, Safety Augmented Value Estimation from\n",
      "Demonstrations (SAVED), which uses supervision that only identifies task\n",
      "completion and a modest set of suboptimal demonstrations to constrain\n",
      "exploration and learn efficiently while handling complex constraints. We then\n",
      "compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms\n",
      "on 6 standard simulation benchmarks involving navigation and manipulation and a\n",
      "physical knot-tying task on the da Vinci surgical robot. Results suggest that\n",
      "SAVED outperforms prior methods in terms of success rate, constraint\n",
      "satisfaction, and sample efficiency, making it feasible to safely learn a\n",
      "control policy directly on a real robot in less than an hour. For tasks on the\n",
      "robot, baselines succeed less than 5% of the time while SAVED has a success\n",
      "rate of over 75% in the first 50 training iterations. Code and supplementary\n",
      "material is available at https://tinyurl.com/saved-rl.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.06472 \n",
      "Title :Dimension of Reservoir Computers\n",
      "  A reservoir computer is a complex dynamical system, often created by coupling\n",
      "nonlinear nodes in a network. The nodes are all driven by a common driving\n",
      "signal. In this work, three dimension estimation methods, false nearest\n",
      "neighbor, covariance and Kaplan-Yorke dimensions, are used to estimate the\n",
      "dimension of the reservoir dynamical system. It is shown that the signals in\n",
      "the reservoir system exist on a relatively low dimensional surface. Changing\n",
      "the spectral radius of the reservoir network can increase the fractal dimension\n",
      "of the reservoir signals, leading to an increase in testing error.\n",
      "\n",
      "**Paper Id :2001.04263 \n",
      "Title :Deep learning to discover and predict dynamics on an inertial manifold\n",
      "  A data-driven framework is developed to represent chaotic dynamics on an\n",
      "inertial manifold (IM), and applied to solutions of the Kuramoto-Sivashinsky\n",
      "equation. A hybrid method combining linear and nonlinear (neural-network)\n",
      "dimension reduction transforms between coordinates in the full state space and\n",
      "on the IM. Additional neural networks predict time-evolution on the IM. The\n",
      "formalism accounts for translation invariance and energy conservation, and\n",
      "substantially outperforms linear dimension reduction, reproducing very well key\n",
      "dynamic and statistical features of the attractor.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.06779 \n",
      "Title :Predictive Precompute with Recurrent Neural Networks\n",
      "  In both mobile and web applications, speeding up user interface response\n",
      "times can often lead to significant improvements in user engagement. A common\n",
      "technique to improve responsiveness is to precompute data ahead of time for\n",
      "specific activities. However, simply precomputing data for all user and\n",
      "activity combinations is prohibitive at scale due to both network constraints\n",
      "and server-side computational costs. It is therefore important to accurately\n",
      "predict per-user application usage in order to minimize wasted precomputation\n",
      "(\"predictive precompute\"). In this paper, we describe the novel application of\n",
      "recurrent neural networks (RNNs) for predictive precompute. We compare their\n",
      "performance with traditional machine learning models, and share findings from\n",
      "their large-scale production use at Facebook. We demonstrate that RNN models\n",
      "improve prediction accuracy, eliminate most feature engineering steps, and\n",
      "reduce the computational cost of serving predictions by an order of magnitude.\n",
      "\n",
      "**Paper Id :2005.11619 \n",
      "Title :Bayesian Neural Networks at Scale: A Performance Analysis and Pruning\n",
      "  Study\n",
      "  Bayesian neural Networks (BNNs) are a promising method of obtaining\n",
      "statistical uncertainties for neural network predictions but with a higher\n",
      "computational overhead which can limit their practical usage. This work\n",
      "explores the use of high performance computing with distributed training to\n",
      "address the challenges of training BNNs at scale. We present a performance and\n",
      "scalability comparison of training the VGG-16 and Resnet-18 models on a\n",
      "Cray-XC40 cluster. We demonstrate that network pruning can speed up inference\n",
      "without accuracy loss and provide an open source software package,\n",
      "{\\it{BPrune}} to automate this pruning. For certain models we find that pruning\n",
      "up to 80\\% of the network results in only a 7.0\\% loss in accuracy. With the\n",
      "development of new hardware accelerators for Deep Learning, BNNs are of\n",
      "considerable interest for benchmarking performance. This analysis of training a\n",
      "BNN at scale outlines the limitations and benefits compared to a conventional\n",
      "neural network.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07044 \n",
      "Title :Theory of neuromorphic computing by waves: machine learning by rogue\n",
      "  waves, dispersive shocks, and solitons\n",
      "  We study artificial neural networks with nonlinear waves as a computing\n",
      "reservoir. We discuss universality and the conditions to learn a dataset in\n",
      "terms of output channels and nonlinearity. A feed-forward three-layer model,\n",
      "with an encoding input layer, a wave layer, and a decoding readout, behaves as\n",
      "a conventional neural network in approximating mathematical functions,\n",
      "real-world datasets, and universal Boolean gates. The rank of the transmission\n",
      "matrix has a fundamental role in assessing the learning abilities of the wave.\n",
      "For a given set of training points, a threshold nonlinearity for universal\n",
      "interpolation exists. When considering the nonlinear Schroedinger equation, the\n",
      "use of highly nonlinear regimes implies that solitons, rogue, and shock waves\n",
      "do have a leading role in training and computing. Our results may enable the\n",
      "realization of novel machine learning devices by using diverse physical\n",
      "systems, as nonlinear optics, hydrodynamics, polaritonics, and Bose-Einstein\n",
      "condensates. The application of these concepts to photonics opens the way to a\n",
      "large class of accelerators and new computational paradigms. In complex wave\n",
      "systems, as multimodal fibers, integrated optical circuits, random, topological\n",
      "devices, and metasurfaces, nonlinear waves can be employed to perform\n",
      "computation and solve complex combinatorial optimization.\n",
      "\n",
      "**Paper Id :1902.09216 \n",
      "Title :Revealing quantum chaos with machine learning\n",
      "  Understanding properties of quantum matter is an outstanding challenge in\n",
      "science. In this paper, we demonstrate how machine-learning methods can be\n",
      "successfully applied for the classification of various regimes in\n",
      "single-particle and many-body systems. We realize neural network algorithms\n",
      "that perform a classification between regular and chaotic behavior in quantum\n",
      "billiard models with remarkably high accuracy. We use the variational\n",
      "autoencoder for autosupervised classification of regular/chaotic wave\n",
      "functions, as well as demonstrating that variational autoencoders could be used\n",
      "as a tool for detection of anomalous quantum states, such as quantum scars. By\n",
      "taking this method further, we show that machine learning techniques allow us\n",
      "to pin down the transition from integrability to many-body quantum chaos in\n",
      "Heisenberg XXZ spin chains. For both cases, we confirm the existence of\n",
      "universal W shapes that characterize the transition. Our results pave the way\n",
      "for exploring the power of machine learning tools for revealing exotic\n",
      "phenomena in quantum many-body systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07123 \n",
      "Title :A novel spike-and-wave automatic detection in EEG signals\n",
      "  Spike-and-wave discharge (SWD) pattern classification in\n",
      "electroencephalography (EEG) signals is a key problem in signal processing. It\n",
      "is particularly important to develop a SWD automatic detection method in\n",
      "long-term EEG recordings since the task of marking the patters manually is time\n",
      "consuming, difficult and error-prone. This paper presents a new detection\n",
      "method with a low computational complexity that can be easily trained if\n",
      "standard medical protocols are respected. The detection procedure is as\n",
      "follows: First, each EEG signal is divided into several time segments and for\n",
      "each time segment, the Morlet 1-D decomposition is applied. Then three\n",
      "parameters are extracted from the wavelet coefficients of each segment: scale\n",
      "(using a generalized Gaussian statistical model), variance and median. This is\n",
      "followed by a k-nearest neighbors (k-NN) classifier to detect the\n",
      "spike-and-wave pattern in each EEG channel from these three parameters. A total\n",
      "of 106 spike-and-wave and 106 non-spike-and-wave were used for training, while\n",
      "69 new annotated EEG segments from six subjects were used for classification.\n",
      "In these circumstances, the proposed methodology achieved 100% accuracy. These\n",
      "results generate new research opportunities for the underlying causes of the\n",
      "so-called absence epilepsy in long-term EEG recordings.\n",
      "\n",
      "**Paper Id :2004.11706 \n",
      "Title :Target specific mining of COVID-19 scholarly articles using one-class\n",
      "  approach\n",
      "  In recent years, several research articles have been published in the field\n",
      "of corona-virus caused diseases like severe acute respiratory syndrome (SARS),\n",
      "middle east respiratory syndrome (MERS) and COVID-19. In the presence of\n",
      "numerous research articles, extracting best-suited articles is time-consuming\n",
      "and manually impractical. The objective of this paper is to extract the\n",
      "activity and trends of corona-virus related research articles using machine\n",
      "learning approaches. The COVID-19 open research dataset (CORD-19) is used for\n",
      "experiments, whereas several target-tasks along with explanations are defined\n",
      "for classification, based on domain knowledge. Clustering techniques are used\n",
      "to create the different clusters of available articles, and later the task\n",
      "assignment is performed using parallel one-class support vector machines\n",
      "(OCSVMs). Experiments with original and reduced features validate the\n",
      "performance of the approach. It is evident that the k-means clustering\n",
      "algorithm, followed by parallel OCSVMs, outperforms other methods for both\n",
      "original and reduced feature space.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07197 \n",
      "Title :Dense Recurrent Neural Networks for Accelerated MRI: History-Cognizant\n",
      "  Unrolling of Optimization Algorithms\n",
      "  Inverse problems for accelerated MRI typically incorporate domain-specific\n",
      "knowledge about the forward encoding operator in a regularized reconstruction\n",
      "framework. Recently physics-driven deep learning (DL) methods have been\n",
      "proposed to use neural networks for data-driven regularization. These methods\n",
      "unroll iterative optimization algorithms to solve the inverse problem objective\n",
      "function, by alternating between domain-specific data consistency and\n",
      "data-driven regularization via neural networks. The whole unrolled network is\n",
      "then trained end-to-end to learn the parameters of the network. Due to\n",
      "simplicity of data consistency updates with gradient descent steps, proximal\n",
      "gradient descent (PGD) is a common approach to unroll physics-driven DL\n",
      "reconstruction methods. However, PGD methods have slow convergence rates,\n",
      "necessitating a higher number of unrolled iterations, leading to memory issues\n",
      "in training and slower reconstruction times in testing. Inspired by efficient\n",
      "variants of PGD methods that use a history of the previous iterates, we propose\n",
      "a history-cognizant unrolling of the optimization algorithm with dense\n",
      "connections across iterations for improved performance. In our approach, the\n",
      "gradient descent steps are calculated at a trainable combination of the outputs\n",
      "of all the previous regularization units. We also apply this idea to unrolling\n",
      "variable splitting methods with quadratic relaxation. Our results in\n",
      "reconstruction of the fastMRI knee dataset show that the proposed\n",
      "history-cognizant approach reduces residual aliasing artifacts compared to its\n",
      "conventional unrolled counterpart without requiring extra computational power\n",
      "or increasing reconstruction time.\n",
      "\n",
      "**Paper Id :1910.09116 \n",
      "Title :Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without\n",
      "  Fully-Sampled Data\n",
      "  Deep learning (DL) has emerged as a tool for improving accelerated MRI\n",
      "reconstruction. A common strategy among DL methods is the physics-based\n",
      "approach, where a regularized iterative algorithm alternating between data\n",
      "consistency and a regularizer is unrolled for a finite number of iterations.\n",
      "This unrolled network is then trained end-to-end in a supervised manner, using\n",
      "fully-sampled data as ground truth for the network output. However, in a number\n",
      "of scenarios, it is difficult to obtain fully-sampled datasets, due to\n",
      "physiological constraints such as organ motion or physical constraints such as\n",
      "signal decay. In this work, we tackle this issue and propose a self-supervised\n",
      "learning strategy that enables physics-based DL reconstruction without\n",
      "fully-sampled data. Our approach is to divide the acquired sub-sampled points\n",
      "for each scan into training and validation subsets. During training, data\n",
      "consistency is enforced over the training subset, while the validation subset\n",
      "is used to define the loss function. Results show that the proposed\n",
      "self-supervised learning method successfully reconstructs images without\n",
      "fully-sampled data, performing similarly to the supervised approach that is\n",
      "trained with fully-sampled references. This has implications for physics-based\n",
      "inverse problem approaches for other settings, where fully-sampled data is not\n",
      "available or possible to acquire.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07286 \n",
      "Title :Variational Quantum Circuits for Quantum State Tomography\n",
      "  Quantum state tomography is a key process in most quantum experiments. In\n",
      "this work, we employ quantum machine learning for state tomography. Given an\n",
      "unknown quantum state, it can be learned by maximizing the fidelity between the\n",
      "output of a variational quantum circuit and this state. The number of\n",
      "parameters of the variational quantum circuit grows linearly with the number of\n",
      "qubits and the circuit depth, so that only polynomial measurements are\n",
      "required, even for highly-entangled states. After that, a subsequent classical\n",
      "circuit simulator is used to transform the information of the target quantum\n",
      "state from the variational quantum circuit into a familiar format. We\n",
      "demonstrate our method by performing numerical simulations for the tomography\n",
      "of the ground state of a one-dimensional quantum spin chain, using a\n",
      "variational quantum circuit simulator. Our method is suitable for near-term\n",
      "quantum computing platforms, and could be used for relatively large-scale\n",
      "quantum state tomography for experimentally relevant quantum states.\n",
      "\n",
      "**Paper Id :1907.06589 \n",
      "Title :Experimental quantum homodyne tomography via machine learning\n",
      "  Complete characterization of states and processes that occur within quantum\n",
      "devices is crucial for understanding and testing their potential to outperform\n",
      "classical technologies for communications and computing. However, solving this\n",
      "task with current state-of-the-art techniques becomes unwieldy for large and\n",
      "complex quantum systems. Here we realize and experimentally demonstrate a\n",
      "method for complete characterization of a quantum harmonic oscillator based on\n",
      "an artificial neural network known as the restricted Boltzmann machine. We\n",
      "apply the method to optical homodyne tomography and show it to allow full\n",
      "estimation of quantum states based on a smaller amount of experimental data\n",
      "compared to state-of-the-art methods. We link this advantage to reduced\n",
      "overfitting. Although our experiment is in the optical domain, our method\n",
      "provides a way of exploring quantum resources in a broad class of large-scale\n",
      "physical systems, such as superconducting circuits, atomic and molecular\n",
      "ensembles, and optomechanical systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07354 \n",
      "Title :Deep learning-based survival prediction for multiple cancer types using\n",
      "  histopathology images\n",
      "  Prognostic information at diagnosis has important implications for cancer\n",
      "treatment and monitoring. Although cancer staging, histopathological\n",
      "assessment, molecular features, and clinical variables can provide useful\n",
      "prognostic insights, improving risk stratification remains an active research\n",
      "area. We developed a deep learning system (DLS) to predict disease specific\n",
      "survival across 10 cancer types from The Cancer Genome Atlas (TCGA). We used a\n",
      "weakly-supervised approach without pixel-level annotations, and tested three\n",
      "different survival loss functions. The DLS was developed using 9,086 slides\n",
      "from 3,664 cases and evaluated using 3,009 slides from 1,216 cases. In\n",
      "multivariable Cox regression analysis of the combined cohort including all 10\n",
      "cancers, the DLS was significantly associated with disease specific survival\n",
      "(hazard ratio of 1.58, 95% CI 1.28-1.70, p<0.0001) after adjusting for cancer\n",
      "type, stage, age, and sex. In a per-cancer adjusted subanalysis, the DLS\n",
      "remained a significant predictor of survival in 5 of 10 cancer types. Compared\n",
      "to a baseline model including stage, age, and sex, the c-index of the model\n",
      "demonstrated an absolute 3.7% improvement (95% CI 1.0-6.5) in the combined\n",
      "cohort. Additionally, our models stratified patients within individual cancer\n",
      "stages, particularly stage II (p=0.025) and stage III (p<0.001). By developing\n",
      "and evaluating prognostic models across multiple cancer types, this work\n",
      "represents one of the most comprehensive studies exploring the direct\n",
      "prediction of clinical outcomes using deep learning and histopathology images.\n",
      "Our analysis demonstrates the potential for this approach to provide prognostic\n",
      "information in multiple cancer types, and even within specific pathologic\n",
      "stages. However, given the relatively small number of clinical events, we\n",
      "observed wide confidence intervals, suggesting that future work will benefit\n",
      "from larger datasets.\n",
      "\n",
      "**Paper Id :1810.10342 \n",
      "Title :Predicting optical coherence tomography-derived diabetic macular edema\n",
      "  grades from fundus photographs using deep learning\n",
      "  Diabetic eye disease is one of the fastest growing causes of preventable\n",
      "blindness. With the advent of anti-VEGF (vascular endothelial growth factor)\n",
      "therapies, it has become increasingly important to detect center-involved\n",
      "diabetic macular edema (ci-DME). However, center-involved diabetic macular\n",
      "edema is diagnosed using optical coherence tomography (OCT), which is not\n",
      "generally available at screening sites because of cost and workflow\n",
      "constraints. Instead, screening programs rely on the detection of hard exudates\n",
      "in color fundus photographs as a proxy for DME, often resulting in high false\n",
      "positive or false negative calls. To improve the accuracy of DME screening, we\n",
      "trained a deep learning model to use color fundus photographs to predict\n",
      "ci-DME. Our model had an ROC-AUC of 0.89 (95% CI: 0.87-0.91), which corresponds\n",
      "to a sensitivity of 85% at a specificity of 80%. In comparison, three retinal\n",
      "specialists had similar sensitivities (82-85%), but only half the specificity\n",
      "(45-50%, p<0.001 for each comparison with model). The positive predictive value\n",
      "(PPV) of the model was 61% (95% CI: 56-66%), approximately double the 36-38% by\n",
      "the retinal specialists. In addition to predicting ci-DME, our model was able\n",
      "to detect the presence of intraretinal fluid with an AUC of 0.81 (95% CI:\n",
      "0.81-0.86) and subretinal fluid with an AUC of 0.88 (95% CI: 0.85-0.91). The\n",
      "ability of deep learning algorithms to make clinically relevant predictions\n",
      "that generally require sophisticated 3D-imaging equipment from simple 2D images\n",
      "has broad relevance to many other applications in medical imaging.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07416 \n",
      "Title :Improved Explanatory Efficacy on Human Affect and Workload through\n",
      "  Interactive Process in Artificial Intelligence\n",
      "  Despite recent advances in the field of explainable artificial intelligence\n",
      "systems, a concrete quantitative measure for evaluating the usability of such\n",
      "systems is nonexistent. Ensuring the success of an explanatory interface in\n",
      "interacting with users requires a cyclic, symbiotic relationship between human\n",
      "and artificial intelligence. We, therefore, propose explanatory efficacy, a\n",
      "novel metric for evaluating the strength of the cyclic relationship the\n",
      "interface exhibits. Furthermore, in a user study, we evaluated the perceived\n",
      "affect and workload and recorded the EEG signals of our participants as they\n",
      "interacted with our custom-built, iterative explanatory interface to build\n",
      "personalized recommendation systems. We found that systems for perceptually\n",
      "driven iterative tasks with greater explanatory efficacy are characterized by\n",
      "statistically significant hemispheric differences in neural signals with 62.4%\n",
      "accuracy, indicating the feasibility of neural correlates as a measure of\n",
      "explanatory efficacy. These findings are beneficial for researchers who aim to\n",
      "study the circular ecosystem of the human-artificial intelligence partnership.\n",
      "\n",
      "**Paper Id :2002.02453 \n",
      "Title :Modeling Engagement in Long-Term, In-Home Socially Assistive Robot\n",
      "  Interventions for Children with Autism Spectrum Disorders\n",
      "  Socially assistive robotics (SAR) has great potential to provide accessible,\n",
      "affordable, and personalized therapeutic interventions for children with autism\n",
      "spectrum disorders (ASD). However, human-robot interaction (HRI) methods are\n",
      "still limited in their ability to autonomously recognize and respond to\n",
      "behavioral cues, especially in atypical users and everyday settings. This work\n",
      "applies supervised machine learning algorithms to model user engagement in the\n",
      "context of long-term, in-home SAR interventions for children with ASD.\n",
      "Specifically, we present two types of engagement models for each user: (i)\n",
      "generalized models trained on data from different users; and (ii)\n",
      "individualized models trained on an early subset of the user's data. The models\n",
      "achieved approximately 90% accuracy (AUROC) for post hoc binary classification\n",
      "of engagement, despite the high variance in data observed across users,\n",
      "sessions, and engagement states. Moreover, temporal patterns in model\n",
      "predictions could be used to reliably initiate re-engagement actions at\n",
      "appropriate times. These results validate the feasibility and challenges of\n",
      "recognition and response to user disengagement in long-term, real-world HRI\n",
      "settings. The contributions of this work also inform the design of engaging and\n",
      "personalized HRI, especially for the ASD community.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.07669 \n",
      "Title :Self-Supervised Learning of Physics-Guided Reconstruction Neural\n",
      "  Networks without Fully-Sampled Reference Data\n",
      "  Purpose: To develop a strategy for training a physics-guided MRI\n",
      "reconstruction neural network without a database of fully-sampled datasets.\n",
      "Theory and Methods: Self-supervised learning via data under-sampling (SSDU) for\n",
      "physics-guided deep learning (DL) reconstruction partitions available\n",
      "measurements into two disjoint sets, one of which is used in the data\n",
      "consistency units in the unrolled network and the other is used to define the\n",
      "loss for training. The proposed training without fully-sampled data is compared\n",
      "to fully-supervised training with ground-truth data, as well as conventional\n",
      "compressed sensing and parallel imaging methods using the publicly available\n",
      "fastMRI knee database. The same physics-guided neural network is used for both\n",
      "proposed SSDU and supervised training. The SSDU training is also applied to\n",
      "prospectively 2-fold accelerated high-resolution brain datasets at different\n",
      "acceleration rates, and compared to parallel imaging. Results: Results on five\n",
      "different knee sequences at acceleration rate of 4 shows that proposed\n",
      "self-supervised approach performs closely with supervised learning, while\n",
      "significantly outperforming conventional compressed sensing and parallel\n",
      "imaging, as characterized by quantitative metrics and a clinical reader study.\n",
      "The results on prospectively sub-sampled brain datasets, where supervised\n",
      "learning cannot be employed due to lack of ground-truth reference, show that\n",
      "the proposed self-supervised approach successfully perform reconstruction at\n",
      "high acceleration rates (4, 6 and 8). Image readings indicate improved visual\n",
      "reconstruction quality with the proposed approach compared to parallel imaging\n",
      "at acquisition acceleration. Conclusion: The proposed SSDU approach allows\n",
      "training of physics-guided DL-MRI reconstruction without fully-sampled data,\n",
      "while achieving comparable results with supervised DL-MRI trained on\n",
      "fully-sampled data.\n",
      "\n",
      "**Paper Id :1910.09116 \n",
      "Title :Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without\n",
      "  Fully-Sampled Data\n",
      "  Deep learning (DL) has emerged as a tool for improving accelerated MRI\n",
      "reconstruction. A common strategy among DL methods is the physics-based\n",
      "approach, where a regularized iterative algorithm alternating between data\n",
      "consistency and a regularizer is unrolled for a finite number of iterations.\n",
      "This unrolled network is then trained end-to-end in a supervised manner, using\n",
      "fully-sampled data as ground truth for the network output. However, in a number\n",
      "of scenarios, it is difficult to obtain fully-sampled datasets, due to\n",
      "physiological constraints such as organ motion or physical constraints such as\n",
      "signal decay. In this work, we tackle this issue and propose a self-supervised\n",
      "learning strategy that enables physics-based DL reconstruction without\n",
      "fully-sampled data. Our approach is to divide the acquired sub-sampled points\n",
      "for each scan into training and validation subsets. During training, data\n",
      "consistency is enforced over the training subset, while the validation subset\n",
      "is used to define the loss function. Results show that the proposed\n",
      "self-supervised learning method successfully reconstructs images without\n",
      "fully-sampled data, performing similarly to the supervised approach that is\n",
      "trained with fully-sampled references. This has implications for physics-based\n",
      "inverse problem approaches for other settings, where fully-sampled data is not\n",
      "available or possible to acquire.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.08177 \n",
      "Title :Lift & Learn: Physics-informed machine learning for large-scale\n",
      "  nonlinear dynamical systems\n",
      "  We present Lift & Learn, a physics-informed method for learning\n",
      "low-dimensional models for large-scale dynamical systems. The method exploits\n",
      "knowledge of a system's governing equations to identify a coordinate\n",
      "transformation in which the system dynamics have quadratic structure. This\n",
      "transformation is called a lifting map because it often adds auxiliary\n",
      "variables to the system state. The lifting map is applied to data obtained by\n",
      "evaluating a model for the original nonlinear system. This lifted data is\n",
      "projected onto its leading principal components, and low-dimensional linear and\n",
      "quadratic matrix operators are fit to the lifted reduced data using a\n",
      "least-squares operator inference procedure. Analysis of our method shows that\n",
      "the Lift & Learn models are able to capture the system physics in the lifted\n",
      "coordinates at least as accurately as traditional intrusive model reduction\n",
      "approaches. This preservation of system physics makes the Lift & Learn models\n",
      "robust to changes in inputs. Numerical experiments on the FitzHugh-Nagumo\n",
      "neuron activation model and the compressible Euler equations demonstrate the\n",
      "generalizability of our model.\n",
      "\n",
      "**Paper Id :1910.00024 \n",
      "Title :Neural Canonical Transformation with Symplectic Flows\n",
      "  Canonical transformation plays a fundamental role in simplifying and solving\n",
      "classical Hamiltonian systems. We construct flexible and powerful canonical\n",
      "transformations as generative models using symplectic neural networks. The\n",
      "model transforms physical variables towards a latent representation with an\n",
      "independent harmonic oscillator Hamiltonian. Correspondingly, the phase space\n",
      "density of the physical system flows towards a factorized Gaussian distribution\n",
      "in the latent space. Since the canonical transformation preserves the\n",
      "Hamiltonian evolution, the model captures nonlinear collective modes in the\n",
      "learned latent representation. We present an efficient implementation of\n",
      "symplectic neural coordinate transformations and two ways to train the model.\n",
      "The variational free energy calculation is based on the analytical form of\n",
      "physical Hamiltonian. While the phase space density estimation only requires\n",
      "samples in the coordinate space for separable Hamiltonians. We demonstrate\n",
      "appealing features of neural canonical transformation using toy problems\n",
      "including two-dimensional ring potential and harmonic chain. Finally, we apply\n",
      "the approach to real-world problems such as identifying slow collective modes\n",
      "in alanine dipeptide and conceptual compression of the MNIST dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.08278 \n",
      "Title :Transfer learning in hybrid classical-quantum neural networks\n",
      "  We extend the concept of transfer learning, widely applied in modern machine\n",
      "learning algorithms, to the emerging context of hybrid neural networks composed\n",
      "of classical and quantum elements. We propose different implementations of\n",
      "hybrid transfer learning, but we focus mainly on the paradigm in which a\n",
      "pre-trained classical network is modified and augmented by a final variational\n",
      "quantum circuit. This approach is particularly attractive in the current era of\n",
      "intermediate-scale quantum technology since it allows to optimally pre-process\n",
      "high dimensional data (e.g., images) with any state-of-the-art classical\n",
      "network and to embed a select set of highly informative features into a quantum\n",
      "processor. We present several proof-of-concept examples of the convenient\n",
      "application of quantum transfer learning for image recognition and quantum\n",
      "state classification. We use the cross-platform software library PennyLane to\n",
      "experimentally test a high-resolution image classifier with two different\n",
      "quantum computers, respectively provided by IBM and Rigetti.\n",
      "\n",
      "**Paper Id :2006.14619 \n",
      "Title :Recurrent Quantum Neural Networks\n",
      "  Recurrent neural networks are the foundation of many sequence-to-sequence\n",
      "models in machine learning, such as machine translation and speech synthesis.\n",
      "In contrast, applied quantum computing is in its infancy. Nevertheless there\n",
      "already exist quantum machine learning models such as variational quantum\n",
      "eigensolvers which have been used successfully e.g. in the context of energy\n",
      "minimization tasks. In this work we construct a quantum recurrent neural\n",
      "network (QRNN) with demonstrable performance on non-trivial tasks such as\n",
      "sequence learning and integer digit classification. The QRNN cell is built from\n",
      "parametrized quantum neurons, which, in conjunction with amplitude\n",
      "amplification, create a nonlinear activation of polynomials of its inputs and\n",
      "cell state, and allow the extraction of a probability distribution over\n",
      "predicted classes at each step. To study the model's performance, we provide an\n",
      "implementation in pytorch, which allows the relatively efficient optimization\n",
      "of parametrized quantum circuits with thousands of parameters. We establish a\n",
      "QRNN training setup by benchmarking optimization hyperparameters, and analyse\n",
      "suitable network topologies for simple memorisation and sequence prediction\n",
      "tasks from Elman's seminal paper (1990) on temporal structure learning. We then\n",
      "proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN\n",
      "each image pixel-by-pixel; and by utilising modern data augmentation as\n",
      "preprocessing step. Finally, we analyse to what extent the unitary nature of\n",
      "the network counteracts the vanishing gradient problem that plagues many\n",
      "existing quantum classifiers and classical RNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.08320 \n",
      "Title :Garbage In, Garbage Out? Do Machine Learning Application Papers in\n",
      "  Social Computing Report Where Human-Labeled Training Data Comes From?\n",
      "  Many machine learning projects for new application areas involve teams of\n",
      "humans who label data for a particular purpose, from hiring crowdworkers to the\n",
      "paper's authors labeling the data themselves. Such a task is quite similar to\n",
      "(or a form of) structured content analysis, which is a longstanding methodology\n",
      "in the social sciences and humanities, with many established best practices. In\n",
      "this paper, we investigate to what extent a sample of machine learning\n",
      "application papers in social computing --- specifically papers from ArXiv and\n",
      "traditional publications performing an ML classification task on Twitter data\n",
      "--- give specific details about whether such best practices were followed. Our\n",
      "team conducted multiple rounds of structured content analysis of each paper,\n",
      "making determinations such as: Does the paper report who the labelers were,\n",
      "what their qualifications were, whether they independently labeled the same\n",
      "items, whether inter-rater reliability metrics were disclosed, what level of\n",
      "training and/or instructions were given to labelers, whether compensation for\n",
      "crowdworkers is disclosed, and if the training data is publicly available. We\n",
      "find a wide divergence in whether such practices were followed and documented.\n",
      "Much of machine learning research and education focuses on what is done once a\n",
      "\"gold standard\" of training data is available, but we discuss issues around the\n",
      "equally-important aspect of whether such data is reliable in the first place.\n",
      "\n",
      "**Paper Id :1910.14464 \n",
      "Title :What Question Answering can Learn from Trivia Nerds\n",
      "  In addition to the traditional task of getting machines to answer questions,\n",
      "a major research question in question answering is to create interesting,\n",
      "challenging questions that can help systems learn how to answer questions and\n",
      "also reveal which systems are the best at answering questions. We argue that\n",
      "creating a question answering dataset -- and the ubiquitous leaderboard that\n",
      "goes with it -- closely resembles running a trivia tournament: you write\n",
      "questions, have agents (either humans or machines) answer the questions, and\n",
      "declare a winner. However, the research community has ignored the decades of\n",
      "hard-learned lessons from decades of the trivia community creating vibrant,\n",
      "fair, and effective question answering competitions. After detailing problems\n",
      "with existing QA datasets, we outline the key lessons -- removing ambiguity,\n",
      "discriminating skill, and adjudicating disputes -- that can transfer to QA\n",
      "research and how they might be implemented for the QA community.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.08333 \n",
      "Title :Learning to grow: control of material self-assembly using evolutionary\n",
      "  reinforcement learning\n",
      "  We show that neural networks trained by evolutionary reinforcement learning\n",
      "can enact efficient molecular self-assembly protocols. Presented with molecular\n",
      "simulation trajectories, networks learn to change temperature and chemical\n",
      "potential in order to promote the assembly of desired structures or choose\n",
      "between competing polymorphs. In the first case, networks reproduce in a\n",
      "qualitative sense the results of previously-known protocols, but faster and\n",
      "with higher fidelity; in the second case they identify strategies previously\n",
      "unknown, from which we can extract physical insight. Networks that take as\n",
      "input the elapsed time of the simulation or microscopic information from the\n",
      "system are both effective, the latter more so. The evolutionary scheme we have\n",
      "used is simple to implement and can be applied to a broad range of examples of\n",
      "experimental self-assembly, whether or not one can monitor the experiment as it\n",
      "proceeds. Our results have been achieved with no human input beyond the\n",
      "specification of which order parameter to promote, pointing the way to the\n",
      "design of synthesis protocols by artificial intelligence.\n",
      "\n",
      "**Paper Id :1904.10797 \n",
      "Title :Machine learning for long-distance quantum communication\n",
      "  Machine learning can help us in solving problems in the context big data\n",
      "analysis and classification, as well as in playing complex games such as Go.\n",
      "But can it also be used to find novel protocols and algorithms for applications\n",
      "such as large-scale quantum communication? Here we show that machine learning\n",
      "can be used to identify central quantum protocols, including teleportation,\n",
      "entanglement purification and the quantum repeater. These schemes are of\n",
      "importance in long-distance quantum communication, and their discovery has\n",
      "shaped the field of quantum information processing. However, the usefulness of\n",
      "learning agents goes beyond the mere re-production of known protocols; the same\n",
      "approach allows one to find improved solutions to long-distance communication\n",
      "problems, in particular when dealing with asymmetric situations where channel\n",
      "noise and segment distance are non-uniform. Our findings are based on the use\n",
      "of projective simulation, a model of a learning agent that combines\n",
      "reinforcement learning and decision making in a physically motivated framework.\n",
      "The learning agent is provided with a universal gate set, and the desired task\n",
      "is specified via a reward scheme. From a technical perspective, the learning\n",
      "agent has to deal with stochastic environments and reactions. We utilize an\n",
      "idea reminiscent of hierarchical skill acquisition, where solutions to\n",
      "sub-problems are learned and re-used in the overall scheme. This is of\n",
      "particular importance in the development of long-distance communication\n",
      "schemes, and opens the way for using machine learning in the design and\n",
      "implementation of quantum networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.08791 \n",
      "Title :Forecasting significant stock price changes using neural networks\n",
      "  Stock price prediction is a rich research topic that has attracted interest\n",
      "from various areas of science. The recent success of machine learning in speech\n",
      "and image recognition has prompted researchers to apply these methods to asset\n",
      "price prediction. The majority of literature has been devoted to predicting\n",
      "either the actual asset price or the direction of price movement. In this\n",
      "paper, we study a hitherto little explored question of predicting significant\n",
      "changes in stock price based on previous changes using machine learning\n",
      "algorithms. We are particularly interested in the performance of neural network\n",
      "classifiers in the given context. To this end, we construct and test three\n",
      "neural network models including multi-layer perceptron, convolutional net, and\n",
      "long short term memory net. As benchmark models we use random forest and\n",
      "relative strength index methods. The models are tested using 10-year daily\n",
      "stock price data of four major US public companies. Test results show that\n",
      "predicting significant changes in stock price can be accomplished with a high\n",
      "degree of accuracy. In particular, we obtain substantially better results than\n",
      "similar studies that forecast the direction of price change.\n",
      "\n",
      "**Paper Id :2006.03541 \n",
      "Title :Sentiment Analysis Based on Deep Learning: A Comparative Study\n",
      "  The study of public opinion can provide us with valuable information. The\n",
      "analysis of sentiment on social networks, such as Twitter or Facebook, has\n",
      "become a powerful means of learning about the users' opinions and has a wide\n",
      "range of applications. However, the efficiency and accuracy of sentiment\n",
      "analysis is being hindered by the challenges encountered in natural language\n",
      "processing (NLP). In recent years, it has been demonstrated that deep learning\n",
      "models are a promising solution to the challenges of NLP. This paper reviews\n",
      "the latest studies that have employed deep learning to solve sentiment analysis\n",
      "problems, such as sentiment polarity. Models using term frequency-inverse\n",
      "document frequency (TF-IDF) and word embedding have been applied to a series of\n",
      "datasets. Finally, a comparative study has been conducted on the experimental\n",
      "results obtained for the different models and input features\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.08949 \n",
      "Title :Enhancing streamflow forecast and extracting insights using long-short\n",
      "  term memory networks with data integration at continental scales\n",
      "  Recent observations with varied schedules and types (moving average,\n",
      "snapshot, or regularly spaced) can help to improve streamflow forecasts, but it\n",
      "is challenging to integrate them effectively. Based on a long short-term memory\n",
      "(LSTM) streamflow model, we tested multiple versions of a flexible procedure we\n",
      "call data integration (DI) to leverage recent discharge measurements to improve\n",
      "forecasts. DI accepts lagged inputs either directly or through a convolutional\n",
      "neural network (CNN) unit. DI ubiquitously elevated streamflow forecast\n",
      "performance to unseen levels, reaching a record continental-scale median\n",
      "Nash-Sutcliffe Efficiency coefficient value of 0.86. Integrating moving-average\n",
      "discharge, discharge from the last few days, or even average discharge from the\n",
      "previous calendar month could all improve daily forecasts. Directly using\n",
      "lagged observations as inputs was comparable in performance to using the CNN\n",
      "unit. Importantly, we obtained valuable insights regarding hydrologic processes\n",
      "impacting LSTM and DI performance. Before applying DI, the base LSTM model\n",
      "worked well in mountainous or snow-dominated regions, but less well in regions\n",
      "with low discharge volumes (due to either low precipitation or high\n",
      "precipitation-energy synchronicity) and large inter-annual storage variability.\n",
      "DI was most beneficial in regions with high flow autocorrelation: it greatly\n",
      "reduced baseflow bias in groundwater-dominated western basins and also improved\n",
      "peak prediction for basins with dynamical surface water storage, such as the\n",
      "Prairie Potholes or Great Lakes regions. However, even DI cannot elevate\n",
      "high-aridity basins with one-day flash peaks. Despite this limitation, there is\n",
      "much promise for a deep-learning-based forecast paradigm due to its\n",
      "performance, automation, efficiency, and flexibility.\n",
      "\n",
      "**Paper Id :2010.03561 \n",
      "Title :Ensembling geophysical models with Bayesian Neural Networks\n",
      "  Ensembles of geophysical models improve projection accuracy and express\n",
      "uncertainties. We develop a novel data-driven ensembling strategy for combining\n",
      "geophysical models using Bayesian Neural Networks, which infers\n",
      "spatiotemporally varying model weights and bias while accounting for\n",
      "heteroscedastic uncertainties in the observations. This produces more accurate\n",
      "and uncertainty-aware projections without sacrificing interpretability. Applied\n",
      "to the prediction of total column ozone from an ensemble of 15\n",
      "chemistry-climate models, we find that the Bayesian neural network ensemble\n",
      "(BayNNE) outperforms existing ensembling methods, achieving a 49.4% reduction\n",
      "in RMSE for temporal extrapolation, and a 67.4% reduction in RMSE for polar\n",
      "data voids, compared to a weighted mean. Uncertainty is also\n",
      "well-characterized, with 90.6% of the data points in our extrapolation\n",
      "validation dataset lying within 2 standard deviations and 98.5% within 3\n",
      "standard deviations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.09132 \n",
      "Title :Mean field theory for deep dropout networks: digging up gradient\n",
      "  backpropagation deeply\n",
      "  In recent years, the mean field theory has been applied to the study of\n",
      "neural networks and has achieved a great deal of success. The theory has been\n",
      "applied to various neural network structures, including CNNs, RNNs, Residual\n",
      "networks, and Batch normalization. Inevitably, recent work has also covered the\n",
      "use of dropout. The mean field theory shows that the existence of depth scales\n",
      "that limit the maximum depth of signal propagation and gradient\n",
      "backpropagation. However, the gradient backpropagation is derived under the\n",
      "gradient independence assumption that weights used during feed forward are\n",
      "drawn independently from the ones used in backpropagation. This is not how\n",
      "neural networks are trained in a real setting. Instead, the same weights used\n",
      "in a feed-forward step needs to be carried over to its corresponding\n",
      "backpropagation. Using this realistic condition, we perform theoretical\n",
      "computation on linear dropout networks and a series of experiments on dropout\n",
      "networks. Our empirical results show an interesting phenomenon that the length\n",
      "gradients can backpropagate for a single input and a pair of inputs are\n",
      "governed by the same depth scale. Besides, we study the relationship between\n",
      "variance and mean of statistical metrics of the gradient and shown an emergence\n",
      "of universality. Finally, we investigate the maximum trainable length for deep\n",
      "dropout networks through a series of experiments using MNIST and CIFAR10 and\n",
      "provide a more precise empirical formula that describes the trainable length\n",
      "than original work.\n",
      "\n",
      "**Paper Id :2003.03633 \n",
      "Title :AL2: Progressive Activation Loss for Learning General Representations in\n",
      "  Classification Neural Networks\n",
      "  The large capacity of neural networks enables them to learn complex\n",
      "functions. To avoid overfitting, networks however require a lot of training\n",
      "data that can be expensive and time-consuming to collect. A common practical\n",
      "approach to attenuate overfitting is the use of network regularization\n",
      "techniques. We propose a novel regularization method that progressively\n",
      "penalizes the magnitude of activations during training. The combined activation\n",
      "signals produced by all neurons in a given layer form the representation of the\n",
      "input image in that feature space. We propose to regularize this representation\n",
      "in the last feature layer before classification layers. Our method's effect on\n",
      "generalization is analyzed with label randomization tests and cumulative\n",
      "ablations. Experimental results show the advantages of our approach in\n",
      "comparison with commonly-used regularizers on standard benchmark datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.09588 \n",
      "Title :Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax\n",
      "  The Gumbel-Softmax is a continuous distribution over the simplex that is\n",
      "often used as a relaxation of discrete distributions. Because it can be readily\n",
      "interpreted and easily reparameterized, it enjoys widespread use. We propose a\n",
      "modular and more flexible family of reparameterizable distributions where\n",
      "Gaussian noise is transformed into a one-hot approximation through an\n",
      "invertible function. This invertible function is composed of a modified softmax\n",
      "and can incorporate diverse transformations that serve different specific\n",
      "purposes. For example, the stick-breaking procedure allows us to extend the\n",
      "reparameterization trick to distributions with countably infinite support, thus\n",
      "enabling the use of our distribution along nonparametric models, or normalizing\n",
      "flows let us increase the flexibility of the distribution. Our construction\n",
      "enjoys theoretical advantages over the Gumbel-Softmax, such as closed form KL,\n",
      "and significantly outperforms it in a variety of experiments. Our code is\n",
      "available at https://github.com/cunningham-lab/igr.\n",
      "\n",
      "**Paper Id :1808.07452 \n",
      "Title :Generalized Canonical Polyadic Tensor Decomposition\n",
      "  Tensor decomposition is a fundamental unsupervised machine learning method in\n",
      "data science, with applications including network analysis and sensor data\n",
      "processing. This work develops a generalized canonical polyadic (GCP) low-rank\n",
      "tensor decomposition that allows other loss functions besides squared error.\n",
      "For instance, we can use logistic loss or Kullback-Leibler divergence, enabling\n",
      "tensor decomposition for binary or count data. We present a variety\n",
      "statistically-motivated loss functions for various scenarios. We provide a\n",
      "generalized framework for computing gradients and handling missing data that\n",
      "enables the use of standard optimization methods for fitting the model. We\n",
      "demonstrate the flexibility of GCP on several real-world examples including\n",
      "interactions in a social network, neural activity in a mouse, and monthly\n",
      "rainfall measurements in India.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.09670 \n",
      "Title :Adversarial symmetric GANs: bridging adversarial samples and adversarial\n",
      "  networks\n",
      "  Generative adversarial networks have achieved remarkable performance on\n",
      "various tasks but suffer from training instability. Despite many training\n",
      "strategies proposed to improve training stability, this issue remains as a\n",
      "challenge. In this paper, we investigate the training instability from the\n",
      "perspective of adversarial samples and reveal that adversarial training on fake\n",
      "samples is implemented in vanilla GANs, but adversarial training on real\n",
      "samples has long been overlooked. Consequently, the discriminator is extremely\n",
      "vulnerable to adversarial perturbation and the gradient given by the\n",
      "discriminator contains non-informative adversarial noises, which hinders the\n",
      "generator from catching the pattern of real samples. Here, we develop\n",
      "adversarial symmetric GANs (AS-GANs) that incorporate adversarial training of\n",
      "the discriminator on real samples into vanilla GANs, making adversarial\n",
      "training symmetrical. The discriminator is therefore more robust and provides\n",
      "more informative gradient with less adversarial noise, thereby stabilizing\n",
      "training and accelerating convergence. The effectiveness of the AS-GANs is\n",
      "verified on image generation on CIFAR-10 , CelebA, and LSUN with varied network\n",
      "architectures. Not only the training is more stabilized, but the FID scores of\n",
      "generated samples are consistently improved by a large margin compared to the\n",
      "baseline. The bridging of adversarial samples and adversarial networks provides\n",
      "a new approach to further develop adversarial networks.\n",
      "\n",
      "**Paper Id :2005.02552 \n",
      "Title :Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder\n",
      "  Whereas adversarial training is employed as the main defence strategy against\n",
      "specific adversarial samples, it has limited generalization capability and\n",
      "incurs excessive time complexity. In this paper, we propose an attack-agnostic\n",
      "defence framework to enhance the intrinsic robustness of neural networks,\n",
      "without jeopardizing the ability of generalizing clean samples. Our Feature\n",
      "Pyramid Decoder (FPD) framework applies to all block-based convolutional neural\n",
      "networks (CNNs). It implants denoising and image restoration modules into a\n",
      "targeted CNN, and it also constraints the Lipschitz constant of the\n",
      "classification layer. Moreover, we propose a two-phase strategy to train the\n",
      "FPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\n",
      "multi-task and self-supervised learning. Evaluated against a variety of\n",
      "white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\n",
      "sufficient robustness against general adversarial samples on MNIST, SVHN and\n",
      "CALTECH. In addition, if we further conduct adversarial training, the\n",
      "FPD-enhanced CNNs perform better than their non-enhanced versions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.10360 \n",
      "Title :Safe and Fast Tracking on a Robot Manipulator: Robust MPC and Neural\n",
      "  Network Control\n",
      "  Fast feedback control and safety guarantees are essential in modern robotics.\n",
      "We present an approach that achieves both by combining novel robust model\n",
      "predictive control (MPC) with function approximation via (deep) neural networks\n",
      "(NNs). The result is a new approach for complex tasks with nonlinear,\n",
      "uncertain, and constrained dynamics as are common in robotics. Specifically, we\n",
      "leverage recent results in MPC research to propose a new robust setpoint\n",
      "tracking MPC algorithm, which achieves reliable and safe tracking of a dynamic\n",
      "setpoint while guaranteeing stability and constraint satisfaction. The\n",
      "presented robust MPC scheme constitutes a one-layer approach that unifies the\n",
      "often separated planning and control layers, by directly computing the control\n",
      "command based on a reference and possibly obstacle positions. As a separate\n",
      "contribution, we show how the computation time of the MPC can be drastically\n",
      "reduced by approximating the MPC law with a NN controller. The NN is trained\n",
      "and validated from offline samples of the MPC, yielding statistical guarantees,\n",
      "and used in lieu thereof at run time. Our experiments on a state-of-the-art\n",
      "robot manipulator are the first to show that both the proposed robust and\n",
      "approximate MPC schemes scale to real-world robotic systems.\n",
      "\n",
      "**Paper Id :2001.08092 \n",
      "Title :Local Policy Optimization for Trajectory-Centric Reinforcement Learning\n",
      "  The goal of this paper is to present a method for simultaneous trajectory and\n",
      "local stabilizing policy optimization to generate local policies for\n",
      "trajectory-centric model-based reinforcement learning (MBRL). This is motivated\n",
      "by the fact that global policy optimization for non-linear systems could be a\n",
      "very challenging problem both algorithmically and numerically. However, a lot\n",
      "of robotic manipulation tasks are trajectory-centric, and thus do not require a\n",
      "global model or policy. Due to inaccuracies in the learned model estimates, an\n",
      "open-loop trajectory optimization process mostly results in very poor\n",
      "performance when used on the real system. Motivated by these problems, we try\n",
      "to formulate the problem of trajectory optimization and local policy synthesis\n",
      "as a single optimization problem. It is then solved simultaneously as an\n",
      "instance of nonlinear programming. We provide some results for analysis as well\n",
      "as achieved performance of the proposed technique under some simplifying\n",
      "assumptions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.11037 \n",
      "Title :Unsupervised Domain Adversarial Self-Calibration for\n",
      "  Electromyographic-based Gesture Recognition\n",
      "  Surface electromyography (sEMG) provides an intuitive and non-invasive\n",
      "interface from which to control machines. However, preserving the myoelectric\n",
      "control system's performance over multiple days is challenging, due to the\n",
      "transient nature of the signals obtained with this recording technique. In\n",
      "practice, if the system is to remain usable, a time-consuming and periodic\n",
      "recalibration is necessary. In the case where the sEMG interface is employed\n",
      "every few days, the user might need to do this recalibration before every use.\n",
      "Thus, severely limiting the practicality of such a control method.\n",
      "Consequently, this paper proposes tackling the especially challenging task of\n",
      "unsupervised adaptation of sEMG signals, when multiple days have elapsed\n",
      "between each recording, by introducing Self-Calibrating Asynchronous Domain\n",
      "Adversarial Neural Network (SCADANN). SCADANN is compared with two\n",
      "state-of-the-art self-calibrating algorithms developed specifically for deep\n",
      "learning within the context of EMG-based gesture recognition and three\n",
      "state-of-the-art domain adversarial algorithms. The comparison is made both on\n",
      "an offline and a dynamic dataset (20 participants per dataset), using two\n",
      "different deep network architectures with two different input modalities\n",
      "(temporal-spatial descriptors and spectrograms). Overall, SCADANN is shown to\n",
      "substantially and systematically improves classification performances over no\n",
      "recalibration and obtains the highest average accuracy for all tested cases\n",
      "across all methods.\n",
      "\n",
      "**Paper Id :2001.05922 \n",
      "Title :Continual Learning for Domain Adaptation in Chest X-ray Classification\n",
      "  Over the last years, Deep Learning has been successfully applied to a broad\n",
      "range of medical applications. Especially in the context of chest X-ray\n",
      "classification, results have been reported which are on par, or even superior\n",
      "to experienced radiologists. Despite this success in controlled experimental\n",
      "environments, it has been noted that the ability of Deep Learning models to\n",
      "generalize to data from a new domain (with potentially different tasks) is\n",
      "often limited. In order to address this challenge, we investigate techniques\n",
      "from the field of Continual Learning (CL) including Joint Training (JT),\n",
      "Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using\n",
      "the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that\n",
      "these methods provide promising options to improve the performance of Deep\n",
      "Learning models on a target domain and to mitigate effectively catastrophic\n",
      "forgetting for the source domain. To this end, the best overall performance was\n",
      "obtained using JT, while for LWF competitive results could be achieved - even\n",
      "without accessing data from the source domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.11084 \n",
      "Title :Where Are We? Using Scopus to Map the Literature at the Intersection\n",
      "  Between Artificial Intelligence and Research on Crime\n",
      "  Research on Artificial Intelligence (AI) applications has spread over many\n",
      "scientific disciplines. Scientists have tested the power of intelligent\n",
      "algorithms developed to predict (or learn from) natural, physical and social\n",
      "phenomena. This also applies to crime-related research problems. Nonetheless,\n",
      "studies that map the current state of the art at the intersection between AI\n",
      "and crime are lacking. What are the current research trends in terms of topics\n",
      "in this area? What is the structure of scientific collaboration when\n",
      "considering works investigating criminal issues using machine learning, deep\n",
      "learning, and AI in general? What are the most active countries in this\n",
      "specific scientific sphere? Using data retrieved from the Scopus database, this\n",
      "work quantitatively analyzes 692 published works at the intersection between AI\n",
      "and crime employing network science to respond to these questions. Results show\n",
      "that researchers are mainly focusing on cyber-related criminal topics and that\n",
      "relevant themes such as algorithmic discrimination, fairness, and ethics are\n",
      "considerably overlooked. Furthermore, data highlight the extremely disconnected\n",
      "structure of co-authorship networks. Such disconnectedness may represent a\n",
      "substantial obstacle to a more solid community of scientists interested in\n",
      "these topics. Additionally, the graph of scientific collaboration indicates\n",
      "that countries that are more prone to engage in international partnerships are\n",
      "generally less central in the network. This means that scholars working in\n",
      "highly productive countries (e.g. the United States, China) tend to mostly\n",
      "collaborate domestically. Finally, current issues and future developments\n",
      "within this scientific area are also discussed.\n",
      "\n",
      "**Paper Id :1906.06843 \n",
      "Title :Predicting Research Trends with Semantic and Neural Networks with an\n",
      "  application in Quantum Physics\n",
      "  The vast and growing number of publications in all disciplines of science\n",
      "cannot be comprehended by a single human researcher. As a consequence,\n",
      "researchers have to specialize in narrow sub-disciplines, which makes it\n",
      "challenging to uncover scientific connections beyond the own field of research.\n",
      "Thus access to structured knowledge from a large corpus of publications could\n",
      "help pushing the frontiers of science. Here we demonstrate a method to build a\n",
      "semantic network from published scientific literature, which we call SemNet. We\n",
      "use SemNet to predict future trends in research and to inspire new,\n",
      "personalized and surprising seeds of ideas in science. We apply it in the\n",
      "discipline of quantum physics, which has seen an unprecedented growth of\n",
      "activity in recent years. In SemNet, scientific knowledge is represented as an\n",
      "evolving network using the content of 750,000 scientific papers published since\n",
      "1919. The nodes of the network correspond to physical concepts, and links\n",
      "between two nodes are drawn when two physical concepts are concurrently studied\n",
      "in research articles. We identify influential and prize-winning research topics\n",
      "from the past inside SemNet thus confirm that it stores useful semantic\n",
      "knowledge. We train a deep neural network using states of SemNet of the past,\n",
      "to predict future developments in quantum physics research, and confirm high\n",
      "quality predictions using historic data. With the neural network and\n",
      "theoretical network tools we are able to suggest new, personalized,\n",
      "out-of-the-box ideas, by identifying pairs of concepts which have unique and\n",
      "extremal semantic network properties. Finally, we consider possible future\n",
      "developments and implications of our findings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.12322 \n",
      "Title :Deep reinforcement learning for complex evaluation of one-loop diagrams\n",
      "  in quantum field theory\n",
      "  In this paper we present a novel technique based on deep reinforcement\n",
      "learning that allows for numerical analytic continuation of integrals that are\n",
      "often encountered in one-loop diagrams in quantum field theory. In order to\n",
      "extract certain quantities of two-point functions, such as spectral densities,\n",
      "mass poles or multi-particle thresholds, it is necessary to perform an analytic\n",
      "continuation of the correlator in question. At one-loop level in Euclidean\n",
      "space, this results in the necessity to deform the integration contour of the\n",
      "loop integral in the complex plane of the square of the loop momentum, in order\n",
      "to avoid non-analyticities in the integration plane. Using a toy model for\n",
      "which an exact solution is known, we train a reinforcement learning agent to\n",
      "perform the required contour deformations. Our study shows great promise for an\n",
      "agent to be deployed in iterative numerical approaches used to compute\n",
      "non-perturbative 2-point functions, such as the quark propagator\n",
      "Dyson-Schwinger equation, or more generally, Fredholm equations of the second\n",
      "kind, in the complex domain.\n",
      "\n",
      "**Paper Id :1911.07571 \n",
      "Title :Casimir effect with machine learning\n",
      "  Vacuum fluctuations of quantum fields between physical objects depend on the\n",
      "shapes, positions, and internal composition of the latter. For objects of\n",
      "arbitrary shapes, even made from idealized materials, the calculation of the\n",
      "associated zero-point (Casimir) energy is an analytically intractable\n",
      "challenge. We propose a new numerical approach to this problem based on\n",
      "machine-learning techniques and illustrate the effectiveness of the method in a\n",
      "(2+1) dimensional scalar field theory. The Casimir energy is first calculated\n",
      "numerically using a Monte-Carlo algorithm for a set of the Dirichlet boundaries\n",
      "of various shapes. Then, a neural network is trained to compute this energy\n",
      "given the Dirichlet domain, treating the latter as black-and-white pixelated\n",
      "images. We show that after the learning phase, the neural network is able to\n",
      "quickly predict the Casimir energy for new boundaries of general shapes with\n",
      "reasonable accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.12576 \n",
      "Title :Privacy-Preserving Public Release of Datasets for Support Vector Machine\n",
      "  Classification\n",
      "  We consider the problem of publicly releasing a dataset for support vector\n",
      "machine classification while not infringing on the privacy of data subjects\n",
      "(i.e., individuals whose private information is stored in the dataset). The\n",
      "dataset is systematically obfuscated using an additive noise for privacy\n",
      "protection. Motivated by the Cramer-Rao bound, inverse of the trace of the\n",
      "Fisher information matrix is used as a measure of the privacy. Conditions are\n",
      "established for ensuring that the classifier extracted from the original\n",
      "dataset and the obfuscated one are close to each other (capturing the utility).\n",
      "The optimal noise distribution is determined by maximizing a weighted sum of\n",
      "the measures of privacy and utility. The optimal privacy-preserving noise is\n",
      "proved to achieve local differential privacy. The results are generalized to a\n",
      "broader class of optimization-based supervised machine learning algorithms.\n",
      "Applicability of the methodology is demonstrated on multiple datasets.\n",
      "\n",
      "**Paper Id :1906.09679 \n",
      "Title :The Value of Collaboration in Convex Machine Learning with Differential\n",
      "  Privacy\n",
      "  In this paper, we apply machine learning to distributed private data owned by\n",
      "multiple data owners, entities with access to non-overlapping training\n",
      "datasets. We use noisy, differentially-private gradients to minimize the\n",
      "fitness cost of the machine learning model using stochastic gradient descent.\n",
      "We quantify the quality of the trained model, using the fitness cost, as a\n",
      "function of privacy budget and size of the distributed datasets to capture the\n",
      "trade-off between privacy and utility in machine learning. This way, we can\n",
      "predict the outcome of collaboration among privacy-aware data owners prior to\n",
      "executing potentially computationally-expensive machine learning algorithms.\n",
      "Particularly, we show that the difference between the fitness of the trained\n",
      "machine learning model using differentially-private gradient queries and the\n",
      "fitness of the trained machine model in the absence of any privacy concerns is\n",
      "inversely proportional to the size of the training datasets squared and the\n",
      "privacy budget squared. We successfully validate the performance prediction\n",
      "with the actual performance of the proposed privacy-aware learning algorithms,\n",
      "applied to: financial datasets for determining interest rates of loans using\n",
      "regression; and detecting credit card frauds using support vector machines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :1912.13163 \n",
      "Title :Federated Learning with Cooperating Devices: A Consensus Approach for\n",
      "  Massive IoT Networks\n",
      "  Federated learning (FL) is emerging as a new paradigm to train machine\n",
      "learning models in distributed systems. Rather than sharing, and disclosing,\n",
      "the training dataset with the server, the model parameters (e.g. neural\n",
      "networks weights and biases) are optimized collectively by large populations of\n",
      "interconnected devices, acting as local learners. FL can be applied to\n",
      "power-constrained IoT devices with slow and sporadic connections. In addition,\n",
      "it does not need data to be exported to third parties, preserving privacy.\n",
      "Despite these benefits, a main limit of existing approaches is the centralized\n",
      "optimization which relies on a server for aggregation and fusion of local\n",
      "parameters; this has the drawback of a single point of failure and scaling\n",
      "issues for increasing network size. The paper proposes a fully distributed (or\n",
      "server-less) learning approach: the proposed FL algorithms leverage the\n",
      "cooperation of devices that perform data operations inside the network by\n",
      "iterating local computations and mutual interactions via consensus-based\n",
      "methods. The approach lays the groundwork for integration of FL within 5G and\n",
      "beyond networks characterized by decentralized connectivity and computing, with\n",
      "intelligence distributed over the end-devices. The proposed methodology is\n",
      "verified by experimental datasets collected inside an industrial IoT\n",
      "environment.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.00254 \n",
      "Title :A Comprehensive and Modularized Statistical Framework for Gradient Norm\n",
      "  Equality in Deep Neural Networks\n",
      "  In recent years, plenty of metrics have been proposed to identify networks\n",
      "that are free of gradient explosion and vanishing. However, due to the\n",
      "diversity of network components and complex serial-parallel hybrid connections\n",
      "in modern DNNs, the evaluation of existing metrics usually requires strong\n",
      "assumptions, complex statistical analysis, or has limited application fields,\n",
      "which constraints their spread in the community. In this paper, inspired by the\n",
      "Gradient Norm Equality and dynamical isometry, we first propose a novel metric\n",
      "called Block Dynamical Isometry, which measures the change of gradient norm in\n",
      "individual block. Because our Block Dynamical Isometry is norm-based, its\n",
      "evaluation needs weaker assumptions compared with the original dynamical\n",
      "isometry. To mitigate the challenging derivation, we propose a highly\n",
      "modularized statistical framework based on free probability. Our framework\n",
      "includes several key theorems to handle complex serial-parallel hybrid\n",
      "connections and a library to cover the diversity of network components.\n",
      "Besides, several sufficient prerequisites are provided. Powered by our metric\n",
      "and framework, we analyze extensive initialization, normalization, and network\n",
      "structures. We find that Gradient Norm Equality is a universal philosophy\n",
      "behind them. Then, we improve some existing methods based on our analysis,\n",
      "including an activation function selection strategy for initialization\n",
      "techniques, a new configuration for weight normalization, and a depth-aware way\n",
      "to derive coefficients in SeLU. Moreover, we propose a novel normalization\n",
      "technique named second moment normalization, which is theoretically 30% faster\n",
      "than batch normalization without accuracy loss. Last but not least, our\n",
      "conclusions and methods are evidenced by extensive experiments on multiple\n",
      "models over CIFAR10 and ImageNet.\n",
      "\n",
      "**Paper Id :1912.02605 \n",
      "Title :Towards Understanding Residual and Dilated Dense Neural Networks via\n",
      "  Convolutional Sparse Coding\n",
      "  Convolutional neural network (CNN) and its variants have led to many\n",
      "state-of-art results in various fields. However, a clear theoretical\n",
      "understanding about them is still lacking. Recently, multi-layer convolutional\n",
      "sparse coding (ML-CSC) has been proposed and proved to equal such simply\n",
      "stacked networks (plain networks). Here, we think three factors in each layer\n",
      "of it including the initialization, the dictionary design and the number of\n",
      "iterations greatly affect the performance of ML-CSC. Inspired by these\n",
      "considerations, we propose two novel multi-layer models--residual convolutional\n",
      "sparse coding model (Res-CSC) and mixed-scale dense convolutional sparse coding\n",
      "model (MSD-CSC), which have close relationship with the residual neural network\n",
      "(ResNet) and mixed-scale (dilated) dense neural network (MSDNet), respectively.\n",
      "Mathematically, we derive the shortcut connection in ResNet as a special case\n",
      "of a new forward propagation rule on ML-CSC. We find a theoretical\n",
      "interpretation of the dilated convolution and dense connection in MSDNet by\n",
      "analyzing MSD-CSC, which gives a clear mathematical understanding about them.\n",
      "We implement the iterative soft thresholding algorithm (ISTA) and its fast\n",
      "version to solve Res-CSC and MSD-CSC, which can employ the unfolding operation\n",
      "for further improvements. At last, extensive numerical experiments and\n",
      "comparison with competing methods demonstrate their effectiveness using three\n",
      "typical datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.00288 \n",
      "Title :Online Similarity Learning with Feedback for Invoice Line Item Matching\n",
      "  The procure to pay process (P2P) in large enterprises is a back-end business\n",
      "process which deals with the procurement of products and services for\n",
      "enterprise operations. Procurement is done by issuing purchase orders to\n",
      "impaneled vendors and invoices submitted by vendors are paid after they go\n",
      "through a rigorous validation process. Agents orchestrating P2P process often\n",
      "encounter the problem of matching a product or service descriptions in the\n",
      "invoice to those in purchase order and verify if the ordered items are what\n",
      "have been supplied or serviced. For example, the description in the invoice and\n",
      "purchase order could be TRES 739mL CD KER Smooth and TRES 0.739L CD KER Smth\n",
      "which look different at word level but refer to the same item. In a typical P2P\n",
      "process, agents are asked to manually select the products which are similar\n",
      "before invoices are posted for payment. This step in the business process is\n",
      "manual, repetitive, cumbersome, and costly. Since descriptions are not\n",
      "well-formed sentences, we cannot apply existing semantic and syntactic text\n",
      "similarity approaches directly. In this paper, we present two approaches to\n",
      "solve the above problem using various types of available agent's recorded\n",
      "feedback data. If the agent's feedback is in the form of a relative ranking\n",
      "between descriptions, we use similarity ranking algorithm. If the agent's\n",
      "feedback is absolute such as match or no-match, we use classification\n",
      "similarity algorithm. We also present the threats to the validity of our\n",
      "approach and present a possible remedy making use of product taxonomy and\n",
      "catalog. We showcase the comparative effectiveness and efficiency of the\n",
      "proposed approaches over many benchmarks and real-world data sets.\n",
      "\n",
      "**Paper Id :1911.01225 \n",
      "Title :Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale\n",
      "  Service Environment\n",
      "  Root cause analysis in a large-scale production environment is challenging\n",
      "due to the complexity of services running across global data centers. Due to\n",
      "the distributed nature of a large-scale system, the various hardware, software,\n",
      "and tooling logs are often maintained separately, making it difficult to review\n",
      "the logs jointly for understanding production issues. Another challenge in\n",
      "reviewing the logs for identifying issues is the scale - there could easily be\n",
      "millions of entities, each described by hundreds of features. In this paper we\n",
      "present a fast dimensional analysis framework that automates the root cause\n",
      "analysis on structured logs with improved scalability.\n",
      "  We first explore item-sets, i.e. combinations of feature values, that could\n",
      "identify groups of samples with sufficient support for the target failures\n",
      "using the Apriori algorithm and a subsequent improvement, FP-Growth. These\n",
      "algorithms were designed for frequent item-set mining and association rule\n",
      "learning over transactional databases. After applying them on structured logs,\n",
      "we select the item-sets that are most unique to the target failures based on\n",
      "lift. We propose pre-processing steps with the use of a large-scale real-time\n",
      "database and post-processing techniques and parallelism to further speed up the\n",
      "analysis and improve interpretability, and demonstrate that such optimization\n",
      "is necessary for handling large-scale production datasets. We have successfully\n",
      "rolled out this approach for root cause investigation purposes in a large-scale\n",
      "infrastructure. We also present the setup and results from multiple production\n",
      "use cases in this paper.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.00329 \n",
      "Title :On Consequentialism and Fairness\n",
      "  Recent work on fairness in machine learning has primarily emphasized how to\n",
      "define, quantify, and encourage \"fair\" outcomes. Less attention has been paid,\n",
      "however, to the ethical foundations which underlie such efforts. Among the\n",
      "ethical perspectives that should be taken into consideration is\n",
      "consequentialism, the position that, roughly speaking, outcomes are all that\n",
      "matter. Although consequentialism is not free from difficulties, and although\n",
      "it does not necessarily provide a tractable way of choosing actions (because of\n",
      "the combined problems of uncertainty, subjectivity, and aggregation), it\n",
      "nevertheless provides a powerful foundation from which to critique the existing\n",
      "literature on machine learning fairness. Moreover, it brings to the fore some\n",
      "of the tradeoffs involved, including the problem of who counts, the pros and\n",
      "cons of using a policy, and the relative value of the distant future. In this\n",
      "paper we provide a consequentialist critique of common definitions of fairness\n",
      "within machine learning, as well as a machine learning perspective on\n",
      "consequentialism. We conclude with a broader discussion of the issues of\n",
      "learning and randomization, which have important implications for the ethics of\n",
      "automated decision making systems.\n",
      "\n",
      "**Paper Id :2008.01916 \n",
      "Title :More Than Privacy: Applying Differential Privacy in Key Areas of\n",
      "  Artificial Intelligence\n",
      "  Artificial Intelligence (AI) has attracted a great deal of attention in\n",
      "recent years. However, alongside all its advancements, problems have also\n",
      "emerged, such as privacy violations, security issues and model fairness.\n",
      "Differential privacy, as a promising mathematical model, has several attractive\n",
      "properties that can help solve these problems, making it quite a valuable tool.\n",
      "For this reason, differential privacy has been broadly applied in AI but to\n",
      "date, no study has documented which differential privacy mechanisms can or have\n",
      "been leveraged to overcome its issues or the properties that make this\n",
      "possible. In this paper, we show that differential privacy can do more than\n",
      "just privacy preservation. It can also be used to improve security, stabilize\n",
      "learning, build fair models, and impose composition in selected areas of AI.\n",
      "With a focus on regular machine learning, distributed machine learning, deep\n",
      "learning, and multi-agent systems, the purpose of this article is to deliver a\n",
      "new view on many possibilities for improving AI performance with differential\n",
      "privacy techniques.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.00781 \n",
      "Title :On the comparability of Pre-trained Language Models\n",
      "  Recent developments in unsupervised representation learning have successfully\n",
      "established the concept of transfer learning in NLP. Mainly three forces are\n",
      "driving the improvements in this area of research: More elaborated\n",
      "architectures are making better use of contextual information. Instead of\n",
      "simply plugging in static pre-trained representations, these are learned based\n",
      "on surrounding context in end-to-end trainable models with more intelligently\n",
      "designed language modelling objectives. Along with this, larger corpora are\n",
      "used as resources for pre-training large language models in a self-supervised\n",
      "fashion which are afterwards fine-tuned on supervised tasks. Advances in\n",
      "parallel computing as well as in cloud computing, made it possible to train\n",
      "these models with growing capacities in the same or even in shorter time than\n",
      "previously established models. These three developments agglomerate in new\n",
      "state-of-the-art (SOTA) results being revealed in a higher and higher\n",
      "frequency. It is not always obvious where these improvements originate from, as\n",
      "it is not possible to completely disentangle the contributions of the three\n",
      "driving forces. We set ourselves to providing a clear and concise overview on\n",
      "several large pre-trained language models, which achieved SOTA results in the\n",
      "last two years, with respect to their use of new architectures and resources.\n",
      "We want to clarify for the reader where the differences between the models are\n",
      "and we furthermore attempt to gain some insight into the single contributions\n",
      "of lexical/computational improvements as well as of architectural changes. We\n",
      "explicitly do not intend to quantify these contributions, but rather see our\n",
      "work as an overview in order to identify potential starting points for\n",
      "benchmark comparisons. Furthermore, we tentatively want to point at potential\n",
      "possibilities for improvement in the field of open-sourcing and reproducible\n",
      "research.\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.01520 \n",
      "Title :Combining data assimilation and machine learning to emulate a dynamical\n",
      "  model from sparse and noisy observations: a case study with the Lorenz 96\n",
      "  model\n",
      "  A novel method, based on the combination of data assimilation and machine\n",
      "learning is introduced. The new hybrid approach is designed for a two-fold\n",
      "scope: (i) emulating hidden, possibly chaotic, dynamics and (ii) predicting\n",
      "their future states. The method consists in applying iteratively a data\n",
      "assimilation step, here an ensemble Kalman filter, and a neural network. Data\n",
      "assimilation is used to optimally combine a surrogate model with sparse noisy\n",
      "data. The output analysis is spatially complete and is used as a training set\n",
      "by the neural network to update the surrogate model. The two steps are then\n",
      "repeated iteratively. Numerical experiments have been carried out using the\n",
      "chaotic 40-variables Lorenz 96 model, proving both convergence and statistical\n",
      "skill of the proposed hybrid approach. The surrogate model shows short-term\n",
      "forecast skill up to two Lyapunov times, the retrieval of positive Lyapunov\n",
      "exponents as well as the more energetic frequencies of the power density\n",
      "spectrum. The sensitivity of the method to critical setup parameters is also\n",
      "presented: the forecast skill decreases smoothly with increased observational\n",
      "noise but drops abruptly if less than half of the model domain is observed. The\n",
      "successful synergy between data assimilation and machine learning, proven here\n",
      "with a low-dimensional system, encourages further investigation of such hybrids\n",
      "with more sophisticated dynamics.\n",
      "\n",
      "**Paper Id :2006.03859 \n",
      "Title :Online learning of both state and dynamics using ensemble Kalman filters\n",
      "  The reconstruction of the dynamics of an observed physical system as a\n",
      "surrogate model has been brought to the fore by recent advances in machine\n",
      "learning. To deal with partial and noisy observations in that endeavor, machine\n",
      "learning representations of the surrogate model can be used within a Bayesian\n",
      "data assimilation framework. However, these approaches require to consider long\n",
      "time series of observational data, meant to be assimilated all together. This\n",
      "paper investigates the possibility to learn both the dynamics and the state\n",
      "online, i.e. to update their estimates at any time, in particular when new\n",
      "observations are acquired. The estimation is based on the ensemble Kalman\n",
      "filter (EnKF) family of algorithms using a rather simple representation for the\n",
      "surrogate model and state augmentation. We consider the implication of learning\n",
      "dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an\n",
      "iterative EnKF and we discuss in each case issues and algorithmic solutions. We\n",
      "then demonstrate numerically the efficiency and assess the accuracy of these\n",
      "methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.01828 \n",
      "Title :Listwise Learning to Rank by Exploring Unique Ratings\n",
      "  In this paper, we propose new listwise learning-to-rank models that mitigate\n",
      "the shortcomings of existing ones. Existing listwise learning-to-rank models\n",
      "are generally derived from the classical Plackett-Luce model, which has three\n",
      "major limitations. (1) Its permutation probabilities overlook ties, i.e., a\n",
      "situation when more than one document has the same rating with respect to a\n",
      "query. This can lead to imprecise permutation probabilities and inefficient\n",
      "training because of selecting documents one by one. (2) It does not favor\n",
      "documents having high relevance. (3) It has a loose assumption that sampling\n",
      "documents at different steps is independent. To overcome the first two\n",
      "limitations, we model ranking as selecting documents from a candidate set based\n",
      "on unique rating levels in decreasing order. The number of steps in training is\n",
      "determined by the number of unique rating levels. We propose a new loss\n",
      "function and associated four models for the entire sequence of weighted\n",
      "classification tasks by assigning high weights to the selected documents with\n",
      "high ratings for optimizing Normalized Discounted Cumulative Gain (NDCG). To\n",
      "overcome the final limitation, we further propose a novel and efficient way of\n",
      "refining prediction scores by combining an adapted Vanilla Recurrent Neural\n",
      "Network (RNN) model with pooling given selected documents at previous steps. We\n",
      "encode all of the documents already selected by an RNN model. In a single step,\n",
      "we rank all of the documents with the same ratings using the last cell of the\n",
      "RNN multiple times. We have implemented our models using three settings: neural\n",
      "networks, neural networks with gradient boosting, and regression trees with\n",
      "gradient boosting. We have conducted experiments on four public datasets. The\n",
      "experiments demonstrate that the models notably outperform state-of-the-art\n",
      "learning-to-rank models.\n",
      "\n",
      "**Paper Id :1905.10702 \n",
      "Title :MDE: Multiple Distance Embeddings for Link Prediction in Knowledge\n",
      "  Graphs\n",
      "  Over the past decade, knowledge graphs became popular for capturing\n",
      "structured domain knowledge. Relational learning models enable the prediction\n",
      "of missing links inside knowledge graphs. More specifically, latent distance\n",
      "approaches model the relationships among entities via a distance between latent\n",
      "representations. Translating embedding models (e.g., TransE) are among the most\n",
      "popular latent distance approaches which use one distance function to learn\n",
      "multiple relation patterns. However, they are mostly inefficient in capturing\n",
      "symmetric relations since the representation vector norm for all the symmetric\n",
      "relations becomes equal to zero. They also lose information when learning\n",
      "relations with reflexive patterns since they become symmetric and transitive.\n",
      "We propose the Multiple Distance Embedding model (MDE) that addresses these\n",
      "limitations and a framework to collaboratively combine variant latent\n",
      "distance-based terms. Our solution is based on two principles: 1) we use a\n",
      "limit-based loss instead of a margin ranking loss and, 2) by learning\n",
      "independent embedding vectors for each of the terms we can collectively train\n",
      "and predict using contradicting distance terms. We further demonstrate that MDE\n",
      "allows modeling relations with (anti)symmetry, inversion, and composition\n",
      "patterns. We propose MDE as a neural network model that allows us to map\n",
      "non-linear relations between the embedding vectors and the expected output of\n",
      "the score function. Our empirical results show that MDE performs competitively\n",
      "to state-of-the-art embedding models on several benchmark datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.01982 \n",
      "Title :Intrinsic Motivation and Episodic Memories for Robot Exploration of\n",
      "  High-Dimensional Sensory Spaces\n",
      "  This work presents an architecture that generates curiosity-driven\n",
      "goal-directed exploration behaviours for an image sensor of a microfarming\n",
      "robot. A combination of deep neural networks for offline unsupervised learning\n",
      "of low-dimensional features from images, and of online learning of shallow\n",
      "neural networks representing the inverse and forward kinematics of the system\n",
      "have been used. The artificial curiosity system assigns interest values to a\n",
      "set of pre-defined goals, and drives the exploration towards those that are\n",
      "expected to maximise the learning progress. We propose the integration of an\n",
      "episodic memory in intrinsic motivation systems to face catastrophic forgetting\n",
      "issues, typically experienced when performing online updates of artificial\n",
      "neural networks. Our results show that adopting an episodic memory system not\n",
      "only prevents the computational models from quickly forgetting knowledge that\n",
      "has been previously acquired, but also provides new avenues for modulating the\n",
      "balance between plasticity and stability of the models.\n",
      "\n",
      "**Paper Id :2006.05832 \n",
      "Title :Adaptive Reinforcement Learning through Evolving Self-Modifying Neural\n",
      "  Networks\n",
      "  The adaptive learning capabilities seen in biological neural networks are\n",
      "largely a product of the self-modifying behavior emerging from online plastic\n",
      "changes in synaptic connectivity. Current methods in Reinforcement Learning\n",
      "(RL) only adjust to new interactions after reflection over a specified time\n",
      "interval, preventing the emergence of online adaptivity. Recent work addressing\n",
      "this by endowing artificial neural networks with neuromodulated plasticity have\n",
      "been shown to improve performance on simple RL tasks trained using\n",
      "backpropagation, but have yet to scale up to larger problems. Here we study the\n",
      "problem of meta-learning in a challenging quadruped domain, where each leg of\n",
      "the quadruped has a chance of becoming unusable, requiring the agent to adapt\n",
      "by continuing locomotion with the remaining limbs. Results demonstrate that\n",
      "agents evolved using self-modifying plastic networks are more capable of\n",
      "adapting to complex meta-learning learning tasks, even outperforming the same\n",
      "network updated using gradient-based algorithms while taking less time to\n",
      "train.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.02469 \n",
      "Title :Limited Angle Tomography for Transmission X-Ray Microscopy Using Deep\n",
      "  Learning\n",
      "  In transmission X-ray microscopy (TXM) systems, the rotation of a scanned\n",
      "sample might be restricted to a limited angular range to avoid collision to\n",
      "other system parts or high attenuation at certain tilting angles. Image\n",
      "reconstruction from such limited angle data suffers from artifacts due to\n",
      "missing data. In this work, deep learning is applied to limited angle\n",
      "reconstruction in TXMs for the first time. With the challenge to obtain\n",
      "sufficient real data for training, training a deep neural network from\n",
      "synthetic data is investigated. Particularly, the U-Net, the state-of-the-art\n",
      "neural network in biomedical imaging, is trained from synthetic ellipsoid data\n",
      "and multi-category data to reduce artifacts in filtered back-projection (FBP)\n",
      "reconstruction images. The proposed method is evaluated on synthetic data and\n",
      "real scanned chlorella data in $100^\\circ$ limited angle tomography. For\n",
      "synthetic test data, the U-Net significantly reduces root-mean-square error\n",
      "(RMSE) from $2.55 \\times 10^{-3}$ {\\mu}m$^{-1}$ in the FBP reconstruction to\n",
      "$1.21 \\times 10^{-3}$ {\\mu}m$^{-1}$ in the U-Net reconstruction, and also\n",
      "improves structural similarity (SSIM) index from 0.625 to 0.920. With penalized\n",
      "weighted least square denoising of measured projections, the RMSE and SSIM are\n",
      "further improved to $1.16 \\times 10^{-3}$ {\\mu}m$^{-1}$ and 0.932,\n",
      "respectively. For real test data, the proposed method remarkably improves the\n",
      "3-D visualization of the subcellular structures in the chlorella cell, which\n",
      "indicates its important value for nano-scale imaging in biology, nanoscience\n",
      "and materials science.\n",
      "\n",
      "**Paper Id :2005.03945 \n",
      "Title :Inferring Vector Magnetic Fields from Stokes Profiles of GST/NIRIS Using\n",
      "  a Convolutional Neural Network\n",
      "  We propose a new machine learning approach to Stokes inversion based on a\n",
      "convolutional neural network (CNN) and the Milne-Eddington (ME) method. The\n",
      "Stokes measurements used in this study were taken by the Near InfraRed Imaging\n",
      "Spectropolarimeter (NIRIS) on the 1.6 m Goode Solar Telescope (GST) at the Big\n",
      "Bear Solar Observatory. By learning the latent patterns in the training data\n",
      "prepared by the physics-based ME tool, the proposed CNN method is able to infer\n",
      "vector magnetic fields from the Stokes profiles of GST/NIRIS. Experimental\n",
      "results show that our CNN method produces smoother and cleaner magnetic maps\n",
      "than the widely used ME method. Furthermore, the CNN method is 4~6 times faster\n",
      "than the ME method, and is able to produce vector magnetic fields in near\n",
      "real-time, which is essential to space weather forecasting. Specifically, it\n",
      "takes ~50 seconds for the CNN method to process an image of 720 x 720 pixels\n",
      "comprising Stokes profiles of GST/NIRIS. Finally, the CNN-inferred results are\n",
      "highly correlated to the ME-calculated results and are closer to the ME's\n",
      "results with the Pearson product-moment correlation coefficient (PPMCC) being\n",
      "closer to 1 on average than those from other machine learning algorithms such\n",
      "as multiple support vector regression and multilayer perceptrons (MLP). In\n",
      "particular, the CNN method outperforms the current best machine learning method\n",
      "(MLP) by 2.6% on average in PPMCC according to our experimental study. Thus,\n",
      "the proposed physics-assisted deep learning-based CNN tool can be considered as\n",
      "an alternative, efficient method for Stokes inversion for high resolution\n",
      "polarimetric observations obtained by GST/NIRIS.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.02568 \n",
      "Title :A Group Norm Regularized Factorization Model for Subspace Segmentation\n",
      "  Subspace segmentation assumes that data comes from the union of different\n",
      "subspaces and the purpose of segmentation is to partition the data into the\n",
      "corresponding subspace. Low-rank representation (LRR) is a classic\n",
      "spectral-type method for solving subspace segmentation problems, that is, one\n",
      "first obtains an affinity matrix by solving a LRR model and then performs\n",
      "spectral clustering for segmentation. This paper proposes a group norm\n",
      "regularized factorization model (GNRFM) inspired by the LRR model for subspace\n",
      "segmentation and then designs an Accelerated Augmented Lagrangian Method (AALM)\n",
      "algorithm to solve this model. Specifically, we adopt group norm regularization\n",
      "to make the columns of the factor matrix sparse, thereby achieving a purpose of\n",
      "low rank, which means no Singular Value Decompositions (SVD) are required and\n",
      "the computational complexity of each step is greatly reduced. We obtain\n",
      "affinity matrices by using different LRR models and then performing cluster\n",
      "testing on different sets of synthetic noisy data and real data, respectively.\n",
      "Compared with traditional models and algorithms, the proposed method is faster\n",
      "and more robust to noise, so the final clustering results are better. Moreover,\n",
      "the numerical results show that our algorithm converges fast and only requires\n",
      "approximately ten iterations.\n",
      "\n",
      "**Paper Id :1901.09178 \n",
      "Title :A general model for plane-based clustering with loss function\n",
      "  In this paper, we propose a general model for plane-based clustering. The\n",
      "general model contains many existing plane-based clustering methods, e.g.,\n",
      "k-plane clustering (kPC), proximal plane clustering (PPC), twin support vector\n",
      "clustering (TWSVC) and its extensions. Under this general model, one may obtain\n",
      "an appropriate clustering method for specific purpose. The general model is a\n",
      "procedure corresponding to an optimization problem, where the optimization\n",
      "problem minimizes the total loss of the samples. Thereinto, the loss of a\n",
      "sample derives from both within-cluster and between-cluster. In theory, the\n",
      "termination conditions are discussed, and we prove that the general model\n",
      "terminates in a finite number of steps at a local or weak local optimal point.\n",
      "Furthermore, based on this general model, we propose a plane-based clustering\n",
      "method by introducing a new loss function to capture the data distribution\n",
      "precisely. Experimental results on artificial and public available datasets\n",
      "verify the effectiveness of the proposed method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.03025 \n",
      "Title :Deep Time-Stream Framework for Click-Through Rate Prediction by Tracking\n",
      "  Interest Evolution\n",
      "  Click-through rate (CTR) prediction is an essential task in industrial\n",
      "applications such as video recommendation. Recently, deep learning models have\n",
      "been proposed to learn the representation of users' overall interests, while\n",
      "ignoring the fact that interests may dynamically change over time. We argue\n",
      "that it is necessary to consider the continuous-time information in CTR models\n",
      "to track user interest trend from rich historical behaviors. In this paper, we\n",
      "propose a novel Deep Time-Stream framework (DTS) which introduces the time\n",
      "information by an ordinary differential equations (ODE). DTS continuously\n",
      "models the evolution of interests using a neural network, and thus is able to\n",
      "tackle the challenge of dynamically representing users' interests based on\n",
      "their historical behaviors. In addition, our framework can be seamlessly\n",
      "applied to any existing deep CTR models by leveraging the additional\n",
      "Time-Stream Module, while no changes are made to the original CTR models.\n",
      "Experiments on public dataset as well as real industry dataset with billions of\n",
      "samples demonstrate the effectiveness of proposed approaches, which achieve\n",
      "superior performance compared with existing methods.\n",
      "\n",
      "**Paper Id :2005.08598 \n",
      "Title :Sequential Recommender via Time-aware Attentive Memory Network\n",
      "  Recommendation systems aim to assist users to discover most preferred\n",
      "contents from an ever-growing corpus of items. Although recommenders have been\n",
      "greatly improved by deep learning, they still faces several challenges: (1)\n",
      "Behaviors are much more complex than words in sentences, so traditional\n",
      "attentive and recurrent models may fail in capturing the temporal dynamics of\n",
      "user preferences. (2) The preferences of users are multiple and evolving, so it\n",
      "is difficult to integrate long-term memory and short-term intent.\n",
      "  In this paper, we propose a temporal gating methodology to improve attention\n",
      "mechanism and recurrent units, so that temporal information can be considered\n",
      "in both information filtering and state transition. Additionally, we propose a\n",
      "Multi-hop Time-aware Attentive Memory network (MTAM) to integrate long-term and\n",
      "short-term preferences. We use the proposed time-aware GRU network to learn the\n",
      "short-term intent and maintain prior records in user memory. We treat the\n",
      "short-term intent as a query and design a multi-hop memory reading operation\n",
      "via the proposed time-aware attention to generate user representation based on\n",
      "the current intent and long-term memory. Our approach is scalable for candidate\n",
      "retrieval tasks and can be viewed as a non-linear generalization of latent\n",
      "factorization for dot-product based Top-K recommendation. Finally, we conduct\n",
      "extensive experiments on six benchmark datasets and the experimental results\n",
      "demonstrate the effectiveness of our MTAM and temporal gating methodology.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.03354 \n",
      "Title :Learning credit assignment\n",
      "  Deep learning has achieved impressive prediction accuracies in a variety of\n",
      "scientific and industrial domains. However, the nested non-linear feature of\n",
      "deep learning makes the learning highly non-transparent, i.e., it is still\n",
      "unknown how the learning coordinates a huge number of parameters to achieve a\n",
      "decision making. To explain this hierarchical credit assignment, we propose a\n",
      "mean-field learning model by assuming that an ensemble of sub-networks, rather\n",
      "than a single network, are trained for a classification task. Surprisingly, our\n",
      "model reveals that apart from some deterministic synaptic weights connecting\n",
      "two neurons at neighboring layers, there exist a large number of connections\n",
      "that can be absent, and other connections can allow for a broad distribution of\n",
      "their weight values. Therefore, synaptic connections can be classified into\n",
      "three categories: very important ones, unimportant ones, and those of\n",
      "variability that may partially encode nuisance factors. Therefore, our model\n",
      "learns the credit assignment leading to the decision, and predicts an ensemble\n",
      "of sub-networks that can accomplish the same task, thereby providing insights\n",
      "toward understanding the macroscopic behavior of deep learning through the lens\n",
      "of distinct roles of synaptic weights.\n",
      "\n",
      "**Paper Id :1911.07662 \n",
      "Title :Variational mean-field theory for training restricted Boltzmann machines\n",
      "  with binary synapses\n",
      "  Unsupervised learning requiring only raw data is not only a fundamental\n",
      "function of the cerebral cortex, but also a foundation for a next generation of\n",
      "artificial neural networks. However, a unified theoretical framework to treat\n",
      "sensory inputs, synapses and neural activity together is still lacking. The\n",
      "computational obstacle originates from the discrete nature of synapses, and\n",
      "complex interactions among these three essential elements of learning. Here, we\n",
      "propose a variational mean-field theory in which the distribution of synaptic\n",
      "weights is considered. The unsupervised learning can then be decomposed into\n",
      "two intertwined steps: a maximization step is carried out as a gradient ascent\n",
      "of the lower-bound on the data log-likelihood, in which the synaptic weight\n",
      "distribution is determined by updating variational parameters, and an\n",
      "expectation step is carried out as a message passing procedure on an equivalent\n",
      "or dual neural network whose parameter is specified by the variational\n",
      "parameters of the weight distribution. Therefore, our framework provides\n",
      "insights on how data (or sensory inputs), synapses and neural activities\n",
      "interact with each other to achieve the goal of extracting statistical\n",
      "regularities in sensory inputs. This variational framework is verified in\n",
      "restricted Boltzmann machines with planted synaptic weights and\n",
      "handwritten-digits learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.03458 \n",
      "Title :Censored Quantile Regression Forest\n",
      "  Random forests are powerful non-parametric regression method but are severely\n",
      "limited in their usage in the presence of randomly censored observations, and\n",
      "naively applied can exhibit poor predictive performance due to the incurred\n",
      "biases. Based on a local adaptive representation of random forests, we develop\n",
      "its regression adjustment for randomly censored regression quantile models.\n",
      "Regression adjustment is based on a new estimating equation that adapts to\n",
      "censoring and leads to quantile score whenever the data do not exhibit\n",
      "censoring. The proposed procedure named {\\it censored quantile regression\n",
      "forest}, allows us to estimate quantiles of time-to-event without any\n",
      "parametric modeling assumption. We establish its consistency under mild model\n",
      "specifications. Numerical studies showcase a clear advantage of the proposed\n",
      "procedure.\n",
      "\n",
      "**Paper Id :1910.13496 \n",
      "Title :Asymptotically unbiased estimation of physical observables with neural\n",
      "  samplers\n",
      "  We propose a general framework for the estimation of observables with\n",
      "generative neural samplers focusing on modern deep generative neural networks\n",
      "that provide an exact sampling probability. In this framework, we present\n",
      "asymptotically unbiased estimators for generic observables, including those\n",
      "that explicitly depend on the partition function such as free energy or\n",
      "entropy, and derive corresponding variance estimators. We demonstrate their\n",
      "practical applicability by numerical experiments for the 2d Ising model which\n",
      "highlight the superiority over existing methods. Our approach greatly enhances\n",
      "the applicability of generative neural samplers to real-world physical systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.03674 \n",
      "Title :Semi-supervised Anomaly Detection using AutoEncoders\n",
      "  Anomaly detection refers to the task of finding unusual instances that stand\n",
      "out from the normal data. In several applications, these outliers or anomalous\n",
      "instances are of greater interest compared to the normal ones. Specifically in\n",
      "the case of industrial optical inspection and infrastructure asset management,\n",
      "finding these defects (anomalous regions) is of extreme importance.\n",
      "Traditionally and even today this process has been carried out manually. Humans\n",
      "rely on the saliency of the defects in comparison to the normal texture to\n",
      "detect the defects. However, manual inspection is slow, tedious, subjective and\n",
      "susceptible to human biases. Therefore, the automation of defect detection is\n",
      "desirable. But for defect detection lack of availability of a large number of\n",
      "anomalous instances and labelled data is a problem. In this paper, we present a\n",
      "convolutional auto-encoder architecture for anomaly detection that is trained\n",
      "only on the defect-free (normal) instances. For the test images, residual masks\n",
      "that are obtained by subtracting the original image from the auto-encoder\n",
      "output are thresholded to obtain the defect segmentation masks. The approach\n",
      "was tested on two data-sets and achieved an impressive average F1 score of\n",
      "0.885. The network learnt to detect the actual shape of the defects even though\n",
      "no defected images were used during the training.\n",
      "\n",
      "**Paper Id :1905.07817 \n",
      "Title :Spatio-Temporal Adversarial Learning for Detecting Unseen Falls\n",
      "  Fall detection is an important problem from both the health and machine\n",
      "learning perspective. A fall can lead to severe injuries, long term impairments\n",
      "or even death in some cases. In terms of machine learning, it presents a\n",
      "severely class imbalance problem with very few or no training data for falls\n",
      "owing to the fact that falls occur rarely. In this paper, we take an alternate\n",
      "philosophy to detect falls in the absence of their training data, by training\n",
      "the classifier on only the normal activities (that are available in abundance)\n",
      "and identifying a fall as an anomaly. To realize such a classifier, we use an\n",
      "adversarial learning framework, which comprises of a spatio-temporal\n",
      "autoencoder for reconstructing input video frames and a spatio-temporal\n",
      "convolution network to discriminate them against original video frames. 3D\n",
      "convolutions are used to learn spatial and temporal features from the input\n",
      "video frames. The adversarial learning of the spatio-temporal autoencoder will\n",
      "enable reconstructing the normal activities of daily living efficiently; thus,\n",
      "rendering detecting unseen falls plausible within this framework. We tested the\n",
      "performance of the proposed framework on camera sensing modalities that may\n",
      "preserve an individual's privacy (fully or partially), such as thermal and\n",
      "depth camera. Our results on three publicly available datasets show that the\n",
      "proposed spatio-temporal adversarial framework performed better than other\n",
      "baseline frame based (or spatial) adversarial learning methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.03715 \n",
      "Title :Derivation of QUBO formulations for sparse estimation\n",
      "  We propose a quadratic unconstrained binary optimization (QUBO) formulation\n",
      "of the l1-norm, which enables us to perform sparse estimation of Ising-type\n",
      "annealing methods such as quantum annealing. The QUBO formulation is derived\n",
      "using the Legendre transformation and the Wolfe theorem, which have recently\n",
      "been employed to derive the QUBO formulations of ReLU-type functions. It is\n",
      "shown that a simple application of the derivation method to the l1-norm case\n",
      "results in a redundant variable. Finally a simplified QUBO formulation is\n",
      "obtained by removing the redundant variable.\n",
      "\n",
      "**Paper Id :1907.11985 \n",
      "Title :The Wang-Landau Algorithm as Stochastic Optimization and Its\n",
      "  Acceleration\n",
      "  We show that the Wang-Landau algorithm can be formulated as a stochastic\n",
      "gradient descent algorithm minimizing a smooth and convex objective function,\n",
      "of which the gradient is estimated using Markov chain Monte Carlo iterations.\n",
      "The optimization formulation provides us a new way to establish the convergence\n",
      "rate of the Wang-Landau algorithm, by exploiting the fact that almost surely,\n",
      "the density estimates (on the logarithmic scale) remain in a compact set, upon\n",
      "which the objective function is strongly convex. The optimization viewpoint\n",
      "motivates us to improve the efficiency of the Wang-Landau algorithm using\n",
      "popular tools including the momentum method and the adaptive learning rate\n",
      "method. We demonstrate the accelerated Wang-Landau algorithm on a\n",
      "two-dimensional Ising model and a two-dimensional ten-state Potts model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.03898 \n",
      "Title :Stepwise Model Selection for Sequence Prediction via Deep Kernel\n",
      "  Learning\n",
      "  An essential problem in automated machine learning (AutoML) is that of model\n",
      "selection. A unique challenge in the sequential setting is the fact that the\n",
      "optimal model itself may vary over time, depending on the distribution of\n",
      "features and labels available up to each point in time. In this paper, we\n",
      "propose a novel Bayesian optimization (BO) algorithm to tackle the challenge of\n",
      "model selection in this setting. This is accomplished by treating the\n",
      "performance at each time step as its own black-box function. In order to solve\n",
      "the resulting multiple black-box function optimization problem jointly and\n",
      "efficiently, we exploit potential correlations among black-box functions using\n",
      "deep kernel learning (DKL). To the best of our knowledge, we are the first to\n",
      "formulate the problem of stepwise model selection (SMS) for sequence\n",
      "prediction, and to design and demonstrate an efficient joint-learning algorithm\n",
      "for this purpose. Using multiple real-world datasets, we verify that our\n",
      "proposed method outperforms both standard BO and multi-objective BO algorithms\n",
      "on a variety of sequence prediction tasks.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.04029 \n",
      "Title :Tangent-Space Gradient Optimization of Tensor Network for Machine\n",
      "  Learning\n",
      "  The gradient-based optimization method for deep machine learning models\n",
      "suffers from gradient vanishing and exploding problems, particularly when the\n",
      "computational graph becomes deep. In this work, we propose the tangent-space\n",
      "gradient optimization (TSGO) for the probabilistic models to keep the gradients\n",
      "from vanishing or exploding. The central idea is to guarantee the orthogonality\n",
      "between the variational parameters and the gradients. The optimization is then\n",
      "implemented by rotating parameter vector towards the direction of gradient. We\n",
      "explain and testify TSGO in tensor network (TN) machine learning, where the TN\n",
      "describes the joint probability distribution as a normalized state $\\left| \\psi\n",
      "\\right\\rangle $ in Hilbert space. We show that the gradient can be restricted\n",
      "in the tangent space of $\\left\\langle \\psi \\right.\\left| \\psi \\right\\rangle =\n",
      "1$ hyper-sphere. Instead of additional adaptive methods to control the learning\n",
      "rate in deep learning, the learning rate of TSGO is naturally determined by the\n",
      "angle $\\theta $ as $\\eta = \\tan \\theta $. Our numerical results reveal better\n",
      "convergence of TSGO in comparison to the off-the-shelf Adam.\n",
      "\n",
      "**Paper Id :1902.08234 \n",
      "Title :An Empirical Study of Large-Batch Stochastic Gradient Descent with\n",
      "  Structured Covariance Noise\n",
      "  The choice of batch-size in a stochastic optimization algorithm plays a\n",
      "substantial role for both optimization and generalization. Increasing the\n",
      "batch-size used typically improves optimization but degrades generalization. To\n",
      "address the problem of improving generalization while maintaining optimal\n",
      "convergence in large-batch training, we propose to add covariance noise to the\n",
      "gradients. We demonstrate that the learning performance of our method is more\n",
      "accurately captured by the structure of the covariance matrix of the noise\n",
      "rather than by the variance of gradients. Moreover, over the convex-quadratic,\n",
      "we prove in theory that it can be characterized by the Frobenius norm of the\n",
      "noise matrix. Our empirical studies with standard deep learning\n",
      "model-architectures and datasets shows that our method not only improves\n",
      "generalization performance in large-batch training, but furthermore, does so in\n",
      "a way where the optimization performance remains desirable and the training\n",
      "duration is not elongated.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.04263 \n",
      "Title :Deep learning to discover and predict dynamics on an inertial manifold\n",
      "  A data-driven framework is developed to represent chaotic dynamics on an\n",
      "inertial manifold (IM), and applied to solutions of the Kuramoto-Sivashinsky\n",
      "equation. A hybrid method combining linear and nonlinear (neural-network)\n",
      "dimension reduction transforms between coordinates in the full state space and\n",
      "on the IM. Additional neural networks predict time-evolution on the IM. The\n",
      "formalism accounts for translation invariance and energy conservation, and\n",
      "substantially outperforms linear dimension reduction, reproducing very well key\n",
      "dynamic and statistical features of the attractor.\n",
      "\n",
      "**Paper Id :1909.13334 \n",
      "Title :Symplectic Recurrent Neural Networks\n",
      "  We propose Symplectic Recurrent Neural Networks (SRNNs) as learning\n",
      "algorithms that capture the dynamics of physical systems from observed\n",
      "trajectories. An SRNN models the Hamiltonian function of the system by a neural\n",
      "network and furthermore leverages symplectic integration, multiple-step\n",
      "training and initial state optimization to address the challenging numerical\n",
      "issues associated with Hamiltonian systems. We show that SRNNs succeed reliably\n",
      "on complex and noisy Hamiltonian systems. We also show how to augment the SRNN\n",
      "integration scheme in order to handle stiff dynamical systems such as bouncing\n",
      "billiards.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.04646 \n",
      "Title :Pedestrian orientation dynamics from high-fidelity measurements\n",
      "  We investigate in real-life conditions and with very high accuracy the\n",
      "dynamics of body rotation, or yawing, of walking pedestrians - an highly\n",
      "complex task due to the wide variety in shapes, postures and walking gestures.\n",
      "We propose a novel measurement method based on a deep neural architecture that\n",
      "we train on the basis of generic physical properties of the motion of\n",
      "pedestrians. Specifically, we leverage on the strong statistical correlation\n",
      "between individual velocity and body orientation: the velocity direction is\n",
      "typically orthogonal with respect to the shoulder line. We make the reasonable\n",
      "assumption that this approximation, although instantaneously slightly\n",
      "imperfect, is correct on average. This enables us to use velocity data as\n",
      "training labels for a highly-accurate point-estimator of individual\n",
      "orientation, that we can train with no dedicated annotation labor. We discuss\n",
      "the measurement accuracy and show the error scaling, both on synthetic and\n",
      "real-life data: we show that our method is capable of estimating orientation\n",
      "with an error as low as 7.5 degrees. This tool opens up new possibilities in\n",
      "the studies of human crowd dynamics where orientation is key. By analyzing the\n",
      "dynamics of body rotation in real-life conditions, we show that the\n",
      "instantaneous velocity direction can be described by the combination of\n",
      "orientation and a random delay, where randomness is provided by an\n",
      "Ornstein-Uhlenbeck process centered on an average delay of 100ms. Quantifying\n",
      "these dynamics could have only been possible thanks to a tool as precise as\n",
      "that proposed.\n",
      "\n",
      "**Paper Id :2007.13866 \n",
      "Title :se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n",
      "  Residuals in Synthetic Domains\n",
      "  Tracking the 6D pose of objects in video sequences is important for robot\n",
      "manipulation. This task, however, introduces multiple challenges: (i) robot\n",
      "manipulation involves significant occlusions; (ii) data and annotations are\n",
      "troublesome and difficult to collect for 6D poses, which complicates machine\n",
      "learning solutions, and (iii) incremental error drift often accumulates in long\n",
      "term tracking to necessitate re-initialization of the object's pose. This work\n",
      "proposes a data-driven optimization approach for long-term, 6D pose tracking.\n",
      "It aims to identify the optimal relative pose given the current RGB-D\n",
      "observation and a synthetic image conditioned on the previous best estimate and\n",
      "the object's model. The key contribution in this context is a novel neural\n",
      "network architecture, which appropriately disentangles the feature encoding to\n",
      "help reduce domain shift, and an effective 3D orientation representation via\n",
      "Lie Algebra. Consequently, even when the network is trained only with synthetic\n",
      "data can work effectively over real images. Comprehensive experiments over\n",
      "benchmarks - existing ones as well as a new dataset with significant occlusions\n",
      "related to object manipulation - show that the proposed approach achieves\n",
      "consistently robust estimates and outperforms alternatives, even though they\n",
      "have been trained with real images. The approach is also the most\n",
      "computationally efficient among the alternatives and achieves a tracking\n",
      "frequency of 90.9Hz.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.04678 \n",
      "Title :Smooth markets: A basic mechanism for organizing gradient-based learners\n",
      "  With the success of modern machine learning, it is becoming increasingly\n",
      "important to understand and control how learning algorithms interact.\n",
      "Unfortunately, negative results from game theory show there is little hope of\n",
      "understanding or controlling general n-player games. We therefore introduce\n",
      "smooth markets (SM-games), a class of n-player games with pairwise zero sum\n",
      "interactions. SM-games codify a common design pattern in machine learning that\n",
      "includes (some) GANs, adversarial training, and other recent algorithms. We\n",
      "show that SM-games are amenable to analysis and optimization using first-order\n",
      "methods.\n",
      "\n",
      "**Paper Id :1906.09831 \n",
      "Title :Foolproof Cooperative Learning\n",
      "  This paper extends the notion of learning equilibrium in game theory from\n",
      "matrix games to stochastic games. We introduce Foolproof Cooperative Learning\n",
      "(FCL), an algorithm that converges to a Tit-for-Tat behavior. It allows\n",
      "cooperative strategies when played against itself while being not exploitable\n",
      "by selfish players. We prove that in repeated symmetric games, this algorithm\n",
      "is a learning equilibrium. We illustrate the behavior of FCL on symmetric\n",
      "matrix and grid games, and its robustness to selfish learners.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.04692 \n",
      "Title :Unsupervised Domain Adaptation for Mobile Semantic Segmentation based on\n",
      "  Cycle Consistency and Feature Alignment\n",
      "  The supervised training of deep networks for semantic segmentation requires a\n",
      "huge amount of labeled real world data. To solve this issue, a commonly\n",
      "exploited workaround is to use synthetic data for training, but deep networks\n",
      "show a critical performance drop when analyzing data with slightly different\n",
      "statistical properties with respect to the training set. In this work, we\n",
      "propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the\n",
      "domain shift issue between real world and synthetic representations. An\n",
      "adversarial model, based on the cycle consistency framework, performs the\n",
      "mapping between the synthetic and real domain. The data is then fed to a\n",
      "MobileNet-v2 architecture that performs the semantic segmentation task. An\n",
      "additional couple of discriminators, working at the feature level of the\n",
      "MobileNet-v2, allows to better align the features of the two domain\n",
      "distributions and to further improve the performance. Finally, the consistency\n",
      "of the semantic maps is exploited. After an initial supervised training on\n",
      "synthetic data, the whole UDA architecture is trained end-to-end considering\n",
      "all its components at once. Experimental results show how the proposed strategy\n",
      "is able to obtain impressive performance in adapting a segmentation network\n",
      "trained on synthetic data to real world scenarios. The usage of the lightweight\n",
      "MobileNet-v2 architecture allows its deployment on devices with limited\n",
      "computational resources as the ones employed in autonomous vehicles.\n",
      "\n",
      "**Paper Id :2003.09284 \n",
      "Title :Acoustic Scene Classification with Squeeze-Excitation Residual Networks\n",
      "  Acoustic scene classification (ASC) is a problem related to the field of\n",
      "machine listening whose objective is to classify/tag an audio clip in a\n",
      "predefined label describing a scene location (e. g. park, airport, etc.). Many\n",
      "state-of-the-art solutions to ASC incorporate data augmentation techniques and\n",
      "model ensembles. However, considerable improvements can also be achieved only\n",
      "by modifying the architecture of convolutional neural networks (CNNs). In this\n",
      "work we propose two novel squeeze-excitation blocks to improve the accuracy of\n",
      "a CNN-based ASC framework based on residual learning. The main idea of\n",
      "squeeze-excitation blocks is to learn spatial and channel-wise feature maps\n",
      "independently instead of jointly as standard CNNs do. This is usually achieved\n",
      "by some global grouping operators, linear operators and a final calibration\n",
      "between the input of the block and its obtained relationships. The behavior of\n",
      "the block that implements such operators and, therefore, the entire neural\n",
      "network, can be modified depending on the input to the block, the established\n",
      "residual configurations and the selected non-linear activations. The analysis\n",
      "has been carried out using the TAU Urban Acoustic Scenes 2019 dataset\n",
      "(https://zenodo.org/record/2589280) presented in the 2019 edition of the DCASE\n",
      "challenge. All configurations discussed in this document exceed the performance\n",
      "of the baseline proposed by the DCASE organization by 13\\% percentage points.\n",
      "In turn, the novel configurations proposed in this paper outperform the\n",
      "residual configurations proposed in previous works.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.04754 \n",
      "Title :Learning Overlapping Representations for the Estimation of\n",
      "  Individualized Treatment Effects\n",
      "  The choice of making an intervention depends on its potential benefit or harm\n",
      "in comparison to alternatives. Estimating the likely outcome of alternatives\n",
      "from observational data is a challenging problem as all outcomes are never\n",
      "observed, and selection bias precludes the direct comparison of differently\n",
      "intervened groups. Despite their empirical success, we show that algorithms\n",
      "that learn domain-invariant representations of inputs (on which to make\n",
      "predictions) are often inappropriate, and develop generalization bounds that\n",
      "demonstrate the dependence on domain overlap and highlight the need for\n",
      "invertible latent maps. Based on these results, we develop a deep kernel\n",
      "regression algorithm and posterior regularization framework that substantially\n",
      "outperforms the state-of-the-art on a variety of benchmarks data sets.\n",
      "\n",
      "**Paper Id :1910.04817 \n",
      "Title :Estimation of Bounds on Potential Outcomes For Decision Making\n",
      "  Estimation of individual treatment effects is commonly used as the basis for\n",
      "contextual decision making in fields such as healthcare, education, and\n",
      "economics. However, it is often sufficient for the decision maker to have\n",
      "estimates of upper and lower bounds on the potential outcomes of decision\n",
      "alternatives to assess risks and benefits. We show that, in such cases, we can\n",
      "improve sample efficiency by estimating simple functions that bound these\n",
      "outcomes instead of estimating their conditional expectations, which may be\n",
      "complex and hard to estimate. Our analysis highlights a trade-off between the\n",
      "complexity of the learning task and the confidence with which the learned\n",
      "bounds hold. Guided by these findings, we develop an algorithm for learning\n",
      "upper and lower bounds on potential outcomes which optimize an objective\n",
      "function defined by the decision maker, subject to the probability that bounds\n",
      "are violated being small. Using a clinical dataset and a well-known causality\n",
      "benchmark, we demonstrate that our algorithm outperforms baselines, providing\n",
      "tighter, more reliable bounds.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05313 \n",
      "Title :Tensor Graph Convolutional Networks for Text Classification\n",
      "  Compared to sequential learning models, graph-based neural networks exhibit\n",
      "some excellent properties, such as ability capturing global information. In\n",
      "this paper, we investigate graph-based neural networks for text classification\n",
      "problem. A new framework TensorGCN (tensor graph convolutional networks), is\n",
      "presented for this task. A text graph tensor is firstly constructed to describe\n",
      "semantic, syntactic, and sequential contextual information. Then, two kinds of\n",
      "propagation learning perform on the text graph tensor. The first is intra-graph\n",
      "propagation used for aggregating information from neighborhood nodes in a\n",
      "single graph. The second is inter-graph propagation used for harmonizing\n",
      "heterogeneous information between graphs. Extensive experiments are conducted\n",
      "on benchmark datasets, and the results illustrate the effectiveness of our\n",
      "proposed framework. Our proposed TensorGCN presents an effective way to\n",
      "harmonize and integrate heterogeneous information from different kinds of\n",
      "graphs.\n",
      "\n",
      "**Paper Id :2003.08420 \n",
      "Title :Unsupervised Hierarchical Graph Representation Learning by Mutual\n",
      "  Information Maximization\n",
      "  Graph representation learning based on graph neural networks (GNNs) can\n",
      "greatly improve the performance of downstream tasks, such as node and graph\n",
      "classification. However, the general GNN models do not aggregate node\n",
      "information in a hierarchical manner, and can miss key higher-order structural\n",
      "features of many graphs. The hierarchical aggregation also enables the graph\n",
      "representations to be explainable. In addition, supervised graph representation\n",
      "learning requires labeled data, which is expensive and error-prone. To address\n",
      "these issues, we present an unsupervised graph representation learning method,\n",
      "Unsupervised Hierarchical Graph Representation (UHGR), which can generate\n",
      "hierarchical representations of graphs. Our method focuses on maximizing mutual\n",
      "information between \"local\" and high-level \"global\" representations, which\n",
      "enables us to learn the node embeddings and graph embeddings without any\n",
      "labeled data. To demonstrate the effectiveness of the proposed method, we\n",
      "perform the node and graph classification using the learned node and graph\n",
      "embeddings. The results show that the proposed method achieves comparable\n",
      "results to state-of-the-art supervised methods on several benchmarks. In\n",
      "addition, our visualization of hierarchical representations indicates that our\n",
      "method can capture meaningful and interpretable clusters.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05361 \n",
      "Title :Learning the Ising Model with Generative Neural Networks\n",
      "  Recent advances in deep learning and neural networks have led to an increased\n",
      "interest in the application of generative models in statistical and condensed\n",
      "matter physics. In particular, restricted Boltzmann machines (RBMs) and\n",
      "variational autoencoders (VAEs) as specific classes of neural networks have\n",
      "been successfully applied in the context of physical feature extraction and\n",
      "representation learning. Despite these successes, however, there is only\n",
      "limited understanding of their representational properties and limitations. To\n",
      "better understand the representational characteristics of RBMs and VAEs, we\n",
      "study their ability to capture physical features of the Ising model at\n",
      "different temperatures. This approach allows us to quantitatively assess\n",
      "learned representations by comparing sample features with corresponding\n",
      "theoretical predictions. Our results suggest that the considered RBMs and\n",
      "convolutional VAEs are able to capture the temperature dependence of\n",
      "magnetization, energy, and spin-spin correlations. The samples generated by\n",
      "RBMs are more evenly distributed across temperature than those generated by\n",
      "VAEs. We also find that convolutional layers in VAEs are important to model\n",
      "spin correlations whereas RBMs achieve similar or even better performances\n",
      "without convolutional filters.\n",
      "\n",
      "**Paper Id :1902.04057 \n",
      "Title :Deep autoregressive models for the efficient variational simulation of\n",
      "  many-body quantum systems\n",
      "  Artificial Neural Networks were recently shown to be an efficient\n",
      "representation of highly-entangled many-body quantum states. In practical\n",
      "applications, neural-network states inherit numerical schemes used in\n",
      "Variational Monte Carlo, most notably the use of Markov-Chain Monte-Carlo\n",
      "(MCMC) sampling to estimate quantum expectations. The local stochastic sampling\n",
      "in MCMC caps the potential advantages of neural networks in two ways: (i) Its\n",
      "intrinsic computational cost sets stringent practical limits on the width and\n",
      "depth of the networks, and therefore limits their expressive capacity; (ii) Its\n",
      "difficulty in generating precise and uncorrelated samples can result in\n",
      "estimations of observables that are very far from their true value. Inspired by\n",
      "the state-of-the-art generative models used in machine learning, we propose a\n",
      "specialized Neural Network architecture that supports efficient and exact\n",
      "sampling, completely circumventing the need for Markov Chain sampling. We\n",
      "demonstrate our approach for two-dimensional interacting spin models,\n",
      "showcasing the ability to obtain accurate results on larger system sizes than\n",
      "those currently accessible to neural-network quantum states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05472 \n",
      "Title :Machine learning transfer efficiencies for noisy quantum walks\n",
      "  Quantum effects are known to provide an advantage in particle transfer across\n",
      "networks. In order to achieve this advantage, requirements on both a graph type\n",
      "and a quantum system coherence must be found. Here we show that the process of\n",
      "finding these requirements can be automated by learning from simulated\n",
      "examples. The automation is done by using a convolutional neural network of a\n",
      "particular type that learns to understand with which network and under which\n",
      "coherence requirements quantum advantage is possible. Our machine learning\n",
      "approach is applied to study noisy quantum walks on cycle graphs of different\n",
      "sizes. We found that it is possible to predict the existence of quantum\n",
      "advantage for the entire decoherence parameter range, even for graphs outside\n",
      "of the training set. Our results are of importance for demonstration of\n",
      "advantage in quantum experiments and pave the way towards automating scientific\n",
      "research and discoveries.\n",
      "\n",
      "**Paper Id :1904.10797 \n",
      "Title :Machine learning for long-distance quantum communication\n",
      "  Machine learning can help us in solving problems in the context big data\n",
      "analysis and classification, as well as in playing complex games such as Go.\n",
      "But can it also be used to find novel protocols and algorithms for applications\n",
      "such as large-scale quantum communication? Here we show that machine learning\n",
      "can be used to identify central quantum protocols, including teleportation,\n",
      "entanglement purification and the quantum repeater. These schemes are of\n",
      "importance in long-distance quantum communication, and their discovery has\n",
      "shaped the field of quantum information processing. However, the usefulness of\n",
      "learning agents goes beyond the mere re-production of known protocols; the same\n",
      "approach allows one to find improved solutions to long-distance communication\n",
      "problems, in particular when dealing with asymmetric situations where channel\n",
      "noise and segment distance are non-uniform. Our findings are based on the use\n",
      "of projective simulation, a model of a learning agent that combines\n",
      "reinforcement learning and decision making in a physically motivated framework.\n",
      "The learning agent is provided with a universal gate set, and the desired task\n",
      "is specified via a reward scheme. From a technical perspective, the learning\n",
      "agent has to deal with stochastic environments and reactions. We utilize an\n",
      "idea reminiscent of hierarchical skill acquisition, where solutions to\n",
      "sub-problems are learned and re-used in the overall scheme. This is of\n",
      "particular importance in the development of long-distance communication\n",
      "schemes, and opens the way for using machine learning in the design and\n",
      "implementation of quantum networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05537 \n",
      "Title :Accelerated Dual-Averaging Primal-Dual Method for Composite Convex\n",
      "  Minimization\n",
      "  Dual averaging-type methods are widely used in industrial machine learning\n",
      "applications due to their ability to promoting solution structure (e.g.,\n",
      "sparsity) efficiently. In this paper, we propose a novel accelerated\n",
      "dual-averaging primal-dual algorithm for minimizing a composite convex\n",
      "function. We also derive a stochastic version of the proposed method which\n",
      "solves empirical risk minimization, and its advantages on handling sparse data\n",
      "are demonstrated both theoretically and empirically.\n",
      "\n",
      "**Paper Id :2007.13243 \n",
      "Title :Scalable Derivative-Free Optimization for Nonlinear Least-Squares\n",
      "  Problems\n",
      "  Derivative-free - or zeroth-order - optimization (DFO) has gained recent\n",
      "attention for its ability to solve problems in a variety of application areas,\n",
      "including machine learning, particularly involving objectives which are\n",
      "stochastic and/or expensive to compute. In this work, we develop a novel\n",
      "model-based DFO method for solving nonlinear least-squares problems. We improve\n",
      "on state-of-the-art DFO by performing dimensionality reduction in the\n",
      "observational space using sketching methods, avoiding the construction of a\n",
      "full local model. Our approach has a per-iteration computational cost which is\n",
      "linear in problem dimension in a big data regime, and numerical evidence\n",
      "demonstrates that, compared to existing software, it has dramatically improved\n",
      "runtime performance on overdetermined least-squares problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05559 \n",
      "Title :Mode-Assisted Unsupervised Learning of Restricted Boltzmann Machines\n",
      "  Restricted Boltzmann machines (RBMs) are a powerful class of generative\n",
      "models, but their training requires computing a gradient that, unlike\n",
      "supervised backpropagation on typical loss functions, is notoriously difficult\n",
      "even to approximate. Here, we show that properly combining standard gradient\n",
      "updates with an off-gradient direction, constructed from samples of the RBM\n",
      "ground state (mode), improves their training dramatically over traditional\n",
      "gradient methods. This approach, which we call mode training, promotes faster\n",
      "training and stability, in addition to lower converged relative entropy (KL\n",
      "divergence). Along with the proofs of stability and convergence of this method,\n",
      "we also demonstrate its efficacy on synthetic datasets where we can compute KL\n",
      "divergences exactly, as well as on a larger machine learning standard, MNIST.\n",
      "The mode training we suggest is quite versatile, as it can be applied in\n",
      "conjunction with any given gradient method, and is easily extended to more\n",
      "general energy-based neural network structures such as deep, convolutional and\n",
      "unrestricted Boltzmann machines.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05591 \n",
      "Title :Distributed, partially collapsed MCMC for Bayesian Nonparametrics\n",
      "  Bayesian nonparametric (BNP) models provide elegant methods for discovering\n",
      "underlying latent features within a data set, but inference in such models can\n",
      "be slow. We exploit the fact that completely random measures, which commonly\n",
      "used models like the Dirichlet process and the beta-Bernoulli process can be\n",
      "expressed as, are decomposable into independent sub-measures. We use this\n",
      "decomposition to partition the latent measure into a finite measure containing\n",
      "only instantiated components, and an infinite measure containing all other\n",
      "components. We then select different inference algorithms for the two\n",
      "components: uncollapsed samplers mix well on the finite measure, while\n",
      "collapsed samplers mix well on the infinite, sparsely occupied tail. The\n",
      "resulting hybrid algorithm can be applied to a wide class of models, and can be\n",
      "easily distributed to allow scalable inference without sacrificing asymptotic\n",
      "convergence guarantees.\n",
      "\n",
      "**Paper Id :2003.07070 \n",
      "Title :Merge-split Markov chain Monte Carlo for community detection\n",
      "  We present a Markov chain Monte Carlo scheme based on merges and splits of\n",
      "groups that is capable of efficiently sampling from the posterior distribution\n",
      "of network partitions, defined according to the stochastic block model (SBM).\n",
      "We demonstrate how schemes based on the move of single nodes between groups\n",
      "systematically fail at correctly sampling from the posterior distribution even\n",
      "on small networks, and how our merge-split approach behaves significantly\n",
      "better, and improves the mixing time of the Markov chain by several orders of\n",
      "magnitude in typical cases. We also show how the scheme can be\n",
      "straightforwardly extended to nested versions of the SBM, yielding\n",
      "asymptotically exact samples of hierarchical network partitions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05857 \n",
      "Title :The Effect of Data Ordering in Image Classification\n",
      "  The success stories from deep learning models increase every day spanning\n",
      "different tasks from image classification to natural language understanding.\n",
      "With the increasing popularity of these models, scientists spend more and more\n",
      "time finding the optimal parameters and best model architectures for their\n",
      "tasks. In this paper, we focus on the ingredient that feeds these machines: the\n",
      "data. We hypothesize that the data ordering affects how well a model performs.\n",
      "To that end, we conduct experiments on an image classification task using\n",
      "ImageNet dataset and show that some data orderings are better than others in\n",
      "terms of obtaining higher classification accuracies. Experimental results show\n",
      "that independent of model architecture, learning rate and batch size, ordering\n",
      "of the data significantly affects the outcome. We show these findings using\n",
      "different metrics: NDCG, accuracy @ 1 and accuracy @ 5. Our goal here is to\n",
      "show that not only parameters and model architectures but also the data\n",
      "ordering has a say in obtaining better results.\n",
      "\n",
      "**Paper Id :2003.13300 \n",
      "Title :Weighted Random Search for CNN Hyperparameter Optimization\n",
      "  Nearly all model algorithms used in machine learning use two different sets\n",
      "of parameters: the training parameters and the meta-parameters\n",
      "(hyperparameters). While the training parameters are learned during the\n",
      "training phase, the values of the hyperparameters have to be specified before\n",
      "learning starts. For a given dataset, we would like to find the optimal\n",
      "combination of hyperparameter values, in a reasonable amount of time. This is a\n",
      "challenging task because of its computational complexity. In previous work\n",
      "[11], we introduced the Weighted Random Search (WRS) method, a combination of\n",
      "Random Search (RS) and probabilistic greedy heuristic. In the current paper, we\n",
      "compare the WRS method with several state-of-the art hyperparameter\n",
      "optimization methods with respect to Convolutional Neural Network (CNN)\n",
      "hyperparameter optimization. The criterion is the classification accuracy\n",
      "achieved within the same number of tested combinations of hyperparameter\n",
      "values. According to our experiments, the WRS algorithm outperforms the other\n",
      "methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.05922 \n",
      "Title :Continual Learning for Domain Adaptation in Chest X-ray Classification\n",
      "  Over the last years, Deep Learning has been successfully applied to a broad\n",
      "range of medical applications. Especially in the context of chest X-ray\n",
      "classification, results have been reported which are on par, or even superior\n",
      "to experienced radiologists. Despite this success in controlled experimental\n",
      "environments, it has been noted that the ability of Deep Learning models to\n",
      "generalize to data from a new domain (with potentially different tasks) is\n",
      "often limited. In order to address this challenge, we investigate techniques\n",
      "from the field of Continual Learning (CL) including Joint Training (JT),\n",
      "Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using\n",
      "the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that\n",
      "these methods provide promising options to improve the performance of Deep\n",
      "Learning models on a target domain and to mitigate effectively catastrophic\n",
      "forgetting for the source domain. To this end, the best overall performance was\n",
      "obtained using JT, while for LWF competitive results could be achieved - even\n",
      "without accessing data from the source domain.\n",
      "\n",
      "**Paper Id :2009.07560 \n",
      "Title :Similarity-based data mining for online domain adaptation of a sonar ATR\n",
      "  system\n",
      "  Due to the expensive nature of field data gathering, the lack of training\n",
      "data often limits the performance of Automatic Target Recognition (ATR)\n",
      "systems. This problem is often addressed with domain adaptation techniques,\n",
      "however the currently existing methods fail to satisfy the constraints of\n",
      "resource and time-limited underwater systems. We propose to address this issue\n",
      "via an online fine-tuning of the ATR algorithm using a novel data-selection\n",
      "method. Our proposed data-mining approach relies on visual similarity and\n",
      "outperforms the traditionally employed hard-mining methods. We present a\n",
      "comparative performance analysis in a wide range of simulated environments and\n",
      "highlight the benefits of using our method for the rapid adaptation to\n",
      "previously unseen environments.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.06137 \n",
      "Title :Graph Inference Learning for Semi-supervised Classification\n",
      "  In this work, we address semi-supervised classification of graph data, where\n",
      "the categories of those unlabeled nodes are inferred from labeled nodes as well\n",
      "as graph structures. Recent works often solve this problem via advanced graph\n",
      "convolution in a conventionally supervised manner, but the performance could\n",
      "degrade significantly when labeled data is scarce. To this end, we propose a\n",
      "Graph Inference Learning (GIL) framework to boost the performance of\n",
      "semi-supervised node classification by learning the inference of node labels on\n",
      "graph topology. To bridge the connection between two nodes, we formally define\n",
      "a structure relation by encapsulating node attributes, between-node paths, and\n",
      "local topological structures together, which can make the inference\n",
      "conveniently deduced from one node to another node. For learning the inference\n",
      "process, we further introduce meta-optimization on structure relations from\n",
      "training nodes to validation nodes, such that the learnt graph inference\n",
      "capability can be better self-adapted to testing nodes. Comprehensive\n",
      "evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and\n",
      "NELL) demonstrate the superiority of our proposed GIL when compared against\n",
      "state-of-the-art methods on the semi-supervised node classification task.\n",
      "\n",
      "**Paper Id :2002.07366 \n",
      "Title :Adversarial Deep Network Embedding for Cross-network Node Classification\n",
      "  In this paper, the task of cross-network node classification, which leverages\n",
      "the abundant labeled nodes from a source network to help classify unlabeled\n",
      "nodes in a target network, is studied. The existing domain adaptation\n",
      "algorithms generally fail to model the network structural information, and the\n",
      "current network embedding models mainly focus on single-network applications.\n",
      "Thus, both of them cannot be directly applied to solve the cross-network node\n",
      "classification problem. This motivates us to propose an adversarial\n",
      "cross-network deep network embedding (ACDNE) model to integrate adversarial\n",
      "domain adaptation with deep network embedding so as to learn network-invariant\n",
      "node representations that can also well preserve the network structural\n",
      "information. In ACDNE, the deep network embedding module utilizes two feature\n",
      "extractors to jointly preserve attributed affinity and topological proximities\n",
      "between nodes. In addition, a node classifier is incorporated to make node\n",
      "representations label-discriminative. Moreover, an adversarial domain\n",
      "adaptation technique is employed to make node representations\n",
      "network-invariant. Extensive experimental results demonstrate that the proposed\n",
      "ACDNE model achieves the state-of-the-art performance in cross-network node\n",
      "classification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.06270 \n",
      "Title :Bayesian inference of chaotic dynamics by merging data assimilation,\n",
      "  machine learning and expectation-maximization\n",
      "  The reconstruction from observations of high-dimensional chaotic dynamics\n",
      "such as geophysical flows is hampered by (i) the partial and noisy observations\n",
      "that can realistically be obtained, (ii) the need to learn from long time\n",
      "series of data, and (iii) the unstable nature of the dynamics. To achieve such\n",
      "inference from the observations over long time series, it has been suggested to\n",
      "combine data assimilation and machine learning in several ways. We show how to\n",
      "unify these approaches from a Bayesian perspective using\n",
      "expectation-maximization and coordinate descents. In doing so, the model, the\n",
      "state trajectory and model error statistics are estimated all together.\n",
      "Implementations and approximations of these methods are discussed. Finally, we\n",
      "numerically and successfully test the approach on two relevant low-order\n",
      "chaotic models with distinct identifiability.\n",
      "\n",
      "**Paper Id :2006.03859 \n",
      "Title :Online learning of both state and dynamics using ensemble Kalman filters\n",
      "  The reconstruction of the dynamics of an observed physical system as a\n",
      "surrogate model has been brought to the fore by recent advances in machine\n",
      "learning. To deal with partial and noisy observations in that endeavor, machine\n",
      "learning representations of the surrogate model can be used within a Bayesian\n",
      "data assimilation framework. However, these approaches require to consider long\n",
      "time series of observational data, meant to be assimilated all together. This\n",
      "paper investigates the possibility to learn both the dynamics and the state\n",
      "online, i.e. to update their estimates at any time, in particular when new\n",
      "observations are acquired. The estimation is based on the ensemble Kalman\n",
      "filter (EnKF) family of algorithms using a rather simple representation for the\n",
      "surrogate model and state augmentation. We consider the implication of learning\n",
      "dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an\n",
      "iterative EnKF and we discuss in each case issues and algorithmic solutions. We\n",
      "then demonstrate numerically the efficiency and assess the accuracy of these\n",
      "methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.06814 \n",
      "Title :Distributionally Robust Bayesian Quadrature Optimization\n",
      "  Bayesian quadrature optimization (BQO) maximizes the expectation of an\n",
      "expensive black-box integrand taken over a known probability distribution. In\n",
      "this work, we study BQO under distributional uncertainty in which the\n",
      "underlying probability distribution is unknown except for a limited set of its\n",
      "i.i.d. samples. A standard BQO approach maximizes the Monte Carlo estimate of\n",
      "the true expected objective given the fixed sample set. Though Monte Carlo\n",
      "estimate is unbiased, it has high variance given a small set of samples; thus\n",
      "can result in a spurious objective function. We adopt the distributionally\n",
      "robust optimization perspective to this problem by maximizing the expected\n",
      "objective under the most adversarial distribution. In particular, we propose a\n",
      "novel posterior sampling based algorithm, namely distributionally robust BQO\n",
      "(DRBQO) for this purpose. We demonstrate the empirical effectiveness of our\n",
      "proposed framework in synthetic and real-world problems, and characterize its\n",
      "theoretical convergence via Bayesian regret.\n",
      "\n",
      "**Paper Id :2009.00666 \n",
      "Title :Robust, Accurate Stochastic Optimization for Variational Inference\n",
      "  We consider the problem of fitting variational posterior approximations using\n",
      "stochastic optimization methods. The performance of these approximations\n",
      "depends on (1) how well the variational family matches the true posterior\n",
      "distribution,(2) the choice of divergence, and (3) the optimization of the\n",
      "variational objective. We show that even in the best-case scenario when the\n",
      "exact posterior belongs to the assumed variational family, common stochastic\n",
      "optimization methods lead to poor variational approximations if the problem\n",
      "dimension is moderately large. We also demonstrate that these methods are not\n",
      "robust across diverse model types. Motivated by these findings, we develop a\n",
      "more robust and accurate stochastic optimization framework by viewing the\n",
      "underlying optimization algorithm as producing a Markov chain. Our approach is\n",
      "theoretically motivated and includes a diagnostic for convergence and a novel\n",
      "stopping rule, both of which are robust to noisy evaluations of the objective\n",
      "function. We show empirically that the proposed framework works well on a\n",
      "diverse set of models: it can automatically detect stochastic optimization\n",
      "failure or inaccurate variational approximation\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.07267 \n",
      "Title :Digital synthesis of histological stains using micro-structured and\n",
      "  multiplexed virtual staining of label-free tissue\n",
      "  Histological staining is a vital step used to diagnose various diseases and\n",
      "has been used for more than a century to provide contrast to tissue sections,\n",
      "rendering the tissue constituents visible for microscopic analysis by medical\n",
      "experts. However, this process is time-consuming, labor-intensive, expensive\n",
      "and destructive to the specimen. Recently, the ability to virtually-stain\n",
      "unlabeled tissue sections, entirely avoiding the histochemical staining step,\n",
      "has been demonstrated using tissue-stain specific deep neural networks. Here,\n",
      "we present a new deep learning-based framework which generates\n",
      "virtually-stained images using label-free tissue, where different stains are\n",
      "merged following a micro-structure map defined by the user. This approach uses\n",
      "a single deep neural network that receives two different sources of information\n",
      "at its input: (1) autofluorescence images of the label-free tissue sample, and\n",
      "(2) a digital staining matrix which represents the desired microscopic map of\n",
      "different stains to be virtually generated at the same tissue section. This\n",
      "digital staining matrix is also used to virtually blend existing stains,\n",
      "digitally synthesizing new histological stains. We trained and blindly tested\n",
      "this virtual-staining network using unlabeled kidney tissue sections to\n",
      "generate micro-structured combinations of Hematoxylin and Eosin (H&E), Jones\n",
      "silver stain, and Masson's Trichrome stain. Using a single network, this\n",
      "approach multiplexes virtual staining of label-free tissue with multiple types\n",
      "of stains and paves the way for synthesizing new digital histological stains\n",
      "that can be created on the same tissue cross-section, which is currently not\n",
      "feasible with standard histochemical staining methods.\n",
      "\n",
      "**Paper Id :1911.04357 \n",
      "Title :Limited View and Sparse Photoacoustic Tomography for Neuroimaging with\n",
      "  Deep Learning\n",
      "  Photoacoustic tomography (PAT) is a nonionizing imaging modality capable of\n",
      "acquiring high contrast and resolution images of optical absorption at depths\n",
      "greater than traditional optical imaging techniques. Practical considerations\n",
      "with instrumentation and geometry limit the number of available acoustic\n",
      "sensors and their view of the imaging target, which result in significant image\n",
      "reconstruction artifacts degrading image quality. Iterative reconstruction\n",
      "methods can be used to reduce artifacts but are computationally expensive. In\n",
      "this work, we propose a novel deep learning approach termed pixelwise deep\n",
      "learning (PixelDL) that first employs pixelwise interpolation governed by the\n",
      "physics of photoacoustic wave propagation and then uses a convolution neural\n",
      "network to directly reconstruct an image. Simulated photoacoustic data from\n",
      "synthetic vasculature phantom and mouse-brain vasculature were used for\n",
      "training and testing, respectively. Results demonstrated that PixelDL achieved\n",
      "comparable performance to iterative methods and outperformed other CNN-based\n",
      "approaches for correcting artifacts. PixelDL is a computationally efficient\n",
      "approach that enables for realtime PAT rendering and for improved image\n",
      "quality, quantification, and interpretation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.07744 \n",
      "Title :Improving Label Ranking Ensembles using Boosting Techniques\n",
      "  Label ranking is a prediction task which deals with learning a mapping\n",
      "between an instance and a ranking (i.e., order) of labels from a finite set,\n",
      "representing their relevance to the instance. Boosting is a well-known and\n",
      "reliable ensemble technique that was shown to often outperform other learning\n",
      "algorithms. While boosting algorithms were developed for a multitude of machine\n",
      "learning tasks, label ranking tasks were overlooked. In this paper, we propose\n",
      "a boosting algorithm which was specifically designed for label ranking tasks.\n",
      "Extensive evaluation of the proposed algorithm on 24 semi-synthetic and\n",
      "real-world label ranking datasets shows that it significantly outperforms\n",
      "existing state-of-the-art label ranking algorithms.\n",
      "\n",
      "**Paper Id :2002.12764 \n",
      "Title :Towards Learning a Universal Non-Semantic Representation of Speech\n",
      "  The ultimate goal of transfer learning is to reduce labeled data requirements\n",
      "by exploiting a pre-existing embedding model trained for different datasets or\n",
      "tasks. The visual and language communities have established benchmarks to\n",
      "compare embeddings, but the speech community has yet to do so. This paper\n",
      "proposes a benchmark for comparing speech representations on non-semantic\n",
      "tasks, and proposes a representation based on an unsupervised triplet-loss\n",
      "objective. The proposed representation outperforms other representations on the\n",
      "benchmark, and even exceeds state-of-the-art performance on a number of\n",
      "transfer learning tasks. The embedding is trained on a publicly available\n",
      "dataset, and it is tested on a variety of low-resource downstream tasks,\n",
      "including personalization tasks and medical domain. The benchmark, models, and\n",
      "evaluation code are publicly released.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08053 \n",
      "Title :Contextualized Embeddings in Named-Entity Recognition: An Empirical\n",
      "  Study on Generalization\n",
      "  Contextualized embeddings use unsupervised language model pretraining to\n",
      "compute word representations depending on their context. This is intuitively\n",
      "useful for generalization, especially in Named-Entity Recognition where it is\n",
      "crucial to detect mentions never seen during training. However, standard\n",
      "English benchmarks overestimate the importance of lexical over contextual\n",
      "features because of an unrealistic lexical overlap between train and test\n",
      "mentions. In this paper, we perform an empirical analysis of the generalization\n",
      "capabilities of state-of-the-art contextualized embeddings by separating\n",
      "mentions by novelty and with out-of-domain evaluation. We show that they are\n",
      "particularly beneficial for unseen mentions detection, especially\n",
      "out-of-domain. For models trained on CoNLL03, language model contextualization\n",
      "leads to a +1.2% maximal relative micro-F1 score increase in-domain against\n",
      "+13% out-of-domain on the WNUT dataset\n",
      "\n",
      "**Paper Id :2004.00588 \n",
      "Title :Better Sign Language Translation with STMC-Transformer\n",
      "  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\n",
      "system to extract sign language glosses from videos. Then, a translation system\n",
      "generates spoken language translations from the sign language glosses. This\n",
      "paper focuses on the translation system and introduces the STMC-Transformer\n",
      "which improves on the current state-of-the-art by over 5 and 7 BLEU\n",
      "respectively on gloss-to-text and video-to-text translation of the\n",
      "PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\n",
      "of over 16 BLEU.\n",
      "  We also demonstrate the problem in current methods that rely on gloss\n",
      "supervision. The video-to-text translation of our STMC-Transformer outperforms\n",
      "translation of GT glosses. This contradicts previous claims that GT gloss\n",
      "translation acts as an upper bound for SLT performance and reveals that glosses\n",
      "are an inefficient representation of sign language. For future SLT research, we\n",
      "therefore suggest an end-to-end training of the recognition and translation\n",
      "models, or using a different sign language annotation scheme.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08092 \n",
      "Title :Local Policy Optimization for Trajectory-Centric Reinforcement Learning\n",
      "  The goal of this paper is to present a method for simultaneous trajectory and\n",
      "local stabilizing policy optimization to generate local policies for\n",
      "trajectory-centric model-based reinforcement learning (MBRL). This is motivated\n",
      "by the fact that global policy optimization for non-linear systems could be a\n",
      "very challenging problem both algorithmically and numerically. However, a lot\n",
      "of robotic manipulation tasks are trajectory-centric, and thus do not require a\n",
      "global model or policy. Due to inaccuracies in the learned model estimates, an\n",
      "open-loop trajectory optimization process mostly results in very poor\n",
      "performance when used on the real system. Motivated by these problems, we try\n",
      "to formulate the problem of trajectory optimization and local policy synthesis\n",
      "as a single optimization problem. It is then solved simultaneously as an\n",
      "instance of nonlinear programming. We provide some results for analysis as well\n",
      "as achieved performance of the proposed technique under some simplifying\n",
      "assumptions.\n",
      "\n",
      "**Paper Id :2007.10284 \n",
      "Title :Learning High-Level Policies for Model Predictive Control\n",
      "  The combination of policy search and deep neural networks holds the promise\n",
      "of automating a variety of decision-making tasks. Model Predictive\n",
      "Control~(MPC) provides robust solutions to robot control tasks by making use of\n",
      "a dynamical model of the system and solving an optimization problem online over\n",
      "a short planning horizon. In this work, we leverage probabilistic\n",
      "decision-making approaches and the generalization capability of artificial\n",
      "neural networks to the powerful online optimization by learning a deep\n",
      "high-level policy for the MPC~(High-MPC). Conditioning on robot's local\n",
      "observations, the trained neural network policy is capable of adaptively\n",
      "selecting high-level decision variables for the low-level MPC controller, which\n",
      "then generates optimal control commands for the robot. First, we formulate the\n",
      "search of high-level decision variables for MPC as a policy search problem,\n",
      "specifically, a probabilistic inference problem. The problem can be solved in a\n",
      "closed-form solution. Second, we propose a self-supervised learning algorithm\n",
      "for learning a neural network high-level policy, which is useful for online\n",
      "hyperparameter adaptations in highly dynamic environments. We demonstrate the\n",
      "importance of incorporating the online adaption into autonomous robots by using\n",
      "the proposed method to solve a challenging control problem, where the task is\n",
      "to control a simulated quadrotor to fly through a swinging gate. We show that\n",
      "our approach can handle situations that are difficult for standard MPC.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08255 \n",
      "Title :A Probabilistic Framework for Imitating Human Race Driver Behavior\n",
      "  Understanding and modeling human driver behavior is crucial for advanced\n",
      "vehicle development. However, unique driving styles, inconsistent behavior, and\n",
      "complex decision processes render it a challenging task, and existing\n",
      "approaches often lack variability or robustness. To approach this problem, we\n",
      "propose Probabilistic Modeling of Driver behavior (ProMoD), a modular framework\n",
      "which splits the task of driver behavior modeling into multiple modules. A\n",
      "global target trajectory distribution is learned with Probabilistic Movement\n",
      "Primitives, clothoids are utilized for local path generation, and the\n",
      "corresponding choice of actions is performed by a neural network. Experiments\n",
      "in a simulated car racing setting show considerable advantages in imitation\n",
      "accuracy and robustness compared to other imitation learning algorithms. The\n",
      "modular architecture of the proposed framework facilitates straightforward\n",
      "extensibility in driving line adaptation and sequencing of multiple movement\n",
      "primitives for future research.\n",
      "\n",
      "**Paper Id :2002.05878 \n",
      "Title :An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset\n",
      "  The Waymo Open Dataset has been released recently, providing a platform to\n",
      "crowdsource some fundamental challenges for automated vehicles (AVs), such as\n",
      "3D detection and tracking. While~the dataset provides a large amount of\n",
      "high-quality and multi-source driving information, people in academia are more\n",
      "interested in the underlying driving policy programmed in Waymo self-driving\n",
      "cars, which is inaccessible due to AV manufacturers' proprietary protection.\n",
      "Accordingly, academic researchers have to make various assumptions to implement\n",
      "AV components in their models or simulations, which may not represent the\n",
      "realistic interactions in real-world traffic. Thus, this paper introduces an\n",
      "approach to learn a long short-term memory (LSTM)-based model for imitating the\n",
      "behavior of Waymo's self-driving model. The proposed model has been evaluated\n",
      "based on Mean Absolute Error (MAE). The experimental results show that our\n",
      "model outperforms several baseline models in driving action prediction. In\n",
      "addition, a visualization tool is presented for verifying the performance of\n",
      "the model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08345 \n",
      "Title :Target-Embedding Autoencoders for Supervised Representation Learning\n",
      "  Autoencoder-based learning has emerged as a staple for disciplining\n",
      "representations in unsupervised and semi-supervised settings. This paper\n",
      "analyzes a framework for improving generalization in a purely supervised\n",
      "setting, where the target space is high-dimensional. We motivate and formalize\n",
      "the general framework of target-embedding autoencoders (TEA) for supervised\n",
      "prediction, learning intermediate latent representations jointly optimized to\n",
      "be both predictable from features as well as predictive of targets---encoding\n",
      "the prior that variations in targets are driven by a compact set of underlying\n",
      "factors. As our theoretical contribution, we provide a guarantee of\n",
      "generalization for linear TEAs by demonstrating uniform stability, interpreting\n",
      "the benefit of the auxiliary reconstruction task as a form of regularization.\n",
      "As our empirical contribution, we extend validation of this approach beyond\n",
      "existing static classification applications to multivariate sequence\n",
      "forecasting, verifying their advantage on both linear and nonlinear recurrent\n",
      "architectures---thereby underscoring the further generality of this framework\n",
      "beyond feedforward instantiations.\n",
      "\n",
      "**Paper Id :2008.11426 \n",
      "Title :Disentangled Adversarial Autoencoder for Subject-Invariant Physiological\n",
      "  Feature Extraction\n",
      "  Recent developments in biosignal processing have enabled users to exploit\n",
      "their physiological status for manipulating devices in a reliable and safe\n",
      "manner. One major challenge of physiological sensing lies in the variability of\n",
      "biosignals across different users and tasks. To address this issue, we propose\n",
      "an adversarial feature extractor for transfer learning to exploit disentangled\n",
      "universal representations. We consider the trade-off between task-relevant\n",
      "features and user-discriminative information by introducing additional\n",
      "adversary and nuisance networks in order to manipulate the latent\n",
      "representations such that the learned feature extractor is applicable to\n",
      "unknown users and various tasks. Results on cross-subject transfer evaluations\n",
      "exhibit the benefits of the proposed framework, with up to 8.8% improvement in\n",
      "average accuracy of classification, and demonstrate adaptability to a broader\n",
      "range of subjects.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08546 \n",
      "Title :CheckThat! at CLEF 2020: Enabling the Automatic Identification and\n",
      "  Verification of Claims in Social Media\n",
      "  We describe the third edition of the CheckThat! Lab, which is part of the\n",
      "2020 Cross-Language Evaluation Forum (CLEF). CheckThat! proposes four\n",
      "complementary tasks and a related task from previous lab editions, offered in\n",
      "English, Arabic, and Spanish. Task 1 asks to predict which tweets in a Twitter\n",
      "stream are worth fact-checking. Task 2 asks to determine whether a claim posted\n",
      "in a tweet can be verified using a set of previously fact-checked claims. Task\n",
      "3 asks to retrieve text snippets from a given set of Web pages that would be\n",
      "useful for verifying a target tweet's claim. Task 4 asks to predict the\n",
      "veracity of a target tweet's claim using a set of Web pages and potentially\n",
      "useful snippets in them. Finally, the lab offers a fifth task that asks to\n",
      "predict the check-worthiness of the claims made in English political debates\n",
      "and speeches. CheckThat! features a full evaluation framework. The evaluation\n",
      "is carried out using mean average precision or precision at rank k for ranking\n",
      "tasks, and F1 for classification tasks.\n",
      "\n",
      "**Paper Id :2007.07997 \n",
      "Title :Overview of CheckThat! 2020: Automatic Identification and Verification\n",
      "  of Claims in Social Media\n",
      "  We present an overview of the third edition of the CheckThat! Lab at CLEF\n",
      "2020. The lab featured five tasks in two different languages: English and\n",
      "Arabic. The first four tasks compose the full pipeline of claim verification in\n",
      "social media: Task 1 on check-worthiness estimation, Task 2 on retrieving\n",
      "previously fact-checked claims, Task 3 on evidence retrieval, and Task 4 on\n",
      "claim verification. The lab is completed with Task 5 on check-worthiness\n",
      "estimation in political debates and speeches. A total of 67 teams registered to\n",
      "participate in the lab (up from 47 at CLEF 2019), and 23 of them actually\n",
      "submitted runs (compared to 14 at CLEF 2019). Most teams used deep neural\n",
      "networks based on BERT, LSTMs, or CNNs, and achieved sizable improvements over\n",
      "the baselines on all tasks. Here we describe the tasks setup, the evaluation\n",
      "results, and a summary of the approaches used by the participants, and we\n",
      "discuss some lessons learned. Last but not least, we release to the research\n",
      "community all datasets from the lab as well as the evaluation scripts, which\n",
      "should enable further research in the important tasks of check-worthiness\n",
      "estimation and automatic claim verification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08861 \n",
      "Title :Encoding Physical Constraints in Differentiable Newton-Euler Algorithm\n",
      "  The recursive Newton-Euler Algorithm (RNEA) is a popular technique for\n",
      "computing the dynamics of robots. RNEA can be framed as a differentiable\n",
      "computational graph, enabling the dynamics parameters of the robot to be\n",
      "learned from data via modern auto-differentiation toolboxes. However, the\n",
      "dynamics parameters learned in this manner can be physically implausible. In\n",
      "this work, we incorporate physical constraints in the learning by adding\n",
      "structure to the learned parameters. This results in a framework that can learn\n",
      "physically plausible dynamics via gradient descent, improving the training\n",
      "speed as well as generalization of the learned dynamics models. We evaluate our\n",
      "method on real-time inverse dynamics control tasks on a 7 degree of freedom\n",
      "robot arm, both in simulation and on the real robot. Our experiments study a\n",
      "spectrum of structure added to the parameters of the differentiable RNEA\n",
      "algorithm, and compare their performance and generalization.\n",
      "\n",
      "**Paper Id :2007.02168 \n",
      "Title :Scalable Differentiable Physics for Learning and Control\n",
      "  Differentiable physics is a powerful approach to learning and control\n",
      "problems that involve physical objects and environments. While notable progress\n",
      "has been made, the capabilities of differentiable physics solvers remain\n",
      "limited. We develop a scalable framework for differentiable physics that can\n",
      "support a large number of objects and their interactions. To accommodate\n",
      "objects with arbitrary geometry and topology, we adopt meshes as our\n",
      "representation and leverage the sparsity of contacts for scalable\n",
      "differentiable collision handling. Collisions are resolved in localized regions\n",
      "to minimize the number of optimization variables even when the number of\n",
      "simulated objects is high. We further accelerate implicit differentiation of\n",
      "optimization with nonlinear constraints. Experiments demonstrate that the\n",
      "presented framework requires up to two orders of magnitude less memory and\n",
      "computation in comparison to recent particle-based methods. We further validate\n",
      "the approach on inverse problems and control scenarios, where it outperforms\n",
      "derivative-free and model-free baselines by at least an order of magnitude.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08896 \n",
      "Title :Compressing Language Models using Doped Kronecker Products\n",
      "  Kronecker Products (KP) have been used to compress IoT RNN Applications by\n",
      "15-38x compression factors, achieving better results than traditional\n",
      "compression methods. However when KP is applied to large Natural Language\n",
      "Processing tasks, it leads to significant accuracy loss (approx 26%). This\n",
      "paper proposes a way to recover accuracy otherwise lost when applying KP to\n",
      "large NLP tasks, by allowing additional degrees of freedom in the KP matrix.\n",
      "More formally, we propose doping, a process of adding an extremely sparse\n",
      "overlay matrix on top of the pre-defined KP structure. We call this compression\n",
      "method doped kronecker product compression. To train these models, we present a\n",
      "new solution to the phenomenon of co-matrix adaption (CMA), which uses a new\n",
      "regularization scheme called co matrix dropout regularization (CMR). We present\n",
      "experimental results that demonstrate compression of a large language model\n",
      "with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At\n",
      "25x compression, an equivalent pruned network leads to 7.9% loss in perplexity\n",
      "score, while HMD and LMF lead to 15% and 27% loss in perplexity score\n",
      "respectively.\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.08985 \n",
      "Title :Identification of Chimera using Machine Learning\n",
      "  Chimera state refers to coexistence of coherent and non-coherent phases in\n",
      "identically coupled dynamical units found in various complex dynamical systems.\n",
      "Identification of Chimera, on one hand is essential due to its applicability in\n",
      "various areas including neuroscience, and on other hand is challenging due to\n",
      "its widely varied appearance in different systems and the peculiar nature of\n",
      "its profile. Therefore, a simple yet universal method for its identification\n",
      "remains an open problem. Here, we present a very distinctive approach using\n",
      "machine learning techniques to characterize different dynamical phases and\n",
      "identify the chimera state from given spatial profiles generated using various\n",
      "different models. The experimental results show that the performance of the\n",
      "classification algorithms varies for different dynamical models. The machine\n",
      "learning algorithms, namely random forest, oblique random forest based on\n",
      "tikhonov, parallel-axis split and null space regularization achieved more than\n",
      "$96\\% $ accuracy for the Kuramoto model. For the logistic-maps, random forest\n",
      "and tikhonov regularization based oblique random forest showed more than $90\\%$\n",
      "accuracy, and for the H\\'enon-Map model, random forest, null-space and\n",
      "axis-parallel split regularization based oblique random forest achieved more\n",
      "than $80\\%$ accuracy. The oblique random forest with null space regularization\n",
      "achieved consistent performance (more than $83\\%$ accuracy) across different\n",
      "dynamical models while the auto-encoder based random vector functional link\n",
      "neural network showed relatively lower performance. This work provides a\n",
      "direction for employing machine learning techniques to identify dynamical\n",
      "patterns arising in coupled non-linear units on large-scale, and for\n",
      "characterizing complex spatio-temporal patterns in real-world systems for\n",
      "various applications.\n",
      "\n",
      "**Paper Id :2007.00936 \n",
      "Title :Deep Neural Networks for Nonlinear Model Order Reduction of Unsteady\n",
      "  Flows\n",
      "  Unsteady fluid systems are nonlinear high-dimensional dynamical systems that\n",
      "may exhibit multiple complex phenomena both in time and space. Reduced Order\n",
      "Modeling (ROM) of fluid flows has been an active research topic in the recent\n",
      "decade with the primary goal to decompose complex flows to a set of features\n",
      "most important for future state prediction and control, typically using a\n",
      "dimensionality reduction technique. In this work, a novel data-driven technique\n",
      "based on the power of deep neural networks for reduced order modeling of the\n",
      "unsteady fluid flows is introduced. An autoencoder network is used for\n",
      "nonlinear dimension reduction and feature extraction as an alternative for\n",
      "singular value decomposition (SVD). Then, the extracted features are used as an\n",
      "input for long short-term memory network (LSTM) to predict the velocity field\n",
      "at future time instances. The proposed autoencoder-LSTM method is compared with\n",
      "non-intrusive reduced order models based on dynamic mode decomposition (DMD)\n",
      "and proper orthogonal decomposition (POD). Moreover, an autoencoder-DMD\n",
      "algorithm is introduced for reduced order modeling, which uses the autoencoder\n",
      "network for dimensionality reduction rather than SVD rank truncation. Results\n",
      "show that the autoencoder-LSTM method is considerably capable of predicting\n",
      "fluid flow evolution, where higher values for coefficient of determination\n",
      "$R^{2}$ are obtained using autoencoder-LSTM compared to other models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.09330 \n",
      "Title :Intent Classification in Question-Answering Using LSTM Architectures\n",
      "  Question-answering (QA) is certainly the best known and probably also one of\n",
      "the most complex problem within Natural Language Processing (NLP) and\n",
      "artificial intelligence (AI). Since the complete solution to the problem of\n",
      "finding a generic answer still seems far away, the wisest thing to do is to\n",
      "break down the problem by solving single simpler parts. Assuming a modular\n",
      "approach to the problem, we confine our research to intent classification for\n",
      "an answer, given a question. Through the use of an LSTM network, we show how\n",
      "this type of classification can be approached effectively and efficiently, and\n",
      "how it can be properly used within a basic prototype responder.\n",
      "\n",
      "**Paper Id :1905.11481 \n",
      "Title :AI Feynman: a Physics-Inspired Method for Symbolic Regression\n",
      "  A core challenge for both physics and artificial intellicence (AI) is\n",
      "symbolic regression: finding a symbolic expression that matches data from an\n",
      "unknown function. Although this problem is likely to be NP-hard in principle,\n",
      "functions of practical interest often exhibit symmetries, separability,\n",
      "compositionality and other simplifying properties. In this spirit, we develop a\n",
      "recursive multidimensional symbolic regression algorithm that combines neural\n",
      "network fitting with a suite of physics-inspired techniques. We apply it to 100\n",
      "equations from the Feynman Lectures on Physics, and it discovers all of them,\n",
      "while previous publicly available software cracks only 71; for a more difficult\n",
      "test set, we improve the state of the art success rate from 15% to 90%.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.09332 \n",
      "Title :An Analysis of Word2Vec for the Italian Language\n",
      "  Word representation is fundamental in NLP tasks, because it is precisely from\n",
      "the coding of semantic closeness between words that it is possible to think of\n",
      "teaching a machine to understand text. Despite the spread of word embedding\n",
      "concepts, still few are the achievements in linguistic contexts other than\n",
      "English. In this work, analysing the semantic capacity of the Word2Vec\n",
      "algorithm, an embedding for the Italian language is produced. Parameter setting\n",
      "such as the number of epochs, the size of the context window and the number of\n",
      "negatively backpropagated samples is explored.\n",
      "\n",
      "**Paper Id :1911.04975 \n",
      "Title :word2ket: Space-efficient Word Embeddings inspired by Quantum\n",
      "  Entanglement\n",
      "  Deep learning natural language processing models often use vector word\n",
      "embeddings, such as word2vec or GloVe, to represent words. A discrete sequence\n",
      "of words can be much more easily integrated with downstream neural layers if it\n",
      "is represented as a sequence of continuous vectors. Also, semantic\n",
      "relationships between words, learned from a text corpus, can be encoded in the\n",
      "relative configurations of the embedding vectors. However, storing and\n",
      "accessing embedding vectors for all words in a dictionary requires large amount\n",
      "of space, and may stain systems with limited GPU memory. Here, we used\n",
      "approaches inspired by quantum computing to propose two related methods, {\\em\n",
      "word2ket} and {\\em word2ketXS}, for storing word embedding matrix during\n",
      "training and inference in a highly efficient way. Our approach achieves a\n",
      "hundred-fold or more reduction in the space required to store the embeddings\n",
      "with almost no relative drop in accuracy in practical natural language\n",
      "processing tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.09382 \n",
      "Title :GraphAF: a Flow-based Autoregressive Model for Molecular Graph\n",
      "  Generation\n",
      "  Molecular graph generation is a fundamental problem for drug discovery and\n",
      "has been attracting growing attention. The problem is challenging since it\n",
      "requires not only generating chemically valid molecular structures but also\n",
      "optimizing their chemical properties in the meantime. Inspired by the recent\n",
      "progress in deep generative models, in this paper we propose a flow-based\n",
      "autoregressive model for graph generation called GraphAF. GraphAF combines the\n",
      "advantages of both autoregressive and flow-based approaches and enjoys: (1)\n",
      "high model flexibility for data density estimation; (2) efficient parallel\n",
      "computation for training; (3) an iterative sampling process, which allows\n",
      "leveraging chemical domain knowledge for valency checking. Experimental results\n",
      "show that GraphAF is able to generate 68% chemically valid molecules even\n",
      "without chemical knowledge rules and 100% valid molecules with chemical rules.\n",
      "The training process of GraphAF is two times faster than the existing\n",
      "state-of-the-art approach GCPN. After fine-tuning the model for goal-directed\n",
      "property optimization with reinforcement learning, GraphAF achieves\n",
      "state-of-the-art performance on both chemical property optimization and\n",
      "constrained property optimization.\n",
      "\n",
      "**Paper Id :2011.07225 \n",
      "Title :Reinforced Molecular Optimization with Neighborhood-Controlled Grammars\n",
      "  A major challenge in the pharmaceutical industry is to design novel molecules\n",
      "with specific desired properties, especially when the property evaluation is\n",
      "costly. Here, we propose MNCE-RL, a graph convolutional policy network for\n",
      "molecular optimization with molecular neighborhood-controlled embedding\n",
      "grammars through reinforcement learning. We extend the original\n",
      "neighborhood-controlled embedding grammars to make them applicable to molecular\n",
      "graph generation and design an efficient algorithm to infer grammatical\n",
      "production rules from given molecules. The use of grammars guarantees the\n",
      "validity of the generated molecular structures. By transforming molecular\n",
      "graphs to parse trees with the inferred grammars, the molecular structure\n",
      "generation task is modeled as a Markov decision process where a policy gradient\n",
      "strategy is utilized. In a series of experiments, we demonstrate that our\n",
      "approach achieves state-of-the-art performance in a diverse range of molecular\n",
      "optimization tasks and exhibits significant superiority in optimizing molecular\n",
      "properties with a limited number of property evaluations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.09841 \n",
      "Title :Coronary Artery Disease Diagnosis; Ranking the Significant Features\n",
      "  Using Random Trees Model\n",
      "  Heart disease is one of the most common diseases in middle-aged citizens.\n",
      "Among the vast number of heart diseases, the coronary artery disease (CAD) is\n",
      "considered as a common cardiovascular disease with a high death rate. The most\n",
      "popular tool for diagnosing CAD is the use of medical imaging, e.g.,\n",
      "angiography. However, angiography is known for being costly and also associated\n",
      "with a number of side effects. Hence, the purpose of this study is to increase\n",
      "the accuracy of coronary heart disease diagnosis through selecting significant\n",
      "predictive features in order of their ranking. In this study, we propose an\n",
      "integrated method using machine learning. The machine learning methods of\n",
      "random trees (RTs), decision tree of C5.0, support vector machine (SVM),\n",
      "decision tree of Chi-squared automatic interaction detection (CHAID) are used\n",
      "in this study. The proposed method shows promising results and the study\n",
      "confirms that RTs model outperforms other models.\n",
      "\n",
      "**Paper Id :2003.13145 \n",
      "Title :Can AI help in screening Viral and COVID-19 pneumonia?\n",
      "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\n",
      "caused thousands of causalities and infected several millions of people\n",
      "worldwide. Any technological tool enabling rapid screening of the COVID-19\n",
      "infection with high accuracy can be crucially helpful to healthcare\n",
      "professionals. The main clinical tool currently in use for the diagnosis of\n",
      "COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\n",
      "is expensive, less-sensitive and requires specialized medical personnel. X-ray\n",
      "imaging is an easily accessible tool that can be an excellent alternative in\n",
      "the COVID-19 diagnosis. This research was taken to investigate the utility of\n",
      "artificial intelligence (AI) in the rapid and accurate detection of COVID-19\n",
      "from chest X-ray images. The aim of this paper is to propose a robust technique\n",
      "for automatic detection of COVID-19 pneumonia from digital chest X-ray images\n",
      "applying pre-trained deep-learning algorithms while maximizing the detection\n",
      "accuracy. A public database was created by the authors combining several public\n",
      "databases and also by collecting images from recently published articles. The\n",
      "database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\n",
      "normal chest X-ray images. Transfer learning technique was used with the help\n",
      "of image augmentation to train and validate several pre-trained deep\n",
      "Convolutional Neural Networks (CNNs). The networks were trained to classify two\n",
      "different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\n",
      "COVID-19 pneumonia with and without image augmentation. The classification\n",
      "accuracy, precision, sensitivity, and specificity for both the schemes were\n",
      "99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\n",
      "respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.09902 \n",
      "Title :Predicting Yield Performance of Parents in Plant Breeding: A Neural\n",
      "  Collaborative Filtering Approach\n",
      "  Experimental corn hybrids are created in plant breeding programs by crossing\n",
      "two parents, so-called inbred and tester, together. Identification of best\n",
      "parent combinations for crossing is challenging since the total number of\n",
      "possible cross combinations of parents is large and it is impractical to test\n",
      "all possible cross combinations due to limited resources of time and budget. In\n",
      "the 2020 Syngenta Crop Challenge, Syngenta released several large datasets that\n",
      "recorded the historical yield performances of around 4% of total cross\n",
      "combinations of 593 inbreds with 496 testers which were planted in 280\n",
      "locations between 2016 and 2018 and asked participants to predict the yield\n",
      "performance of cross combinations of inbreds and testers that have not been\n",
      "planted based on the historical yield data collected from crossing other\n",
      "inbreds and testers. In this paper, we present a collaborative filtering method\n",
      "which is an ensemble of matrix factorization method and neural networks to\n",
      "solve this problem. Our computational results suggested that the proposed model\n",
      "significantly outperformed other models such as LASSO, random forest (RF), and\n",
      "neural networks. Presented method and results were produced within the 2020\n",
      "Syngenta Crop Challenge.\n",
      "\n",
      "**Paper Id :2005.13140 \n",
      "Title :SSM-Net for Plants Disease Identification in Low Data Regime\n",
      "  Plant disease detection is an essential factor in increasing agricultural\n",
      "production. Due to the difficulty of disease detection, farmers spray various\n",
      "pesticides on their crops to protect them, causing great harm to crop growth\n",
      "and food standards. Deep learning can offer critical aid in detecting such\n",
      "diseases. However, it is highly inconvenient to collect a large volume of data\n",
      "on all forms of the diseases afflicting a specific plant species. In this\n",
      "paper, we propose a new metrics-based few-shot learning SSM net architecture,\n",
      "which consists of stacked siamese and matching network components to address\n",
      "the problem of disease detection in low data regimes. We demonstrated our\n",
      "experiments on two datasets: mini-leaves diseases and sugarcane diseases\n",
      "dataset. We have showcased that the SSM-Net approach can achieve better\n",
      "decision boundaries with an accuracy of 92.7% on the mini-leaves dataset and\n",
      "94.3% on the sugarcane dataset. The accuracy increased by ~10% and ~5%\n",
      "respectively, compared to the widely used VGG16 transfer learning approach.\n",
      "Furthermore, we attained F1 score of 0.90 using SSM Net on the sugarcane\n",
      "dataset and 0.91 on the mini-leaves dataset. Our code implementation is\n",
      "available on Github: https://github.com/shruti-jadon/PlantsDiseaseDetection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.10283 \n",
      "Title :Real-time calibration of coherent-state receivers: learning by trial and\n",
      "  error\n",
      "  The optimal discrimination of coherent states of light with current\n",
      "technology is a key problem in classical and quantum communication, whose\n",
      "solution would enable the realization of efficient receivers for long-distance\n",
      "communications in free-space and optical fiber channels. In this article, we\n",
      "show that reinforcement learning (RL) protocols allow an agent to learn\n",
      "near-optimal coherent-state receivers made of passive linear optics,\n",
      "photodetectors and classical adaptive control. Each agent is trained and tested\n",
      "in real time over several runs of independent discrimination experiments and\n",
      "has no knowledge about the energy of the states nor the receiver setup nor the\n",
      "quantum-mechanical laws governing the experiments. Based exclusively on the\n",
      "observed photodetector outcomes, the agent adaptively chooses among a set of ~3\n",
      "10^3 possible receiver setups, and obtains a reward at the end of each\n",
      "experiment if its guess is correct. At variance with previous applications of\n",
      "RL in quantum physics, the information gathered in each run is intrinsically\n",
      "stochastic and thus insufficient to evaluate exactly the performance of the\n",
      "chosen receiver. Nevertheless, we present families of agents that: (i) discover\n",
      "a receiver beating the best Gaussian receiver after ~3 10^2 experiments; (ii)\n",
      "surpass the cumulative reward of the best Gaussian receiver after ~10^3\n",
      "experiments; (iii) simultaneously discover a near-optimal receiver and attain\n",
      "its cumulative reward after ~10^5 experiments. Our results show that RL\n",
      "techniques are suitable for on-line control of quantum receivers and can be\n",
      "employed for long-distance communications over potentially unknown channels.\n",
      "\n",
      "**Paper Id :2002.01068 \n",
      "Title :Policy Gradient based Quantum Approximate Optimization Algorithm\n",
      "  The quantum approximate optimization algorithm (QAOA), as a hybrid\n",
      "quantum/classical algorithm, has received much interest recently. QAOA can also\n",
      "be viewed as a variational ansatz for quantum control. However, its direct\n",
      "application to emergent quantum technology encounters additional physical\n",
      "constraints: (i) the states of the quantum system are not observable; (ii)\n",
      "obtaining the derivatives of the objective function can be computationally\n",
      "expensive or even inaccessible in experiments, and (iii) the values of the\n",
      "objective function may be sensitive to various sources of uncertainty, as is\n",
      "the case for noisy intermediate-scale quantum (NISQ) devices. Taking such\n",
      "constraints into account, we show that policy-gradient-based reinforcement\n",
      "learning (RL) algorithms are well suited for optimizing the variational\n",
      "parameters of QAOA in a noise-robust fashion, opening up the way for developing\n",
      "RL techniques for continuous quantum control. This is advantageous to help\n",
      "mitigate and monitor the potentially unknown sources of errors in modern\n",
      "quantum simulators. We analyze the performance of the algorithm for quantum\n",
      "state transfer problems in single- and multi-qubit systems, subject to various\n",
      "sources of noise such as error terms in the Hamiltonian, or quantum uncertainty\n",
      "in the measurement process. We show that, in noisy setups, it is capable of\n",
      "outperforming state-of-the-art existing optimization algorithms.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.10477 \n",
      "Title :Statistical Limits of Supervised Quantum Learning\n",
      "  Within the framework of statistical learning theory it is possible to bound\n",
      "the minimum number of samples required by a learner to reach a target accuracy.\n",
      "We show that if the bound on the accuracy is taken into account, quantum\n",
      "machine learning algorithms for supervised learning---for which statistical\n",
      "guarantees are available---cannot achieve polylogarithmic runtimes in the input\n",
      "dimension. We conclude that, when no further assumptions on the problem are\n",
      "made, quantum machine learning algorithms for supervised learning can have at\n",
      "most polynomial speedups over efficient classical algorithms, even in cases\n",
      "where quantum access to the data is naturally available.\n",
      "\n",
      "**Paper Id :1912.03283 \n",
      "Title :A quantum active learning algorithm for sampling against adversarial\n",
      "  attacks\n",
      "  Adversarial attacks represent a serious menace for learning algorithms and\n",
      "may compromise the security of future autonomous systems. A theorem by Khoury\n",
      "and Hadfield-Menell (KH), provides sufficient conditions to guarantee the\n",
      "robustness of machine learning algorithms, but comes with a caveat: it is\n",
      "crucial to know the smallest distance among the classes of the corresponding\n",
      "classification problem. We propose a theoretical framework that allows us to\n",
      "think of active learning as sampling the most promising new points to be\n",
      "classified, so that the minimum distance between classes can be found and the\n",
      "theorem KH used. Additionally, we introduce a quantum active learning algorithm\n",
      "that makes use of such framework and whose complexity is polylogarithmic in the\n",
      "dimension of the space, $m$, and the size of the initial training data $n$,\n",
      "provided the use of qRAMs; and polynomial in the precision, achieving an\n",
      "exponential speedup over the equivalent classical algorithm in $n$ and $m$.\n",
      "This algorithm may be nevertheless `dequantized' reducing the advantage to\n",
      "polynomial.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.10505 \n",
      "Title :Data-driven control of micro-climate in buildings: an event-triggered\n",
      "  reinforcement learning approach\n",
      "  Smart buildings have great potential for shaping an energy-efficient,\n",
      "sustainable, and more economic future for our planet as buildings account for\n",
      "approximately 40% of the global energy consumption. Future of the smart\n",
      "buildings lies in using sensory data for adaptive decision making and control\n",
      "that is currently gloomed by the key challenge of learning a good control\n",
      "policy in a short period of time in an online and continuing fashion. To tackle\n",
      "this challenge, an event-triggered -- as opposed to classic time-triggered --\n",
      "paradigm, is proposed in which learning and control decisions are made when\n",
      "events occur and enough information is collected. Events are characterized by\n",
      "certain design conditions and they occur when the conditions are met, for\n",
      "instance, when a certain state threshold is reached. By systematically\n",
      "adjusting the time of learning and control decisions, the proposed framework\n",
      "can potentially reduce the variance in learning, and consequently, improve the\n",
      "control process. We formulate the micro-climate control problem based on\n",
      "semi-Markov decision processes that allow for variable-time state transitions\n",
      "and decision making. Using extended policy gradient theorems and temporal\n",
      "difference methods in a reinforcement learning set-up, we propose two learning\n",
      "algorithms for event-triggered control of micro-climate in buildings. We show\n",
      "the efficacy of our proposed approach via designing a smart learning thermostat\n",
      "that simultaneously optimizes energy consumption and occupants' comfort in a\n",
      "test building.\n",
      "\n",
      "**Paper Id :1905.10891 \n",
      "Title :A hybrid model for predicting human physical activity status from\n",
      "  lifelogging data\n",
      "  One trend in the recent healthcare transformations is people are encouraged\n",
      "to monitor and manage their health based on their daily diets and physical\n",
      "activity habits. However, much attention of the use of operational research and\n",
      "analytical models in healthcare has been paid to the systematic level such as\n",
      "country or regional policy making or organisational issues. This paper proposes\n",
      "a model concerned with healthcare analytics at the individual level, which can\n",
      "predict human physical activity status from sequential lifelogging data\n",
      "collected from wearable sensors. The model has a two-stage hybrid structure (in\n",
      "short, MOGP-HMM) -- a multi-objective genetic programming (MOGP) algorithm in\n",
      "the first stage to reduce the dimensions of lifelogging data and a hidden\n",
      "Markov model (HMM) in the second stage for activity status prediction over\n",
      "time. It can be used as a decision support tool to provide real-time\n",
      "monitoring, statistical analysis and personalized advice to individuals,\n",
      "encouraging positive attitudes towards healthy lifestyles. We validate the\n",
      "model with the real data collected from a group of participants in the UK, and\n",
      "compare it with other popular two-stage hybrid models. Our experimental results\n",
      "show that the MOGP-HMM can achieve comparable performance. To the best of our\n",
      "knowledge, this is the very first study that uses the MOGP in the hybrid\n",
      "two-stage structure for individuals' activity status prediction. It fits\n",
      "seamlessly with the current trend in the UK healthcare transformation of\n",
      "patient empowerment as well as contributing to a strategic development for more\n",
      "efficient and cost-effective provision of healthcare.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.10529 \n",
      "Title :Submodular Rank Aggregation on Score-based Permutations for Distributed\n",
      "  Automatic Speech Recognition\n",
      "  Distributed automatic speech recognition (ASR) requires to aggregate outputs\n",
      "of distributed deep neural network (DNN)-based models. This work studies the\n",
      "use of submodular functions to design a rank aggregation on score-based\n",
      "permutations, which can be used for distributed ASR systems in both supervised\n",
      "and unsupervised modes. Specifically, we compose an aggregation rank function\n",
      "based on the Lovasz Bregman divergence for setting up linear structured convex\n",
      "and nested structured concave functions. The algorithm is based on stochastic\n",
      "gradient descent (SGD) and can obtain well-trained aggregation models. Our\n",
      "experiments on the distributed ASR system show that the submodular rank\n",
      "aggregation can obtain higher speech recognition accuracy than traditional\n",
      "aggregation methods like Adaboost. Code is available\n",
      "online~\\footnote{https://github.com/uwjunqi/Subrank}.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.10685 \n",
      "Title :PulseSatellite: A tool using human-AI feedback loops for satellite image\n",
      "  analysis in humanitarian contexts\n",
      "  Humanitarian response to natural disasters and conflicts can be assisted by\n",
      "satellite image analysis. In a humanitarian context, very specific satellite\n",
      "image analysis tasks must be done accurately and in a timely manner to provide\n",
      "operational support. We present PulseSatellite, a collaborative satellite image\n",
      "analysis tool which leverages neural network models that can be retrained\n",
      "on-the fly and adapted to specific humanitarian contexts and geographies. We\n",
      "present two case studies, in mapping shelters and floods respectively, that\n",
      "illustrate the capabilities of PulseSatellite.\n",
      "\n",
      "**Paper Id :2010.11010 \n",
      "Title :Complex data labeling with deep learning methods: Lessons from fisheries\n",
      "  acoustics\n",
      "  Quantitative and qualitative analysis of acoustic backscattered signals from\n",
      "the seabed bottom to the sea surface is used worldwide for fish stocks\n",
      "assessment and marine ecosystem monitoring. Huge amounts of raw data are\n",
      "collected yet require tedious expert labeling. This paper focuses on a case\n",
      "study where the ground truth labels are non-obvious: echograms labeling, which\n",
      "is time-consuming and critical for the quality of fisheries and ecological\n",
      "analysis. We investigate how these tasks can benefit from supervised learning\n",
      "algorithms and demonstrate that convolutional neural networks trained with\n",
      "non-stationary datasets can be used to stress parts of a new dataset needing\n",
      "human expert correction. Further development of this approach paves the way\n",
      "toward a standardization of the labeling process in fisheries acoustics and is\n",
      "a good case study for non-obvious data labeling processes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11085 \n",
      "Title :Deep Channel Learning For Large Intelligent Surfaces Aided mm-Wave\n",
      "  Massive MIMO Systems\n",
      "  This letter presents the first work introducing a deep learning (DL)\n",
      "framework for channel estimation in large intelligent surface (LIS) assisted\n",
      "massive MIMO (multiple-input multiple-output) systems. A twin convolutional\n",
      "neural network (CNN) architecture is designed and it is fed with the received\n",
      "pilot signals to estimate both direct and cascaded channels. In a multi-user\n",
      "scenario, each user has access to the CNN to estimate its own channel. The\n",
      "performance of the proposed DL approach is evaluated and compared with\n",
      "state-of-the-art DL-based techniques and its superior performance is\n",
      "demonstrated.\n",
      "\n",
      "**Paper Id :2002.09821 \n",
      "Title :A Multi-view CNN-based Acoustic Classification System for Automatic\n",
      "  Animal Species Identification\n",
      "  Automatic identification of animal species by their vocalization is an\n",
      "important and challenging task. Although many kinds of audio monitoring system\n",
      "have been proposed in the literature, they suffer from several disadvantages\n",
      "such as non-trivial feature selection, accuracy degradation because of\n",
      "environmental noise or intensive local computation. In this paper, we propose a\n",
      "deep learning based acoustic classification framework for Wireless Acoustic\n",
      "Sensor Network (WASN). The proposed framework is based on cloud architecture\n",
      "which relaxes the computational burden on the wireless sensor node. To improve\n",
      "the recognition accuracy, we design a multi-view Convolution Neural Network\n",
      "(CNN) to extract the short-, middle-, and long-term dependencies in parallel.\n",
      "The evaluation on two real datasets shows that the proposed architecture can\n",
      "achieve high accuracy and outperforms traditional classification systems\n",
      "significantly when the environmental noise dominate the audio signal (low SNR).\n",
      "Moreover, we implement and deploy the proposed system on a testbed and analyse\n",
      "the system performance in real-world environments. Both simulation and\n",
      "real-world evaluation demonstrate the accuracy and robustness of the proposed\n",
      "acoustic classification system in distinguishing species of animals.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11263 \n",
      "Title :Sound field reconstruction in rooms: inpainting meets super-resolution\n",
      "  In this paper, a deep-learning-based method for sound field reconstruction is\n",
      "proposed. It is shown the possibility to reconstruct the magnitude of the sound\n",
      "pressure in the frequency band 30-300 Hz for an entire room by using a very low\n",
      "number of irregularly distributed microphones arbitrarily arranged. Moreover,\n",
      "the approach is agnostic to the location of the measurements in the Euclidean\n",
      "space. In particular, the presented approach uses a limited number of arbitrary\n",
      "discrete measurements of the magnitude of the sound field pressure in order to\n",
      "extrapolate this field to a higher-resolution grid of discrete points in space\n",
      "with a low computational complexity. The method is based on a U-net-like neural\n",
      "network with partial convolutions trained solely on simulated data, which\n",
      "itself is constructed from numerical simulations of Green's function across\n",
      "thousands of common rectangular rooms. Although extensible to three dimensions\n",
      "and different room shapes, the method focuses on reconstructing a\n",
      "two-dimensional plane of a rectangular room from measurements of the\n",
      "three-dimensional sound field. Experiments using simulated data together with\n",
      "an experimental validation in a real listening room are shown. The results\n",
      "suggest a performance which may exceed conventional reconstruction techniques\n",
      "for a low number of microphones and computational requirements.\n",
      "\n",
      "**Paper Id :1911.07571 \n",
      "Title :Casimir effect with machine learning\n",
      "  Vacuum fluctuations of quantum fields between physical objects depend on the\n",
      "shapes, positions, and internal composition of the latter. For objects of\n",
      "arbitrary shapes, even made from idealized materials, the calculation of the\n",
      "associated zero-point (Casimir) energy is an analytically intractable\n",
      "challenge. We propose a new numerical approach to this problem based on\n",
      "machine-learning techniques and illustrate the effectiveness of the method in a\n",
      "(2+1) dimensional scalar field theory. The Casimir energy is first calculated\n",
      "numerically using a Monte-Carlo algorithm for a set of the Dirichlet boundaries\n",
      "of various shapes. Then, a neural network is trained to compute this energy\n",
      "given the Dirichlet domain, treating the latter as black-and-white pixelated\n",
      "images. We show that after the learning phase, the neural network is able to\n",
      "quickly predict the Casimir energy for new boundaries of general shapes with\n",
      "reasonable accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11268 \n",
      "Title :Data Mining in Clinical Trial Text: Transformers for Classification and\n",
      "  Question Answering Tasks\n",
      "  This research on data extraction methods applies recent advances in natural\n",
      "language processing to evidence synthesis based on medical texts. Texts of\n",
      "interest include abstracts of clinical trials in English and in multilingual\n",
      "contexts. The main focus is on information characterized via the Population,\n",
      "Intervention, Comparator, and Outcome (PICO) framework, but data extraction is\n",
      "not limited to these fields. Recent neural network architectures based on\n",
      "transformers show capacities for transfer learning and increased performance on\n",
      "downstream natural language processing tasks such as universal reading\n",
      "comprehension, brought forward by this architecture's use of contextualized\n",
      "word embeddings and self-attention mechanisms. This paper contributes to\n",
      "solving problems related to ambiguity in PICO sentence prediction tasks, as\n",
      "well as highlighting how annotations for training named entity recognition\n",
      "systems are used to train a high-performing, but nevertheless flexible\n",
      "architecture for question answering in systematic review automation.\n",
      "Additionally, it demonstrates how the problem of insufficient amounts of\n",
      "training annotations for PICO entity extraction is tackled by augmentation. All\n",
      "models in this paper were created with the aim to support systematic review\n",
      "(semi)automation. They achieve high F1 scores, and demonstrate the feasibility\n",
      "of applying transformer-based classification methods to support data mining in\n",
      "the biomedical literature.\n",
      "\n",
      "**Paper Id :2010.11727 \n",
      "Title :Vision-Based Layout Detection from Scientific Literature using Recurrent\n",
      "  Convolutional Neural Networks\n",
      "  We present an approach for adapting convolutional neural networks for object\n",
      "recognition and classification to scientific literature layout detection\n",
      "(SLLD), a shared subtask of several information extraction problems. Scientific\n",
      "publications contain multiple types of information sought by researchers in\n",
      "various disciplines, organized into an abstract, bibliography, and sections\n",
      "documenting related work, experimental methods, and results; however, there is\n",
      "no effective way to extract this information due to their diverse layout. In\n",
      "this paper, we present a novel approach to developing an end-to-end learning\n",
      "framework to segment and classify major regions of a scientific document. We\n",
      "consider scientific document layout analysis as an object detection task over\n",
      "digital images, without any additional text features that need to be added into\n",
      "the network during the training process. Our technical objective is to\n",
      "implement transfer learning via fine-tuning of pre-trained networks and thereby\n",
      "demonstrate that this deep learning architecture is suitable for tasks that\n",
      "lack very large document corpora for training ab initio. As part of the\n",
      "experimental test bed for empirical evaluation of this approach, we created a\n",
      "merged multi-corpus data set for scientific publication layout detection tasks.\n",
      "Our results show good improvement with fine-tuning of a pre-trained base\n",
      "network using this merged data set, compared to the baseline convolutional\n",
      "neural network architecture.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11274 \n",
      "Title :Scalable Psychological Momentum Forecasting in Esports\n",
      "  The world of competitive Esports and video gaming has seen and continues to\n",
      "experience steady growth in popularity and complexity. Correspondingly, more\n",
      "research on the topic is being published, ranging from social network analyses\n",
      "to the benchmarking of advanced artificial intelligence systems in playing\n",
      "against humans. In this paper, we present ongoing work on an intelligent agent\n",
      "recommendation engine that suggests actions to players in order to maximise\n",
      "success and enjoyment, both in the space of in-game choices, as well as\n",
      "decisions made around play session timing in the broader context. By leveraging\n",
      "temporal data and appropriate models, we show that a learned representation of\n",
      "player psychological momentum, and of tilt, can be used, in combination with\n",
      "player expertise, to achieve state-of-the-art performance in pre- and\n",
      "post-draft win prediction. Our progress toward fulfilling the potential for\n",
      "deriving optimal recommendations is documented.\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11569 \n",
      "Title :Tiny noise, big mistakes: Adversarial perturbations induce errors in\n",
      "  Brain-Computer Interface spellers\n",
      "  An electroencephalogram (EEG) based brain-computer interface (BCI) speller\n",
      "allows a user to input text to a computer by thought. It is particularly useful\n",
      "to severely disabled individuals, e.g., amyotrophic lateral sclerosis patients,\n",
      "who have no other effective means of communication with another person or a\n",
      "computer. Most studies so far focused on making EEG-based BCI spellers faster\n",
      "and more reliable; however, few have considered their security. This study, for\n",
      "the first time, shows that P300 and steady-state visual evoked potential BCI\n",
      "spellers are very vulnerable, i.e., they can be severely attacked by\n",
      "adversarial perturbations, which are too tiny to be noticed when added to EEG\n",
      "signals, but can mislead the spellers to spell anything the attacker wants. The\n",
      "consequence could range from merely user frustration to severe misdiagnosis in\n",
      "clinical applications. We hope our research can attract more attention to the\n",
      "security of EEG-based BCI spellers, and more broadly, EEG-based BCIs, which has\n",
      "received little attention before.\n",
      "\n",
      "**Paper Id :2004.06286 \n",
      "Title :Transfer Learning for EEG-Based Brain-Computer Interfaces: A Review of\n",
      "  Progress Made Since 2016\n",
      "  A brain-computer interface (BCI) enables a user to communicate with a\n",
      "computer directly using brain signals. The most common non-invasive BCI\n",
      "modality, electroencephalogram (EEG), is sensitive to noise/artifact and\n",
      "suffers between-subject/within-subject non-stationarity. Therefore, it is\n",
      "difficult to build a generic pattern recognition model in an EEG-based BCI\n",
      "system that is optimal for different subjects, during different sessions, for\n",
      "different devices and tasks. Usually, a calibration session is needed to\n",
      "collect some training data for a new subject, which is time-consuming and user\n",
      "unfriendly. Transfer learning (TL), which utilizes data or knowledge from\n",
      "similar or relevant subjects/sessions/devices/tasks to facilitate learning for\n",
      "a new subject/session/device/task, is frequently used to reduce the amount of\n",
      "calibration effort. This paper reviews journal publications on TL approaches in\n",
      "EEG-based BCIs in the last few years, i.e., since 2016. Six paradigms and\n",
      "applications -- motor imagery, event-related potentials, steady-state visual\n",
      "evoked potentials, affective BCIs, regression problems, and adversarial attacks\n",
      "-- are considered. For each paradigm/application, we group the TL approaches\n",
      "into cross-subject/session, cross-device, and cross-task settings and review\n",
      "them separately. Observations and conclusions are made at the end of the paper,\n",
      "which may point to future research directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11612 \n",
      "Title :Search for Better Students to Learn Distilled Knowledge\n",
      "  Knowledge Distillation, as a model compression technique, has received great\n",
      "attention. The knowledge of a well-performed teacher is distilled to a student\n",
      "with a small architecture. The architecture of the small student is often\n",
      "chosen to be similar to their teacher's, with fewer layers or fewer channels,\n",
      "or both. However, even with the same number of FLOPs or parameters, the\n",
      "students with different architecture can achieve different generalization\n",
      "ability. The configuration of a student architecture requires intensive network\n",
      "architecture engineering. In this work, instead of designing a good student\n",
      "architecture manually, we propose to search for the optimal student\n",
      "automatically. Based on L1-norm optimization, a subgraph from the teacher\n",
      "network topology graph is selected as a student, the goal of which is to\n",
      "minimize the KL-divergence between student's and teacher's outputs. We verify\n",
      "the proposal on CIFAR10 and CIFAR100 datasets. The empirical experiments show\n",
      "that the learned student architecture achieves better performance than ones\n",
      "specified manually. We also visualize and understand the architecture of the\n",
      "found student.\n",
      "\n",
      "**Paper Id :2009.09140 \n",
      "Title :Introspective Learning by Distilling Knowledge from Online\n",
      "  Self-explanation\n",
      "  In recent years, many explanation methods have been proposed to explain\n",
      "individual classifications of deep neural networks. However, how to leverage\n",
      "the created explanations to improve the learning process has been less\n",
      "explored. As the privileged information, the explanations of a model can be\n",
      "used to guide the learning process of the model itself. In the community,\n",
      "another intensively investigated privileged information used to guide the\n",
      "training of a model is the knowledge from a powerful teacher model. The goal of\n",
      "this work is to leverage the self-explanation to improve the learning process\n",
      "by borrowing ideas from knowledge distillation. We start by investigating the\n",
      "effective components of the knowledge transferred from the teacher network to\n",
      "the student network. Our investigation reveals that both the responses in\n",
      "non-ground-truth classes and class-similarity information in teacher's outputs\n",
      "contribute to the success of the knowledge distillation. Motivated by the\n",
      "conclusion, we propose an implementation of introspective learning by\n",
      "distilling knowledge from online self-explanations. The models trained with the\n",
      "introspective learning procedure outperform the ones trained with the standard\n",
      "learning procedure, as well as the ones trained with different regularization\n",
      "methods. When compared to the models learned from peer networks or teacher\n",
      "networks, our models also show competitive performance and requires neither\n",
      "peers nor teachers.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11767 \n",
      "Title :Automatic lung segmentation in routine imaging is primarily a data\n",
      "  diversity problem, not a methodology problem\n",
      "  Automated segmentation of anatomical structures is a crucial step in image\n",
      "analysis. For lung segmentation in computed tomography, a variety of approaches\n",
      "exist, involving sophisticated pipelines trained and validated on different\n",
      "datasets. However, the clinical applicability of these approaches across\n",
      "diseases remains limited. We compared four generic deep learning approaches\n",
      "trained on various datasets and two readily available lung segmentation\n",
      "algorithms. We performed evaluation on routine imaging data with more than six\n",
      "different disease patterns and three published data sets. Using different deep\n",
      "learning approaches, mean Dice similarity coefficients (DSCs) on test datasets\n",
      "varied not over 0.02. When trained on a diverse routine dataset (n = 36) a\n",
      "standard approach (U-net) yields a higher DSC (0.97 $\\pm$ 0.05) compared to\n",
      "training on public datasets such as Lung Tissue Research Consortium (0.94 $\\pm$\n",
      "0.13, p = 0.024) or Anatomy 3 (0.92 $\\pm$ 0.15, p = 0.001). Trained on routine\n",
      "data (n = 231) covering multiple diseases, U-net compared to reference methods\n",
      "yields a DSC of 0.98 $\\pm$ 0.03 versus 0.94 $\\pm$ 0.12 (p = 0.024).\n",
      "\n",
      "**Paper Id :1912.07354 \n",
      "Title :Deep learning-based survival prediction for multiple cancer types using\n",
      "  histopathology images\n",
      "  Prognostic information at diagnosis has important implications for cancer\n",
      "treatment and monitoring. Although cancer staging, histopathological\n",
      "assessment, molecular features, and clinical variables can provide useful\n",
      "prognostic insights, improving risk stratification remains an active research\n",
      "area. We developed a deep learning system (DLS) to predict disease specific\n",
      "survival across 10 cancer types from The Cancer Genome Atlas (TCGA). We used a\n",
      "weakly-supervised approach without pixel-level annotations, and tested three\n",
      "different survival loss functions. The DLS was developed using 9,086 slides\n",
      "from 3,664 cases and evaluated using 3,009 slides from 1,216 cases. In\n",
      "multivariable Cox regression analysis of the combined cohort including all 10\n",
      "cancers, the DLS was significantly associated with disease specific survival\n",
      "(hazard ratio of 1.58, 95% CI 1.28-1.70, p<0.0001) after adjusting for cancer\n",
      "type, stage, age, and sex. In a per-cancer adjusted subanalysis, the DLS\n",
      "remained a significant predictor of survival in 5 of 10 cancer types. Compared\n",
      "to a baseline model including stage, age, and sex, the c-index of the model\n",
      "demonstrated an absolute 3.7% improvement (95% CI 1.0-6.5) in the combined\n",
      "cohort. Additionally, our models stratified patients within individual cancer\n",
      "stages, particularly stage II (p=0.025) and stage III (p<0.001). By developing\n",
      "and evaluating prognostic models across multiple cancer types, this work\n",
      "represents one of the most comprehensive studies exploring the direct\n",
      "prediction of clinical outcomes using deep learning and histopathology images.\n",
      "Our analysis demonstrates the potential for this approach to provide prognostic\n",
      "information in multiple cancer types, and even within specific pathologic\n",
      "stages. However, given the relatively small number of clinical events, we\n",
      "observed wide confidence intervals, suggesting that future work will benefit\n",
      "from larger datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2001.11818 \n",
      "Title :Community Detection in Bipartite Networks with Stochastic Blockmodels\n",
      "  In bipartite networks, community structures are restricted to being\n",
      "disassortative, in that nodes of one type are grouped according to common\n",
      "patterns of connection with nodes of the other type. This makes the stochastic\n",
      "block model (SBM), a highly flexible generative model for networks with block\n",
      "structure, an intuitive choice for bipartite community detection. However,\n",
      "typical formulations of the SBM do not make use of the special structure of\n",
      "bipartite networks. Here we introduce a Bayesian nonparametric formulation of\n",
      "the SBM and a corresponding algorithm to efficiently find communities in\n",
      "bipartite networks which parsimoniously chooses the number of communities. The\n",
      "biSBM improves community detection results over general SBMs when data are\n",
      "noisy, improves the model resolution limit by a factor of $\\sqrt{2}$, and\n",
      "expands our understanding of the complicated optimization landscape associated\n",
      "with community detection tasks. A direct comparison of certain terms of the\n",
      "prior distributions in the biSBM and a related high-resolution hierarchical SBM\n",
      "also reveals a counterintuitive regime of community detection problems,\n",
      "populated by smaller and sparser networks, where nonhierarchical models\n",
      "outperform their more flexible counterpart.\n",
      "\n",
      "**Paper Id :2004.04704 \n",
      "Title :Heuristics for Link Prediction in Multiplex Networks\n",
      "  Link prediction, or the inference of future or missing connections between\n",
      "entities, is a well-studied problem in network analysis. A multitude of\n",
      "heuristics exist for link prediction in ordinary networks with a single type of\n",
      "connection. However, link prediction in multiplex networks, or networks with\n",
      "multiple types of connections, is not a well understood problem. We propose a\n",
      "novel general framework and three families of heuristics for multiplex network\n",
      "link prediction that are simple, interpretable, and take advantage of the rich\n",
      "connection type correlation structure that exists in many real world networks.\n",
      "We further derive a theoretical threshold for determining when to use a\n",
      "different connection type based on the number of links that overlap with an\n",
      "Erdos-Renyi random graph. Through experiments with simulated and real world\n",
      "scientific collaboration, transportation and global trade networks, we\n",
      "demonstrate that the proposed heuristics show increased performance with the\n",
      "richness of connection type correlation structure and significantly outperform\n",
      "their baseline heuristics for ordinary networks with a single connection type.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.00011 \n",
      "Title :Age-Conditioned Synthesis of Pediatric Computed Tomography with\n",
      "  Auxiliary Classifier Generative Adversarial Networks\n",
      "  Deep learning is a popular and powerful tool in computed tomography (CT)\n",
      "image processing such as organ segmentation, but its requirement of large\n",
      "training datasets remains a challenge. Even though there is a large anatomical\n",
      "variability for children during their growth, the training datasets for\n",
      "pediatric CT scans are especially hard to obtain due to risks of radiation to\n",
      "children. In this paper, we propose a method to conditionally synthesize\n",
      "realistic pediatric CT images using a new auxiliary classifier generative\n",
      "adversarial network (ACGAN) architecture by taking age information into\n",
      "account. The proposed network generated age-conditioned high-resolution CT\n",
      "images to enrich pediatric training datasets.\n",
      "\n",
      "**Paper Id :1911.08105 \n",
      "Title :Three-dimensional Generative Adversarial Nets for Unsupervised Metal\n",
      "  Artifact Reduction\n",
      "  The reduction of metal artifacts in computed tomography (CT) images,\n",
      "specifically for strong artifacts generated from multiple metal objects, is a\n",
      "challenging issue in medical imaging research. Although there have been some\n",
      "studies on supervised metal artifact reduction through the learning of\n",
      "synthesized artifacts, it is difficult for simulated artifacts to cover the\n",
      "complexity of the real physical phenomena that may be observed in X-ray\n",
      "propagation. In this paper, we introduce metal artifact reduction methods based\n",
      "on an unsupervised volume-to-volume translation learned from clinical CT\n",
      "images. We construct three-dimensional adversarial nets with a regularized loss\n",
      "function designed for metal artifacts from multiple dental fillings. The\n",
      "results of experiments using 915 CT volumes from real patients demonstrate that\n",
      "the proposed framework has an outstanding capacity to reduce strong artifacts\n",
      "and to recover underlying missing voxels, while preserving the anatomical\n",
      "features of soft tissues and tooth structures from the original images.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.00544 \n",
      "Title :Tensor-to-Vector Regression for Multi-channel Speech Enhancement based\n",
      "  on Tensor-Train Network\n",
      "  We propose a tensor-to-vector regression approach to multi-channel speech\n",
      "enhancement in order to address the issue of input size explosion and\n",
      "hidden-layer size expansion. The key idea is to cast the conventional deep\n",
      "neural network (DNN) based vector-to-vector regression formulation under a\n",
      "tensor-train network (TTN) framework. TTN is a recently emerged solution for\n",
      "compact representation of deep models with fully connected hidden layers. Thus\n",
      "TTN maintains DNN's expressive power yet involves a much smaller amount of\n",
      "trainable parameters. Furthermore, TTN can handle a multi-dimensional tensor\n",
      "input by design, which exactly matches the desired setting in multi-channel\n",
      "speech enhancement. We first provide a theoretical extension from DNN to TTN\n",
      "based regression. Next, we show that TTN can attain speech enhancement quality\n",
      "comparable with that for DNN but with much fewer parameters, e.g., a reduction\n",
      "from 27 million to only 5 million parameters is observed in a single-channel\n",
      "scenario. TTN also improves PESQ over DNN from 2.86 to 2.96 by slightly\n",
      "increasing the number of trainable parameters. Finally, in 8-channel\n",
      "conditions, a PESQ of 3.12 is achieved using 20 million parameters for TTN,\n",
      "whereas a DNN with 68 million parameters can only attain a PESQ of 3.06. Our\n",
      "implementation is available online\n",
      "https://github.com/uwjunqi/Tensor-Train-Neural-Network.\n",
      "\n",
      "**Paper Id :2003.04296 \n",
      "Title :Propagating Asymptotic-Estimated Gradients for Low Bitwidth Quantized\n",
      "  Neural Networks\n",
      "  The quantized neural networks (QNNs) can be useful for neural network\n",
      "acceleration and compression, but during the training process they pose a\n",
      "challenge: how to propagate the gradient of loss function through the graph\n",
      "flow with a derivative of 0 almost everywhere. In response to this\n",
      "non-differentiable situation, we propose a novel Asymptotic-Quantized Estimator\n",
      "(AQE) to estimate the gradient. In particular, during back-propagation, the\n",
      "graph that relates inputs to output remains smoothness and differentiability.\n",
      "At the end of training, the weights and activations have been quantized to\n",
      "low-precision because of the asymptotic behaviour of AQE. Meanwhile, we propose\n",
      "a M-bit Inputs and N-bit Weights Network (MINW-Net) trained by AQE, a quantized\n",
      "neural network with 1-3 bits weights and activations. In the inference phase,\n",
      "we can use XNOR or SHIFT operations instead of convolution operations to\n",
      "accelerate the MINW-Net. Our experiments on CIFAR datasets demonstrate that our\n",
      "AQE is well defined, and the QNNs with AQE perform better than that with\n",
      "Straight-Through Estimator (STE). For example, in the case of the same ConvNet\n",
      "that has 1-bit weights and activations, our MINW-Net with AQE can achieve a\n",
      "prediction accuracy 1.5\\% higher than the Binarized Neural Network (BNN) with\n",
      "STE. The MINW-Net, which is trained from scratch by AQE, can achieve comparable\n",
      "classification accuracy as 32-bit counterparts on CIFAR test sets. Extensive\n",
      "experimental results on ImageNet dataset show great superiority of the proposed\n",
      "AQE and our MINW-Net achieves comparable results with other state-of-the-art\n",
      "QNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.00655 \n",
      "Title :Dynamic Parameter Allocation in Parameter Servers\n",
      "  To keep up with increasing dataset sizes and model complexity, distributed\n",
      "training has become a necessity for large machine learning tasks. Parameter\n",
      "servers ease the implementation of distributed parameter management---a key\n",
      "concern in distributed training---, but can induce severe communication\n",
      "overhead. To reduce communication overhead, distributed machine learning\n",
      "algorithms use techniques to increase parameter access locality (PAL),\n",
      "achieving up to linear speed-ups. We found that existing parameter servers\n",
      "provide only limited support for PAL techniques, however, and therefore prevent\n",
      "efficient training. In this paper, we explore whether and to what extent PAL\n",
      "techniques can be supported, and whether such support is beneficial. We propose\n",
      "to integrate dynamic parameter allocation into parameter servers, describe an\n",
      "efficient implementation of such a parameter server called Lapse, and\n",
      "experimentally compare its performance to existing parameter servers across a\n",
      "number of machine learning tasks. We found that Lapse provides near-linear\n",
      "scaling and can be orders of magnitude faster than existing parameter servers.\n",
      "\n",
      "**Paper Id :1810.05934 \n",
      "Title :A System for Massively Parallel Hyperparameter Tuning\n",
      "  Modern learning models are characterized by large hyperparameter spaces and\n",
      "long training times. These properties, coupled with the rise of parallel\n",
      "computing and the growing demand to productionize machine learning workloads,\n",
      "motivate the need to develop mature hyperparameter optimization functionality\n",
      "in distributed computing settings. We address this challenge by first\n",
      "introducing a simple and robust hyperparameter optimization algorithm called\n",
      "ASHA, which exploits parallelism and aggressive early-stopping to tackle\n",
      "large-scale hyperparameter optimization problems. Our extensive empirical\n",
      "results show that ASHA outperforms existing state-of-the-art hyperparameter\n",
      "optimization methods; scales linearly with the number of workers in distributed\n",
      "settings; and is suitable for massive parallelism, as demonstrated on a task\n",
      "with 500 workers. We then describe several design decisions we encountered,\n",
      "along with our associated solutions, when integrating ASHA in Determined AI's\n",
      "end-to-end production-quality machine learning system that offers\n",
      "hyperparameter tuning as a service.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.00743 \n",
      "Title :Unsupervised Multilingual Alignment using Wasserstein Barycenter\n",
      "  We study unsupervised multilingual alignment, the problem of finding\n",
      "word-to-word translations between multiple languages without using any parallel\n",
      "data. One popular strategy is to reduce multilingual alignment to the much\n",
      "simplified bilingual setting, by picking one of the input languages as the\n",
      "pivot language that we transit through. However, it is well-known that\n",
      "transiting through a poorly chosen pivot language (such as English) may\n",
      "severely degrade the translation quality, since the assumed transitive\n",
      "relations among all pairs of languages may not be enforced in the training\n",
      "process. Instead of going through a rather arbitrarily chosen pivot language,\n",
      "we propose to use the Wasserstein barycenter as a more informative \"mean\"\n",
      "language: it encapsulates information from all languages and minimizes all\n",
      "pairwise transportation costs. We evaluate our method on standard benchmarks\n",
      "and demonstrate state-of-the-art performances.\n",
      "\n",
      "**Paper Id :1911.09812 \n",
      "Title :Zero-Resource Cross-Lingual Named Entity Recognition\n",
      "  Recently, neural methods have achieved state-of-the-art (SOTA) results in\n",
      "Named Entity Recognition (NER) tasks for many languages without the need for\n",
      "manually crafted features. However, these models still require manually\n",
      "annotated training data, which is not available for many languages. In this\n",
      "paper, we propose an unsupervised cross-lingual NER model that can transfer NER\n",
      "knowledge from one language to another in a completely unsupervised way without\n",
      "relying on any bilingual dictionary or parallel data. Our model achieves this\n",
      "through word-level adversarial learning and augmented fine-tuning with\n",
      "parameter sharing and feature augmentation. Experiments on five different\n",
      "languages demonstrate the effectiveness of our approach, outperforming existing\n",
      "models by a good margin and setting a new SOTA for each language pair.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.01068 \n",
      "Title :Policy Gradient based Quantum Approximate Optimization Algorithm\n",
      "  The quantum approximate optimization algorithm (QAOA), as a hybrid\n",
      "quantum/classical algorithm, has received much interest recently. QAOA can also\n",
      "be viewed as a variational ansatz for quantum control. However, its direct\n",
      "application to emergent quantum technology encounters additional physical\n",
      "constraints: (i) the states of the quantum system are not observable; (ii)\n",
      "obtaining the derivatives of the objective function can be computationally\n",
      "expensive or even inaccessible in experiments, and (iii) the values of the\n",
      "objective function may be sensitive to various sources of uncertainty, as is\n",
      "the case for noisy intermediate-scale quantum (NISQ) devices. Taking such\n",
      "constraints into account, we show that policy-gradient-based reinforcement\n",
      "learning (RL) algorithms are well suited for optimizing the variational\n",
      "parameters of QAOA in a noise-robust fashion, opening up the way for developing\n",
      "RL techniques for continuous quantum control. This is advantageous to help\n",
      "mitigate and monitor the potentially unknown sources of errors in modern\n",
      "quantum simulators. We analyze the performance of the algorithm for quantum\n",
      "state transfer problems in single- and multi-qubit systems, subject to various\n",
      "sources of noise such as error terms in the Hamiltonian, or quantum uncertainty\n",
      "in the measurement process. We show that, in noisy setups, it is capable of\n",
      "outperforming state-of-the-art existing optimization algorithms.\n",
      "\n",
      "**Paper Id :1910.01155 \n",
      "Title :Stochastic gradient descent for hybrid quantum-classical optimization\n",
      "  Within the context of hybrid quantum-classical optimization, gradient descent\n",
      "based optimizers typically require the evaluation of expectation values with\n",
      "respect to the outcome of parameterized quantum circuits. In this work, we\n",
      "explore the consequences of the prior observation that estimation of these\n",
      "quantities on quantum hardware results in a form of stochastic gradient descent\n",
      "optimization. We formalize this notion, which allows us to show that in many\n",
      "relevant cases, including VQE, QAOA and certain quantum classifiers, estimating\n",
      "expectation values with $k$ measurement outcomes results in optimization\n",
      "algorithms whose convergence properties can be rigorously well understood, for\n",
      "any value of $k$. In fact, even using single measurement outcomes for the\n",
      "estimation of expectation values is sufficient. Moreover, in many settings the\n",
      "required gradients can be expressed as linear combinations of expectation\n",
      "values -- originating, e.g., from a sum over local terms of a Hamiltonian, a\n",
      "parameter shift rule, or a sum over data-set instances -- and we show that in\n",
      "these cases $k$-shot expectation value estimation can be combined with sampling\n",
      "over terms of the linear combination, to obtain ``doubly stochastic'' gradient\n",
      "descent optimizers. For all algorithms we prove convergence guarantees,\n",
      "providing a framework for the derivation of rigorous optimization results in\n",
      "the context of near-term quantum devices. Additionally, we explore numerically\n",
      "these methods on benchmark VQE, QAOA and quantum-enhanced machine learning\n",
      "tasks and show that treating the stochastic settings as hyper-parameters allows\n",
      "for state-of-the-art results with significantly fewer circuit executions and\n",
      "measurements.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.01136 \n",
      "Title :On Positive-Unlabeled Classification in GAN\n",
      "  This paper defines a positive and unlabeled classification problem for\n",
      "standard GANs, which then leads to a novel technique to stabilize the training\n",
      "of the discriminator in GANs. Traditionally, real data are taken as positive\n",
      "while generated data are negative. This positive-negative classification\n",
      "criterion was kept fixed all through the learning process of the discriminator\n",
      "without considering the gradually improved quality of generated data, even if\n",
      "they could be more realistic than real data at times. In contrast, it is more\n",
      "reasonable to treat the generated data as unlabeled, which could be positive or\n",
      "negative according to their quality. The discriminator is thus a classifier for\n",
      "this positive and unlabeled classification problem, and we derive a new\n",
      "Positive-Unlabeled GAN (PUGAN). We theoretically discuss the global optimality\n",
      "the proposed model will achieve and the equivalent optimization goal.\n",
      "Empirically, we find that PUGAN can achieve comparable or even better\n",
      "performance than those sophisticated discriminator stabilization methods.\n",
      "\n",
      "**Paper Id :1912.01834 \n",
      "Title :PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting\n",
      "  The latest methods based on deep learning have achieved amazing results\n",
      "regarding the complex work of inpainting large missing areas in an image. But\n",
      "this type of method generally attempts to generate one single \"optimal\" result,\n",
      "ignoring many other plausible results. Considering the uncertainty of the\n",
      "inpainting task, one sole result can hardly be regarded as a desired\n",
      "regeneration of the missing area. In view of this weakness, which is related to\n",
      "the design of the previous algorithms, we propose a novel deep generative model\n",
      "equipped with a brand new style extractor which can extract the style feature\n",
      "(latent vector) from the ground truth. Once obtained, the extracted style\n",
      "feature and the ground truth are both input into the generator. We also craft a\n",
      "consistency loss that guides the generated image to approximate the ground\n",
      "truth. After iterations, our generator is able to learn the mapping of styles\n",
      "corresponding to multiple sets of vectors. The proposed model can generate a\n",
      "large number of results consistent with the context semantics of the image.\n",
      "Moreover, we evaluated the effectiveness of our model on three datasets, i.e.,\n",
      "CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting\n",
      "methods, this model is able to offer desirable inpainting results with both\n",
      "better quality and higher diversity. The code and model will be made available\n",
      "on https://github.com/vivitsai/PiiGAN.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.01365 \n",
      "Title :Compositional Languages Emerge in a Neural Iterated Learning Model\n",
      "  The principle of compositionality, which enables natural language to\n",
      "represent complex concepts via a structured combination of simpler ones, allows\n",
      "us to convey an open-ended set of messages using a limited vocabulary. If\n",
      "compositionality is indeed a natural property of language, we may expect it to\n",
      "appear in communication protocols that are created by neural agents in language\n",
      "games. In this paper, we propose an effective neural iterated learning (NIL)\n",
      "algorithm that, when applied to interacting neural agents, facilitates the\n",
      "emergence of a more structured type of language. Indeed, these languages\n",
      "provide learning speed advantages to neural agents during training, which can\n",
      "be incrementally amplified via NIL. We provide a probabilistic model of NIL and\n",
      "an explanation of why the advantage of compositional language exist. Our\n",
      "experiments confirm our analysis, and also demonstrate that the emerged\n",
      "languages largely improve the generalizing power of the neural agent\n",
      "communication.\n",
      "\n",
      "**Paper Id :1904.10797 \n",
      "Title :Machine learning for long-distance quantum communication\n",
      "  Machine learning can help us in solving problems in the context big data\n",
      "analysis and classification, as well as in playing complex games such as Go.\n",
      "But can it also be used to find novel protocols and algorithms for applications\n",
      "such as large-scale quantum communication? Here we show that machine learning\n",
      "can be used to identify central quantum protocols, including teleportation,\n",
      "entanglement purification and the quantum repeater. These schemes are of\n",
      "importance in long-distance quantum communication, and their discovery has\n",
      "shaped the field of quantum information processing. However, the usefulness of\n",
      "learning agents goes beyond the mere re-production of known protocols; the same\n",
      "approach allows one to find improved solutions to long-distance communication\n",
      "problems, in particular when dealing with asymmetric situations where channel\n",
      "noise and segment distance are non-uniform. Our findings are based on the use\n",
      "of projective simulation, a model of a learning agent that combines\n",
      "reinforcement learning and decision making in a physically motivated framework.\n",
      "The learning agent is provided with a universal gate set, and the desired task\n",
      "is specified via a reward scheme. From a technical perspective, the learning\n",
      "agent has to deal with stochastic environments and reactions. We utilize an\n",
      "idea reminiscent of hierarchical skill acquisition, where solutions to\n",
      "sub-problems are learned and re-used in the overall scheme. This is of\n",
      "particular importance in the development of long-distance communication\n",
      "schemes, and opens the way for using machine learning in the design and\n",
      "implementation of quantum networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.01490 \n",
      "Title :Pseudo-dimension of quantum circuits\n",
      "  We characterize the expressive power of quantum circuits with the\n",
      "pseudo-dimension, a measure of complexity for probabilistic concept classes. We\n",
      "prove pseudo-dimension bounds on the output probability distributions of\n",
      "quantum circuits; the upper bounds are polynomial in circuit depth and number\n",
      "of gates. Using these bounds, we exhibit a class of circuit output states out\n",
      "of which at least one has exponential state complexity, and moreover\n",
      "demonstrate that quantum circuits of known polynomial size and depth are\n",
      "PAC-learnable.\n",
      "\n",
      "**Paper Id :1804.02484 \n",
      "Title :Approximating Hamiltonian dynamics with the Nystr\\\"om method\n",
      "  Simulating the time-evolution of quantum mechanical systems is BQP-hard and\n",
      "expected to be one of the foremost applications of quantum computers. We\n",
      "consider classical algorithms for the approximation of Hamiltonian dynamics\n",
      "using subsampling methods from randomized numerical linear algebra. We derive a\n",
      "simulation technique whose runtime scales polynomially in the number of qubits\n",
      "and the Frobenius norm of the Hamiltonian. As an immediate application, we show\n",
      "that sample based quantum simulation, a type of evolution where the Hamiltonian\n",
      "is a density matrix, can be efficiently classically simulated under specific\n",
      "structural conditions. Our main technical contribution is a randomized\n",
      "algorithm for approximating Hermitian matrix exponentials. The proof leverages\n",
      "a low-rank, symmetric approximation via the Nystr\\\"om method. Our results\n",
      "suggest that under strong sampling assumptions there exist classical\n",
      "poly-logarithmic time simulations of quantum computations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.01664 \n",
      "Title :Identification of Indian Languages using Ghost-VLAD pooling\n",
      "  In this work, we propose a new pooling strategy for language identification\n",
      "by considering Indian languages. The idea is to obtain utterance level features\n",
      "for any variable length audio for robust language recognition. We use the\n",
      "GhostVLAD approach to generate an utterance level feature vector for any\n",
      "variable length input audio by aggregating the local frame level features\n",
      "across time. The generated feature vector is shown to have very good language\n",
      "discriminative features and helps in getting state of the art results for\n",
      "language identification task. We conduct our experiments on 635Hrs of audio\n",
      "data for 7 Indian languages. Our method outperforms the previous state of the\n",
      "art x-vector [11] method by an absolute improvement of 1.88% in F1-score and\n",
      "achieves 98.43% F1-score on the held-out test data. We compare our system with\n",
      "various pooling approaches and show that GhostVLAD is the best pooling approach\n",
      "for this task. We also provide visualization of the utterance level embeddings\n",
      "generated using Ghost-VLAD pooling and show that this method creates embeddings\n",
      "which has very good language discriminative features.\n",
      "\n",
      "**Paper Id :2004.11714 \n",
      "Title :Residual Energy-Based Models for Text Generation\n",
      "  Text generation is ubiquitous in many NLP tasks, from summarization, to\n",
      "dialogue and machine translation. The dominant parametric approach is based on\n",
      "locally normalized models which predict one word at a time. While these work\n",
      "remarkably well, they are plagued by exposure bias due to the greedy nature of\n",
      "the generation process. In this work, we investigate un-normalized energy-based\n",
      "models (EBMs) which operate not at the token but at the sequence level. In\n",
      "order to make training tractable, we first work in the residual of a pretrained\n",
      "locally normalized language model and second we train using noise contrastive\n",
      "estimation. Furthermore, since the EBM works at the sequence level, we can\n",
      "leverage pretrained bi-directional contextual representations, such as BERT and\n",
      "RoBERTa. Our experiments on two large language modeling datasets show that\n",
      "residual EBMs yield lower perplexity compared to locally normalized baselines.\n",
      "Moreover, generation via importance sampling is very efficient and of higher\n",
      "quality than the baseline models according to human evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.01793 \n",
      "Title :Proximity Preserving Binary Code using Signed Graph-Cut\n",
      "  We introduce a binary embedding framework, called Proximity Preserving Code\n",
      "(PPC), which learns similarity and dissimilarity between data points to create\n",
      "a compact and affinity-preserving binary code. This code can be used to apply\n",
      "fast and memory-efficient approximation to nearest-neighbor searches. Our\n",
      "framework is flexible, enabling different proximity definitions between data\n",
      "points. In contrast to previous methods that extract binary codes based on\n",
      "unsigned graph partitioning, our system models the attractive and repulsive\n",
      "forces in the data by incorporating positive and negative graph weights. The\n",
      "proposed framework is shown to boil down to finding the minimal cut of a signed\n",
      "graph, a problem known to be NP-hard. We offer an efficient approximation and\n",
      "achieve superior results by constructing the code bit after bit. We show that\n",
      "the proposed approximation is superior to the commonly used spectral methods\n",
      "with respect to both accuracy and complexity. Thus, it is useful for many other\n",
      "problems that can be translated into signed graph cut.\n",
      "\n",
      "**Paper Id :1907.02929 \n",
      "Title :Improved local search for graph edit distance\n",
      "  The graph edit distance (GED) measures the dissimilarity between two graphs\n",
      "as the minimal cost of a sequence of elementary operations transforming one\n",
      "graph into another. This measure is fundamental in many areas such as\n",
      "structural pattern recognition or classification. However, exactly computing\n",
      "GED is NP-hard. Among different classes of heuristic algorithms that were\n",
      "proposed to compute approximate solutions, local search based algorithms\n",
      "provide the tightest upper bounds for GED. In this paper, we present K-REFINE\n",
      "and RANDPOST. K-REFINE generalizes and improves an existing local search\n",
      "algorithm and performs particularly well on small graphs. RANDPOST is a general\n",
      "warm start framework that stochastically generates promising initial solutions\n",
      "to be used by any local search based GED algorithm. It is particularly\n",
      "efficient on large graphs. An extensive empirical evaluation demonstrates that\n",
      "both K-REFINE and RANDPOST perform excellently in practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02008 \n",
      "Title :Detecting Changes in Asset Co-Movement Using the Autoencoder\n",
      "  Reconstruction Ratio\n",
      "  Detecting changes in asset co-movements is of much importance to financial\n",
      "practitioners, with numerous risk management benefits arising from the timely\n",
      "detection of breakdowns in historical correlations. In this article, we propose\n",
      "a real-time indicator to detect temporary increases in asset co-movements, the\n",
      "Autoencoder Reconstruction Ratio, which measures how well a basket of asset\n",
      "returns can be modelled using a lower-dimensional set of latent variables. The\n",
      "ARR uses a deep sparse denoising autoencoder to perform the dimensionality\n",
      "reduction on the returns vector, which replaces the PCA approach of the\n",
      "standard Absorption Ratio, and provides a better model for non-Gaussian\n",
      "returns. Through a systemic risk application on forecasting on the CRSP US\n",
      "Total Market Index, we show that lower ARR values coincide with higher\n",
      "volatility and larger drawdowns, indicating that increased asset co-movement\n",
      "does correspond with periods of market weakness. We also demonstrate that\n",
      "short-term (i.e. 5-min and 1-hour) predictors for realised volatility and\n",
      "market crashes can be improved by including additional ARR inputs.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02453 \n",
      "Title :Modeling Engagement in Long-Term, In-Home Socially Assistive Robot\n",
      "  Interventions for Children with Autism Spectrum Disorders\n",
      "  Socially assistive robotics (SAR) has great potential to provide accessible,\n",
      "affordable, and personalized therapeutic interventions for children with autism\n",
      "spectrum disorders (ASD). However, human-robot interaction (HRI) methods are\n",
      "still limited in their ability to autonomously recognize and respond to\n",
      "behavioral cues, especially in atypical users and everyday settings. This work\n",
      "applies supervised machine learning algorithms to model user engagement in the\n",
      "context of long-term, in-home SAR interventions for children with ASD.\n",
      "Specifically, we present two types of engagement models for each user: (i)\n",
      "generalized models trained on data from different users; and (ii)\n",
      "individualized models trained on an early subset of the user's data. The models\n",
      "achieved approximately 90% accuracy (AUROC) for post hoc binary classification\n",
      "of engagement, despite the high variance in data observed across users,\n",
      "sessions, and engagement states. Moreover, temporal patterns in model\n",
      "predictions could be used to reliably initiate re-engagement actions at\n",
      "appropriate times. These results validate the feasibility and challenges of\n",
      "recognition and response to user disengagement in long-term, real-world HRI\n",
      "settings. The contributions of this work also inform the design of engaging and\n",
      "personalized HRI, especially for the ASD community.\n",
      "\n",
      "**Paper Id :1912.07416 \n",
      "Title :Improved Explanatory Efficacy on Human Affect and Workload through\n",
      "  Interactive Process in Artificial Intelligence\n",
      "  Despite recent advances in the field of explainable artificial intelligence\n",
      "systems, a concrete quantitative measure for evaluating the usability of such\n",
      "systems is nonexistent. Ensuring the success of an explanatory interface in\n",
      "interacting with users requires a cyclic, symbiotic relationship between human\n",
      "and artificial intelligence. We, therefore, propose explanatory efficacy, a\n",
      "novel metric for evaluating the strength of the cyclic relationship the\n",
      "interface exhibits. Furthermore, in a user study, we evaluated the perceived\n",
      "affect and workload and recorded the EEG signals of our participants as they\n",
      "interacted with our custom-built, iterative explanatory interface to build\n",
      "personalized recommendation systems. We found that systems for perceptually\n",
      "driven iterative tasks with greater explanatory efficacy are characterized by\n",
      "statistically significant hemispheric differences in neural signals with 62.4%\n",
      "accuracy, indicating the feasibility of neural correlates as a measure of\n",
      "explanatory efficacy. These findings are beneficial for researchers who aim to\n",
      "study the circular ecosystem of the human-artificial intelligence partnership.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02534 \n",
      "Title :Fast inference of Boosted Decision Trees in FPGAs for particle physics\n",
      "  We describe the implementation of Boosted Decision Trees in the hls4ml\n",
      "library, which allows the translation of a trained model into FPGA firmware\n",
      "through an automated conversion process. Thanks to its fully on-chip\n",
      "implementation, hls4ml performs inference of Boosted Decision Tree models with\n",
      "extremely low latency. With a typical latency less than 100 ns, this solution\n",
      "is suitable for FPGA-based real-time processing, such as in the Level-1 Trigger\n",
      "system of a collider experiment. These developments open up prospects for\n",
      "physicists to deploy BDTs in FPGAs for identifying the origin of jets, better\n",
      "reconstructing the energies of muons, and enabling better selection of rare\n",
      "signal processes.\n",
      "\n",
      "**Paper Id :1905.11825 \n",
      "Title :Fast Data-Driven Simulation of Cherenkov Detectors Using Generative\n",
      "  Adversarial Networks\n",
      "  The increasing luminosities of future Large Hadron Collider runs and next\n",
      "generation of collider experiments will require an unprecedented amount of\n",
      "simulated events to be produced. Such large scale productions are extremely\n",
      "demanding in terms of computing resources. Thus new approaches to event\n",
      "generation and simulation of detector responses are needed. In LHCb, the\n",
      "accurate simulation of Cherenkov detectors takes a sizeable fraction of CPU\n",
      "time. An alternative approach is described here, when one generates high-level\n",
      "reconstructed observables using a generative neural network to bypass low level\n",
      "details. This network is trained to reproduce the particle species likelihood\n",
      "function values based on the track kinematic parameters and detector occupancy.\n",
      "The fast simulation is trained using real data samples collected by LHCb during\n",
      "run 2. We demonstrate that this approach provides high-fidelity results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02664 \n",
      "Title :Short sighted deep learning\n",
      "  A theory explaining how deep learning works is yet to be developed. Previous\n",
      "work suggests that deep learning performs a coarse graining, similar in spirit\n",
      "to the renormalization group (RG). This idea has been explored in the setting\n",
      "of a local (nearest neighbor interactions) Ising spin lattice. We extend the\n",
      "discussion to the setting of a long range spin lattice. Markov Chain Monte\n",
      "Carlo (MCMC) simulations determine both the critical temperature and scaling\n",
      "dimensions of the system. The model is used to train both a single RBM\n",
      "(restricted Boltzmann machine) network, as well as a stacked RBM network.\n",
      "Following earlier Ising model studies, the trained weights of a single layer\n",
      "RBM network define a flow of lattice models. In contrast to results for nearest\n",
      "neighbor Ising, the RBM flow for the long ranged model does not converge to the\n",
      "correct values for the spin and energy scaling dimension. Further, correlation\n",
      "functions between visible and hidden nodes exhibit key differences between the\n",
      "stacked RBM and RG flows. The stacked RBM flow appears to move towards low\n",
      "temperatures whereas the RG flow moves towards high temperature. This again\n",
      "differs from results obtained for nearest neighbor Ising.\n",
      "\n",
      "**Paper Id :1810.08179 \n",
      "Title :Thermodynamics and Feature Extraction by Machine Learning\n",
      "  Machine learning methods are powerful in distinguishing different phases of\n",
      "matter in an automated way and provide a new perspective on the study of\n",
      "physical phenomena. We train a Restricted Boltzmann Machine (RBM) on data\n",
      "constructed with spin configurations sampled from the Ising Hamiltonian at\n",
      "different values of temperature and external magnetic field using Monte Carlo\n",
      "methods. From the trained machine we obtain the flow of iterative\n",
      "reconstruction of spin state configurations to faithfully reproduce the\n",
      "observables of the physical system. We find that the flow of the trained RBM\n",
      "approaches the spin configurations of the maximal possible specific heat which\n",
      "resemble the near criticality region of the Ising model. In the special case of\n",
      "the vanishing magnetic field the trained RBM converges to the critical point of\n",
      "the Renormalization Group (RG) flow of the lattice model. Our results suggest\n",
      "an alternative explanation of how the machine identifies the physical phase\n",
      "transitions, by recognizing certain properties of the configuration like the\n",
      "maximization of the specific heat, instead of associating directly the\n",
      "recognition procedure with the RG flow and its fixed points. Then from the\n",
      "reconstructed data we deduce the critical exponent associated to the\n",
      "magnetization to find satisfactory agreement with the actual physical value. We\n",
      "assume no prior knowledge about the criticality of the system and its\n",
      "Hamiltonian.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02848 \n",
      "Title :Unsupervised pretraining transfers well across languages\n",
      "  Cross-lingual and multi-lingual training of Automatic Speech Recognition\n",
      "(ASR) has been extensively investigated in the supervised setting. This assumes\n",
      "the existence of a parallel corpus of speech and orthographic transcriptions.\n",
      "Recently, contrastive predictive coding (CPC) algorithms have been proposed to\n",
      "pretrain ASR systems with unlabelled data. In this work, we investigate whether\n",
      "unsupervised pretraining transfers well across languages. We show that a slight\n",
      "modification of the CPC pretraining extracts features that transfer well to\n",
      "other languages, being on par or even outperforming supervised pretraining.\n",
      "This shows the potential of unsupervised methods for languages with few\n",
      "linguistic resources.\n",
      "\n",
      "**Paper Id :1911.09812 \n",
      "Title :Zero-Resource Cross-Lingual Named Entity Recognition\n",
      "  Recently, neural methods have achieved state-of-the-art (SOTA) results in\n",
      "Named Entity Recognition (NER) tasks for many languages without the need for\n",
      "manually crafted features. However, these models still require manually\n",
      "annotated training data, which is not available for many languages. In this\n",
      "paper, we propose an unsupervised cross-lingual NER model that can transfer NER\n",
      "knowledge from one language to another in a completely unsupervised way without\n",
      "relying on any bilingual dictionary or parallel data. Our model achieves this\n",
      "through word-level adversarial learning and augmented fine-tuning with\n",
      "parameter sharing and feature augmentation. Experiments on five different\n",
      "languages demonstrate the effectiveness of our approach, outperforming existing\n",
      "models by a good margin and setting a new SOTA for each language pair.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02886 \n",
      "Title :Weakly-Supervised Disentanglement Without Compromises\n",
      "  Intelligent agents should be able to learn useful representations by\n",
      "observing changes in their environment. We model such observations as pairs of\n",
      "non-i.i.d. images sharing at least one of the underlying factors of variation.\n",
      "First, we theoretically show that only knowing how many factors have changed,\n",
      "but not which ones, is sufficient to learn disentangled representations.\n",
      "Second, we provide practical algorithms that learn disentangled representations\n",
      "from pairs of images without requiring annotation of groups, individual\n",
      "factors, or the number of factors that have changed. Third, we perform a\n",
      "large-scale empirical study and show that such pairs of observations are\n",
      "sufficient to reliably learn disentangled representations on several benchmark\n",
      "data sets. Finally, we evaluate our learned representations and find that they\n",
      "are simultaneously useful on a diverse suite of tasks, including generalization\n",
      "under covariate shifts, fairness, and abstract reasoning. Overall, our results\n",
      "demonstrate that weak supervision enables learning of useful disentangled\n",
      "representations in realistic scenarios.\n",
      "\n",
      "**Paper Id :1905.01258 \n",
      "Title :Disentangling Factors of Variation Using Few Labels\n",
      "  Learning disentangled representations is considered a cornerstone problem in\n",
      "representation learning. Recently, Locatello et al. (2019) demonstrated that\n",
      "unsupervised disentanglement learning without inductive biases is theoretically\n",
      "impossible and that existing inductive biases and unsupervised methods do not\n",
      "allow to consistently learn disentangled representations. However, in many\n",
      "practical settings, one might have access to a limited amount of supervision,\n",
      "for example through manual labeling of (some) factors of variation in a few\n",
      "training examples. In this paper, we investigate the impact of such supervision\n",
      "on state-of-the-art disentanglement methods and perform a large scale study,\n",
      "training over 52000 models under well-defined and reproducible experimental\n",
      "conditions. We observe that a small number of labeled examples (0.01--0.5\\% of\n",
      "the data set), with potentially imprecise and incomplete labels, is sufficient\n",
      "to perform model selection on state-of-the-art unsupervised models. Further, we\n",
      "investigate the benefit of incorporating supervision into the training process.\n",
      "Overall, we empirically validate that with little and imprecise supervision it\n",
      "is possible to reliably learn disentangled representations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02913 \n",
      "Title :Learning Autoencoders with Relational Regularization\n",
      "  A new algorithmic framework is proposed for learning autoencoders of data\n",
      "distributions. We minimize the discrepancy between the model and target\n",
      "distributions, with a \\emph{relational regularization} on the learnable latent\n",
      "prior. This regularization penalizes the fused Gromov-Wasserstein (FGW)\n",
      "distance between the latent prior and its corresponding posterior, allowing one\n",
      "to flexibly learn a structured prior distribution associated with the\n",
      "generative model. Moreover, it helps co-training of multiple autoencoders even\n",
      "if they have heterogeneous architectures and incomparable latent spaces. We\n",
      "implement the framework with two scalable algorithms, making it applicable for\n",
      "both probabilistic and deterministic autoencoders. Our relational regularized\n",
      "autoencoder (RAE) outperforms existing methods, $e.g.$, the variational\n",
      "autoencoder, Wasserstein autoencoder, and their variants, on generating images.\n",
      "Additionally, our relational co-training strategy for autoencoders achieves\n",
      "encouraging results in both synthesis and real-world multi-view learning tasks.\n",
      "The code is at https://github.com/HongtengXu/ Relational-AutoEncoders.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.02959 \n",
      "Title :Revisiting Spatial Invariance with Low-Rank Local Connectivity\n",
      "  Convolutional neural networks are among the most successful architectures in\n",
      "deep learning with this success at least partially attributable to the efficacy\n",
      "of spatial invariance as an inductive bias. Locally connected layers, which\n",
      "differ from convolutional layers only in their lack of spatial invariance,\n",
      "usually perform poorly in practice. However, these observations still leave\n",
      "open the possibility that some degree of relaxation of spatial invariance may\n",
      "yield a better inductive bias than either convolution or local connectivity. To\n",
      "test this hypothesis, we design a method to relax the spatial invariance of a\n",
      "network layer in a controlled manner; we create a \\textit{low-rank} locally\n",
      "connected layer, where the filter bank applied at each position is constructed\n",
      "as a linear combination of basis set of filter banks with spatially varying\n",
      "combining weights. By varying the number of basis filter banks, we can control\n",
      "the degree of relaxation of spatial invariance. In experiments with small\n",
      "convolutional networks, we find that relaxing spatial invariance improves\n",
      "classification accuracy over both convolution and locally connected layers\n",
      "across MNIST, CIFAR-10, and CelebA datasets, thus suggesting that spatial\n",
      "invariance may be an overly restrictive prior.\n",
      "\n",
      "**Paper Id :1904.06194 \n",
      "Title :Compressing deep neural networks by matrix product operators\n",
      "  A deep neural network is a parametrization of a multilayer mapping of signals\n",
      "in terms of many alternatively arranged linear and nonlinear transformations.\n",
      "The linear transformations, which are generally used in the fully connected as\n",
      "well as convolutional layers, contain most of the variational parameters that\n",
      "are trained and stored. Compressing a deep neural network to reduce its number\n",
      "of variational parameters but not its prediction power is an important but\n",
      "challenging problem toward the establishment of an optimized scheme in training\n",
      "efficiently these parameters and in lowering the risk of overfitting. Here we\n",
      "show that this problem can be effectively solved by representing linear\n",
      "transformations with matrix product operators (MPOs), which is a tensor network\n",
      "originally proposed in physics to characterize the short-range entanglement in\n",
      "one-dimensional quantum states. We have tested this approach in five typical\n",
      "neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\n",
      "widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\n",
      "representation indeed sets up a faithful and efficient mapping between input\n",
      "and output signals, which can keep or even improve the prediction accuracy with\n",
      "a dramatically reduced number of parameters. Our method greatly simplifies the\n",
      "representations in deep learning, and opens a possible route toward\n",
      "establishing a framework of modern neural networks which might be simpler and\n",
      "cheaper, but more efficient.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.03113 \n",
      "Title :Projective Preferential Bayesian Optimization\n",
      "  Bayesian optimization is an effective method for finding extrema of a\n",
      "black-box function. We propose a new type of Bayesian optimization for learning\n",
      "user preferences in high-dimensional spaces. The central assumption is that the\n",
      "underlying objective function cannot be evaluated directly, but instead a\n",
      "minimizer along a projection can be queried, which we call a projective\n",
      "preferential query. The form of the query allows for feedback that is natural\n",
      "for a human to give, and which enables interaction. This is demonstrated in a\n",
      "user experiment in which the user feedback comes in the form of optimal\n",
      "position and orientation of a molecule adsorbing to a surface. We demonstrate\n",
      "that our framework is able to find a global minimum of a high-dimensional\n",
      "black-box function, which is an infeasible task for existing preferential\n",
      "Bayesian optimization frameworks that are based on pairwise comparisons.\n",
      "\n",
      "**Paper Id :1905.00820 \n",
      "Title :On the smoothness of nonlinear system identification\n",
      "  We shed new light on the \\textit{smoothness} of optimization problems arising\n",
      "in prediction error parameter estimation of linear and nonlinear systems. We\n",
      "show that for regions of the parameter space where the model is not\n",
      "contractive, the Lipschitz constant and $\\beta$-smoothness of the objective\n",
      "function might blow up exponentially with the simulation length, making it hard\n",
      "to numerically find minima within those regions or, even, to escape from them.\n",
      "In addition to providing theoretical understanding of this problem, this paper\n",
      "also proposes the use of multiple shooting as a viable solution. The proposed\n",
      "method minimizes the error between a prediction model and the observed values.\n",
      "Rather than running the prediction model over the entire dataset, multiple\n",
      "shooting splits the data into smaller subsets and runs the prediction model\n",
      "over each subset, making the simulation length a design parameter and making it\n",
      "possible to solve problems that would be infeasible using a standard approach.\n",
      "The equivalence to the original problem is obtained by including constraints in\n",
      "the optimization. The new method is illustrated by estimating the parameters of\n",
      "nonlinear systems with chaotic or unstable behavior, as well as neural\n",
      "networks. We also present a comparative analysis of the proposed method with\n",
      "multi-step-ahead prediction error minimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.03181 \n",
      "Title :Capsule Network Performance with Autonomous Navigation\n",
      "  Capsule Networks (CapsNets) have been proposed as an alternative to\n",
      "Convolutional Neural Networks (CNNs). This paper showcases how CapsNets are\n",
      "more capable than CNNs for autonomous agent exploration of realistic scenarios.\n",
      "In real world navigation, rewards external to agents may be rare. In turn,\n",
      "reinforcement learning algorithms can struggle to form meaningful policy\n",
      "functions. This paper's approach Capsules Exploration Module (Caps-EM) pairs a\n",
      "CapsNets architecture with an Advantage Actor Critic algorithm. Other\n",
      "approaches for navigating sparse environments require intrinsic reward\n",
      "generators, such as the Intrinsic Curiosity Module (ICM) and Augmented\n",
      "Curiosity Modules (ACM). Caps-EM uses a more compact architecture without need\n",
      "for intrinsic rewards. Tested using ViZDoom, the Caps-EM uses 44% and 83% fewer\n",
      "trainable network parameters than the ICM and Depth-Augmented Curiosity Module\n",
      "(D-ACM), respectively, for 1141% and 437% average time improvement over the ICM\n",
      "and D-ACM, respectively, for converging to a policy function across \"My Way\n",
      "Home\" scenarios.\n",
      "\n",
      "**Paper Id :2008.03787 \n",
      "Title :Neural Manipulation Planning on Constraint Manifolds\n",
      "  The presence of task constraints imposes a significant challenge to motion\n",
      "planning. Despite all recent advancements, existing algorithms are still\n",
      "computationally expensive for most planning problems. In this paper, we present\n",
      "Constrained Motion Planning Networks (CoMPNet), the first neural planner for\n",
      "multimodal kinematic constraints. Our approach comprises the following\n",
      "components: i) constraint and environment perception encoders; ii) neural robot\n",
      "configuration generator that outputs configurations on/near the constraint\n",
      "manifold(s), and iii) a bidirectional planning algorithm that takes the\n",
      "generated configurations to create a feasible robot motion trajectory. We show\n",
      "that CoMPNet solves practical motion planning tasks involving both\n",
      "unconstrained and constrained problems. Furthermore, it generalizes to new\n",
      "unseen locations of the objects, i.e., not seen during training, in the given\n",
      "environments with high success rates. When compared to the state-of-the-art\n",
      "constrained motion planning algorithms, CoMPNet outperforms by order of\n",
      "magnitude improvement in computational speed with a significantly lower\n",
      "variance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.03229 \n",
      "Title :Supervised Quantile Normalization for Low-rank Matrix Approximation\n",
      "  Low rank matrix factorization is a fundamental building block in machine\n",
      "learning, used for instance to summarize gene expression profile data or\n",
      "word-document counts. To be robust to outliers and differences in scale across\n",
      "features, a matrix factorization step is usually preceded by ad-hoc feature\n",
      "normalization steps, such as \\texttt{tf-idf} scaling or data whitening. We\n",
      "propose in this work to learn these normalization operators jointly with the\n",
      "factorization itself. More precisely, given a $d\\times n$ matrix $X$ of $d$\n",
      "features measured on $n$ individuals, we propose to learn the parameters of\n",
      "quantile normalization operators that can operate row-wise on the values of $X$\n",
      "and/or of its factorization $UV$ to improve the quality of the low-rank\n",
      "representation of $X$ itself. This optimization is facilitated by the\n",
      "introduction of a new differentiable quantile normalization operator built\n",
      "using optimal transport, providing new results on top of existing work by\n",
      "(Cuturi et al. 2019). We demonstrate the applicability of these techniques on\n",
      "synthetic and genomics datasets.\n",
      "\n",
      "**Paper Id :2006.14293 \n",
      "Title :Neural Decomposition: Functional ANOVA with Variational Autoencoders\n",
      "  Variational Autoencoders (VAEs) have become a popular approach for\n",
      "dimensionality reduction. However, despite their ability to identify latent\n",
      "low-dimensional structures embedded within high-dimensional data, these latent\n",
      "representations are typically hard to interpret on their own. Due to the\n",
      "black-box nature of VAEs, their utility for healthcare and genomics\n",
      "applications has been limited. In this paper, we focus on characterising the\n",
      "sources of variation in Conditional VAEs. Our goal is to provide a\n",
      "feature-level variance decomposition, i.e. to decompose variation in the data\n",
      "by separating out the marginal additive effects of latent variables z and fixed\n",
      "inputs c from their non-linear interactions. We propose to achieve this through\n",
      "what we call Neural Decomposition - an adaptation of the well-known concept of\n",
      "functional ANOVA variance decomposition from classical statistics to deep\n",
      "learning models. We show how identifiability can be achieved by training models\n",
      "subject to constraints on the marginal properties of the decoder networks. We\n",
      "demonstrate the utility of our Neural Decomposition on a series of synthetic\n",
      "examples as well as high-dimensional genomics data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.03780 \n",
      "Title :Photonic tensor cores for machine learning\n",
      "  With an ongoing trend in computing hardware towards increased heterogeneity,\n",
      "domain-specific co-processors are emerging as alternatives to centralized\n",
      "paradigms. The tensor core unit (TPU) has shown to outperform graphic process\n",
      "units by almost 3-orders of magnitude enabled by higher signal throughout and\n",
      "energy efficiency. In this context, photons bear a number of synergistic\n",
      "physical properties while phase-change materials allow for local nonvolatile\n",
      "mnemonic functionality in these emerging distributed non van-Neumann\n",
      "architectures. While several photonic neural network designs have been\n",
      "explored, a photonic TPU to perform matrix vector multiplication and summation\n",
      "is yet outstanding. Here we introduced an integrated photonics-based TPU by\n",
      "strategically utilizing a) photonic parallelism via wavelength division\n",
      "multiplexing, b) high 2 Peta-operations-per second throughputs enabled by 10s\n",
      "of picosecond-short delays from optoelectronics and compact photonic integrated\n",
      "circuitry, and c) zero power-consuming novel photonic multi-state memories\n",
      "based on phase-change materials featuring vanishing losses in the amorphous\n",
      "state. Combining these physical synergies of material, function, and system, we\n",
      "show that the performance of this 8-bit photonic TPU can be 2-3 orders higher\n",
      "compared to an electrical TPU whilst featuring similar chip areas. This work\n",
      "shows that photonic specialized processors have the potential to augment\n",
      "electronic systems and may perform exceptionally well in network-edge devices\n",
      "in the looming 5G networks and beyond.\n",
      "\n",
      "**Paper Id :1811.04047 \n",
      "Title :A Microprocessor implemented in 65nm CMOS with Configurable and\n",
      "  Bit-scalable Accelerator for Programmable In-memory Computing\n",
      "  This paper presents a programmable in-memory-computing processor,\n",
      "demonstrated in a 65nm CMOS technology. For data-centric workloads, such as\n",
      "deep neural networks, data movement often dominates when implemented with\n",
      "today's computing architectures. This has motivated spatial architectures,\n",
      "where the arrangement of data-storage and compute hardware is distributed and\n",
      "explicitly aligned to the computation dataflow, most notably for matrix-vector\n",
      "multiplication. In-memory computing is a spatial architecture where processing\n",
      "elements correspond to dense bit cells, providing local storage and compute,\n",
      "typically employing analog operation. Though this raises the potential for high\n",
      "energy efficiency and throughput, analog operation has significantly limited\n",
      "robustness, scale, and programmability. This paper describes a 590kb\n",
      "in-memory-computing accelerator integrated in a programmable processor\n",
      "architecture, by exploiting recent approaches to charge-domain in-memory\n",
      "computing. The architecture takes the approach of tight coupling with an\n",
      "embedded CPU, through accelerator interfaces enabling integration in the\n",
      "standard processor memory space. Additionally, a near-memory-computing datapath\n",
      "both enables diverse computations locally, to address operations required\n",
      "across applications, and enables bit-precision scalability for\n",
      "matrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme.\n",
      "Chip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput\n",
      "of 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element\n",
      "precisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b\n",
      "weights and activations for CIFAR-10 classification consume 5.3/105.2\n",
      "$\\mu$J/image at 176/23 fps, with accuracy at the level of digital/software\n",
      "implementation (89.3/92.4 $\\%$ accuracy).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.03893 \n",
      "Title :Novel Machine Learning Algorithms for Centrality and Cliques Detection\n",
      "  in Youtube Social Networks\n",
      "  The goal of this research project is to analyze the dynamics of social\n",
      "networks using machine learning techniques to locate maximal cliques and to\n",
      "find clusters for the purpose of identifying a target demographic. Unsupervised\n",
      "machine learning techniques are designed and implemented in this project to\n",
      "analyze a dataset from YouTube to discover communities in the social network\n",
      "and find central nodes. Different clustering algorithms are implemented and\n",
      "applied to the YouTube dataset. The well-known Bron-Kerbosch algorithm is used\n",
      "effectively in this research to find maximal cliques. The results obtained from\n",
      "this research could be used for advertising purposes and for building smart\n",
      "recommendation systems. All algorithms were implemented using Python\n",
      "programming language. The experimental results show that we were able to\n",
      "successfully find central nodes through clique-centrality and degree\n",
      "centrality. By utilizing clique detection algorithms, the research shown how\n",
      "machine learning algorithms can detect close knit groups within a larger\n",
      "network.\n",
      "\n",
      "**Paper Id :2005.06599 \n",
      "Title :Phishing URL Detection Through Top-level Domain Analysis: A Descriptive\n",
      "  Approach\n",
      "  Phishing is considered to be one of the most prevalent cyber-attacks because\n",
      "of its immense flexibility and alarmingly high success rate. Even with adequate\n",
      "training and high situational awareness, it can still be hard for users to\n",
      "continually be aware of the URL of the website they are visiting. Traditional\n",
      "detection methods rely on blocklists and content analysis, both of which\n",
      "require time-consuming human verification. Thus, there have been attempts\n",
      "focusing on the predictive filtering of such URLs. This study aims to develop a\n",
      "machine-learning model to detect fraudulent URLs which can be used within the\n",
      "Splunk platform. Inspired from similar approaches in the literature, we trained\n",
      "the SVM and Random Forests algorithms using malicious and benign datasets found\n",
      "in the literature and one dataset that we created. We evaluated the algorithms'\n",
      "performance with precision and recall, reaching up to 85% precision and 87%\n",
      "recall in the case of Random Forests while SVM achieved up to 90% precision and\n",
      "88% recall using only descriptive features.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.03967 \n",
      "Title :Regularized Optimal Transport is Ground Cost Adversarial\n",
      "  Regularizing the optimal transport (OT) problem has proven crucial for OT\n",
      "theory to impact the field of machine learning. For instance, it is known that\n",
      "regularizing OT problems with entropy leads to faster computations and better\n",
      "differentiation using the Sinkhorn algorithm, as well as better sample\n",
      "complexity bounds than classic OT. In this work we depart from this practical\n",
      "perspective and propose a new interpretation of regularization as a robust\n",
      "mechanism, and show using Fenchel duality that any convex regularization of OT\n",
      "can be interpreted as ground cost adversarial. This incidentally gives access\n",
      "to a robust dissimilarity measure on the ground space, which can in turn be\n",
      "used in other applications. We propose algorithms to compute this robust cost,\n",
      "and illustrate the interest of this approach empirically.\n",
      "\n",
      "**Paper Id :2006.14763 \n",
      "Title :PAC-Bayesian Bound for the Conditional Value at Risk\n",
      "  Conditional Value at Risk (CVaR) is a family of \"coherent risk measures\"\n",
      "which generalize the traditional mathematical expectation. Widely used in\n",
      "mathematical finance, it is garnering increasing interest in machine learning,\n",
      "e.g., as an alternate approach to regularization, and as a means for ensuring\n",
      "fairness. This paper presents a generalization bound for learning algorithms\n",
      "that minimize the CVaR of the empirical loss. The bound is of PAC-Bayesian type\n",
      "and is guaranteed to be small when the empirical CVaR is small. We achieve this\n",
      "by reducing the problem of estimating CVaR to that of merely estimating an\n",
      "expectation. This then enables us, as a by-product, to obtain concentration\n",
      "inequalities for CVaR even when the random variable in question is unbounded.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04025 \n",
      "Title :Can Graph Neural Networks Count Substructures?\n",
      "  The ability to detect and count certain substructures in graphs is important\n",
      "for solving many tasks on graph-structured data, especially in the contexts of\n",
      "computational chemistry and biology as well as social network analysis.\n",
      "Inspired by this, we propose to study the expressive power of graph neural\n",
      "networks (GNNs) via their ability to count attributed graph substructures,\n",
      "extending recent works that examine their power in graph isomorphism testing\n",
      "and function approximation. We distinguish between two types of substructure\n",
      "counting: induced-subgraph-count and subgraph-count, and establish both\n",
      "positive and negative answers for popular GNN architectures. Specifically, we\n",
      "prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL)\n",
      "and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count\n",
      "of substructures consisting of 3 or more nodes, while they can perform\n",
      "subgraph-count of star-shaped substructures. As an intermediary step, we prove\n",
      "that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs,\n",
      "partly answering an open problem raised in Maron et al. (2019). We also prove\n",
      "positive results for k-WL and k-IGNs as well as negative results for k-WL with\n",
      "a finite number of iterations. We then conduct experiments that support the\n",
      "theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure\n",
      "counting and inspired by Murphy et al. (2019), we propose the Local Relational\n",
      "Pooling model and demonstrate that it is not only effective for substructure\n",
      "counting but also able to achieve competitive performance on molecular\n",
      "prediction tasks.\n",
      "\n",
      "**Paper Id :2002.10306 \n",
      "Title :Adaptive Propagation Graph Convolutional Network\n",
      "  Graph convolutional networks (GCNs) are a family of neural network models\n",
      "that perform inference on graph data by interleaving vertex-wise operations and\n",
      "message-passing exchanges across nodes. Concerning the latter, two key\n",
      "questions arise: (i) how to design a differentiable exchange protocol (e.g., a\n",
      "1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize\n",
      "the trade-off in complexity with respect to the local updates. In this paper,\n",
      "we show that state-of-the-art results can be achieved by adapting the number of\n",
      "communication steps independently at every node. In particular, we endow each\n",
      "node with a halting unit (inspired by Graves' adaptive computation time) that\n",
      "after every exchange decides whether to continue communicating or not. We show\n",
      "that the proposed adaptive propagation GCN (AP-GCN) achieves superior or\n",
      "similar results to the best proposed models so far on a number of benchmarks,\n",
      "while requiring a small overhead in terms of additional parameters. We also\n",
      "investigate a regularization term to enforce an explicit trade-off between\n",
      "communication and accuracy. The code for the AP-GCN experiments is released as\n",
      "an open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04083 \n",
      "Title :Estimating Counterfactual Treatment Outcomes over Time Through\n",
      "  Adversarially Balanced Representations\n",
      "  Identifying when to give treatments to patients and how to select among\n",
      "multiple treatments over time are important medical problems with a few\n",
      "existing solutions. In this paper, we introduce the Counterfactual Recurrent\n",
      "Network (CRN), a novel sequence-to-sequence model that leverages the\n",
      "increasingly available patient observational data to estimate treatment effects\n",
      "over time and answer such medical questions. To handle the bias from\n",
      "time-varying confounders, covariates affecting the treatment assignment policy\n",
      "in the observational data, CRN uses domain adversarial training to build\n",
      "balancing representations of the patient history. At each timestep, CRN\n",
      "constructs a treatment invariant representation which removes the association\n",
      "between patient history and treatment assignments and thus can be reliably used\n",
      "for making counterfactual predictions. On a simulated model of tumour growth,\n",
      "with varying degree of time-dependent confounding, we show how our model\n",
      "achieves lower error in estimating counterfactuals and in choosing the correct\n",
      "treatment and timing of treatment than current state-of-the-art methods.\n",
      "\n",
      "**Paper Id :1902.00450 \n",
      "Title :Time Series Deconfounder: Estimating Treatment Effects over Time in the\n",
      "  Presence of Hidden Confounders\n",
      "  The estimation of treatment effects is a pervasive problem in medicine.\n",
      "Existing methods for estimating treatment effects from longitudinal\n",
      "observational data assume that there are no hidden confounders, an assumption\n",
      "that is not testable in practice and, if it does not hold, leads to biased\n",
      "estimates. In this paper, we develop the Time Series Deconfounder, a method\n",
      "that leverages the assignment of multiple treatments over time to enable the\n",
      "estimation of treatment effects in the presence of multi-cause hidden\n",
      "confounders. The Time Series Deconfounder uses a novel recurrent neural network\n",
      "architecture with multitask output to build a factor model over time and infer\n",
      "latent variables that render the assigned treatments conditionally independent;\n",
      "then, it performs causal inference using these latent variables that act as\n",
      "substitutes for the multi-cause unobserved confounders. We provide a\n",
      "theoretical analysis for obtaining unbiased causal effects of time-varying\n",
      "exposures using the Time Series Deconfounder. Using both simulated and real\n",
      "data we show the effectiveness of our method in deconfounding the estimation of\n",
      "treatment responses over time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04186 \n",
      "Title :Infinity Learning: Learning Markov Chains from Aggregate Steady-State\n",
      "  Observations\n",
      "  We consider the task of learning a parametric Continuous Time Markov Chain\n",
      "(CTMC) sequence model without examples of sequences, where the training data\n",
      "consists entirely of aggregate steady-state statistics. Making the problem\n",
      "harder, we assume that the states we wish to predict are unobserved in the\n",
      "training data. Specifically, given a parametric model over the transition rates\n",
      "of a CTMC and some known transition rates, we wish to extrapolate its steady\n",
      "state distribution to states that are unobserved. A technical roadblock to\n",
      "learn a CTMC from its steady state has been that the chain rule to compute\n",
      "gradients will not work over the arbitrarily long sequences necessary to reach\n",
      "steady state ---from where the aggregate statistics are sampled. To overcome\n",
      "this optimization challenge, we propose $\\infty$-SGD, a principled stochastic\n",
      "gradient descent method that uses randomly-stopped estimators to avoid infinite\n",
      "sums required by the steady state computation, while learning even when only a\n",
      "subset of the CTMC states can be observed. We apply $\\infty$-SGD to a\n",
      "real-world testbed and synthetic experiments showcasing its accuracy, ability\n",
      "to extrapolate the steady state distribution to unobserved states under\n",
      "unobserved conditions (heavy loads, when training under light loads), and\n",
      "succeeding in difficult scenarios where even a tailor-made extension of\n",
      "existing methods fails.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04335 \n",
      "Title :Static and Dynamic Values of Computation in MCTS\n",
      "  Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for\n",
      "planning, and has powered many recent advances in artificial intelligence. In\n",
      "MCTS, one typically performs computations (i.e., simulations) to collect\n",
      "statistics about the possible future consequences of actions, and then chooses\n",
      "accordingly. Many popular MCTS methods such as UCT and its variants decide\n",
      "which computations to perform by trading-off exploration and exploitation. In\n",
      "this work, we take a more direct approach, and explicitly quantify the value of\n",
      "a computation based on its expected impact on the quality of the action\n",
      "eventually chosen. Our approach goes beyond the \"myopic\" limitations of\n",
      "existing computation-value-based methods in two senses: (I) we are able to\n",
      "account for the impact of non-immediate (ie, future) computations (II) on\n",
      "non-immediate actions. We show that policies that greedily optimize computation\n",
      "values are optimal under certain assumptions and obtain results that are\n",
      "competitive with the state-of-the-art.\n",
      "\n",
      "**Paper Id :2003.04108 \n",
      "Title :Stable Policy Optimization via Off-Policy Divergence Regularization\n",
      "  Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization\n",
      "(PPO) are among the most successful policy gradient approaches in deep\n",
      "reinforcement learning (RL). While these methods achieve state-of-the-art\n",
      "performance across a wide range of challenging tasks, there is room for\n",
      "improvement in the stabilization of the policy learning and how the off-policy\n",
      "data are used. In this paper we revisit the theoretical foundations of these\n",
      "algorithms and propose a new algorithm which stabilizes the policy improvement\n",
      "through a proximity term that constrains the discounted state-action visitation\n",
      "distribution induced by consecutive policies to be close to one another. This\n",
      "proximity term, expressed in terms of the divergence between the visitation\n",
      "distributions, is learned in an off-policy and adversarial manner. We\n",
      "empirically show that our proposed method can have a beneficial effect on\n",
      "stability and improve final performance in benchmark high-dimensional control\n",
      "tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04613 \n",
      "Title :Neural network wave functions and the sign problem\n",
      "  Neural quantum states (NQS) are a promising approach to study many-body\n",
      "quantum physics. However, they face a major challenge when applied to lattice\n",
      "models: Convolutional networks struggle to converge to ground states with a\n",
      "nontrivial sign structure. We tackle this problem by proposing a neural network\n",
      "architecture with a simple, explicit, and interpretable phase ansatz, which can\n",
      "robustly represent such states and achieve state-of-the-art variational\n",
      "energies for both conventional and frustrated antiferromagnets. In the latter\n",
      "case, our approach uncovers low-energy states that exhibit the Marshall sign\n",
      "rule and are therefore inconsistent with the expected ground state. Such states\n",
      "are the likely cause of the obstruction for NQS-based variational Monte Carlo\n",
      "to access the true ground states of these systems. We discuss the implications\n",
      "of this observation and suggest potential strategies to overcome the problem.\n",
      "\n",
      "**Paper Id :2005.00544 \n",
      "Title :Variational Quantum Eigensolver for Frustrated Quantum Systems\n",
      "  Hybrid quantum-classical algorithms have been proposed as a potentially\n",
      "viable application of quantum computers. A particular example - the variational\n",
      "quantum eigensolver, or VQE - is designed to determine a global minimum in an\n",
      "energy landscape specified by a quantum Hamiltonian, which makes it appealing\n",
      "for the needs of quantum chemistry. Experimental realizations have been\n",
      "reported in recent years and theoretical estimates of its efficiency are a\n",
      "subject of intense effort. Here we consider the performance of the VQE\n",
      "technique for a Hubbard-like model describing a one-dimensional chain of\n",
      "fermions with competing nearest- and next-nearest-neighbor interactions. We\n",
      "find that recovering the VQE solution allows one to obtain the correlation\n",
      "function of the ground state consistent with the exact result. We also study\n",
      "the barren plateau phenomenon for the Hamiltonian in question and find that the\n",
      "severity of this effect depends on the encoding of fermions to qubits. Our\n",
      "results are consistent with the current knowledge about the barren plateaus in\n",
      "quantum optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04632 \n",
      "Title :Black-Box Optimization with Local Generative Surrogates\n",
      "  We propose a novel method for gradient-based optimization of black-box\n",
      "simulators using differentiable local surrogate models. In fields such as\n",
      "physics and engineering, many processes are modeled with non-differentiable\n",
      "simulators with intractable likelihoods. Optimization of these forward models\n",
      "is particularly challenging, especially when the simulator is stochastic. To\n",
      "address such cases, we introduce the use of deep generative models to\n",
      "iteratively approximate the simulator in local neighborhoods of the parameter\n",
      "space. We demonstrate that these local surrogates can be used to approximate\n",
      "the gradient of the simulator, and thus enable gradient-based optimization of\n",
      "simulator parameters. In cases where the dependence of the simulator on the\n",
      "parameter space is constrained to a low dimensional submanifold, we observe\n",
      "that our method attains minima faster than baseline methods, including Bayesian\n",
      "optimization, numerical optimization, and approaches using score function\n",
      "gradient estimators.\n",
      "\n",
      "**Paper Id :2009.00666 \n",
      "Title :Robust, Accurate Stochastic Optimization for Variational Inference\n",
      "  We consider the problem of fitting variational posterior approximations using\n",
      "stochastic optimization methods. The performance of these approximations\n",
      "depends on (1) how well the variational family matches the true posterior\n",
      "distribution,(2) the choice of divergence, and (3) the optimization of the\n",
      "variational objective. We show that even in the best-case scenario when the\n",
      "exact posterior belongs to the assumed variational family, common stochastic\n",
      "optimization methods lead to poor variational approximations if the problem\n",
      "dimension is moderately large. We also demonstrate that these methods are not\n",
      "robust across diverse model types. Motivated by these findings, we develop a\n",
      "more robust and accurate stochastic optimization framework by viewing the\n",
      "underlying optimization algorithm as producing a Markov chain. Our approach is\n",
      "theoretically motivated and includes a diagnostic for convergence and a novel\n",
      "stopping rule, both of which are robust to noisy evaluations of the objective\n",
      "function. We show empirically that the proposed framework works well on a\n",
      "diverse set of models: it can automatically detect stochastic optimization\n",
      "failure or inaccurate variational approximation\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04745 \n",
      "Title :On Layer Normalization in the Transformer Architecture\n",
      "  The Transformer is widely used in natural language processing tasks. To train\n",
      "a Transformer however, one usually needs a carefully designed learning rate\n",
      "warm-up stage, which is shown to be crucial to the final performance but will\n",
      "slow down the optimization and bring more hyper-parameter tunings. In this\n",
      "paper, we first study theoretically why the learning rate warm-up stage is\n",
      "essential and show that the location of layer normalization matters.\n",
      "Specifically, we prove with mean field theory that at initialization, for the\n",
      "original-designed Post-LN Transformer, which places the layer normalization\n",
      "between the residual blocks, the expected gradients of the parameters near the\n",
      "output layer are large. Therefore, using a large learning rate on those\n",
      "gradients makes the training unstable. The warm-up stage is practically helpful\n",
      "for avoiding this problem. On the other hand, our theory also shows that if the\n",
      "layer normalization is put inside the residual blocks (recently proposed as\n",
      "Pre-LN Transformer), the gradients are well-behaved at initialization. This\n",
      "motivates us to remove the warm-up stage for the training of Pre-LN\n",
      "Transformers. We show in our experiments that Pre-LN Transformers without the\n",
      "warm-up stage can reach comparable results with baselines while requiring\n",
      "significantly less training time and hyper-parameter tuning on a wide range of\n",
      "applications.\n",
      "\n",
      "**Paper Id :2002.01136 \n",
      "Title :On Positive-Unlabeled Classification in GAN\n",
      "  This paper defines a positive and unlabeled classification problem for\n",
      "standard GANs, which then leads to a novel technique to stabilize the training\n",
      "of the discriminator in GANs. Traditionally, real data are taken as positive\n",
      "while generated data are negative. This positive-negative classification\n",
      "criterion was kept fixed all through the learning process of the discriminator\n",
      "without considering the gradually improved quality of generated data, even if\n",
      "they could be more realistic than real data at times. In contrast, it is more\n",
      "reasonable to treat the generated data as unlabeled, which could be positive or\n",
      "negative according to their quality. The discriminator is thus a classifier for\n",
      "this positive and unlabeled classification problem, and we derive a new\n",
      "Positive-Unlabeled GAN (PUGAN). We theoretically discuss the global optimality\n",
      "the proposed model will achieve and the equivalent optimization goal.\n",
      "Empirically, we find that PUGAN can achieve comparable or even better\n",
      "performance than those sophisticated discriminator stabilization methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04756 \n",
      "Title :Average-case Acceleration Through Spectral Density Estimation\n",
      "  We develop a framework for the average-case analysis of random quadratic\n",
      "problems and derive algorithms that are optimal under this analysis. This\n",
      "yields a new class of methods that achieve acceleration given a model of the\n",
      "Hessian's eigenvalue distribution. We develop explicit algorithms for the\n",
      "uniform, Marchenko-Pastur, and exponential distributions. These methods are\n",
      "momentum-based algorithms, whose hyper-parameters can be estimated without\n",
      "knowledge of the Hessian's smallest singular value, in contrast with classical\n",
      "accelerated methods like Nesterov acceleration and Polyak momentum. Through\n",
      "empirical benchmarks on quadratic and logistic regression problems, we identify\n",
      "regimes in which the the proposed methods improve over classical (worst-case)\n",
      "accelerated methods.\n",
      "\n",
      "**Paper Id :1910.11561 \n",
      "Title :Convergence Analysis of Block Coordinate Algorithms with Determinantal\n",
      "  Sampling\n",
      "  We analyze the convergence rate of the randomized Newton-like method\n",
      "introduced by Qu et. al. (2016) for smooth and convex objectives, which uses\n",
      "random coordinate blocks of a Hessian-over-approximation matrix $\\bM$ instead\n",
      "of the true Hessian. The convergence analysis of the algorithm is challenging\n",
      "because of its complex dependence on the structure of $\\bM$. However, we show\n",
      "that when the coordinate blocks are sampled with probability proportional to\n",
      "their determinant, the convergence rate depends solely on the eigenvalue\n",
      "distribution of matrix $\\bM$, and has an analytically tractable form. To do so,\n",
      "we derive a fundamental new expectation formula for determinantal point\n",
      "processes. We show that determinantal sampling allows us to reason about the\n",
      "optimal subset size of blocks in terms of the spectrum of $\\bM$. Additionally,\n",
      "we provide a numerical evaluation of our analysis, demonstrating cases where\n",
      "determinantal sampling is superior or on par with uniform sampling.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.04881 \n",
      "Title :Learning Flat Latent Manifolds with VAEs\n",
      "  Measuring the similarity between data points often requires domain knowledge,\n",
      "which can in parts be compensated by relying on unsupervised methods such as\n",
      "latent-variable models, where similarity/distance is estimated in a more\n",
      "compact latent space. Prevalent is the use of the Euclidean metric, which has\n",
      "the drawback of ignoring information about similarity of data stored in the\n",
      "decoder, as captured by the framework of Riemannian geometry. We propose an\n",
      "extension to the framework of variational auto-encoders allows learning flat\n",
      "latent manifolds, where the Euclidean metric is a proxy for the similarity\n",
      "between data points. This is achieved by defining the latent space as a\n",
      "Riemannian manifold and by regularising the metric tensor to be a scaled\n",
      "identity matrix. Additionally, we replace the compact prior typically used in\n",
      "variational auto-encoders with a recently presented, more expressive\n",
      "hierarchical one---and formulate the learning problem as a constrained\n",
      "optimisation problem. We evaluate our method on a range of data-sets, including\n",
      "a video-tracking benchmark, where the performance of our unsupervised approach\n",
      "nears that of state-of-the-art supervised approaches, while retaining the\n",
      "computational efficiency of straight-line-based approaches.\n",
      "\n",
      "**Paper Id :2002.05227 \n",
      "Title :Variational Autoencoders with Riemannian Brownian Motion Priors\n",
      "  Variational Autoencoders (VAEs) represent the given data in a low-dimensional\n",
      "latent space, which is generally assumed to be Euclidean. This assumption\n",
      "naturally leads to the common choice of a standard Gaussian prior over\n",
      "continuous latent variables. Recent work has, however, shown that this prior\n",
      "has a detrimental effect on model capacity, leading to subpar performance. We\n",
      "propose that the Euclidean assumption lies at the heart of this failure mode.\n",
      "To counter this, we assume a Riemannian structure over the latent space, which\n",
      "constitutes a more principled geometric view of the latent codes, and replace\n",
      "the standard Gaussian prior with a Riemannian Brownian motion prior. We propose\n",
      "an efficient inference scheme that does not rely on the unknown normalizing\n",
      "factor of this prior. Finally, we demonstrate that this prior significantly\n",
      "increases model capacity using only one additional scalar parameter.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05049 \n",
      "Title :Detect and Correct Bias in Multi-Site Neuroimaging Datasets\n",
      "  The desire to train complex machine learning algorithms and to increase the\n",
      "statistical power in association studies drives neuroimaging research to use\n",
      "ever-larger datasets. The most obvious way to increase sample size is by\n",
      "pooling scans from independent studies. However, simple pooling is often\n",
      "ill-advised as selection, measurement, and confounding biases may creep in and\n",
      "yield spurious correlations. In this work, we combine 35,320 magnetic resonance\n",
      "images of the brain from 17 studies to examine bias in neuroimaging. In the\n",
      "first experiment, Name That Dataset, we provide empirical evidence for the\n",
      "presence of bias by showing that scans can be correctly assigned to their\n",
      "respective dataset with 71.5% accuracy. Given such evidence, we take a closer\n",
      "look at confounding bias, which is often viewed as the main shortcoming in\n",
      "observational studies. In practice, we neither know all potential confounders\n",
      "nor do we have data on them. Hence, we model confounders as unknown, latent\n",
      "variables. Kolmogorov complexity is then used to decide whether the confounded\n",
      "or the causal model provides the simplest factorization of the graphical model.\n",
      "Finally, we present methods for dataset harmonization and study their ability\n",
      "to remove bias in imaging features. In particular, we propose an extension of\n",
      "the recently introduced ComBat algorithm to control for global variation across\n",
      "image features, inspired by adjusting for population stratification in\n",
      "genetics. Our results demonstrate that harmonization can reduce\n",
      "dataset-specific information in image features. Further, confounding bias can\n",
      "be reduced and even turned into a causal relationship. However, harmonziation\n",
      "also requires caution as it can easily remove relevant subject-specific\n",
      "information. Code is available at https://github.com/ai-med/Dataset-Bias.\n",
      "\n",
      "**Paper Id :1907.06011 \n",
      "Title :Extracting Interpretable Physical Parameters from Spatiotemporal Systems\n",
      "  using Unsupervised Learning\n",
      "  Experimental data is often affected by uncontrolled variables that make\n",
      "analysis and interpretation difficult. For spatiotemporal systems, this problem\n",
      "is further exacerbated by their intricate dynamics. Modern machine learning\n",
      "methods are particularly well-suited for analyzing and modeling complex\n",
      "datasets, but to be effective in science, the result needs to be interpretable.\n",
      "We demonstrate an unsupervised learning technique for extracting interpretable\n",
      "physical parameters from noisy spatiotemporal data and for building a\n",
      "transferable model of the system. In particular, we implement a\n",
      "physics-informed architecture based on variational autoencoders that is\n",
      "designed for analyzing systems governed by partial differential equations\n",
      "(PDEs). The architecture is trained end-to-end and extracts latent parameters\n",
      "that parameterize the dynamics of a learned predictive model for the system. To\n",
      "test our method, we train our model on simulated data from a variety of PDEs\n",
      "with varying dynamical parameters that act as uncontrolled variables. Numerical\n",
      "experiments show that our method can accurately identify relevant parameters\n",
      "and extract them from raw and even noisy spatiotemporal data (tested with\n",
      "roughly 10% added noise). These extracted parameters correlate well (linearly\n",
      "with $R^2 > 0.95$) with the ground truth physical parameters used to generate\n",
      "the datasets. We then apply this method to nonlinear fiber propagation data,\n",
      "generated by an ab-initio simulation, to demonstrate its capabilities on a more\n",
      "realistic dataset. Our method for discovering interpretable latent parameters\n",
      "in spatiotemporal systems will allow us to better analyze and understand\n",
      "real-world phenomena and datasets, which often have unknown and uncontrolled\n",
      "variables that alter the system dynamics and cause varying behaviors that are\n",
      "difficult to disentangle.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05056 \n",
      "Title :Quantum Boosting\n",
      "  Suppose we have a weak learning algorithm $\\mathcal{A}$ for a Boolean-valued\n",
      "problem: $\\mathcal{A}$ produces hypotheses whose bias $\\gamma$ is small, only\n",
      "slightly better than random guessing (this could, for instance, be due to\n",
      "implementing $\\mathcal{A}$ on a noisy device), can we boost the performance of\n",
      "$\\mathcal{A}$ so that $\\mathcal{A}$'s output is correct on $2/3$ of the inputs?\n",
      "  Boosting is a technique that converts a weak and inaccurate machine learning\n",
      "algorithm into a strong accurate learning algorithm. The AdaBoost algorithm by\n",
      "Freund and Schapire (for which they were awarded the G\\\"odel prize in 2003) is\n",
      "one of the widely used boosting algorithms, with many applications in theory\n",
      "and practice. Suppose we have a $\\gamma$-weak learner for a Boolean concept\n",
      "class $C$ that takes time $R(C)$, then the time complexity of AdaBoost scales\n",
      "as $VC(C)\\cdot poly(R(C), 1/\\gamma)$, where $VC(C)$ is the $VC$-dimension of\n",
      "$C$. In this paper, we show how quantum techniques can improve the time\n",
      "complexity of classical AdaBoost. To this end, suppose we have a $\\gamma$-weak\n",
      "quantum learner for a Boolean concept class $C$ that takes time $Q(C)$, we\n",
      "introduce a quantum boosting algorithm whose complexity scales as\n",
      "$\\sqrt{VC(C)}\\cdot poly(Q(C),1/\\gamma);$ thereby achieving a quadratic quantum\n",
      "improvement over classical AdaBoost in terms of $VC(C)$.\n",
      "\n",
      "**Paper Id :1905.04559 \n",
      "Title :ForestDSH: A Universal Hash Design for Discrete Probability\n",
      "  Distributions\n",
      "  In this paper, we consider the problem of classification of $M$ high\n",
      "dimensional queries $y^1,\\cdots,y^M\\in B^S$ to $N$ high dimensional classes\n",
      "$x^1,\\cdots,x^N\\in A^S$ where $A$ and $B$ are discrete alphabets and the\n",
      "probabilistic model that relates data to the classes $P(x,y)$ is known. This\n",
      "problem has applications in various fields including the database search\n",
      "problem in mass spectrometry. The problem is analogous to the nearest neighbor\n",
      "search problem, where the goal is to find the data point in a database that is\n",
      "the most similar to a query point. The state of the art method for solving an\n",
      "approximate version of the nearest neighbor search problem in high dimensions\n",
      "is locality sensitive hashing (LSH). LSH is based on designing hash functions\n",
      "that map near points to the same buckets with a probability higher than random\n",
      "(far) points. To solve our high dimensional classification problem, we\n",
      "introduce distribution sensitive hashes that map jointly generated pairs\n",
      "$(x,y)\\sim P$ to the same bucket with probability higher than random pairs\n",
      "$x\\sim P^A$ and $y\\sim P^B$, where $P^A$ and $P^B$ are the marginal probability\n",
      "distributions of $P$. We design distribution sensitive hashes using a forest of\n",
      "decision trees and we show that the complexity of search grows with\n",
      "$O(N^{\\lambda^*(P)})$ where $\\lambda^*(P)$ is expressed in an analytical form.\n",
      "We further show that the proposed hashes perform faster than state of the art\n",
      "approximate nearest neighbor search methods for a range of probability\n",
      "distributions, in both theory and simulations. Finally, we apply our method to\n",
      "the spectral library search problem in mass spectrometry, and show that it is\n",
      "an order of magnitude faster than the state of the art methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05227 \n",
      "Title :Variational Autoencoders with Riemannian Brownian Motion Priors\n",
      "  Variational Autoencoders (VAEs) represent the given data in a low-dimensional\n",
      "latent space, which is generally assumed to be Euclidean. This assumption\n",
      "naturally leads to the common choice of a standard Gaussian prior over\n",
      "continuous latent variables. Recent work has, however, shown that this prior\n",
      "has a detrimental effect on model capacity, leading to subpar performance. We\n",
      "propose that the Euclidean assumption lies at the heart of this failure mode.\n",
      "To counter this, we assume a Riemannian structure over the latent space, which\n",
      "constitutes a more principled geometric view of the latent codes, and replace\n",
      "the standard Gaussian prior with a Riemannian Brownian motion prior. We propose\n",
      "an efficient inference scheme that does not rely on the unknown normalizing\n",
      "factor of this prior. Finally, we demonstrate that this prior significantly\n",
      "increases model capacity using only one additional scalar parameter.\n",
      "\n",
      "**Paper Id :2002.04881 \n",
      "Title :Learning Flat Latent Manifolds with VAEs\n",
      "  Measuring the similarity between data points often requires domain knowledge,\n",
      "which can in parts be compensated by relying on unsupervised methods such as\n",
      "latent-variable models, where similarity/distance is estimated in a more\n",
      "compact latent space. Prevalent is the use of the Euclidean metric, which has\n",
      "the drawback of ignoring information about similarity of data stored in the\n",
      "decoder, as captured by the framework of Riemannian geometry. We propose an\n",
      "extension to the framework of variational auto-encoders allows learning flat\n",
      "latent manifolds, where the Euclidean metric is a proxy for the similarity\n",
      "between data points. This is achieved by defining the latent space as a\n",
      "Riemannian manifold and by regularising the metric tensor to be a scaled\n",
      "identity matrix. Additionally, we replace the compact prior typically used in\n",
      "variational auto-encoders with a recently presented, more expressive\n",
      "hierarchical one---and formulate the learning problem as a constrained\n",
      "optimisation problem. We evaluate our method on a range of data-sets, including\n",
      "a video-tracking benchmark, where the performance of our unsupervised approach\n",
      "nears that of state-of-the-art supervised approaches, while retaining the\n",
      "computational efficiency of straight-line-based approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05368 \n",
      "Title :Effective Reinforcement Learning through Evolutionary Surrogate-Assisted\n",
      "  Prescription\n",
      "  There is now significant historical data available on decision making in\n",
      "organizations, consisting of the decision problem, what decisions were made,\n",
      "and how desirable the outcomes were. Using this data, it is possible to learn a\n",
      "surrogate model, and with that model, evolve a decision strategy that optimizes\n",
      "the outcomes. This paper introduces a general such approach, called\n",
      "Evolutionary Surrogate-Assisted Prescription, or ESP. The surrogate is, for\n",
      "example, a random forest or a neural network trained with gradient descent, and\n",
      "the strategy is a neural network that is evolved to maximize the predictions of\n",
      "the surrogate model. ESP is further extended in this paper to sequential\n",
      "decision-making tasks, which makes it possible to evaluate the framework in\n",
      "reinforcement learning (RL) benchmarks. Because the majority of evaluations are\n",
      "done on the surrogate, ESP is more sample efficient, has lower variance, and\n",
      "lower regret than standard RL approaches. Surprisingly, its solutions are also\n",
      "better because both the surrogate and the strategy network regularize the\n",
      "decision-making behavior. ESP thus forms a promising foundation to decision\n",
      "optimization in real-world problems.\n",
      "\n",
      "**Paper Id :1906.00588 \n",
      "Title :Quantifying Point-Prediction Uncertainty in Neural Networks via Residual\n",
      "  Estimation with an I/O Kernel\n",
      "  Neural Networks (NNs) have been extensively used for a wide spectrum of\n",
      "real-world regression tasks, where the goal is to predict a numerical outcome\n",
      "such as revenue, effectiveness, or a quantitative result. In many such tasks,\n",
      "the point prediction is not enough: the uncertainty (i.e. risk or confidence)\n",
      "of that prediction must also be estimated. Standard NNs, which are most often\n",
      "used in such tasks, do not provide uncertainty information. Existing approaches\n",
      "address this issue by combining Bayesian models with NNs, but these models are\n",
      "hard to implement, more expensive to train, and usually do not predict as\n",
      "accurately as standard NNs. In this paper, a new framework (RIO) is developed\n",
      "that makes it possible to estimate uncertainty in any pretrained standard NN.\n",
      "The behavior of the NN is captured by modeling its prediction residuals with a\n",
      "Gaussian Process, whose kernel includes both the NN's input and its output. The\n",
      "framework is evaluated in twelve real-world datasets, where it is found to (1)\n",
      "provide reliable estimates of uncertainty, (2) reduce the error of the point\n",
      "predictions, and (3) scale well to large datasets. Given that RIO can be\n",
      "applied to any standard NN without modifications to model architecture or\n",
      "training pipeline, it provides an important ingredient for building real-world\n",
      "NN applications.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05487 \n",
      "Title :End-to-end semantic segmentation of personalized deep brain structures\n",
      "  for non-invasive brain stimulation\n",
      "  Electro-stimulation or modulation of deep brain regions is commonly used in\n",
      "clinical procedures for the treatment of several nervous system disorders. In\n",
      "particular, transcranial direct current stimulation (tDCS) is widely used as an\n",
      "affordable clinical application that is applied through electrodes attached to\n",
      "the scalp. However, it is difficult to determine the amount and distribution of\n",
      "the electric field (EF) in the different brain regions due to anatomical\n",
      "complexity and high inter-subject variability. Personalized tDCS is an emerging\n",
      "clinical procedure that is used to tolerate electrode montage for accurate\n",
      "targeting. This procedure is guided by computational head models generated from\n",
      "anatomical images such as MRI. Distribution of the EF in segmented head models\n",
      "can be calculated through simulation studies. Therefore, fast, accurate, and\n",
      "feasible segmentation of different brain structures would lead to a better\n",
      "adjustment for customized tDCS studies. In this study, a single-encoder\n",
      "multi-decoders convolutional neural network is proposed for deep brain\n",
      "segmentation. The proposed architecture is trained to segment seven deep brain\n",
      "structures using T1-weighted MRI. Network generated models are compared with a\n",
      "reference model constructed using a semi-automatic method, and it presents a\n",
      "high matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate\n",
      "(DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution\n",
      "during tDCS in generated and reference models matched well each other,\n",
      "suggesting its potential usefulness in clinical practice.\n",
      "\n",
      "**Paper Id :1910.02420 \n",
      "Title :Deep learning-based development of personalized human head model with\n",
      "  non-uniform conductivity for brain stimulation\n",
      "  Electromagnetic stimulation of the human brain is a key tool for the\n",
      "neurophysiological characterization and diagnosis of several neurological\n",
      "disorders. Transcranial magnetic stimulation (TMS) is one procedure that is\n",
      "commonly used clinically. However, personalized TMS requires a pipeline for\n",
      "accurate head model generation to provide target-specific stimulation. This\n",
      "process includes intensive segmentation of several head tissues based on\n",
      "magnetic resonance imaging (MRI), which has significant potential for\n",
      "segmentation error, especially for low-contrast tissues. Additionally, a\n",
      "uniform electrical conductivity is assigned to each tissue in the model, which\n",
      "is an unrealistic assumption based on conventional volume conductor modeling.\n",
      "This paper proposes a novel approach to the automatic estimation of electric\n",
      "conductivity in the human head for volume conductor models without anatomical\n",
      "segmentation. A convolutional neural network is designed to estimate\n",
      "personalized electrical conductivity values based on anatomical information\n",
      "obtained from T1- and T2-weighted MRI scans. This approach can avoid the\n",
      "time-consuming process of tissue segmentation and maximize the advantages of\n",
      "position-dependent conductivity assignment based on water content values\n",
      "estimated from MRI intensity values. The computational results of the proposed\n",
      "approach provide similar but smoother electric field results for the brain when\n",
      "compared to conventional approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05511 \n",
      "Title :Deep Autotuner: a Pitch Correcting Network for Singing Performances\n",
      "  We introduce a data-driven approach to automatic pitch correction of solo\n",
      "singing performances. The proposed approach predicts note-wise pitch shifts\n",
      "from the relationship between the respective spectrograms of the singing and\n",
      "accompaniment. This approach differs from commercial systems, where vocal track\n",
      "notes are usually shifted to be centered around pitches in a user-defined\n",
      "score, or mapped to the closest pitch among the twelve equal-tempered scale\n",
      "degrees. The proposed system treats pitch as a continuous value rather than\n",
      "relying on a set of discretized notes found in musical scores, thus allowing\n",
      "for improvisation and harmonization in the singing performance. We train our\n",
      "neural network model using a dataset of 4,702 amateur karaoke performances\n",
      "selected for good intonation. Our model is trained on both incorrect\n",
      "intonation, for which it learns a correction, and intentional pitch variation,\n",
      "which it learns to preserve. The proposed deep neural network with gated\n",
      "recurrent units on top of convolutional layers shows promising performance on\n",
      "the real-world score-free singing pitch correction task of autotuning.\n",
      "\n",
      "**Paper Id :2006.09833 \n",
      "Title :Generative Modelling for Controllable Audio Synthesis of Expressive\n",
      "  Piano Performance\n",
      "  We present a controllable neural audio synthesizer based on Gaussian Mixture\n",
      "Variational Autoencoders (GM-VAE), which can generate realistic piano\n",
      "performances in the audio domain that closely follows temporal conditions of\n",
      "two essential style features for piano performances: articulation and dynamics.\n",
      "We demonstrate how the model is able to apply fine-grained style morphing over\n",
      "the course of synthesizing the audio. This is based on conditions which are\n",
      "latent variables that can be sampled from the prior or inferred from other\n",
      "pieces. One of the envisioned use cases is to inspire creative and brand new\n",
      "interpretations for existing pieces of piano music.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05878 \n",
      "Title :An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset\n",
      "  The Waymo Open Dataset has been released recently, providing a platform to\n",
      "crowdsource some fundamental challenges for automated vehicles (AVs), such as\n",
      "3D detection and tracking. While~the dataset provides a large amount of\n",
      "high-quality and multi-source driving information, people in academia are more\n",
      "interested in the underlying driving policy programmed in Waymo self-driving\n",
      "cars, which is inaccessible due to AV manufacturers' proprietary protection.\n",
      "Accordingly, academic researchers have to make various assumptions to implement\n",
      "AV components in their models or simulations, which may not represent the\n",
      "realistic interactions in real-world traffic. Thus, this paper introduces an\n",
      "approach to learn a long short-term memory (LSTM)-based model for imitating the\n",
      "behavior of Waymo's self-driving model. The proposed model has been evaluated\n",
      "based on Mean Absolute Error (MAE). The experimental results show that our\n",
      "model outperforms several baseline models in driving action prediction. In\n",
      "addition, a visualization tool is presented for verifying the performance of\n",
      "the model.\n",
      "\n",
      "**Paper Id :1912.01834 \n",
      "Title :PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting\n",
      "  The latest methods based on deep learning have achieved amazing results\n",
      "regarding the complex work of inpainting large missing areas in an image. But\n",
      "this type of method generally attempts to generate one single \"optimal\" result,\n",
      "ignoring many other plausible results. Considering the uncertainty of the\n",
      "inpainting task, one sole result can hardly be regarded as a desired\n",
      "regeneration of the missing area. In view of this weakness, which is related to\n",
      "the design of the previous algorithms, we propose a novel deep generative model\n",
      "equipped with a brand new style extractor which can extract the style feature\n",
      "(latent vector) from the ground truth. Once obtained, the extracted style\n",
      "feature and the ground truth are both input into the generator. We also craft a\n",
      "consistency loss that guides the generated image to approximate the ground\n",
      "truth. After iterations, our generator is able to learn the mapping of styles\n",
      "corresponding to multiple sets of vectors. The proposed model can generate a\n",
      "large number of results consistent with the context semantics of the image.\n",
      "Moreover, we evaluated the effectiveness of our model on three datasets, i.e.,\n",
      "CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting\n",
      "methods, this model is able to offer desirable inpainting results with both\n",
      "better quality and higher diversity. The code and model will be made available\n",
      "on https://github.com/vivitsai/PiiGAN.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.05909 \n",
      "Title :Deep reconstruction of strange attractors from time series\n",
      "  Experimental measurements of physical systems often have a limited number of\n",
      "independent channels, causing essential dynamical variables to remain\n",
      "unobserved. However, many popular methods for unsupervised inference of latent\n",
      "dynamics from experimental data implicitly assume that the measurements have\n",
      "higher intrinsic dimensionality than the underlying system---making coordinate\n",
      "identification a dimensionality reduction problem. Here, we study the opposite\n",
      "limit, in which hidden governing coordinates must be inferred from only a\n",
      "low-dimensional time series of measurements. Inspired by classical analysis\n",
      "techniques for partial observations of chaotic attractors, we introduce a\n",
      "general embedding technique for univariate and multivariate time series,\n",
      "consisting of an autoencoder trained with a novel latent-space loss function.\n",
      "We show that our technique reconstructs the strange attractors of synthetic and\n",
      "real-world systems better than existing techniques, and that it creates\n",
      "consistent, predictive representations of even stochastic systems. We conclude\n",
      "by using our technique to discover dynamical attractors in diverse systems such\n",
      "as patient electrocardiograms, household electricity usage, neural spiking, and\n",
      "eruptions of the Old Faithful geyser---demonstrating diverse applications of\n",
      "our technique for exploratory data analysis.\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06239 \n",
      "Title :Boosted Locality Sensitive Hashing: Discriminative Binary Codes for\n",
      "  Source Separation\n",
      "  Speech enhancement tasks have seen significant improvements with the advance\n",
      "of deep learning technology, but with the cost of increased computational\n",
      "complexity. In this study, we propose an adaptive boosting approach to learning\n",
      "locality sensitive hash codes, which represent audio spectra efficiently. We\n",
      "use the learned hash codes for single-channel speech denoising tasks as an\n",
      "alternative to a complex machine learning model, particularly to address the\n",
      "resource-constrained environments. Our adaptive boosting algorithm learns\n",
      "simple logistic regressors as the weak learners. Once trained, their binary\n",
      "classification results transform each spectrum of test noisy speech into a bit\n",
      "string. Simple bitwise operations calculate Hamming distance to find the\n",
      "K-nearest matching frames in the dictionary of training noisy speech spectra,\n",
      "whose associated ideal binary masks are averaged to estimate the denoising mask\n",
      "for that test mixture. Our proposed learning algorithm differs from AdaBoost in\n",
      "the sense that the projections are trained to minimize the distances between\n",
      "the self-similarity matrix of the hash codes and that of the original spectra,\n",
      "rather than the misclassification rate. We evaluate our discriminative hash\n",
      "codes on the TIMIT corpus with various noise types, and show comparative\n",
      "performance to deep learning methods in terms of denoising performance and\n",
      "complexity.\n",
      "\n",
      "**Paper Id :2005.03355 \n",
      "Title :Quantum correlation alignment for unsupervised domain adaptation\n",
      "  Correlation alignment (CORAL), a representative domain adaptation (DA)\n",
      "algorithm, decorrelates and aligns a labelled source domain dataset to an\n",
      "unlabelled target domain dataset to minimize the domain shift such that a\n",
      "classifier can be applied to predict the target domain labels. In this paper,\n",
      "we implement the CORAL on quantum devices by two different methods. One method\n",
      "utilizes quantum basic linear algebra subroutines (QBLAS) to implement the\n",
      "CORAL with exponential speedup in the number and dimension of the given data\n",
      "samples. The other method is achieved through a variational hybrid\n",
      "quantum-classical procedure. In addition, the numerical experiments of the\n",
      "CORAL with three different types of data sets, namely the synthetic data, the\n",
      "synthetic-Iris data, the handwritten digit data, are presented to evaluate the\n",
      "performance of our work. The simulation results prove that the variational\n",
      "quantum correlation alignment algorithm (VQCORAL) can achieve competitive\n",
      "performance compared with the classical CORAL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06306 \n",
      "Title :Jelly Bean World: A Testbed for Never-Ending Learning\n",
      "  Machine learning has shown growing success in recent years. However, current\n",
      "machine learning systems are highly specialized, trained for particular\n",
      "problems or domains, and typically on a single narrow dataset. Human learning,\n",
      "on the other hand, is highly general and adaptable. Never-ending learning is a\n",
      "machine learning paradigm that aims to bridge this gap, with the goal of\n",
      "encouraging researchers to design machine learning systems that can learn to\n",
      "perform a wider variety of inter-related tasks in more complex environments. To\n",
      "date, there is no environment or testbed to facilitate the development and\n",
      "evaluation of never-ending learning systems. To this end, we propose the Jelly\n",
      "Bean World testbed. The Jelly Bean World allows experimentation over\n",
      "two-dimensional grid worlds which are filled with items and in which agents can\n",
      "navigate. This testbed provides environments that are sufficiently complex and\n",
      "where more generally intelligent algorithms ought to perform better than\n",
      "current state-of-the-art reinforcement learning approaches. It does so by\n",
      "producing non-stationary environments and facilitating experimentation with\n",
      "multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope\n",
      "that this new freely-available software will prompt new research and interest\n",
      "in the development and evaluation of never-ending learning systems and more\n",
      "broadly, general intelligence systems.\n",
      "\n",
      "**Paper Id :1909.12892 \n",
      "Title :Automated curricula through setter-solver interactions\n",
      "  Reinforcement learning algorithms use correlations between policies and\n",
      "rewards to improve agent performance. But in dynamic or sparsely rewarding\n",
      "environments these correlations are often too small, or rewarding events are\n",
      "too infrequent to make learning feasible. Human education instead relies on\n",
      "curricula--the breakdown of tasks into simpler, static challenges with dense\n",
      "rewards--to build up to complex behaviors. While curricula are also useful for\n",
      "artificial agents, hand-crafting them is time consuming. This has lead\n",
      "researchers to explore automatic curriculum generation. Here we explore\n",
      "automatic curriculum generation in rich, dynamic environments. Using a\n",
      "setter-solver paradigm we show the importance of considering goal validity,\n",
      "goal feasibility, and goal coverage to construct useful curricula. We\n",
      "demonstrate the success of our approach in rich but sparsely rewarding 2D and\n",
      "3D environments, where an agent is tasked to achieve a single goal selected\n",
      "from a set of possible goals that varies between episodes, and identify\n",
      "challenges for future work. Finally, we demonstrate the value of a novel\n",
      "technique that guides agents towards a desired goal distribution. Altogether,\n",
      "these results represent a substantial step towards applying automatic task\n",
      "curricula to learn complex, otherwise unlearnable goals, and to our knowledge\n",
      "are the first to demonstrate automated curriculum generation for\n",
      "goal-conditioned agents in environments where the possible goals vary between\n",
      "episodes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06395 \n",
      "Title :Quantum Bandits\n",
      "  We consider the quantum version of the bandit problem known as {\\em best arm\n",
      "identification} (BAI). We first propose a quantum modeling of the BAI problem,\n",
      "which assumes that both the learning agent and the environment are quantum; we\n",
      "then propose an algorithm based on quantum amplitude amplification to solve\n",
      "BAI. We formally analyze the behavior of the algorithm on all instances of the\n",
      "problem and we show, in particular, that it is able to get the optimal solution\n",
      "quadratically faster than what is known to hold in the classical case.\n",
      "\n",
      "**Paper Id :1912.03283 \n",
      "Title :A quantum active learning algorithm for sampling against adversarial\n",
      "  attacks\n",
      "  Adversarial attacks represent a serious menace for learning algorithms and\n",
      "may compromise the security of future autonomous systems. A theorem by Khoury\n",
      "and Hadfield-Menell (KH), provides sufficient conditions to guarantee the\n",
      "robustness of machine learning algorithms, but comes with a caveat: it is\n",
      "crucial to know the smallest distance among the classes of the corresponding\n",
      "classification problem. We propose a theoretical framework that allows us to\n",
      "think of active learning as sampling the most promising new points to be\n",
      "classified, so that the minimum distance between classes can be found and the\n",
      "theorem KH used. Additionally, we introduce a quantum active learning algorithm\n",
      "that makes use of such framework and whose complexity is polylogarithmic in the\n",
      "dimension of the space, $m$, and the size of the initial training data $n$,\n",
      "provided the use of qRAMs; and polynomial in the precision, achieving an\n",
      "exponential speedup over the equivalent classical algorithm in $n$ and $m$.\n",
      "This algorithm may be nevertheless `dequantized' reducing the advantage to\n",
      "polynomial.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06470 \n",
      "Title :Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep\n",
      "  Learning\n",
      "  Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty\n",
      "estimation is one of the main benchmarks for assessment of ensembling\n",
      "performance. At the same time, deep learning ensembles have provided\n",
      "state-of-the-art results in uncertainty estimation. In this work, we focus on\n",
      "in-domain uncertainty for image classification. We explore the standards for\n",
      "its quantification and point out pitfalls of existing metrics. Avoiding these\n",
      "pitfalls, we perform a broad study of different ensembling techniques. To\n",
      "provide more insight in this study, we introduce the deep ensemble equivalent\n",
      "score (DEE) and show that many sophisticated ensembling techniques are\n",
      "equivalent to an ensemble of only few independently trained networks in terms\n",
      "of test performance.\n",
      "\n",
      "**Paper Id :1909.02729 \n",
      "Title :A Baseline for Few-Shot Image Classification\n",
      "  Fine-tuning a deep network trained with the standard cross-entropy loss is a\n",
      "strong baseline for few-shot learning. When fine-tuned transductively, this\n",
      "outperforms the current state-of-the-art on standard datasets such as\n",
      "Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same\n",
      "hyper-parameters. The simplicity of this approach enables us to demonstrate the\n",
      "first few-shot learning results on the ImageNet-21k dataset. We find that using\n",
      "a large number of meta-training classes results in high few-shot accuracies\n",
      "even for a large number of few-shot classes. We do not advocate our approach as\n",
      "the solution for few-shot learning, but simply use the results to highlight\n",
      "limitations of current benchmarks and few-shot protocols. We perform extensive\n",
      "studies on benchmark datasets to propose a metric that quantifies the\n",
      "\"hardness\" of a few-shot episode. This metric can be used to report the\n",
      "performance of few-shot algorithms in a more systematic way.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06524 \n",
      "Title :Tensor denoising and completion based on ordinal observations\n",
      "  Higher-order tensors arise frequently in applications such as neuroimaging,\n",
      "recommendation system, social network analysis, and psychological studies. We\n",
      "consider the problem of low-rank tensor estimation from possibly incomplete,\n",
      "ordinal-valued observations. Two related problems are studied, one on tensor\n",
      "denoising and another on tensor completion. We propose a multi-linear\n",
      "cumulative link model, develop a rank-constrained M-estimator, and obtain\n",
      "theoretical accuracy guarantees. Our mean squared error bound enjoys a faster\n",
      "convergence rate than previous results, and we show that the proposed estimator\n",
      "is minimax optimal under the class of low-rank models. Furthermore, the\n",
      "procedure developed serves as an efficient completion method which guarantees\n",
      "consistent recovery of an order-$K$ $(d,\\ldots,d)$-dimensional low-rank tensor\n",
      "using only $\\tilde{\\mathcal{O}}(Kd)$ noisy, quantized observations. We\n",
      "demonstrate the outperformance of our approach over previous methods on the\n",
      "tasks of clustering and collaborative filtering.\n",
      "\n",
      "**Paper Id :1811.05076 \n",
      "Title :Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n",
      "  and its Statistical Optimality\n",
      "  We consider the problem of decomposing a higher-order tensor with binary\n",
      "entries. Such data problems arise frequently in applications such as\n",
      "neuroimaging, recommendation system, topic modeling, and sensor network\n",
      "localization. We propose a multilinear Bernoulli model, develop a\n",
      "rank-constrained likelihood-based estimation method, and obtain the theoretical\n",
      "accuracy guarantees. In contrast to continuous-valued problems, the binary\n",
      "tensor problem exhibits an interesting phase transition phenomenon according to\n",
      "the signal-to-noise ratio. The error bound for the parameter tensor estimation\n",
      "is established, and we show that the obtained rate is minimax optimal under the\n",
      "considered model. Furthermore, we develop an alternating optimization algorithm\n",
      "with convergence guarantees. The efficacy of our approach is demonstrated\n",
      "through both simulations and analyses of multiple data sets on the tasks of\n",
      "tensor completion and clustering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06715 \n",
      "Title :BatchEnsemble: An Alternative Approach to Efficient Ensemble and\n",
      "  Lifelong Learning\n",
      "  Ensembles, where multiple neural networks are trained individually and their\n",
      "predictions are averaged, have been shown to be widely successful for improving\n",
      "both the accuracy and predictive uncertainty of single neural networks.\n",
      "However, an ensemble's cost for both training and testing increases linearly\n",
      "with the number of networks, which quickly becomes untenable.\n",
      "  In this paper, we propose BatchEnsemble, an ensemble method whose\n",
      "computational and memory costs are significantly lower than typical ensembles.\n",
      "BatchEnsemble achieves this by defining each weight matrix to be the Hadamard\n",
      "product of a shared weight among all ensemble members and a rank-one matrix per\n",
      "member. Unlike ensembles, BatchEnsemble is not only parallelizable across\n",
      "devices, where one device trains one member, but also parallelizable within a\n",
      "device, where multiple ensemble members are updated simultaneously for a given\n",
      "mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and\n",
      "out-of-distribution tasks, BatchEnsemble yields competitive accuracy and\n",
      "uncertainties as typical ensembles; the speedup at test time is 3X and memory\n",
      "reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to\n",
      "lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable\n",
      "performance to progressive neural networks while having a much lower\n",
      "computational and memory costs. We further show that BatchEnsemble can easily\n",
      "scale up to lifelong learning on Split-ImageNet which involves 100 sequential\n",
      "learning tasks.\n",
      "\n",
      "**Paper Id :2002.00544 \n",
      "Title :Tensor-to-Vector Regression for Multi-channel Speech Enhancement based\n",
      "  on Tensor-Train Network\n",
      "  We propose a tensor-to-vector regression approach to multi-channel speech\n",
      "enhancement in order to address the issue of input size explosion and\n",
      "hidden-layer size expansion. The key idea is to cast the conventional deep\n",
      "neural network (DNN) based vector-to-vector regression formulation under a\n",
      "tensor-train network (TTN) framework. TTN is a recently emerged solution for\n",
      "compact representation of deep models with fully connected hidden layers. Thus\n",
      "TTN maintains DNN's expressive power yet involves a much smaller amount of\n",
      "trainable parameters. Furthermore, TTN can handle a multi-dimensional tensor\n",
      "input by design, which exactly matches the desired setting in multi-channel\n",
      "speech enhancement. We first provide a theoretical extension from DNN to TTN\n",
      "based regression. Next, we show that TTN can attain speech enhancement quality\n",
      "comparable with that for DNN but with much fewer parameters, e.g., a reduction\n",
      "from 27 million to only 5 million parameters is observed in a single-channel\n",
      "scenario. TTN also improves PESQ over DNN from 2.86 to 2.96 by slightly\n",
      "increasing the number of trainable parameters. Finally, in 8-channel\n",
      "conditions, a PESQ of 3.12 is achieved using 20 million parameters for TTN,\n",
      "whereas a DNN with 68 million parameters can only attain a PESQ of 3.06. Our\n",
      "implementation is available online\n",
      "https://github.com/uwjunqi/Tensor-Train-Neural-Network.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06836 \n",
      "Title :Control Frequency Adaptation via Action Persistence in Batch\n",
      "  Reinforcement Learning\n",
      "  The choice of the control frequency of a system has a relevant impact on the\n",
      "ability of reinforcement learning algorithms to learn a highly performing\n",
      "policy. In this paper, we introduce the notion of action persistence that\n",
      "consists in the repetition of an action for a fixed number of decision steps,\n",
      "having the effect of modifying the control frequency. We start analyzing how\n",
      "action persistence affects the performance of the optimal policy, and then we\n",
      "present a novel algorithm, Persistent Fitted Q-Iteration (PFQI), that extends\n",
      "FQI, with the goal of learning the optimal value function at a given\n",
      "persistence. After having provided a theoretical study of PFQI and a heuristic\n",
      "approach to identify the optimal persistence, we present an experimental\n",
      "campaign on benchmark domains to show the advantages of action persistence and\n",
      "proving the effectiveness of our persistence selection method.\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.06890 \n",
      "Title :Amplifying The Uncanny\n",
      "  Deep neural networks have become remarkably good at producing realistic\n",
      "deepfakes, images of people that (to the untrained eye) are indistinguishable\n",
      "from real images. Deepfakes are produced by algorithms that learn to\n",
      "distinguish between real and fake images and are optimised to generate samples\n",
      "that the system deems realistic. This paper, and the resulting series of\n",
      "artworks Being Foiled explore the aesthetic outcome of inverting this process,\n",
      "instead optimising the system to generate images that it predicts as being\n",
      "fake. This maximises the unlikelihood of the data and in turn, amplifies the\n",
      "uncanny nature of these machine hallucinations.\n",
      "\n",
      "**Paper Id :2005.00813 \n",
      "Title :Social Biases in NLP Models as Barriers for Persons with Disabilities\n",
      "  Building equitable and inclusive NLP technologies demands consideration of\n",
      "whether and how social attitudes are represented in ML models. In particular,\n",
      "representations encoded in models often inadvertently perpetuate undesirable\n",
      "social biases from the data on which they are trained. In this paper, we\n",
      "present evidence of such undesirable biases towards mentions of disability in\n",
      "two different English language models: toxicity prediction and sentiment\n",
      "analysis. Next, we demonstrate that the neural embeddings that are the critical\n",
      "first step in most NLP pipelines similarly contain undesirable biases towards\n",
      "mentions of disability. We end by highlighting topical biases in the discourse\n",
      "about disability which may contribute to the observed model biases; for\n",
      "instance, gun violence, homelessness, and drug addiction are over-represented\n",
      "in texts discussing mental illness.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07082 \n",
      "Title :PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks\n",
      "  for Thermal and NIR to Visible Image Transformation\n",
      "  In many real world scenarios, it is difficult to capture the images in the\n",
      "visible light spectrum (VIS) due to bad lighting conditions. However, the\n",
      "images can be captured in such scenarios using Near-Infrared (NIR) and Thermal\n",
      "(THM) cameras. The NIR and THM images contain the limited details. Thus, there\n",
      "is a need to transform the images from THM/NIR to VIS for better understanding.\n",
      "However, it is non-trivial task due to the large domain discrepancies and lack\n",
      "of abundant datasets. Nowadays, Generative Adversarial Network (GAN) is able to\n",
      "transform the images from one domain to another domain. Most of the available\n",
      "GAN based methods use the combination of the adversarial and the pixel-wise\n",
      "losses (like $L_1$ or $L_2$) as the objective function for training. The\n",
      "quality of transformed images in case of THM/NIR to VIS transformation is still\n",
      "not up to the mark using such objective function. Thus, better objective\n",
      "functions are needed to improve the quality, fine details and realism of the\n",
      "transformed images. A new model for THM/NIR to VIS image transformation called\n",
      "Perceptual Cyclic-Synthesized Generative Adversarial Network (PCSGAN) is\n",
      "introduced to address these issues. The PCSGAN uses the combination of the\n",
      "perceptual (i.e., feature based) losses along with the pixel-wise and the\n",
      "adversarial losses. Both the quantitative and qualitative measures are used to\n",
      "judge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR\n",
      "scene datasets. The proposed PCSGAN outperforms the state-of-the-art image\n",
      "transformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in\n",
      "terms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is\n",
      "available at https://github.com/KishanKancharagunta/PCSGAN.\n",
      "\n",
      "**Paper Id :2009.05752 \n",
      "Title :Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial\n",
      "  Networks\n",
      "  Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common\n",
      "procedure for the identification of many respiratory diseases compared to MRI,\n",
      "CT, and PET scans. This paper presents the use of generative adversarial\n",
      "networks (GAN) to perform the task of lung segmentation on a given CXR. GANs\n",
      "are popular to generate realistic data by learning the mapping from one domain\n",
      "to another. In our work, the generator of the GAN is trained to generate a\n",
      "segmented mask of a given input CXR. The discriminator distinguishes between a\n",
      "ground truth and the generated mask, and updates the generator through the\n",
      "adversarial loss measure. The objective is to generate masks for the input CXR,\n",
      "which are as realistic as possible compared to the ground truth masks. The\n",
      "model is trained and evaluated using four different discriminators referred to\n",
      "as D1, D2, D3, and D4, respectively. Experimental results on three different\n",
      "CXR datasets reveal that the proposed model is able to achieve a dice-score of\n",
      "0.9740, and IOU score of 0.943, which are better than other reported\n",
      "state-of-the art results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07217 \n",
      "Title :Decision-Making with Auto-Encoding Variational Bayes\n",
      "  To make decisions based on a model fit with auto-encoding variational Bayes\n",
      "(AEVB), practitioners often let the variational distribution serve as a\n",
      "surrogate for the posterior distribution. This approach yields biased estimates\n",
      "of the expected risk, and therefore leads to poor decisions for two reasons.\n",
      "First, the model fit with AEVB may not equal the underlying data distribution.\n",
      "Second, the variational distribution may not equal the posterior distribution\n",
      "under the fitted model. We explore how fitting the variational distribution\n",
      "based on several objective functions other than the ELBO, while continuing to\n",
      "fit the generative model based on the ELBO, affects the quality of downstream\n",
      "decisions. For the probabilistic principal component analysis model, we\n",
      "investigate how importance sampling error, as well as the bias of the model\n",
      "parameter estimates, varies across several approximate posteriors when used as\n",
      "proposal distributions. Our theoretical results suggest that a posterior\n",
      "approximation distinct from the variational distribution should be used for\n",
      "making decisions. Motivated by these theoretical results, we propose learning\n",
      "several approximate proposals for the best model and combining them using\n",
      "multiple importance sampling for decision-making. In addition to toy examples,\n",
      "we present a full-fledged case study of single-cell RNA sequencing. In this\n",
      "challenging instance of multiple hypothesis testing, our proposed approach\n",
      "surpasses the current state of the art.\n",
      "\n",
      "**Paper Id :2009.00666 \n",
      "Title :Robust, Accurate Stochastic Optimization for Variational Inference\n",
      "  We consider the problem of fitting variational posterior approximations using\n",
      "stochastic optimization methods. The performance of these approximations\n",
      "depends on (1) how well the variational family matches the true posterior\n",
      "distribution,(2) the choice of divergence, and (3) the optimization of the\n",
      "variational objective. We show that even in the best-case scenario when the\n",
      "exact posterior belongs to the assumed variational family, common stochastic\n",
      "optimization methods lead to poor variational approximations if the problem\n",
      "dimension is moderately large. We also demonstrate that these methods are not\n",
      "robust across diverse model types. Motivated by these findings, we develop a\n",
      "more robust and accurate stochastic optimization framework by viewing the\n",
      "underlying optimization algorithm as producing a Markov chain. Our approach is\n",
      "theoretically motivated and includes a diagnostic for convergence and a novel\n",
      "stopping rule, both of which are robust to noisy evaluations of the objective\n",
      "function. We show empirically that the proposed framework works well on a\n",
      "diverse set of models: it can automatically detect stochastic optimization\n",
      "failure or inaccurate variational approximation\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07366 \n",
      "Title :Adversarial Deep Network Embedding for Cross-network Node Classification\n",
      "  In this paper, the task of cross-network node classification, which leverages\n",
      "the abundant labeled nodes from a source network to help classify unlabeled\n",
      "nodes in a target network, is studied. The existing domain adaptation\n",
      "algorithms generally fail to model the network structural information, and the\n",
      "current network embedding models mainly focus on single-network applications.\n",
      "Thus, both of them cannot be directly applied to solve the cross-network node\n",
      "classification problem. This motivates us to propose an adversarial\n",
      "cross-network deep network embedding (ACDNE) model to integrate adversarial\n",
      "domain adaptation with deep network embedding so as to learn network-invariant\n",
      "node representations that can also well preserve the network structural\n",
      "information. In ACDNE, the deep network embedding module utilizes two feature\n",
      "extractors to jointly preserve attributed affinity and topological proximities\n",
      "between nodes. In addition, a node classifier is incorporated to make node\n",
      "representations label-discriminative. Moreover, an adversarial domain\n",
      "adaptation technique is employed to make node representations\n",
      "network-invariant. Extensive experimental results demonstrate that the proposed\n",
      "ACDNE model achieves the state-of-the-art performance in cross-network node\n",
      "classification.\n",
      "\n",
      "**Paper Id :2001.06137 \n",
      "Title :Graph Inference Learning for Semi-supervised Classification\n",
      "  In this work, we address semi-supervised classification of graph data, where\n",
      "the categories of those unlabeled nodes are inferred from labeled nodes as well\n",
      "as graph structures. Recent works often solve this problem via advanced graph\n",
      "convolution in a conventionally supervised manner, but the performance could\n",
      "degrade significantly when labeled data is scarce. To this end, we propose a\n",
      "Graph Inference Learning (GIL) framework to boost the performance of\n",
      "semi-supervised node classification by learning the inference of node labels on\n",
      "graph topology. To bridge the connection between two nodes, we formally define\n",
      "a structure relation by encapsulating node attributes, between-node paths, and\n",
      "local topological structures together, which can make the inference\n",
      "conveniently deduced from one node to another node. For learning the inference\n",
      "process, we further introduce meta-optimization on structure relations from\n",
      "training nodes to validation nodes, such that the learnt graph inference\n",
      "capability can be better self-adapted to testing nodes. Comprehensive\n",
      "evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and\n",
      "NELL) demonstrate the superiority of our proposed GIL when compared against\n",
      "state-of-the-art methods on the semi-supervised node classification task.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "**Paper Id :2010.15969 \n",
      "Title :Greedy Optimization Provably Wins the Lottery: Logarithmic Number of\n",
      "  Winning Tickets is Enough\n",
      "  Despite the great success of deep learning, recent works show that large deep\n",
      "neural networks are often highly redundant and can be significantly reduced in\n",
      "size. However, the theoretical question of how much we can prune a neural\n",
      "network given a specified tolerance of accuracy drop is still open. This paper\n",
      "provides one answer to this question by proposing a greedy optimization based\n",
      "pruning method. The proposed method has the guarantee that the discrepancy\n",
      "between the pruned network and the original network decays with exponentially\n",
      "fast rate w.r.t. the size of the pruned network, under weak assumptions that\n",
      "apply for most practical settings. Empirically, our method improves prior arts\n",
      "on pruning various network architectures including ResNet, MobilenetV2/V3 on\n",
      "ImageNet.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07469 \n",
      "Title :A Neural Network Based on First Principles\n",
      "  In this paper, a Neural network is derived from first principles, assuming\n",
      "only that each layer begins with a linear dimension-reducing transformation.\n",
      "The approach appeals to the principle of Maximum Entropy (MaxEnt) to find the\n",
      "posterior distribution of the input data of each layer, conditioned on the\n",
      "layer output variables. This posterior has a well-defined mean, the conditional\n",
      "mean estimator, that is calculated using a type of neural network with\n",
      "theoretically-derived activation functions similar to sigmoid, softplus, and\n",
      "relu. This implicitly provides a theoretical justification for their use. A\n",
      "theorem that finds the conditional distribution and conditional mean estimator\n",
      "under the MaxEnt prior is proposed, unifying results for special cases.\n",
      "Combining layers results in an auto-encoder with conventional feed-forward\n",
      "analysis network and a type of linear Bayesian belief network in the\n",
      "reconstruction path.\n",
      "\n",
      "**Paper Id :1912.09132 \n",
      "Title :Mean field theory for deep dropout networks: digging up gradient\n",
      "  backpropagation deeply\n",
      "  In recent years, the mean field theory has been applied to the study of\n",
      "neural networks and has achieved a great deal of success. The theory has been\n",
      "applied to various neural network structures, including CNNs, RNNs, Residual\n",
      "networks, and Batch normalization. Inevitably, recent work has also covered the\n",
      "use of dropout. The mean field theory shows that the existence of depth scales\n",
      "that limit the maximum depth of signal propagation and gradient\n",
      "backpropagation. However, the gradient backpropagation is derived under the\n",
      "gradient independence assumption that weights used during feed forward are\n",
      "drawn independently from the ones used in backpropagation. This is not how\n",
      "neural networks are trained in a real setting. Instead, the same weights used\n",
      "in a feed-forward step needs to be carried over to its corresponding\n",
      "backpropagation. Using this realistic condition, we perform theoretical\n",
      "computation on linear dropout networks and a series of experiments on dropout\n",
      "networks. Our empirical results show an interesting phenomenon that the length\n",
      "gradients can backpropagate for a single input and a pair of inputs are\n",
      "governed by the same depth scale. Besides, we study the relationship between\n",
      "variance and mean of statistical metrics of the gradient and shown an emergence\n",
      "of universality. Finally, we investigate the maximum trainable length for deep\n",
      "dropout networks through a series of experiments using MNIST and CIFAR10 and\n",
      "provide a more precise empirical formula that describes the trainable length\n",
      "than original work.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07596 \n",
      "Title :Coordination without communication: optimal regret in two players\n",
      "  multi-armed bandits\n",
      "  We consider two agents playing simultaneously the same stochastic three-armed\n",
      "bandit problem. The two agents are cooperating but they cannot communicate. We\n",
      "propose a strategy with no collisions at all between the players (with very\n",
      "high probability), and with near-optimal regret $O(\\sqrt{T \\log(T)})$. We also\n",
      "argue that the extra logarithmic term $\\sqrt{\\log(T)}$ should be necessary by\n",
      "proving a lower bound for a full information variant of the problem.\n",
      "\n",
      "**Paper Id :2007.01160 \n",
      "Title :Tight Bounds on Minimax Regret under Logarithmic Loss via\n",
      "  Self-Concordance\n",
      "  We consider the classical problem of sequential probability assignment under\n",
      "logarithmic loss while competing against an arbitrary, potentially\n",
      "nonparametric class of experts. We obtain tight bounds on the minimax regret\n",
      "via a new approach that exploits the self-concordance property of the\n",
      "logarithmic loss. We show that for any expert class with (sequential) metric\n",
      "entropy $\\mathcal{O}(\\gamma^{-p})$ at scale $\\gamma$, the minimax regret is\n",
      "$\\mathcal{O}(n^{p/(p+1)})$, and that this rate cannot be improved without\n",
      "additional assumptions on the expert class under consideration. As an\n",
      "application of our techniques, we resolve the minimax regret for nonparametric\n",
      "Lipschitz classes of experts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07775 \n",
      "Title :An enhanced Tree-LSTM architecture for sentence semantic modeling using\n",
      "  typed dependencies\n",
      "  Tree-based Long short term memory (LSTM) network has become state-of-the-art\n",
      "for modeling the meaning of language texts as they can effectively exploit the\n",
      "grammatical syntax and thereby non-linear dependencies among words of the\n",
      "sentence. However, most of these models cannot recognize the difference in\n",
      "meaning caused by a change in semantic roles of words or phrases because they\n",
      "do not acknowledge the type of grammatical relations, also known as typed\n",
      "dependencies, in sentence structure. This paper proposes an enhanced LSTM\n",
      "architecture, called relation gated LSTM, which can model the relationship\n",
      "between two inputs of a sequence using a control input. We also introduce a\n",
      "Tree-LSTM model called Typed Dependency Tree-LSTM that uses the sentence\n",
      "dependency parse structure as well as the dependency type to embed sentence\n",
      "meaning into a dense vector. The proposed model outperformed its type-unaware\n",
      "counterpart in two typical NLP tasks - Semantic Relatedness Scoring and\n",
      "Sentiment Analysis, in a lesser number of training epochs. The results were\n",
      "comparable or competitive with other state-of-the-art models. Qualitative\n",
      "analysis showed that changes in the voice of sentences had little effect on the\n",
      "model's predicted scores, while changes in nominal (noun) words had a more\n",
      "significant impact. The model recognized subtle semantic relationships in\n",
      "sentence pairs. The magnitudes of learned typed dependencies embeddings were\n",
      "also in agreement with human intuitions. The research findings imply the\n",
      "significance of grammatical relations in sentence modeling. The proposed models\n",
      "would serve as a base for future researches in this direction.\n",
      "\n",
      "**Paper Id :2003.11644 \n",
      "Title :Multi-Label Text Classification using Attention-based Graph Neural\n",
      "  Network\n",
      "  In Multi-Label Text Classification (MLTC), one sample can belong to more than\n",
      "one class. It is observed that most MLTC tasks, there are dependencies or\n",
      "correlations among labels. Existing methods tend to ignore the relationship\n",
      "among labels. In this paper, a graph attention network-based model is proposed\n",
      "to capture the attentive dependency structure among the labels. The graph\n",
      "attention network uses a feature matrix and a correlation matrix to capture and\n",
      "explore the crucial dependencies between the labels and generate classifiers\n",
      "for the task. The generated classifiers are applied to sentence feature vectors\n",
      "obtained from the text feature extraction network (BiLSTM) to enable end-to-end\n",
      "training. Attention allows the system to assign different weights to neighbor\n",
      "nodes per label, thus allowing it to learn the dependencies among labels\n",
      "implicitly. The results of the proposed model are validated on five real-world\n",
      "MLTC datasets. The proposed model achieves similar or better performance\n",
      "compared to the previous state-of-the-art models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.07803 \n",
      "Title :Latent Poisson models for networks with heterogeneous density\n",
      "  Empirical networks are often globally sparse, with a small average number of\n",
      "connections per node, when compared to the total size of the network. However,\n",
      "this sparsity tends not to be homogeneous, and networks can also be locally\n",
      "dense, for example with a few nodes connecting to a large fraction of the rest\n",
      "of the network, or with small groups of nodes with a large probability of\n",
      "connections between them. Here we show how latent Poisson models which generate\n",
      "hidden multigraphs can be effective at capturing this density heterogeneity,\n",
      "while being more tractable mathematically than some of the alternatives that\n",
      "model simple graphs directly. We show how these latent multigraphs can be\n",
      "reconstructed from data on simple graphs, and how this allows us to disentangle\n",
      "disassortative degree-degree correlations from the constraints of imposed\n",
      "degree sequences, and to improve the identification of community structure in\n",
      "empirically relevant scenarios.\n",
      "\n",
      "**Paper Id :2004.14764 \n",
      "Title :Hierarchical clustering of bipartite data sets based on the statistical\n",
      "  significance of coincidences\n",
      "  When some 'entities' are related by the 'features' they share they are\n",
      "amenable to a bipartite network representation. Plant-pollinator ecological\n",
      "communities, co-authorship of scientific papers, customers and purchases, or\n",
      "answers in a poll, are but a few examples. Analyzing clustering of such\n",
      "entities in the network is a useful tool with applications in many fields, like\n",
      "internet technology, recommender systems, or detection of diseases. The\n",
      "algorithms most widely applied to find clusters in bipartite networks are\n",
      "variants of modularity optimization. Here we provide an hierarchical clustering\n",
      "algorithm based on a dissimilarity between entities that quantifies the\n",
      "probability that the features shared by two entities is due to mere chance. The\n",
      "algorithm performance is $O(n^2)$ when applied to a set of n entities, and its\n",
      "outcome is a dendrogram exhibiting the connections of those entities. Through\n",
      "the introduction of a 'susceptibility' measure we can provide an 'optimal'\n",
      "choice for the clustering as well as quantify its quality. The dendrogram\n",
      "reveals further useful structural information though -- like the existence of\n",
      "sub-clusters within clusters or of nodes that do not fit in any cluster. We\n",
      "illustrate the algorithm by applying it first to a set of synthetic networks,\n",
      "and then to a selection of examples. We also illustrate how to transform our\n",
      "algorithm into a valid alternative for one-mode networks as well, and show that\n",
      "it performs at least as well as the standard, modularity-based algorithms --\n",
      "with a higher numerical performance. We provide an implementation of the\n",
      "algorithm in Python freely accessible from GitHub.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08204 \n",
      "Title :SYMOG: learning symmetric mixture of Gaussian modes for improved\n",
      "  fixed-point quantization\n",
      "  Deep neural networks (DNNs) have been proven to outperform classical methods\n",
      "on several machine learning benchmarks. However, they have high computational\n",
      "complexity and require powerful processing units. Especially when deployed on\n",
      "embedded systems, model size and inference time must be significantly reduced.\n",
      "We propose SYMOG (symmetric mixture of Gaussian modes), which significantly\n",
      "decreases the complexity of DNNs through low-bit fixed-point quantization.\n",
      "SYMOG is a novel soft quantization method such that the learning task and the\n",
      "quantization are solved simultaneously. During training the weight distribution\n",
      "changes from an unimodal Gaussian distribution to a symmetric mixture of\n",
      "Gaussians, where each mean value belongs to a particular fixed-point mode. We\n",
      "evaluate our approach with different architectures (LeNet5, VGG7, VGG11,\n",
      "DenseNet) on common benchmark data sets (MNIST, CIFAR-10, CIFAR-100) and we\n",
      "compare with state-of-the-art quantization approaches. We achieve excellent\n",
      "results and outperform 2-bit state-of-the-art performance with an error rate of\n",
      "only 5.71% on CIFAR-10 and 27.65% on CIFAR-100.\n",
      "\n",
      "**Paper Id :1910.09495 \n",
      "Title :S4NN: temporal backpropagation for spiking neural networks with one\n",
      "  spike per neuron\n",
      "  We propose a new supervised learning rule for multilayer spiking neural\n",
      "networks (SNNs) that use a form of temporal coding known as rank-order-coding.\n",
      "With this coding scheme, all neurons fire exactly one spike per stimulus, but\n",
      "the firing order carries information. In particular, in the readout layer, the\n",
      "first neuron to fire determines the class of the stimulus. We derive a new\n",
      "learning rule for this sort of network, named S4NN, akin to traditional error\n",
      "backpropagation, yet based on latencies. We show how approximated error\n",
      "gradients can be computed backward in a feedforward network with any number of\n",
      "layers. This approach reaches state-of-the-art performance with supervised\n",
      "multi fully-connected layer SNNs: test accuracy of 97.4% for the MNIST dataset,\n",
      "and 99.2% for the Caltech Face/Motorbike dataset. Yet, the neuron model that we\n",
      "use, non-leaky integrate-and-fire, is much simpler than the one used in all\n",
      "previous works. The source codes of the proposed S4NN are publicly available at\n",
      "https://github.com/SRKH/S4NN.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08247 \n",
      "Title :Learning Global Transparent Models Consistent with Local Contrastive\n",
      "  Explanations\n",
      "  There is a rich and growing literature on producing local\n",
      "contrastive/counterfactual explanations for black-box models (e.g. neural\n",
      "networks).\n",
      "  In these methods, for an input, an explanation is in the form of a contrast\n",
      "point differing in very few features from the original input and lying in a\n",
      "different class. Other works try to build globally interpretable models like\n",
      "decision trees and rule lists based on the data using actual labels or based on\n",
      "the black-box models predictions. Although these interpretable global models\n",
      "can be useful, they may not be consistent with local explanations from a\n",
      "specific black-box of choice. In this work, we explore the question: Can we\n",
      "produce a transparent global model that is simultaneously accurate and\n",
      "consistent with the local (contrastive) explanations of the black-box model? We\n",
      "introduce a natural local consistency metric that quantifies if the local\n",
      "explanations and predictions of the black-box model are also consistent with\n",
      "the proxy global transparent model. Based on a key insight we propose a novel\n",
      "method where we create custom boolean features from sparse local contrastive\n",
      "explanations of the black-box model and then train a globally transparent model\n",
      "on just these, and showcase empirically that such models have higher local\n",
      "consistency compared with other known strategies, while still being close in\n",
      "performance to models that are trained with access to the original data.\n",
      "\n",
      "**Paper Id :2003.06005 \n",
      "Title :Model Agnostic Multilevel Explanations\n",
      "  In recent years, post-hoc local instance-level and global dataset-level\n",
      "explainability of black-box models has received a lot of attention. Much less\n",
      "attention has been given to obtaining insights at intermediate or group levels,\n",
      "which is a need outlined in recent works that study the challenges in realizing\n",
      "the guidelines in the General Data Protection Regulation (GDPR). In this paper,\n",
      "we propose a meta-method that, given a typical local explainability method, can\n",
      "build a multilevel explanation tree. The leaves of this tree correspond to the\n",
      "local explanations, the root corresponds to the global explanation, and\n",
      "intermediate levels correspond to explanations for groups of data points that\n",
      "it automatically clusters. The method can also leverage side information, where\n",
      "users can specify points for which they may want the explanations to be\n",
      "similar. We argue that such a multilevel structure can also be an effective\n",
      "form of communication, where one could obtain few explanations that\n",
      "characterize the entire dataset by considering an appropriate level in our\n",
      "explanation tree. Explanations for novel test points can be cost-efficiently\n",
      "obtained by associating them with the closest training points. When the local\n",
      "explainability technique is generalized additive (viz. LIME, GAMs), we develop\n",
      "a fast approximate algorithm for building the multilevel tree and study its\n",
      "convergence behavior. We validate the effectiveness of the proposed technique\n",
      "based on two human studies -- one with experts and the other with non-expert\n",
      "users -- on real world datasets, and show that we produce high fidelity sparse\n",
      "explanations on several other public datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08274 \n",
      "Title :Residual Correlation in Graph Neural Network Regression\n",
      "  A graph neural network transforms features in each vertex's neighborhood into\n",
      "a vector representation of the vertex. Afterward, each vertex's representation\n",
      "is used independently for predicting its label. This standard pipeline\n",
      "implicitly assumes that vertex labels are conditionally independent given their\n",
      "neighborhood features. However, this is a strong assumption, and we show that\n",
      "it is far from true on many real-world graph datasets. Focusing on regression\n",
      "tasks, we find that this conditional independence assumption severely limits\n",
      "predictive power. This should not be that surprising, given that traditional\n",
      "graph-based semi-supervised learning methods such as label propagation work in\n",
      "the opposite fashion by explicitly modeling the correlation in predicted\n",
      "outcomes.\n",
      "  Here, we address this problem with an interpretable and efficient framework\n",
      "that can improve any graph neural network architecture simply by exploiting\n",
      "correlation structure in the regression residuals. In particular, we model the\n",
      "joint distribution of residuals on vertices with a parameterized multivariate\n",
      "Gaussian, and estimate the parameters by maximizing the marginal likelihood of\n",
      "the observed labels. Our framework achieves substantially higher accuracy than\n",
      "competing baselines, and the learned parameters can be interpreted as the\n",
      "strength of correlation among connected vertices. Furthermore, we develop\n",
      "linear time algorithms for low-variance, unbiased model parameter estimates,\n",
      "allowing us to scale to large networks. We also provide a basic version of our\n",
      "method that makes stronger assumptions on correlation structure but is painless\n",
      "to implement, often leading to great practical performance with minimal\n",
      "overhead.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08327 \n",
      "Title :Fawkes: Protecting Privacy against Unauthorized Deep Learning Models\n",
      "  Today's proliferation of powerful facial recognition systems poses a real\n",
      "threat to personal privacy. As Clearview.ai demonstrated, anyone can canvas the\n",
      "Internet for data and train highly accurate facial recognition models of\n",
      "individuals without their knowledge. We need tools to protect ourselves from\n",
      "potential misuses of unauthorized facial recognition systems. Unfortunately, no\n",
      "practical or effective solutions exist.\n",
      "  In this paper, we propose Fawkes, a system that helps individuals inoculate\n",
      "their images against unauthorized facial recognition models. Fawkes achieves\n",
      "this by helping users add imperceptible pixel-level changes (we call them\n",
      "\"cloaks\") to their own photos before releasing them. When used to train facial\n",
      "recognition models, these \"cloaked\" images produce functional models that\n",
      "consistently cause normal images of the user to be misidentified. We\n",
      "experimentally demonstrate that Fawkes provides 95+% protection against user\n",
      "recognition regardless of how trackers train their models. Even when clean,\n",
      "uncloaked images are \"leaked\" to the tracker and used for training, Fawkes can\n",
      "still maintain an 80+% protection success rate. We achieve 100% success in\n",
      "experiments against today's state-of-the-art facial recognition services.\n",
      "Finally, we show that Fawkes is robust against a variety of countermeasures\n",
      "that try to detect or disrupt image cloaks.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08396 \n",
      "Title :Keep Doing What Worked: Behavioral Modelling Priors for Offline\n",
      "  Reinforcement Learning\n",
      "  Off-policy reinforcement learning algorithms promise to be applicable in\n",
      "settings where only a fixed data-set (batch) of environment interactions is\n",
      "available and no new experience can be acquired. This property makes these\n",
      "algorithms appealing for real world problems such as robot control. In\n",
      "practice, however, standard off-policy algorithms fail in the batch setting for\n",
      "continuous control. In this paper, we propose a simple solution to this\n",
      "problem. It admits the use of data generated by arbitrary behavior policies and\n",
      "uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias\n",
      "the RL policy towards actions that have previously been executed and are likely\n",
      "to be successful on the new task. Our method can be seen as an extension of\n",
      "recent work on batch-RL that enables stable learning from conflicting\n",
      "data-sources. We find improvements on competitive baselines in a variety of RL\n",
      "tasks -- including standard continuous control benchmarks and multi-task\n",
      "learning for simulated and real-world robots.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08596 \n",
      "Title :Interpretability of machine learning based prediction models in\n",
      "  healthcare\n",
      "  There is a need of ensuring machine learning models that are interpretable.\n",
      "Higher interpretability of the model means easier comprehension and explanation\n",
      "of future predictions for end-users. Further, interpretable machine learning\n",
      "models allow healthcare experts to make reasonable and data-driven decisions to\n",
      "provide personalized decisions that can ultimately lead to higher quality of\n",
      "service in healthcare. Generally, we can classify interpretability approaches\n",
      "in two groups where the first focuses on personalized interpretation (local\n",
      "interpretability) while the second summarizes prediction models on a population\n",
      "level (global interpretability). Alternatively, we can group interpretability\n",
      "methods into model-specific techniques, which are designed to interpret\n",
      "predictions generated by a specific model, such as a neural network, and\n",
      "model-agnostic approaches, which provide easy-to-understand explanations of\n",
      "predictions made by any machine learning model. Here, we give an overview of\n",
      "interpretability approaches and provide examples of practical interpretability\n",
      "of machine learning in different areas of healthcare, including prediction of\n",
      "health-related outcomes, optimizing treatments or improving the efficiency of\n",
      "screening for specific conditions. Further, we outline future directions for\n",
      "interpretable machine learning and highlight the importance of developing\n",
      "algorithmic solutions that can enable machine-learning driven decision making\n",
      "in high-stakes healthcare problems.\n",
      "\n",
      "**Paper Id :1910.09358 \n",
      "Title :A Decision-Theoretic Approach for Model Interpretability in Bayesian\n",
      "  Framework\n",
      "  A salient approach to interpretable machine learning is to restrict modeling\n",
      "to simple models. In the Bayesian framework, this can be pursued by restricting\n",
      "the model structure and prior to favor interpretable models. Fundamentally,\n",
      "however, interpretability is about users' preferences, not the data generation\n",
      "mechanism; it is more natural to formulate interpretability as a utility\n",
      "function. In this work, we propose an interpretability utility, which\n",
      "explicates the trade-off between explanation fidelity and interpretability in\n",
      "the Bayesian framework. The method consists of two steps. First, a reference\n",
      "model, possibly a black-box Bayesian predictive model which does not compromise\n",
      "accuracy, is fitted to the training data. Second, a proxy model from an\n",
      "interpretable model family that best mimics the predictive behaviour of the\n",
      "reference model is found by optimizing the interpretability utility function.\n",
      "The approach is model agnostic -- neither the interpretable model nor the\n",
      "reference model are restricted to a certain class of models -- and the\n",
      "optimization problem can be solved using standard tools. Through experiments on\n",
      "real-word data sets, using decision trees as interpretable models and Bayesian\n",
      "additive regression models as reference models, we show that for the same level\n",
      "of interpretability, our approach generates more accurate models than the\n",
      "alternative of restricting the prior. We also propose a systematic way to\n",
      "measure stability of interpretabile models constructed by different\n",
      "interpretability approaches and show that our proposed approach generates more\n",
      "stable models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08801 \n",
      "Title :Guiding attention in Sequence-to-sequence models for Dialogue Act\n",
      "  prediction\n",
      "  The task of predicting dialog acts (DA) based on conversational dialog is a\n",
      "key component in the development of conversational agents. Accurately\n",
      "predicting DAs requires a precise modeling of both the conversation and the\n",
      "global tag dependencies. We leverage seq2seq approaches widely adopted in\n",
      "Neural Machine Translation (NMT) to improve the modelling of tag sequentiality.\n",
      "Seq2seq models are known to learn complex global dependencies while currently\n",
      "proposed approaches using linear conditional random fields (CRF) only model\n",
      "local tag dependencies. In this work, we introduce a seq2seq model tailored for\n",
      "DA classification using: a hierarchical encoder, a novel guided attention\n",
      "mechanism and beam search applied to both training and inference. Compared to\n",
      "the state of the art our model does not require handcrafted features and is\n",
      "trained end-to-end. Furthermore, the proposed approach achieves an unmatched\n",
      "accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on\n",
      "MRDA.\n",
      "\n",
      "**Paper Id :2004.11714 \n",
      "Title :Residual Energy-Based Models for Text Generation\n",
      "  Text generation is ubiquitous in many NLP tasks, from summarization, to\n",
      "dialogue and machine translation. The dominant parametric approach is based on\n",
      "locally normalized models which predict one word at a time. While these work\n",
      "remarkably well, they are plagued by exposure bias due to the greedy nature of\n",
      "the generation process. In this work, we investigate un-normalized energy-based\n",
      "models (EBMs) which operate not at the token but at the sequence level. In\n",
      "order to make training tractable, we first work in the residual of a pretrained\n",
      "locally normalized language model and second we train using noise contrastive\n",
      "estimation. Furthermore, since the EBM works at the sequence level, we can\n",
      "leverage pretrained bi-directional contextual representations, such as BERT and\n",
      "RoBERTa. Our experiments on two large language modeling datasets show that\n",
      "residual EBMs yield lower perplexity compared to locally normalized baselines.\n",
      "Moreover, generation via importance sampling is very efficient and of higher\n",
      "quality than the baseline models according to human evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.08953 \n",
      "Title :Predicting Many Properties of a Quantum System from Very Few\n",
      "  Measurements\n",
      "  Predicting properties of complex, large-scale quantum systems is essential\n",
      "for developing quantum technologies. We present an efficient method for\n",
      "constructing an approximate classical description of a quantum state using very\n",
      "few measurements of the state. This description, called a classical shadow, can\n",
      "be used to predict many different properties: order $\\log M$ measurements\n",
      "suffice to accurately predict $M$ different functions of the state with high\n",
      "success probability. The number of measurements is independent of the system\n",
      "size, and saturates information-theoretic lower bounds. Moreover, target\n",
      "properties to predict can be selected after the measurements are completed. We\n",
      "support our theoretical findings with extensive numerical experiments. We apply\n",
      "classical shadows to predict quantum fidelities, entanglement entropies,\n",
      "two-point correlation functions, expectation values of local observables, and\n",
      "the energy variance of many-body local Hamiltonians. The numerical results\n",
      "highlight the advantages of classical shadows relative to previously known\n",
      "methods.\n",
      "\n",
      "**Paper Id :1912.07286 \n",
      "Title :Variational Quantum Circuits for Quantum State Tomography\n",
      "  Quantum state tomography is a key process in most quantum experiments. In\n",
      "this work, we employ quantum machine learning for state tomography. Given an\n",
      "unknown quantum state, it can be learned by maximizing the fidelity between the\n",
      "output of a variational quantum circuit and this state. The number of\n",
      "parameters of the variational quantum circuit grows linearly with the number of\n",
      "qubits and the circuit depth, so that only polynomial measurements are\n",
      "required, even for highly-entangled states. After that, a subsequent classical\n",
      "circuit simulator is used to transform the information of the target quantum\n",
      "state from the variational quantum circuit into a familiar format. We\n",
      "demonstrate our method by performing numerical simulations for the tomography\n",
      "of the ground state of a one-dimensional quantum spin chain, using a\n",
      "variational quantum circuit simulator. Our method is suitable for near-term\n",
      "quantum computing platforms, and could be used for relatively large-scale\n",
      "quantum state tomography for experimentally relevant quantum states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09253 \n",
      "Title :Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven\n",
      "  Exploration\n",
      "  Developmental machine learning studies how artificial agents can model the\n",
      "way children learn open-ended repertoires of skills. Such agents need to create\n",
      "and represent goals, select which ones to pursue and learn to achieve them.\n",
      "Recent approaches have considered goal spaces that were either fixed and\n",
      "hand-defined or learned using generative models of states. This limited agents\n",
      "to sample goals within the distribution of known effects. We argue that the\n",
      "ability to imagine out-of-distribution goals is key to enable creative\n",
      "discoveries and open-ended learning. Children do so by leveraging the\n",
      "compositionality of language as a tool to imagine descriptions of outcomes they\n",
      "never experienced before, targeting them as goals during play. We introduce\n",
      "IMAGINE, an intrinsically motivated deep reinforcement learning architecture\n",
      "that models this ability. Such imaginative agents, like children, benefit from\n",
      "the guidance of a social peer who provides language descriptions. To take\n",
      "advantage of goal imagination, agents must be able to leverage these\n",
      "descriptions to interpret their imagined out-of-distribution goals. This\n",
      "generalization is made possible by modularity: a decomposition between learned\n",
      "goal-achievement reward function and policy relying on deep sets, gated\n",
      "attention and object-centered representations. We introduce the Playground\n",
      "environment and study how this form of goal imagination improves generalization\n",
      "and exploration over agents lacking this capacity. In addition, we identify the\n",
      "properties of goal imagination that enable these results and study the impacts\n",
      "of modularity and social interactions.\n",
      "\n",
      "**Paper Id :1909.12892 \n",
      "Title :Automated curricula through setter-solver interactions\n",
      "  Reinforcement learning algorithms use correlations between policies and\n",
      "rewards to improve agent performance. But in dynamic or sparsely rewarding\n",
      "environments these correlations are often too small, or rewarding events are\n",
      "too infrequent to make learning feasible. Human education instead relies on\n",
      "curricula--the breakdown of tasks into simpler, static challenges with dense\n",
      "rewards--to build up to complex behaviors. While curricula are also useful for\n",
      "artificial agents, hand-crafting them is time consuming. This has lead\n",
      "researchers to explore automatic curriculum generation. Here we explore\n",
      "automatic curriculum generation in rich, dynamic environments. Using a\n",
      "setter-solver paradigm we show the importance of considering goal validity,\n",
      "goal feasibility, and goal coverage to construct useful curricula. We\n",
      "demonstrate the success of our approach in rich but sparsely rewarding 2D and\n",
      "3D environments, where an agent is tasked to achieve a single goal selected\n",
      "from a set of possible goals that varies between episodes, and identify\n",
      "challenges for future work. Finally, we demonstrate the value of a novel\n",
      "technique that guides agents towards a desired goal distribution. Altogether,\n",
      "these results represent a substantial step towards applying automatic task\n",
      "curricula to learn complex, otherwise unlearnable goals, and to our knowledge\n",
      "are the first to demonstrate automated curriculum generation for\n",
      "goal-conditioned agents in environments where the possible goals vary between\n",
      "episodes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09259 \n",
      "Title :Binary Probability Model for Learning Based Image Compression\n",
      "  In this paper, we propose to enhance learned image compression systems with a\n",
      "richer probability model for the latent variables. Previous works model the\n",
      "latents with a Gaussian or a Laplace distribution. Inspired by binary\n",
      "arithmetic coding , we propose to signal the latents with three binary values\n",
      "and one integer, with different probability models. A relaxation method is\n",
      "designed to perform gradient-based training. The richer probability model\n",
      "results in a better entropy coding leading to lower rate. Experiments under the\n",
      "Challenge on Learned Image Compression (CLIC) test conditions demonstrate that\n",
      "this method achieves 18% rate saving compared to Gaussian or Laplace models.\n",
      "\n",
      "**Paper Id :2001.08896 \n",
      "Title :Compressing Language Models using Doped Kronecker Products\n",
      "  Kronecker Products (KP) have been used to compress IoT RNN Applications by\n",
      "15-38x compression factors, achieving better results than traditional\n",
      "compression methods. However when KP is applied to large Natural Language\n",
      "Processing tasks, it leads to significant accuracy loss (approx 26%). This\n",
      "paper proposes a way to recover accuracy otherwise lost when applying KP to\n",
      "large NLP tasks, by allowing additional degrees of freedom in the KP matrix.\n",
      "More formally, we propose doping, a process of adding an extremely sparse\n",
      "overlay matrix on top of the pre-defined KP structure. We call this compression\n",
      "method doped kronecker product compression. To train these models, we present a\n",
      "new solution to the phenomenon of co-matrix adaption (CMA), which uses a new\n",
      "regularization scheme called co matrix dropout regularization (CMR). We present\n",
      "experimental results that demonstrate compression of a large language model\n",
      "with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At\n",
      "25x compression, an equivalent pruned network leads to 7.9% loss in perplexity\n",
      "score, while HMD and LMF lead to 15% and 27% loss in perplexity score\n",
      "respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09309 \n",
      "Title :Efficiently Sampling Functions from Gaussian Process Posteriors\n",
      "  Gaussian processes are the gold standard for many real-world modeling\n",
      "problems, especially in cases where a model's success hinges upon its ability\n",
      "to faithfully represent predictive uncertainty. These problems typically exist\n",
      "as parts of larger frameworks, wherein quantities of interest are ultimately\n",
      "defined by integrating over posterior distributions. These quantities are\n",
      "frequently intractable, motivating the use of Monte Carlo methods. Despite\n",
      "substantial progress in scaling up Gaussian processes to large training sets,\n",
      "methods for accurately generating draws from their posterior distributions\n",
      "still scale cubically in the number of test locations. We identify a\n",
      "decomposition of Gaussian processes that naturally lends itself to scalable\n",
      "sampling by separating out the prior from the data. Building off of this\n",
      "factorization, we propose an easy-to-use and general-purpose approach for fast\n",
      "posterior sampling, which seamlessly pairs with sparse approximations to afford\n",
      "scalability both during training and at test time. In a series of experiments\n",
      "designed to test competing sampling schemes' statistical properties and\n",
      "practical ramifications, we demonstrate how decoupled sample paths accurately\n",
      "represent Gaussian process posteriors at a fraction of the usual cost.\n",
      "\n",
      "**Paper Id :2004.12157 \n",
      "Title :A Bayesian machine scientist to aid in the solution of challenging\n",
      "  scientific problems\n",
      "  Closed-form, interpretable mathematical models have been instrumental for\n",
      "advancing our understanding of the world; with the data revolution, we may now\n",
      "be in a position to uncover new such models for many systems from physics to\n",
      "the social sciences. However, to deal with increasing amounts of data, we need\n",
      "\"machine scientists\" that are able to extract these models automatically from\n",
      "data. Here, we introduce a Bayesian machine scientist, which establishes the\n",
      "plausibility of models using explicit approximations to the exact marginal\n",
      "posterior over models and establishes its prior expectations about models by\n",
      "learning from a large empirical corpus of mathematical expressions. It explores\n",
      "the space of models using Markov chain Monte Carlo. We show that this approach\n",
      "uncovers accurate models for synthetic and real data and provides out-of-sample\n",
      "predictions that are more accurate than those of existing approaches and of\n",
      "other nonparametric methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09436 \n",
      "Title :Likelihood-free inference of experimental Neutrino Oscillations using\n",
      "  Neural Spline Flows\n",
      "  In machine learning, likelihood-free inference refers to the task of\n",
      "performing an analysis driven by data instead of an analytical expression. We\n",
      "discuss the application of Neural Spline Flows, a neural density estimation\n",
      "algorithm, to the likelihood-free inference problem of the measurement of\n",
      "neutrino oscillation parameters in Long Baseline neutrino experiments. A method\n",
      "adapted to physics parameter inference is developed and applied to the case of\n",
      "the disappearance muon neutrino analysis at the T2K experiment.\n",
      "\n",
      "**Paper Id :1909.04305 \n",
      "Title :Inverse Ising inference from high-temperature re-weighting of\n",
      "  observations\n",
      "  Maximum Likelihood Estimation (MLE) is the bread and butter of system\n",
      "inference for stochastic systems. In some generality, MLE will converge to the\n",
      "correct model in the infinite data limit. In the context of physical approaches\n",
      "to system inference, such as Boltzmann machines, MLE requires the arduous\n",
      "computation of partition functions summing over all configurations, both\n",
      "observed and unobserved. We present here a conceptually and computationally\n",
      "transparent data-driven approach to system inference that is based on the\n",
      "simple question: How should the Boltzmann weights of observed configurations be\n",
      "modified to make the probability distribution of observed configurations close\n",
      "to a flat distribution? This algorithm gives accurate inference by using only\n",
      "observed configurations for systems with a large number of degrees of freedom\n",
      "where other approaches are intractable.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09615 \n",
      "Title :Preference Modeling with Context-Dependent Salient Features\n",
      "  We consider the problem of estimating a ranking on a set of items from noisy\n",
      "pairwise comparisons given item features. We address the fact that pairwise\n",
      "comparison data often reflects irrational choice, e.g. intransitivity. Our key\n",
      "observation is that two items compared in isolation from other items may be\n",
      "compared based on only a salient subset of features. Formalizing this\n",
      "framework, we propose the salient feature preference model and prove a finite\n",
      "sample complexity result for learning the parameters of our model and the\n",
      "underlying ranking with maximum likelihood estimation. We also provide\n",
      "empirical results that support our theoretical bounds and illustrate how our\n",
      "model explains systematic intransitivity. Finally we demonstrate strong\n",
      "performance of maximum likelihood estimation of our model on both synthetic\n",
      "data and two real data sets: the UT Zappos50K data set and comparison data\n",
      "about the compactness of legislative districts in the US.\n",
      "\n",
      "**Paper Id :1902.02495 \n",
      "Title :Cost-Effective Incentive Allocation via Structured Counterfactual\n",
      "  Inference\n",
      "  We address a practical problem ubiquitous in modern marketing campaigns, in\n",
      "which a central agent tries to learn a policy for allocating strategic\n",
      "financial incentives to customers and observes only bandit feedback. In\n",
      "contrast to traditional policy optimization frameworks, we take into account\n",
      "the additional reward structure and budget constraints common in this setting,\n",
      "and develop a new two-step method for solving this constrained counterfactual\n",
      "policy optimization problem. Our method first casts the reward estimation\n",
      "problem as a domain adaptation problem with supplementary structure, and then\n",
      "subsequently uses the estimators for optimizing the policy with constraints. We\n",
      "also establish theoretical error bounds for our estimation procedure and we\n",
      "empirically show that the approach leads to significant improvement on both\n",
      "synthetic and real datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09821 \n",
      "Title :A Multi-view CNN-based Acoustic Classification System for Automatic\n",
      "  Animal Species Identification\n",
      "  Automatic identification of animal species by their vocalization is an\n",
      "important and challenging task. Although many kinds of audio monitoring system\n",
      "have been proposed in the literature, they suffer from several disadvantages\n",
      "such as non-trivial feature selection, accuracy degradation because of\n",
      "environmental noise or intensive local computation. In this paper, we propose a\n",
      "deep learning based acoustic classification framework for Wireless Acoustic\n",
      "Sensor Network (WASN). The proposed framework is based on cloud architecture\n",
      "which relaxes the computational burden on the wireless sensor node. To improve\n",
      "the recognition accuracy, we design a multi-view Convolution Neural Network\n",
      "(CNN) to extract the short-, middle-, and long-term dependencies in parallel.\n",
      "The evaluation on two real datasets shows that the proposed architecture can\n",
      "achieve high accuracy and outperforms traditional classification systems\n",
      "significantly when the environmental noise dominate the audio signal (low SNR).\n",
      "Moreover, we implement and deploy the proposed system on a testbed and analyse\n",
      "the system performance in real-world environments. Both simulation and\n",
      "real-world evaluation demonstrate the accuracy and robustness of the proposed\n",
      "acoustic classification system in distinguishing species of animals.\n",
      "\n",
      "**Paper Id :2006.08122 \n",
      "Title :Classification and Recognition of Encrypted EEG Data Neural Network\n",
      "  With the rapid development of Machine Learning technology applied in\n",
      "electroencephalography (EEG) signals, Brain-Computer Interface (BCI) has\n",
      "emerged as a novel and convenient human-computer interaction for smart home,\n",
      "intelligent medical and other Internet of Things (IoT) scenarios. However,\n",
      "security issues such as sensitive information disclosure and unauthorized\n",
      "operations have not received sufficient concerns. There are still some defects\n",
      "with the existing solutions to encrypted EEG data such as low accuracy, high\n",
      "time complexity or slow processing speed. For this reason, a classification and\n",
      "recognition method of encrypted EEG data based on neural network is proposed,\n",
      "which adopts Paillier encryption algorithm to encrypt EEG data and meanwhile\n",
      "resolves the problem of floating point operations. In addition, it improves\n",
      "traditional feed-forward neural network (FNN) by using the approximate function\n",
      "instead of activation function and realizes multi-classification of encrypted\n",
      "EEG data. Extensive experiments are conducted to explore the effect of several\n",
      "metrics (such as the hidden neuron size and the learning rate updated by\n",
      "improved simulated annealing algorithm) on the recognition results. Followed by\n",
      "security and time cost analysis, the proposed model and approach are validated\n",
      "and evaluated on public EEG datasets provided by PhysioNet, BCI Competition IV\n",
      "and EPILEPSIAE. The experimental results show that our proposal has the\n",
      "satisfactory accuracy, efficiency and feasibility compared with other\n",
      "solutions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09841 \n",
      "Title :SetRank: A Setwise Bayesian Approach for Collaborative Ranking from\n",
      "  Implicit Feedback\n",
      "  The recent development of online recommender systems has a focus on\n",
      "collaborative ranking from implicit feedback, such as user clicks and\n",
      "purchases. Different from explicit ratings, which reflect graded user\n",
      "preferences, the implicit feedback only generates positive and unobserved\n",
      "labels. While considerable efforts have been made in this direction, the\n",
      "well-known pairwise and listwise approaches have still been limited by various\n",
      "challenges. Specifically, for the pairwise approaches, the assumption of\n",
      "independent pairwise preference is not always held in practice. Also, the\n",
      "listwise approaches cannot efficiently accommodate \"ties\" due to the\n",
      "precondition of the entire list permutation. To this end, in this paper, we\n",
      "propose a novel setwise Bayesian approach for collaborative ranking, namely\n",
      "SetRank, to inherently accommodate the characteristics of implicit feedback in\n",
      "recommender system. Specifically, SetRank aims at maximizing the posterior\n",
      "probability of novel setwise preference comparisons and can be implemented with\n",
      "matrix factorization and neural networks. Meanwhile, we also present the\n",
      "theoretical analysis of SetRank to show that the bound of excess risk can be\n",
      "proportional to $\\sqrt{M/N}$, where $M$ and $N$ are the numbers of items and\n",
      "users, respectively. Finally, extensive experiments on four real-world datasets\n",
      "clearly validate the superiority of SetRank compared with various\n",
      "state-of-the-art baselines.\n",
      "\n",
      "**Paper Id :2006.06922 \n",
      "Title :Incorporating User Micro-behaviors and Item Knowledge into Multi-task\n",
      "  Learning for Session-based Recommendation\n",
      "  Session-based recommendation (SR) has become an important and popular\n",
      "component of various e-commerce platforms, which aims to predict the next\n",
      "interacted item based on a given session. Most of existing SR models only focus\n",
      "on exploiting the consecutive items in a session interacted by a certain user,\n",
      "to capture the transition pattern among the items. Although some of them have\n",
      "been proven effective, the following two insights are often neglected. First, a\n",
      "user's micro-behaviors, such as the manner in which the user locates an item,\n",
      "the activities that the user commits on an item (e.g., reading comments, adding\n",
      "to cart), offer fine-grained and deep understanding of the user's preference.\n",
      "Second, the item attributes, also known as item knowledge, provide side\n",
      "information to model the transition pattern among interacted items and\n",
      "alleviate the data sparsity problem. These insights motivate us to propose a\n",
      "novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors\n",
      "and item Knowledge into Multi-task learning for Session-based Recommendation.\n",
      "Specifically, a given session is modeled on micro-behavior level in MKM-SR,\n",
      "i.e., with a sequence of item-operation pairs rather than a sequence of items,\n",
      "to capture the transition pattern in the session sufficiently. Furthermore, we\n",
      "propose a multi-task learning paradigm to involve learning knowledge embeddings\n",
      "which plays a role as an auxiliary task to promote the major task of SR. It\n",
      "enables our model to obtain better session representations, resulting in more\n",
      "precise SR recommendation results. The extensive evaluations on two benchmark\n",
      "datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models,\n",
      "justifying the strategy of incorporating knowledge learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.09970 \n",
      "Title :Computer-inspired Quantum Experiments\n",
      "  The design of new devices and experiments in science and engineering has\n",
      "historically relied on the intuitions of human experts. This credo, however,\n",
      "has changed. In many disciplines, computer-inspired design processes, also\n",
      "known as inverse-design, have augmented the capability of scientists. Here we\n",
      "visit different fields of physics in which computer-inspired designs are\n",
      "applied. We will meet vastly diverse computational approaches based on\n",
      "topological optimization, evolutionary strategies, deep learning, reinforcement\n",
      "learning or automated reasoning. Then we draw our attention specifically on\n",
      "quantum physics. In the quest for designing new quantum experiments, we face\n",
      "two challenges: First, quantum phenomena are unintuitive. Second, the number of\n",
      "possible configurations of quantum experiments explodes combinatorially. To\n",
      "overcome these challenges, physicists began to use algorithms for\n",
      "computer-designed quantum experiments. We focus on the most mature and\n",
      "\\textit{practical} approaches that scientists used to find new complex quantum\n",
      "experiments, which experimentalists subsequently have realized in the\n",
      "laboratories. The underlying idea is a highly-efficient topological search,\n",
      "which allows for scientific interpretability. In that way, some of the\n",
      "computer-designs have led to the discovery of new scientific concepts and ideas\n",
      "-- demonstrating how computer algorithm can genuinely contribute to science by\n",
      "providing unexpected inspirations. We discuss several extensions and\n",
      "alternatives based on optimization and machine learning techniques, with the\n",
      "potential of accelerating the discovery of practical computer-inspired\n",
      "experiments or concepts in the future. Finally, we discuss what we can learn\n",
      "from the different approaches in the fields of physics, and raise several\n",
      "fascinating possibilities for future research.\n",
      "\n",
      "**Paper Id :1906.06843 \n",
      "Title :Predicting Research Trends with Semantic and Neural Networks with an\n",
      "  application in Quantum Physics\n",
      "  The vast and growing number of publications in all disciplines of science\n",
      "cannot be comprehended by a single human researcher. As a consequence,\n",
      "researchers have to specialize in narrow sub-disciplines, which makes it\n",
      "challenging to uncover scientific connections beyond the own field of research.\n",
      "Thus access to structured knowledge from a large corpus of publications could\n",
      "help pushing the frontiers of science. Here we demonstrate a method to build a\n",
      "semantic network from published scientific literature, which we call SemNet. We\n",
      "use SemNet to predict future trends in research and to inspire new,\n",
      "personalized and surprising seeds of ideas in science. We apply it in the\n",
      "discipline of quantum physics, which has seen an unprecedented growth of\n",
      "activity in recent years. In SemNet, scientific knowledge is represented as an\n",
      "evolving network using the content of 750,000 scientific papers published since\n",
      "1919. The nodes of the network correspond to physical concepts, and links\n",
      "between two nodes are drawn when two physical concepts are concurrently studied\n",
      "in research articles. We identify influential and prize-winning research topics\n",
      "from the past inside SemNet thus confirm that it stores useful semantic\n",
      "knowledge. We train a deep neural network using states of SemNet of the past,\n",
      "to predict future developments in quantum physics research, and confirm high\n",
      "quality predictions using historic data. With the neural network and\n",
      "theoretical network tools we are able to suggest new, personalized,\n",
      "out-of-the-box ideas, by identifying pairs of concepts which have unique and\n",
      "extremal semantic network properties. Finally, we consider possible future\n",
      "developments and implications of our findings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10199 \n",
      "Title :Better Classifier Calibration for Small Data Sets\n",
      "  Classifier calibration does not always go hand in hand with the classifier's\n",
      "ability to separate the classes. There are applications where good classifier\n",
      "calibration, i.e. the ability to produce accurate probability estimates, is\n",
      "more important than class separation. When the amount of data for training is\n",
      "limited, the traditional approach to improve calibration starts to crumble. In\n",
      "this article we show how generating more data for calibration is able to\n",
      "improve calibration algorithm performance in many cases where a classifier is\n",
      "not naturally producing well-calibrated outputs and the traditional approach\n",
      "fails. The proposed approach adds computational cost but considering that the\n",
      "main use case is with small data sets this extra computational cost stays\n",
      "insignificant and is comparable to other methods in prediction time. From the\n",
      "tested classifiers the largest improvement was detected with the random forest\n",
      "and naive Bayes classifiers. Therefore, the proposed approach can be\n",
      "recommended at least for those classifiers when the amount of data available\n",
      "for training is limited and good calibration is essential.\n",
      "\n",
      "**Paper Id :2008.04222 \n",
      "Title :Accuracy of neural networks for the simulation of chaotic dynamics:\n",
      "  precision of training data vs precision of the algorithm\n",
      "  We explore the influence of precision of the data and the algorithm for the\n",
      "simulation of chaotic dynamics by neural networks techniques. For this purpose,\n",
      "we simulate the Lorenz system with different precisions using three different\n",
      "neural network techniques adapted to time series, namely reservoir computing\n",
      "(using ESN), LSTM and TCN, for both short and long time predictions, and assess\n",
      "their efficiency and accuracy. Our results show that the ESN network is better\n",
      "at predicting accurately the dynamics of the system, and that in all cases the\n",
      "precision of the algorithm is more important than the precision of the training\n",
      "data for the accuracy of the predictions. This result gives support to the idea\n",
      "that neural networks can perform time-series predictions in many practical\n",
      "applications for which data are necessarily of limited precision, in line with\n",
      "recent results. It also suggests that for a given set of data the reliability\n",
      "of the predictions can be significantly improved by using a network with higher\n",
      "precision than the one of the data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10221 \n",
      "Title :The Archimedean trap: Why traditional reinforcement learning will\n",
      "  probably not yield AGI\n",
      "  After generalizing the Archimedean property of real numbers in such a way as\n",
      "to make it adaptable to non-numeric structures, we demonstrate that the real\n",
      "numbers cannot be used to accurately measure non-Archimedean structures. We\n",
      "argue that, since an agent with Artificial General Intelligence (AGI) should\n",
      "have no problem engaging in tasks that inherently involve non-Archimedean\n",
      "rewards, and since traditional reinforcement learning rewards are real numbers,\n",
      "therefore traditional reinforcement learning probably will not lead to AGI. We\n",
      "indicate two possible ways traditional reinforcement learning could be altered\n",
      "to remove this roadblock.\n",
      "\n",
      "**Paper Id :2002.01365 \n",
      "Title :Compositional Languages Emerge in a Neural Iterated Learning Model\n",
      "  The principle of compositionality, which enables natural language to\n",
      "represent complex concepts via a structured combination of simpler ones, allows\n",
      "us to convey an open-ended set of messages using a limited vocabulary. If\n",
      "compositionality is indeed a natural property of language, we may expect it to\n",
      "appear in communication protocols that are created by neural agents in language\n",
      "games. In this paper, we propose an effective neural iterated learning (NIL)\n",
      "algorithm that, when applied to interacting neural agents, facilitates the\n",
      "emergence of a more structured type of language. Indeed, these languages\n",
      "provide learning speed advantages to neural agents during training, which can\n",
      "be incrementally amplified via NIL. We provide a probabilistic model of NIL and\n",
      "an explanation of why the advantage of compositional language exist. Our\n",
      "experiments confirm our analysis, and also demonstrate that the emerged\n",
      "languages largely improve the generalizing power of the neural agent\n",
      "communication.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10306 \n",
      "Title :Adaptive Propagation Graph Convolutional Network\n",
      "  Graph convolutional networks (GCNs) are a family of neural network models\n",
      "that perform inference on graph data by interleaving vertex-wise operations and\n",
      "message-passing exchanges across nodes. Concerning the latter, two key\n",
      "questions arise: (i) how to design a differentiable exchange protocol (e.g., a\n",
      "1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize\n",
      "the trade-off in complexity with respect to the local updates. In this paper,\n",
      "we show that state-of-the-art results can be achieved by adapting the number of\n",
      "communication steps independently at every node. In particular, we endow each\n",
      "node with a halting unit (inspired by Graves' adaptive computation time) that\n",
      "after every exchange decides whether to continue communicating or not. We show\n",
      "that the proposed adaptive propagation GCN (AP-GCN) achieves superior or\n",
      "similar results to the best proposed models so far on a number of benchmarks,\n",
      "while requiring a small overhead in terms of additional parameters. We also\n",
      "investigate a regularization term to enforce an explicit trade-off between\n",
      "communication and accuracy. The code for the AP-GCN experiments is released as\n",
      "an open-source library.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10399 \n",
      "Title :Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference\n",
      "  Setting\n",
      "  Parameter estimation, statistical tests and confidence sets are the\n",
      "cornerstones of classical statistics that allow scientists to make inferences\n",
      "about the underlying process that generated the observed data. A key question\n",
      "is whether one can still construct hypothesis tests and confidence sets with\n",
      "proper coverage and high power in a so-called likelihood-free inference (LFI)\n",
      "setting; that is, a setting where the likelihood is not explicitly known but\n",
      "one can forward-simulate observable data according to a stochastic model. In\n",
      "this paper, we present $\\texttt{ACORE}$ (Approximate Computation via Odds Ratio\n",
      "Estimation), a frequentist approach to LFI that first formulates the classical\n",
      "likelihood ratio test (LRT) as a parametrized classification problem, and then\n",
      "uses the equivalence of tests and confidence sets to build confidence regions\n",
      "for parameters of interest. We also present a goodness-of-fit procedure for\n",
      "checking whether the constructed tests and confidence regions are valid.\n",
      "$\\texttt{ACORE}$ is based on the key observation that the LRT statistic, the\n",
      "rejection probability of the test, and the coverage of the confidence set are\n",
      "conditional distribution functions which often vary smoothly as a function of\n",
      "the parameters of interest. Hence, instead of relying solely on samples\n",
      "simulated at fixed parameter settings (as is the convention in standard Monte\n",
      "Carlo solutions), one can leverage machine learning tools and data simulated in\n",
      "the neighborhood of a parameter to improve estimates of quantities of interest.\n",
      "We demonstrate the efficacy of $\\texttt{ACORE}$ with both theoretical and\n",
      "empirical results. Our implementation is available on Github.\n",
      "\n",
      "**Paper Id :2002.07217 \n",
      "Title :Decision-Making with Auto-Encoding Variational Bayes\n",
      "  To make decisions based on a model fit with auto-encoding variational Bayes\n",
      "(AEVB), practitioners often let the variational distribution serve as a\n",
      "surrogate for the posterior distribution. This approach yields biased estimates\n",
      "of the expected risk, and therefore leads to poor decisions for two reasons.\n",
      "First, the model fit with AEVB may not equal the underlying data distribution.\n",
      "Second, the variational distribution may not equal the posterior distribution\n",
      "under the fitted model. We explore how fitting the variational distribution\n",
      "based on several objective functions other than the ELBO, while continuing to\n",
      "fit the generative model based on the ELBO, affects the quality of downstream\n",
      "decisions. For the probabilistic principal component analysis model, we\n",
      "investigate how importance sampling error, as well as the bias of the model\n",
      "parameter estimates, varies across several approximate posteriors when used as\n",
      "proposal distributions. Our theoretical results suggest that a posterior\n",
      "approximation distinct from the variational distribution should be used for\n",
      "making decisions. Motivated by these theoretical results, we propose learning\n",
      "several approximate proposals for the best model and combining them using\n",
      "multiple importance sampling for decision-making. In addition to toy examples,\n",
      "we present a full-fledged case study of single-cell RNA sequencing. In this\n",
      "challenging instance of multiple hypothesis testing, our proposed approach\n",
      "surpasses the current state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10433 \n",
      "Title :From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the\n",
      "  World of AI\n",
      "  This paper reviews the field of Game AI, which not only deals with creating\n",
      "agents that can play a certain game, but also with areas as diverse as creating\n",
      "game content automatically, game analytics, or player modelling. While Game AI\n",
      "was for a long time not very well recognized by the larger scientific\n",
      "community, it has established itself as a research area for developing and\n",
      "testing the most advanced forms of AI algorithms and articles covering advances\n",
      "in mastering video games such as StarCraft 2 and Quake III appear in the most\n",
      "prestigious journals. Because of the growth of the field, a single review\n",
      "cannot cover it completely. Therefore, we put a focus on important recent\n",
      "developments, including that advances in Game AI are starting to be extended to\n",
      "areas outside of games, such as robotics or the synthesis of chemicals. In this\n",
      "article, we review the algorithms and methods that have paved the way for these\n",
      "breakthroughs, report on the other important areas of Game AI research, and\n",
      "also point out exciting directions for the future of Game AI.\n",
      "\n",
      "**Paper Id :2005.06540 \n",
      "Title :Deep Learning for Political Science\n",
      "  Political science, and social science in general, have traditionally been\n",
      "using computational methods to study areas such as voting behavior, policy\n",
      "making, international conflict, and international development. More recently,\n",
      "increasingly available quantities of data are being combined with improved\n",
      "algorithms and affordable computational resources to predict, learn, and\n",
      "discover new insights from data that is large in volume and variety. New\n",
      "developments in the areas of machine learning, deep learning, natural language\n",
      "processing (NLP), and, more generally, artificial intelligence (AI) are opening\n",
      "up new opportunities for testing theories and evaluating the impact of\n",
      "interventions and programs in a more dynamic and effective way. Applications\n",
      "using large volumes of structured and unstructured data are becoming common in\n",
      "government and industry, and increasingly also in social science research. This\n",
      "chapter offers an introduction to such methods drawing examples from political\n",
      "science. Focusing on the areas where the strengths of the methods coincide with\n",
      "challenges in these fields, the chapter first presents an introduction to AI\n",
      "and its core technology - machine learning, with its rapidly developing\n",
      "subfield of deep learning. The discussion of deep neural networks is\n",
      "illustrated with the NLP tasks that are relevant to political science. The\n",
      "latest advances in deep learning methods for NLP are also reviewed, together\n",
      "with their potential for improving information extraction and pattern\n",
      "recognition from political science texts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10953 \n",
      "Title :Predicting Coronal Mass Ejections Using SDO/HMI Vector Magnetic Data\n",
      "  Products and Recurrent Neural Networks\n",
      "  We present two recurrent neural networks (RNNs), one based on gated recurrent\n",
      "units and the other based on long short-term memory, for predicting whether an\n",
      "active region (AR) that produces an M- or X-class flare will also produce a\n",
      "coronal mass ejection (CME). We model data samples in an AR as time series and\n",
      "use the RNNs to capture temporal information of the data samples. Each data\n",
      "sample has 18 physical parameters, or features, derived from photospheric\n",
      "vector magnetic field data taken by the Helioseismic and Magnetic Imager (HMI)\n",
      "on board the Solar Dynamics Observatory (SDO). We survey M- and X-class flares\n",
      "that occurred from 2010 May to 2019 May using the Geostationary Operational\n",
      "Environmental Satellite's X-ray flare catalogs provided by the National Centers\n",
      "for Environmental Information (NCEI), and select those flares with identified\n",
      "ARs in the NCEI catalogs. In addition, we extract the associations of flares\n",
      "and CMEs from the Space Weather Database Of Notifications, Knowledge,\n",
      "Information (DONKI). We use the information gathered above to build the labels\n",
      "(positive versus negative) of the data samples at hand. Experimental results\n",
      "demonstrate the superiority of our RNNs over closely related machine learning\n",
      "methods in predicting the labels of the data samples. We also discuss an\n",
      "extension of our approach to predict a probabilistic estimate of how likely an\n",
      "M- or X-class flare will initiate a CME, with good performance results. To our\n",
      "knowledge this is the first time that RNNs have been used for CME prediction.\n",
      "\n",
      "**Paper Id :2005.03945 \n",
      "Title :Inferring Vector Magnetic Fields from Stokes Profiles of GST/NIRIS Using\n",
      "  a Convolutional Neural Network\n",
      "  We propose a new machine learning approach to Stokes inversion based on a\n",
      "convolutional neural network (CNN) and the Milne-Eddington (ME) method. The\n",
      "Stokes measurements used in this study were taken by the Near InfraRed Imaging\n",
      "Spectropolarimeter (NIRIS) on the 1.6 m Goode Solar Telescope (GST) at the Big\n",
      "Bear Solar Observatory. By learning the latent patterns in the training data\n",
      "prepared by the physics-based ME tool, the proposed CNN method is able to infer\n",
      "vector magnetic fields from the Stokes profiles of GST/NIRIS. Experimental\n",
      "results show that our CNN method produces smoother and cleaner magnetic maps\n",
      "than the widely used ME method. Furthermore, the CNN method is 4~6 times faster\n",
      "than the ME method, and is able to produce vector magnetic fields in near\n",
      "real-time, which is essential to space weather forecasting. Specifically, it\n",
      "takes ~50 seconds for the CNN method to process an image of 720 x 720 pixels\n",
      "comprising Stokes profiles of GST/NIRIS. Finally, the CNN-inferred results are\n",
      "highly correlated to the ME-calculated results and are closer to the ME's\n",
      "results with the Pearson product-moment correlation coefficient (PPMCC) being\n",
      "closer to 1 on average than those from other machine learning algorithms such\n",
      "as multiple support vector regression and multilayer perceptrons (MLP). In\n",
      "particular, the CNN method outperforms the current best machine learning method\n",
      "(MLP) by 2.6% on average in PPMCC according to our experimental study. Thus,\n",
      "the proposed physics-assisted deep learning-based CNN tool can be considered as\n",
      "an alternative, efficient method for Stokes inversion for high resolution\n",
      "polarimetric observations obtained by GST/NIRIS.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10981 \n",
      "Title :AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent\n",
      "  Videos with Deep Learning\n",
      "  In movie productions, the Foley Artist is responsible for creating an overlay\n",
      "soundtrack that helps the movie come alive for the audience. This requires the\n",
      "artist to first identify the sounds that will enhance the experience for the\n",
      "listener thereby reinforcing the Directors's intention for a given scene. In\n",
      "this paper, we present AutoFoley, a fully-automated deep learning tool that can\n",
      "be used to synthesize a representative audio track for videos. AutoFoley can be\n",
      "used in the applications where there is either no corresponding audio file\n",
      "associated with the video or in cases where there is a need to identify\n",
      "critical scenarios and provide a synthesized, reinforced soundtrack. An\n",
      "important performance criterion of the synthesized soundtrack is to be\n",
      "time-synchronized with the input video, which provides for a realistic and\n",
      "believable portrayal of the synthesized sound. Unlike existing sound prediction\n",
      "and generation architectures, our algorithm is capable of precise recognition\n",
      "of actions as well as inter-frame relations in fast moving video clips by\n",
      "incorporating an interpolation technique and Temporal Relationship Networks\n",
      "(TRN). We employ a robust multi-scale Recurrent Neural Network (RNN) associated\n",
      "with a Convolutional Neural Network (CNN) for a better understanding of the\n",
      "intricate input-to-output associations over time. To evaluate AutoFoley, we\n",
      "create and introduce a large scale audio-video dataset containing a variety of\n",
      "sounds frequently used as Foley effects in movies. Our experiments show that\n",
      "the synthesized sounds are realistically portrayed with accurate temporal\n",
      "synchronization of the associated visual inputs. Human qualitative testing of\n",
      "AutoFoley show over 73% of the test subjects considered the generated\n",
      "soundtrack as original, which is a noteworthy improvement in cross-modal\n",
      "research in sound synthesis.\n",
      "\n",
      "**Paper Id :2006.14348 \n",
      "Title :Audeo: Audio Generation for a Silent Performance Video\n",
      "  We present a novel system that gets as an input video frames of a musician\n",
      "playing the piano and generates the music for that video. Generation of music\n",
      "from visual cues is a challenging problem and it is not clear whether it is an\n",
      "attainable goal at all. Our main aim in this work is to explore the\n",
      "plausibility of such a transformation and to identify cues and components able\n",
      "to carry the association of sounds with visual events. To achieve the\n",
      "transformation we built a full pipeline named `\\textit{Audeo}' containing three\n",
      "components. We first translate the video frames of the keyboard and the\n",
      "musician hand movements into raw mechanical musical symbolic representation\n",
      "Piano-Roll (Roll) for each video frame which represents the keys pressed at\n",
      "each time step. We then adapt the Roll to be amenable for audio synthesis by\n",
      "including temporal correlations. This step turns out to be critical for\n",
      "meaningful audio generation. As a last step, we implement Midi synthesizers to\n",
      "generate realistic music. \\textit{Audeo} converts video to audio smoothly and\n",
      "clearly with only a few setup constraints. We evaluate \\textit{Audeo} on `in\n",
      "the wild' piano performance videos and obtain that their generated music is of\n",
      "reasonable audio quality and can be successfully recognized with high precision\n",
      "by popular music identification software.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.10986 \n",
      "Title :A Deep Learning Framework for Simulation and Defect Prediction Applied\n",
      "  in Microelectronics\n",
      "  The prediction of upcoming events in industrial processes has been a\n",
      "long-standing research goal since it enables optimization of manufacturing\n",
      "parameters, planning of equipment maintenance and more importantly prediction\n",
      "and eventually prevention of defects. While existing approaches have\n",
      "accomplished substantial progress, they are mostly limited to processing of one\n",
      "dimensional signals or require parameter tuning to model environmental\n",
      "parameters. In this paper, we propose an alternative approach based on deep\n",
      "neural networks that simulates changes in the 3D structure of a monitored\n",
      "object in a batch based on previous 3D measurements. In particular, we propose\n",
      "an architecture based on 3D Convolutional Neural Networks (3DCNN) in order to\n",
      "model the geometric variations in manufacturing parameters and predict upcoming\n",
      "events related to sub-optimal performance. We validate our framework on a\n",
      "microelectronics use-case using the recently published PCB scans dataset where\n",
      "we simulate changes on the shape and volume of glue deposited on an Liquid\n",
      "Crystal Polymer (LCP) substrate before the attachment of integrated circuits\n",
      "(IC). Experimental evaluation examines the impact of different choices in the\n",
      "cost function during training and shows that the proposed method can be\n",
      "efficiently used for defect prediction.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.11044 \n",
      "Title :Regression with Deep Learning for Sensor Performance Optimization\n",
      "  Neural networks with at least two hidden layers are called deep networks.\n",
      "Recent developments in AI and computer programming in general has led to\n",
      "development of tools such as Tensorflow, Keras, NumPy etc. making it easier to\n",
      "model and draw conclusions from data. In this work we re-approach non-linear\n",
      "regression with deep learning enabled by Keras and Tensorflow. In particular,\n",
      "we use deep learning to parametrize a non-linear multivariate relationship\n",
      "between inputs and outputs of an industrial sensor with an intent to optimize\n",
      "the sensor performance based on selected key metrics.\n",
      "\n",
      "**Paper Id :1904.06517 \n",
      "Title :Improving detection of protein-ligand binding sites with 3D segmentation\n",
      "  In recent years machine learning (ML) took bio- and cheminformatics fields by\n",
      "storm, providing new solutions for a vast repertoire of problems related to\n",
      "protein sequence, structure, and interactions analysis. ML techniques, deep\n",
      "neural networks especially, were proven more effective than classical models\n",
      "for tasks like predicting binding affinity for molecular complex. In this work\n",
      "we investigated the earlier stage of drug discovery process - finding druggable\n",
      "pockets on protein surface, that can be later used to design active molecules.\n",
      "For this purpose we developed a 3D fully convolutional neural network capable\n",
      "of binding site segmentation. Our solution has high prediction accuracy and\n",
      "provides intuitive representations of the results, which makes it easy to\n",
      "incorporate into drug discovery projects. The model's source code, together\n",
      "with scripts for most common use-cases is freely available at\n",
      "http://gitlab.com/cheminfIBB/kalasanty\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.11434 \n",
      "Title :Towards Interpretable Semantic Segmentation via Gradient-weighted Class\n",
      "  Activation Mapping\n",
      "  Convolutional neural networks have become state-of-the-art in a wide range of\n",
      "image recognition tasks. The interpretation of their predictions, however, is\n",
      "an active area of research. Whereas various interpretation methods have been\n",
      "suggested for image classification, the interpretation of image segmentation\n",
      "still remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a\n",
      "gradient-based method for interpreting semantic segmentation. Our method is an\n",
      "extension of the widely-used Grad-CAM method, applied locally to produce\n",
      "heatmaps showing the relevance of individual pixels for semantic segmentation.\n",
      "\n",
      "**Paper Id :1910.08978 \n",
      "Title :Attention Enriched Deep Learning Model for Breast Tumor Segmentation in\n",
      "  Ultrasound Images\n",
      "  Incorporating human domain knowledge for breast tumor diagnosis is\n",
      "challenging, since shape, boundary, curvature, intensity, or other common\n",
      "medical priors vary significantly across patients and cannot be employed. This\n",
      "work proposes a new approach for integrating visual saliency into a deep\n",
      "learning model for breast tumor segmentation in ultrasound images. Visual\n",
      "saliency refers to image maps containing regions that are more likely to\n",
      "attract radiologists visual attention. The proposed approach introduces\n",
      "attention blocks into a U-Net architecture, and learns feature representations\n",
      "that prioritize spatial regions with high saliency levels. The validation\n",
      "results demonstrate increased accuracy for tumor segmentation relative to\n",
      "models without salient attention layers. The approach achieved a Dice\n",
      "similarity coefficient of 90.5 percent on a dataset of 510 images. The salient\n",
      "attention model has potential to enhance accuracy and robustness in processing\n",
      "medical images of other organs, by providing a means to incorporate\n",
      "task-specific knowledge into deep learning architectures.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.11952 \n",
      "Title :Autonomous robotic nanofabrication with reinforcement learning\n",
      "  The ability to handle single molecules as effectively as macroscopic\n",
      "building-blocks would enable the construction of complex supramolecular\n",
      "structures inaccessible to self-assembly. The fundamental challenges\n",
      "obstructing this goal are the uncontrolled variability and poor observability\n",
      "of atomic-scale conformations. Here, we present a strategy to work around both\n",
      "obstacles, and demonstrate autonomous robotic nanofabrication by manipulating\n",
      "single molecules. Our approach employs reinforcement learning (RL), which finds\n",
      "solution strategies even in the face of large uncertainty and sparse feedback.\n",
      "We demonstrate the potential of our RL approach by removing molecules\n",
      "autonomously with a scanning probe microscope from a supramolecular structure\n",
      "-- an exemplary task of subtractive manufacturing at the nanoscale. Our RL\n",
      "agent reaches an excellent performance, enabling us to automate a task which\n",
      "previously had to be performed by a human. We anticipate that our work opens\n",
      "the way towards autonomous agents for the robotic construction of functional\n",
      "supramolecular structures with speed, precision and perseverance beyond our\n",
      "current capabilities.\n",
      "\n",
      "**Paper Id :1907.06589 \n",
      "Title :Experimental quantum homodyne tomography via machine learning\n",
      "  Complete characterization of states and processes that occur within quantum\n",
      "devices is crucial for understanding and testing their potential to outperform\n",
      "classical technologies for communications and computing. However, solving this\n",
      "task with current state-of-the-art techniques becomes unwieldy for large and\n",
      "complex quantum systems. Here we realize and experimentally demonstrate a\n",
      "method for complete characterization of a quantum harmonic oscillator based on\n",
      "an artificial neural network known as the restricted Boltzmann machine. We\n",
      "apply the method to optical homodyne tomography and show it to allow full\n",
      "estimation of quantum states based on a smaller amount of experimental data\n",
      "compared to state-of-the-art methods. We link this advantage to reduced\n",
      "overfitting. Although our experiment is in the optical domain, our method\n",
      "provides a way of exploring quantum resources in a broad class of large-scale\n",
      "physical systems, such as superconducting circuits, atomic and molecular\n",
      "ensembles, and optomechanical systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.12411 \n",
      "Title :Cognitively-Inspired Model for Incremental Learning Using a Few Examples\n",
      "  Incremental learning attempts to develop a classifier which learns\n",
      "continuously from a stream of data segregated into different classes. Deep\n",
      "learning approaches suffer from catastrophic forgetting when learning classes\n",
      "incrementally, while most incremental learning approaches require a large\n",
      "amount of training data per class. We examine the problem of incremental\n",
      "learning using only a few training examples, referred to as Few-Shot\n",
      "Incremental Learning (FSIL). To solve this problem, we propose a novel approach\n",
      "inspired by the concept learning model of the hippocampus and the neocortex\n",
      "that represents each image class as centroids and does not suffer from\n",
      "catastrophic forgetting. We evaluate our approach on three class-incremental\n",
      "learning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental\n",
      "and few-shot incremental learning and show that our approach achieves\n",
      "state-of-the-art results in terms of classification accuracy over all learned\n",
      "classes.\n",
      "\n",
      "**Paper Id :1909.02729 \n",
      "Title :A Baseline for Few-Shot Image Classification\n",
      "  Fine-tuning a deep network trained with the standard cross-entropy loss is a\n",
      "strong baseline for few-shot learning. When fine-tuned transductively, this\n",
      "outperforms the current state-of-the-art on standard datasets such as\n",
      "Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same\n",
      "hyper-parameters. The simplicity of this approach enables us to demonstrate the\n",
      "first few-shot learning results on the ImageNet-21k dataset. We find that using\n",
      "a large number of meta-training classes results in high few-shot accuracies\n",
      "even for a large number of few-shot classes. We do not advocate our approach as\n",
      "the solution for few-shot learning, but simply use the results to highlight\n",
      "limitations of current benchmarks and few-shot protocols. We perform extensive\n",
      "studies on benchmark datasets to propose a metric that quantifies the\n",
      "\"hardness\" of a few-shot episode. This metric can be used to report the\n",
      "performance of few-shot algorithms in a more systematic way.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.12414 \n",
      "Title :On the Convergence of Nesterov's Accelerated Gradient Method in\n",
      "  Stochastic Settings\n",
      "  We study Nesterov's accelerated gradient method with constant step-size and\n",
      "momentum parameters in the stochastic approximation setting (unbiased gradients\n",
      "with bounded variance) and the finite-sum setting (where randomness is due to\n",
      "sampling mini-batches). To build better insight into the behavior of Nesterov's\n",
      "method in stochastic settings, we focus throughout on objectives that are\n",
      "smooth, strongly-convex, and twice continuously differentiable. In the\n",
      "stochastic approximation setting, Nesterov's method converges to a neighborhood\n",
      "of the optimal point at the same accelerated rate as in the deterministic\n",
      "setting. Perhaps surprisingly, in the finite-sum setting, we prove that\n",
      "Nesterov's method may diverge with the usual choice of step-size and momentum,\n",
      "unless additional conditions on the problem related to conditioning and data\n",
      "coherence are satisfied. Our results shed light as to why Nesterov's method may\n",
      "fail to converge or achieve acceleration in the finite-sum setting.\n",
      "\n",
      "**Paper Id :2002.04756 \n",
      "Title :Average-case Acceleration Through Spectral Density Estimation\n",
      "  We develop a framework for the average-case analysis of random quadratic\n",
      "problems and derive algorithms that are optimal under this analysis. This\n",
      "yields a new class of methods that achieve acceleration given a model of the\n",
      "Hessian's eigenvalue distribution. We develop explicit algorithms for the\n",
      "uniform, Marchenko-Pastur, and exponential distributions. These methods are\n",
      "momentum-based algorithms, whose hyper-parameters can be estimated without\n",
      "knowledge of the Hessian's smallest singular value, in contrast with classical\n",
      "accelerated methods like Nesterov acceleration and Polyak momentum. Through\n",
      "empirical benchmarks on quadratic and logistic regression problems, we identify\n",
      "regimes in which the the proposed methods improve over classical (worst-case)\n",
      "accelerated methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.12455 \n",
      "Title :Is the Meta-Learning Idea Able to Improve the Generalization of Deep\n",
      "  Neural Networks on the Standard Supervised Learning?\n",
      "  Substantial efforts have been made on improving the generalization abilities\n",
      "of deep neural networks (DNNs) in order to obtain better performances without\n",
      "introducing more parameters. On the other hand, meta-learning approaches\n",
      "exhibit powerful generalization on new tasks in few-shot learning. Intuitively,\n",
      "few-shot learning is more challenging than the standard supervised learning as\n",
      "each target class only has a very few or no training samples. The natural\n",
      "question that arises is whether the meta-learning idea can be used for\n",
      "improving the generalization of DNNs on the standard supervised learning. In\n",
      "this paper, we propose a novel meta-learning based training procedure (MLTP)\n",
      "for DNNs and demonstrate that the meta-learning idea can indeed improve the\n",
      "generalization abilities of DNNs. MLTP simulates the meta-training process by\n",
      "considering a batch of training samples as a task. The key idea is that the\n",
      "gradient descent step for improving the current task performance should also\n",
      "improve a new task performance, which is ignored by the current standard\n",
      "procedure for training neural networks. MLTP also benefits from all the\n",
      "existing training techniques such as dropout, weight decay, and batch\n",
      "normalization. We evaluate MLTP by training a variety of small and large neural\n",
      "networks on three benchmark datasets, i.e., CIFAR-10, CIFAR-100, and Tiny\n",
      "ImageNet. The experimental results show a consistently improved generalization\n",
      "performance on all the DNNs with different sizes, which verifies the promise of\n",
      "MLTP and demonstrates that the meta-learning idea is indeed able to improve the\n",
      "generalization of DNNs on the standard supervised learning.\n",
      "\n",
      "**Paper Id :1910.05493 \n",
      "Title :Deep Transfer Learning for Source Code Modeling\n",
      "  In recent years, deep learning models have shown great potential in source\n",
      "code modeling and analysis. Generally, deep learning-based approaches are\n",
      "problem-specific and data-hungry. A challenging issue of these approaches is\n",
      "that they require training from starch for a different related problem. In this\n",
      "work, we propose a transfer learning-based approach that significantly improves\n",
      "the performance of deep learning-based source code models. In contrast to\n",
      "traditional learning paradigms, transfer learning can transfer the knowledge\n",
      "learned in solving one problem into another related problem. First, we present\n",
      "two recurrent neural network-based models RNN and GRU for the purpose of\n",
      "transfer learning in the domain of source code modeling. Next, via transfer\n",
      "learning, these pre-trained (RNN and GRU) models are used as feature\n",
      "extractors. Then, these extracted features are combined into attention learner\n",
      "for different downstream tasks. The attention learner leverages from the\n",
      "learned knowledge of pre-trained models and fine-tunes them for a specific\n",
      "downstream task. We evaluate the performance of the proposed approach with\n",
      "extensive experiments with the source code suggestion task. The results\n",
      "indicate that the proposed approach outperforms the state-of-the-art models in\n",
      "terms of accuracy, precision, recall, and F-measure without training the models\n",
      "from scratch.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.12764 \n",
      "Title :Towards Learning a Universal Non-Semantic Representation of Speech\n",
      "  The ultimate goal of transfer learning is to reduce labeled data requirements\n",
      "by exploiting a pre-existing embedding model trained for different datasets or\n",
      "tasks. The visual and language communities have established benchmarks to\n",
      "compare embeddings, but the speech community has yet to do so. This paper\n",
      "proposes a benchmark for comparing speech representations on non-semantic\n",
      "tasks, and proposes a representation based on an unsupervised triplet-loss\n",
      "objective. The proposed representation outperforms other representations on the\n",
      "benchmark, and even exceeds state-of-the-art performance on a number of\n",
      "transfer learning tasks. The embedding is trained on a publicly available\n",
      "dataset, and it is tested on a variety of low-resource downstream tasks,\n",
      "including personalization tasks and medical domain. The benchmark, models, and\n",
      "evaluation code are publicly released.\n",
      "\n",
      "**Paper Id :1903.03096 \n",
      "Title :Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few\n",
      "  Examples\n",
      "  Few-shot classification refers to learning a classifier for new classes given\n",
      "only a few examples. While a plethora of models have emerged to tackle it, we\n",
      "find the procedure and datasets that are used to assess their progress lacking.\n",
      "To address this limitation, we propose Meta-Dataset: a new benchmark for\n",
      "training and evaluating models that is large-scale, consists of diverse\n",
      "datasets, and presents more realistic tasks. We experiment with popular\n",
      "baselines and meta-learners on Meta-Dataset, along with a competitive method\n",
      "that we propose. We analyze performance as a function of various\n",
      "characteristics of test tasks and examine the models' ability to leverage\n",
      "diverse training sources for improving their generalization. We also propose a\n",
      "new set of baselines for quantifying the benefit of meta-learning in\n",
      "Meta-Dataset. Our extensive experimentation has uncovered important research\n",
      "challenges and we hope to inspire work in these directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2002.12826 \n",
      "Title :A Deep Generative Model for Fragment-Based Molecule Generation\n",
      "  Molecule generation is a challenging open problem in cheminformatics.\n",
      "Currently, deep generative approaches addressing the challenge belong to two\n",
      "broad categories, differing in how molecules are represented. One approach\n",
      "encodes molecular graphs as strings of text, and learns their corresponding\n",
      "character-based language model. Another, more expressive, approach operates\n",
      "directly on the molecular graph. In this work, we address two limitations of\n",
      "the former: generation of invalid and duplicate molecules. To improve validity\n",
      "rates, we develop a language model for small molecular substructures called\n",
      "fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug\n",
      "Design. In other words, we generate molecules fragment by fragment, instead of\n",
      "atom by atom. To improve uniqueness rates, we present a frequency-based masking\n",
      "strategy that helps generate molecules with infrequent fragments. We show\n",
      "experimentally that our model largely outperforms other language model-based\n",
      "competitors, reaching state-of-the-art performances typical of graph-based\n",
      "approaches. Moreover, generated molecules display molecular properties similar\n",
      "to those in the training sample, even in absence of explicit task-specific\n",
      "supervision.\n",
      "\n",
      "**Paper Id :2001.09382 \n",
      "Title :GraphAF: a Flow-based Autoregressive Model for Molecular Graph\n",
      "  Generation\n",
      "  Molecular graph generation is a fundamental problem for drug discovery and\n",
      "has been attracting growing attention. The problem is challenging since it\n",
      "requires not only generating chemically valid molecular structures but also\n",
      "optimizing their chemical properties in the meantime. Inspired by the recent\n",
      "progress in deep generative models, in this paper we propose a flow-based\n",
      "autoregressive model for graph generation called GraphAF. GraphAF combines the\n",
      "advantages of both autoregressive and flow-based approaches and enjoys: (1)\n",
      "high model flexibility for data density estimation; (2) efficient parallel\n",
      "computation for training; (3) an iterative sampling process, which allows\n",
      "leveraging chemical domain knowledge for valency checking. Experimental results\n",
      "show that GraphAF is able to generate 68% chemically valid molecules even\n",
      "without chemical knowledge rules and 100% valid molecules with chemical rules.\n",
      "The training process of GraphAF is two times faster than the existing\n",
      "state-of-the-art approach GCPN. After fine-tuning the model for goal-directed\n",
      "property optimization with reinforcement learning, GraphAF achieves\n",
      "state-of-the-art performance on both chemical property optimization and\n",
      "constrained property optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.00430 \n",
      "Title :A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\n",
      "  We propose a novel hybrid stochastic policy gradient estimator by combining\n",
      "an unbiased policy gradient estimator, the REINFORCE estimator, with another\n",
      "biased one, an adapted SARAH estimator for policy optimization. The hybrid\n",
      "policy gradient estimator is shown to be biased, but has variance reduced\n",
      "property. Using this estimator, we develop a new Proximal Hybrid Stochastic\n",
      "Policy Gradient Algorithm (ProxHSPGA) to solve a composite policy optimization\n",
      "problem that allows us to handle constraints or regularizers on the policy\n",
      "parameters. We first propose a single-looped algorithm then introduce a more\n",
      "practical restarting variant. We prove that both algorithms can achieve the\n",
      "best-known trajectory complexity $\\mathcal{O}\\left(\\varepsilon^{-3}\\right)$ to\n",
      "attain a first-order stationary point for the composite problem which is better\n",
      "than existing REINFORCE/GPOMDP $\\mathcal{O}\\left(\\varepsilon^{-4}\\right)$ and\n",
      "SVRPG $\\mathcal{O}\\left(\\varepsilon^{-10/3}\\right)$ in the non-composite\n",
      "setting. We evaluate the performance of our algorithm on several well-known\n",
      "examples in reinforcement learning. Numerical results show that our algorithm\n",
      "outperforms two existing methods on these examples. Moreover, the composite\n",
      "settings indeed have some advantages compared to the non-composite ones on\n",
      "certain problems.\n",
      "\n",
      "**Paper Id :2006.00731 \n",
      "Title :Second-Order Provable Defenses against Adversarial Attacks\n",
      "  A robustness certificate is the minimum distance of a given input to the\n",
      "decision boundary of the classifier (or its lower bound). For {\\it any} input\n",
      "perturbations with a magnitude smaller than the certificate value, the\n",
      "classification output will provably remain unchanged. Exactly computing the\n",
      "robustness certificates for neural networks is difficult since it requires\n",
      "solving a non-convex optimization. In this paper, we provide\n",
      "computationally-efficient robustness certificates for neural networks with\n",
      "differentiable activation functions in two steps. First, we show that if the\n",
      "eigenvalues of the Hessian of the network are bounded, we can compute a\n",
      "robustness certificate in the $l_2$ norm efficiently using convex optimization.\n",
      "Second, we derive a computationally-efficient differentiable upper bound on the\n",
      "curvature of a deep network. We also use the curvature bound as a\n",
      "regularization term during the training of the network to boost its certified\n",
      "robustness. Putting these results together leads to our proposed {\\bf\n",
      "C}urvature-based {\\bf R}obustness {\\bf C}ertificate (CRC) and {\\bf\n",
      "C}urvature-based {\\bf R}obust {\\bf T}raining (CRT). Our numerical results show\n",
      "that CRT leads to significantly higher certified robust accuracy compared to\n",
      "interval-bound propagation (IBP) based training. We achieve certified robust\n",
      "accuracy 69.79\\%, 57.78\\% and 53.19\\% while IBP-based methods achieve 44.96\\%,\n",
      "44.74\\% and 44.66\\% on 2,3 and 4 layer networks respectively on the\n",
      "MNIST-dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.00682 \n",
      "Title :Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images\n",
      "  Lung disease is common throughout the world. These include chronic\n",
      "obstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc.\n",
      "Timely diagnosis of lung disease is essential. Many image processing and\n",
      "machine learning models have been developed for this purpose. Different forms\n",
      "of existing deep learning techniques including convolutional neural network\n",
      "(CNN), vanilla neural network, visual geometry group based neural network\n",
      "(VGG), and capsule network are applied for lung disease prediction.The basic\n",
      "CNN has poor performance for rotated, tilted, or other abnormal image\n",
      "orientation. Therefore, we propose a new hybrid deep learning framework by\n",
      "combining VGG, data augmentation and spatial transformer network (STN) with\n",
      "CNN. This new hybrid method is termed here as VGG Data STN with CNN (VDSNet).\n",
      "As implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The\n",
      "new model is applied to NIH chest X-ray image dataset collected from Kaggle\n",
      "repository. Full and sample versions of the dataset are considered. For both\n",
      "full and sample datasets, VDSNet outperforms existing methods in terms of a\n",
      "number of metrics including precision, recall, F0.5 score and validation\n",
      "accuracy. For the case of full dataset, VDSNet exhibits a validation accuracy\n",
      "of 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified\n",
      "capsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%,\n",
      "respectively. When sample dataset rather than full dataset is used, VDSNet\n",
      "requires much lower training time at the expense of a slightly lower validation\n",
      "accuracy. Hence, the proposed VDSNet framework will simplify the detection of\n",
      "lung disease for experts as well as for doctors.\n",
      "\n",
      "**Paper Id :2003.13145 \n",
      "Title :Can AI help in screening Viral and COVID-19 pneumonia?\n",
      "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\n",
      "caused thousands of causalities and infected several millions of people\n",
      "worldwide. Any technological tool enabling rapid screening of the COVID-19\n",
      "infection with high accuracy can be crucially helpful to healthcare\n",
      "professionals. The main clinical tool currently in use for the diagnosis of\n",
      "COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\n",
      "is expensive, less-sensitive and requires specialized medical personnel. X-ray\n",
      "imaging is an easily accessible tool that can be an excellent alternative in\n",
      "the COVID-19 diagnosis. This research was taken to investigate the utility of\n",
      "artificial intelligence (AI) in the rapid and accurate detection of COVID-19\n",
      "from chest X-ray images. The aim of this paper is to propose a robust technique\n",
      "for automatic detection of COVID-19 pneumonia from digital chest X-ray images\n",
      "applying pre-trained deep-learning algorithms while maximizing the detection\n",
      "accuracy. A public database was created by the authors combining several public\n",
      "databases and also by collecting images from recently published articles. The\n",
      "database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\n",
      "normal chest X-ray images. Transfer learning technique was used with the help\n",
      "of image augmentation to train and validate several pre-trained deep\n",
      "Convolutional Neural Networks (CNNs). The networks were trained to classify two\n",
      "different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\n",
      "COVID-19 pneumonia with and without image augmentation. The classification\n",
      "accuracy, precision, sensitivity, and specificity for both the schemes were\n",
      "99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\n",
      "respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01113 \n",
      "Title :Warwick Electron Microscopy Datasets\n",
      "  Large, carefully partitioned datasets are essential to train neural networks\n",
      "and standardize performance benchmarks. As a result, we have set up new\n",
      "repositories to make our electron microscopy datasets available to the wider\n",
      "community. There are three main datasets containing 19769 scanning transmission\n",
      "electron micrographs, 17266 transmission electron micrographs, and 98340\n",
      "simulated exit wavefunctions, and multiple variants of each dataset for\n",
      "different applications. To visualize image datasets, we trained variational\n",
      "autoencoders to encode data as 64-dimensional multivariate normal\n",
      "distributions, which we cluster in two dimensions by t-distributed stochastic\n",
      "neighbor embedding. In addition, we have improved dataset visualization with\n",
      "variational autoencoders by introducing encoding normalization and\n",
      "regularization, adding an image gradient loss, and extending t-distributed\n",
      "stochastic neighbor embedding to account for encoded standard deviations. Our\n",
      "datasets, source code, pretrained models, and interactive visualizations are\n",
      "openly available at https://github.com/Jeffrey-Ede/datasets.\n",
      "\n",
      "**Paper Id :1905.13667 \n",
      "Title :Partial Scanning Transmission Electron Microscopy with Deep Learning\n",
      "  Compressed sensing algorithms are used to decrease electron microscope scan\n",
      "time and electron beam exposure with minimal information loss. Following\n",
      "successful applications of deep learning to compressed sensing, we have\n",
      "developed a two-stage multiscale generative adversarial neural network to\n",
      "complete realistic 512$\\times$512 scanning transmission electron micrographs\n",
      "from spiral, jittered gridlike, and other partial scans. For spiral scans and\n",
      "mean squared error based pre-training, this enables electron beam coverage to\n",
      "be decreased by 17.9$\\times$ with a 3.8\\% test set root mean squared intensity\n",
      "error, and by 87.0$\\times$ with a 6.2\\% error. Our generator networks are\n",
      "trained on partial scans created from a new dataset of 16227 scanning\n",
      "transmission electron micrographs. High performance is achieved with adaptive\n",
      "learning rate clipping of loss spikes and an auxiliary trainer network. Our\n",
      "source code, new dataset, and pre-trained models have been made publicly\n",
      "available at https://github.com/Jeffrey-Ede/partial-STEM\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01410 \n",
      "Title :ABC-LMPC: Safe Sample-Based Learning MPC for Stochastic Nonlinear\n",
      "  Dynamical Systems with Adjustable Boundary Conditions\n",
      "  Sample-based learning model predictive control (LMPC) strategies have\n",
      "recently attracted attention due to their desirable theoretical properties and\n",
      "their good empirical performance on robotic tasks. However, prior analysis of\n",
      "LMPC controllers for stochastic systems has mainly focused on linear systems in\n",
      "the iterative learning control setting. We present a novel LMPC algorithm,\n",
      "Adjustable Boundary Condition LMPC (ABC-LMPC), which enables rapid adaptation\n",
      "to novel start and goal configurations and theoretically show that the\n",
      "resulting controller guarantees iterative improvement in expectation for\n",
      "stochastic nonlinear systems. We present results with a practical instantiation\n",
      "of this algorithm and experimentally demonstrate that the resulting controller\n",
      "adapts to a variety of initial and terminal conditions on 3 stochastic\n",
      "continuous control tasks.\n",
      "\n",
      "**Paper Id :1912.10360 \n",
      "Title :Safe and Fast Tracking on a Robot Manipulator: Robust MPC and Neural\n",
      "  Network Control\n",
      "  Fast feedback control and safety guarantees are essential in modern robotics.\n",
      "We present an approach that achieves both by combining novel robust model\n",
      "predictive control (MPC) with function approximation via (deep) neural networks\n",
      "(NNs). The result is a new approach for complex tasks with nonlinear,\n",
      "uncertain, and constrained dynamics as are common in robotics. Specifically, we\n",
      "leverage recent results in MPC research to propose a new robust setpoint\n",
      "tracking MPC algorithm, which achieves reliable and safe tracking of a dynamic\n",
      "setpoint while guaranteeing stability and constraint satisfaction. The\n",
      "presented robust MPC scheme constitutes a one-layer approach that unifies the\n",
      "often separated planning and control layers, by directly computing the control\n",
      "command based on a reference and possibly obstacle positions. As a separate\n",
      "contribution, we show how the computation time of the MPC can be drastically\n",
      "reduced by approximating the MPC law with a NN controller. The NN is trained\n",
      "and validated from offline samples of the MPC, yielding statistical guarantees,\n",
      "and used in lieu thereof at run time. Our experiments on a state-of-the-art\n",
      "robot manipulator are the first to show that both the proposed robust and\n",
      "approximate MPC schemes scale to real-world robotic systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01456 \n",
      "Title :Implicit Functions in Feature Space for 3D Shape Reconstruction and\n",
      "  Completion\n",
      "  While many works focus on 3D reconstruction from images, in this paper, we\n",
      "focus on 3D shape reconstruction and completion from a variety of 3D inputs,\n",
      "which are deficient in some respect: low and high resolution voxels, sparse and\n",
      "dense point clouds, complete or incomplete. Processing of such 3D inputs is an\n",
      "increasingly important problem as they are the output of 3D scanners, which are\n",
      "becoming more accessible, and are the intermediate output of 3D computer vision\n",
      "algorithms. Recently, learned implicit functions have shown great promise as\n",
      "they produce continuous reconstructions. However, we identified two limitations\n",
      "in reconstruction from 3D inputs: 1) details present in the input data are not\n",
      "retained, and 2) poor reconstruction of articulated humans. To solve this, we\n",
      "propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,\n",
      "can handle multiple topologies, and complete shapes for missing or sparse input\n",
      "data retaining the nice properties of recent learned implicit functions, but\n",
      "critically they can also retain detail when it is present in the input data,\n",
      "and can reconstruct articulated humans. Our work differs from prior work in two\n",
      "crucial aspects. First, instead of using a single vector to encode a 3D shape,\n",
      "we extract a learnable 3-dimensional multi-scale tensor of deep features, which\n",
      "is aligned with the original Euclidean space embedding the shape. Second,\n",
      "instead of classifying x-y-z point coordinates directly, we classify deep\n",
      "features extracted from the tensor at a continuous query point. We show that\n",
      "this forces our model to make decisions based on global and local shape\n",
      "structure, as opposed to point coordinates, which are arbitrary under Euclidean\n",
      "transformations. Experiments demonstrate that IF-Nets clearly outperform prior\n",
      "work in 3D object reconstruction in ShapeNet, and obtain significantly more\n",
      "accurate 3D human reconstructions.\n",
      "\n",
      "**Paper Id :2010.13938 \n",
      "Title :Neural Unsigned Distance Fields for Implicit Function Learning\n",
      "  In this work we target a learnable output representation that allows\n",
      "continuous, high resolution outputs of arbitrary shape. Recent works represent\n",
      "3D surfaces implicitly with a Neural Network, thereby breaking previous\n",
      "barriers in resolution, and ability to represent diverse topologies. However,\n",
      "neural implicit representations are limited to closed surfaces, which divide\n",
      "the space into inside and outside. Many real world objects such as walls of a\n",
      "scene scanned by a sensor, clothing, or a car with inner structures are not\n",
      "closed. This constitutes a significant barrier, in terms of data pre-processing\n",
      "(objects need to be artificially closed creating artifacts), and the ability to\n",
      "output open surfaces. In this work, we propose Neural Distance Fields (NDF), a\n",
      "neural network based model which predicts the unsigned distance field for\n",
      "arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high\n",
      "resolutions as prior implicit models, but do not require closed surface data,\n",
      "and significantly broaden the class of representable shapes in the output. NDF\n",
      "allow to extract the surface as very dense point clouds and as meshes. We also\n",
      "show that NDF allow for surface normal calculation and can be rendered using a\n",
      "slight modification of sphere tracing. We find NDF can be used for multi-target\n",
      "regression (multiple outputs for one input) with techniques that have been\n",
      "exclusively used for rendering in graphics. Experiments on ShapeNet show that\n",
      "NDF, while simple, is the state-of-the art, and allows to reconstruct shapes\n",
      "with inner structures, such as the chairs inside a bus. Notably, we show that\n",
      "NDF are not restricted to 3D shapes, and can approximate more general open\n",
      "surfaces such as curves, manifolds, and functions. Code is available for\n",
      "research at https://virtualhumans.mpi-inf.mpg.de/ndf/.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01504 \n",
      "Title :Towards Novel Insights in Lattice Field Theory with Explainable Machine\n",
      "  Learning\n",
      "  Machine learning has the potential to aid our understanding of phase\n",
      "structures in lattice quantum field theories through the statistical analysis\n",
      "of Monte Carlo samples. Available algorithms, in particular those based on deep\n",
      "learning, often demonstrate remarkable performance in the search for previously\n",
      "unidentified features, but tend to lack transparency if applied naively. To\n",
      "address these shortcomings, we propose representation learning in combination\n",
      "with interpretability methods as a framework for the identification of\n",
      "observables. More specifically, we investigate action parameter regression as a\n",
      "pretext task while using layer-wise relevance propagation (LRP) to identify the\n",
      "most important observables depending on the location in the phase diagram. The\n",
      "approach is put to work in the context of a scalar Yukawa model in (2+1)d.\n",
      "First, we investigate a multilayer perceptron to determine an importance\n",
      "hierarchy of several predefined, standard observables. The method is then\n",
      "applied directly to the raw field configurations using a convolutional network,\n",
      "demonstrating the ability to reconstruct all order parameters from the learned\n",
      "filter weights. Based on our results, we argue that due to its broad\n",
      "applicability, attribution methods such as LRP could prove a useful and\n",
      "versatile tool in our search for new physical insights. In the case of the\n",
      "Yukawa model, it facilitates the construction of an observable that\n",
      "characterises the symmetric phase.\n",
      "\n",
      "**Paper Id :2003.13418 \n",
      "Title :Machine Learning Enabled Discovery of Application Dependent Design\n",
      "  Principles for Two-dimensional Materials\n",
      "  The large-scale search for high-performing candidate 2D materials is limited\n",
      "to calculating a few simple descriptors, usually with first-principles density\n",
      "functional theory calculations. In this work, we alleviate this issue by\n",
      "extending and generalizing crystal graph convolutional neural networks to\n",
      "systems with planar periodicity, and train an ensemble of models to predict\n",
      "thermodynamic, mechanical, and electronic properties. To demonstrate the\n",
      "utility of this approach, we carry out a screening of nearly 45,000 structures\n",
      "for two largely disjoint applications: namely, mechanically robust composites\n",
      "and photovoltaics. An analysis of the uncertainty associated with our methods\n",
      "indicates the ensemble of neural networks is well-calibrated and has errors\n",
      "comparable with those from accurate first-principles density functional theory\n",
      "calculations. The ensemble of models allows us to gauge the confidence of our\n",
      "predictions, and to find the candidates most likely to exhibit effective\n",
      "performance in their applications. Since the datasets used in our screening\n",
      "were combinatorically generated, we are also able to investigate, using an\n",
      "innovative method, structural and compositional design principles that impact\n",
      "the properties of the structures surveyed and which can act as a generative\n",
      "model basis for future material discovery through reverse engineering. Our\n",
      "approach allowed us to recover some well-accepted design principles: for\n",
      "instance, we find that hybrid organic-inorganic perovskites with lead and tin\n",
      "tend to be good candidates for solar cell applications.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01582 \n",
      "Title :Reconfigurable Design for Omni-adaptive Grasp Learning\n",
      "  The engineering design of robotic grippers presents an ample design space for\n",
      "optimization towards robust grasping. In this paper, we adopt the\n",
      "reconfigurable design of the robotic gripper using a novel soft finger\n",
      "structure with omni-directional adaptation, which generates a large number of\n",
      "possible gripper configurations by rearranging these fingers. Such\n",
      "reconfigurable design with these omni-adaptive fingers enables us to\n",
      "systematically investigate the optimal arrangement of the fingers towards\n",
      "robust grasping. Furthermore, we adopt a learning-based method as the baseline\n",
      "to benchmark the effectiveness of each design configuration. As a result, we\n",
      "found that a 3-finger and 4-finger radial configuration is the most effective\n",
      "one achieving an average 96\\% grasp success rate on seen and novel objects\n",
      "selected from the YCB dataset. We also discussed the influence of the\n",
      "frictional surface on the finger to improve the grasp robustness.\n",
      "\n",
      "**Paper Id :2011.05516 \n",
      "Title :Probability-Density-Based Deep Learning Paradigm for the Fuzzy Design of\n",
      "  Functional Metastructures\n",
      "  In quantum mechanics, a norm squared wave function can be interpreted as the\n",
      "probability density that describes the likelihood of a particle to be measured\n",
      "in a given position or momentum. This statistical property is at the core of\n",
      "the fuzzy structure of microcosmos. Recently, hybrid neural structures raised\n",
      "intense attention, resulting in various intelligent systems with far-reaching\n",
      "influence. Here, we propose a probability-density-based deep learning paradigm\n",
      "for the fuzzy design of functional meta-structures. In contrast to other\n",
      "inverse design methods, our probability-density-based neural network can\n",
      "efficiently evaluate and accurately capture all plausible meta-structures in a\n",
      "high-dimensional parameter space. Local maxima in probability density\n",
      "distribution correspond to the most likely candidates to meet the desired\n",
      "performances. We verify this universally adaptive approach in but not limited\n",
      "to acoustics by designing multiple meta-structures for each targeted\n",
      "transmission spectrum, with experiments unequivocally demonstrating the\n",
      "effectiveness and generalization of the inverse design.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01643 \n",
      "Title :Single-exposure absorption imaging of ultracold atoms using deep\n",
      "  learning\n",
      "  Absorption imaging is the most common probing technique in experiments with\n",
      "ultracold atoms. The standard procedure involves the division of two frames\n",
      "acquired at successive exposures, one with the atomic absorption signal and one\n",
      "without. A well-known problem is the presence of residual structured noise in\n",
      "the final image, due to small differences between the imaging light in the two\n",
      "exposures. Here we solve this problem by performing absorption imaging with\n",
      "only a single exposure, where instead of a second exposure the reference frame\n",
      "is generated by an unsupervised image-completion autoencoder neural network.\n",
      "The network is trained on images without absorption signal such that it can\n",
      "infer the noise overlaying the atomic signal based only on the information in\n",
      "the region encircling the signal. We demonstrate our approach on data captured\n",
      "with a quantum degenerate Fermi gas. The average residual noise in the\n",
      "resulting images is below that of the standard double-shot technique. Our\n",
      "method simplifies the experimental sequence, reduces the hardware requirements,\n",
      "and can improve the accuracy of extracted physical observables. The trained\n",
      "network and its generating scripts are available as an open-source repository\n",
      "(http://absDL.github.io/).\n",
      "\n",
      "**Paper Id :1911.09083 \n",
      "Title :Machine-learning non-stationary noise out of gravitational wave\n",
      "  detectors\n",
      "  Signal extraction out of background noise is a common challenge in high\n",
      "precision physics experiments, where the measurement output is often a\n",
      "continuous data stream. To improve the signal to noise ratio of the detection,\n",
      "witness sensors are often used to independently measure background noises and\n",
      "subtract them from the main signal. If the noise coupling is linear and\n",
      "stationary, optimal techniques already exist and are routinely implemented in\n",
      "many experiments. However, when the noise coupling is non-stationary, linear\n",
      "techniques often fail or are sub-optimal. Inspired by the properties of the\n",
      "background noise in gravitational wave detectors, this work develops a novel\n",
      "algorithm to efficiently characterize and remove non-stationary noise\n",
      "couplings, provided there exist witnesses of the noise source and of the\n",
      "modulation. In this work, the algorithm is described in its most general\n",
      "formulation, and its efficiency is demonstrated with examples from the data of\n",
      "the Advanced LIGO gravitational wave observatory, where we could obtain an\n",
      "improvement of the detector gravitational wave reach without introducing any\n",
      "bias on the source parameter estimation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01668 \n",
      "Title :Model Assertions for Monitoring and Improving ML Models\n",
      "  ML models are increasingly deployed in settings with real world interactions\n",
      "such as vehicles, but unfortunately, these models can fail in systematic ways.\n",
      "To prevent errors, ML engineering teams monitor and continuously improve these\n",
      "models. We propose a new abstraction, model assertions, that adapts the\n",
      "classical use of program assertions as a way to monitor and improve ML models.\n",
      "Model assertions are arbitrary functions over a model's input and output that\n",
      "indicate when errors may be occurring, e.g., a function that triggers if an\n",
      "object rapidly changes its class in a video. We propose methods of using model\n",
      "assertions at all stages of ML system deployment, including runtime monitoring,\n",
      "validating labels, and continuously improving ML models. For runtime\n",
      "monitoring, we show that model assertions can find high confidence errors,\n",
      "where a model returns the wrong output with high confidence, which\n",
      "uncertainty-based monitoring techniques would not detect. For training, we\n",
      "propose two methods of using model assertions. First, we propose a bandit-based\n",
      "active learning algorithm that can sample from data flagged by assertions and\n",
      "show that it can reduce labeling costs by up to 40% over traditional\n",
      "uncertainty-based methods. Second, we propose an API for generating\n",
      "\"consistency assertions\" (e.g., the class change example) and weak labels for\n",
      "inputs where the consistency assertions fail, and show that these weak labels\n",
      "can improve relative model quality by up to 46%. We evaluate model assertions\n",
      "on four real-world tasks with video, LIDAR, and ECG data.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01695 \n",
      "Title :Robust data encodings for quantum classifiers\n",
      "  Data representation is crucial for the success of machine learning models. In\n",
      "the context of quantum machine learning with near-term quantum computers,\n",
      "equally important considerations of how to efficiently input (encode) data and\n",
      "effectively deal with noise arise. In this work, we study data encodings for\n",
      "binary quantum classification and investigate their properties both with and\n",
      "without noise. For the common classifier we consider, we show that encodings\n",
      "determine the classes of learnable decision boundaries as well as the set of\n",
      "points which retain the same classification in the presence of noise. After\n",
      "defining the notion of a robust data encoding, we prove several results on\n",
      "robustness for different channels, discuss the existence of robust encodings,\n",
      "and prove an upper bound on the number of robust points in terms of fidelities\n",
      "between noisy and noiseless states. Numerical results for several example\n",
      "implementations are provided to reinforce our findings.\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01770 \n",
      "Title :Error bounds in estimating the out-of-sample prediction error using\n",
      "  leave-one-out cross validation in high-dimensions\n",
      "  We study the problem of out-of-sample risk estimation in the high dimensional\n",
      "regime where both the sample size $n$ and number of features $p$ are large, and\n",
      "$n/p$ can be less than one. Extensive empirical evidence confirms the accuracy\n",
      "of leave-one-out cross validation (LO) for out-of-sample risk estimation. Yet,\n",
      "a unifying theoretical evaluation of the accuracy of LO in high-dimensional\n",
      "problems has remained an open problem. This paper aims to fill this gap for\n",
      "penalized regression in the generalized linear family. With minor assumptions\n",
      "about the data generating process, and without any sparsity assumptions on the\n",
      "regression coefficients, our theoretical analysis obtains finite sample upper\n",
      "bounds on the expected squared error of LO in estimating the out-of-sample\n",
      "error. Our bounds show that the error goes to zero as $n,p \\rightarrow \\infty$,\n",
      "even when the dimension $p$ of the feature vectors is comparable with or\n",
      "greater than the sample size $n$. One technical advantage of the theory is that\n",
      "it can be used to clarify and connect some results from the recent literature\n",
      "on scalable approximate LO.\n",
      "\n",
      "**Paper Id :1905.00820 \n",
      "Title :On the smoothness of nonlinear system identification\n",
      "  We shed new light on the \\textit{smoothness} of optimization problems arising\n",
      "in prediction error parameter estimation of linear and nonlinear systems. We\n",
      "show that for regions of the parameter space where the model is not\n",
      "contractive, the Lipschitz constant and $\\beta$-smoothness of the objective\n",
      "function might blow up exponentially with the simulation length, making it hard\n",
      "to numerically find minima within those regions or, even, to escape from them.\n",
      "In addition to providing theoretical understanding of this problem, this paper\n",
      "also proposes the use of multiple shooting as a viable solution. The proposed\n",
      "method minimizes the error between a prediction model and the observed values.\n",
      "Rather than running the prediction model over the entire dataset, multiple\n",
      "shooting splits the data into smaller subsets and runs the prediction model\n",
      "over each subset, making the simulation length a design parameter and making it\n",
      "possible to solve problems that would be infeasible using a standard approach.\n",
      "The equivalence to the original problem is obtained by including constraints in\n",
      "the optimization. The new method is illustrated by estimating the parameters of\n",
      "nonlinear systems with chaotic or unstable behavior, as well as neural\n",
      "networks. We also present a comparative analysis of the proposed method with\n",
      "multi-step-ahead prediction error minimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01805 \n",
      "Title :Adaptive Hyper-box Matching for Interpretable Individualized Treatment\n",
      "  Effect Estimation\n",
      "  We propose a matching method for observational data that matches units with\n",
      "others in unit-specific, hyper-box-shaped regions of the covariate space. These\n",
      "regions are large enough that many matches are created for each unit and small\n",
      "enough that the treatment effect is roughly constant throughout. The regions\n",
      "are found as either the solution to a mixed integer program, or using a (fast)\n",
      "approximation algorithm. The result is an interpretable and tailored estimate\n",
      "of a causal effect for each unit.\n",
      "\n",
      "**Paper Id :1905.00820 \n",
      "Title :On the smoothness of nonlinear system identification\n",
      "  We shed new light on the \\textit{smoothness} of optimization problems arising\n",
      "in prediction error parameter estimation of linear and nonlinear systems. We\n",
      "show that for regions of the parameter space where the model is not\n",
      "contractive, the Lipschitz constant and $\\beta$-smoothness of the objective\n",
      "function might blow up exponentially with the simulation length, making it hard\n",
      "to numerically find minima within those regions or, even, to escape from them.\n",
      "In addition to providing theoretical understanding of this problem, this paper\n",
      "also proposes the use of multiple shooting as a viable solution. The proposed\n",
      "method minimizes the error between a prediction model and the observed values.\n",
      "Rather than running the prediction model over the entire dataset, multiple\n",
      "shooting splits the data into smaller subsets and runs the prediction model\n",
      "over each subset, making the simulation length a design parameter and making it\n",
      "possible to solve problems that would be infeasible using a standard approach.\n",
      "The equivalence to the original problem is obtained by including constraints in\n",
      "the optimization. The new method is illustrated by estimating the parameters of\n",
      "nonlinear systems with chaotic or unstable behavior, as well as neural\n",
      "networks. We also present a comparative analysis of the proposed method with\n",
      "multi-step-ahead prediction error minimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.01852 \n",
      "Title :q-VAE for Disentangled Representation Learning and Latent Dynamical\n",
      "  Systems\n",
      "  A variational autoencoder (VAE) derived from Tsallis statistics called q-VAE\n",
      "is proposed. In the proposed method, a standard VAE is employed to\n",
      "statistically extract latent space hidden in sampled data, and this latent\n",
      "space helps make robots controllable in feasible computational time and cost.\n",
      "To improve the usefulness of the latent space, this paper focuses on\n",
      "disentangled representation learning, e.g., $\\beta$-VAE, which is the baseline\n",
      "for it. Starting from a Tsallis statistics perspective, a new lower bound for\n",
      "the proposed q-VAE is derived to maximize the likelihood of the sampled data,\n",
      "which can be considered an adaptive $\\beta$-VAE with deformed Kullback-Leibler\n",
      "divergence. To verify the benefits of the proposed q-VAE, a benchmark task to\n",
      "extract the latent space from the MNIST dataset was performed. The results\n",
      "demonstrate that the proposed q-VAE improved disentangled representation while\n",
      "maintaining the reconstruction accuracy of the data. In addition, it relaxes\n",
      "the independency condition between data, which is demonstrated by learning the\n",
      "latent dynamics of nonlinear dynamical systems. By combining disentangled\n",
      "representation, the proposed q-VAE achieves stable and accurate long-term state\n",
      "prediction from the initial state and the action sequence.\n",
      "\n",
      "**Paper Id :2004.05988 \n",
      "Title :ControlVAE: Controllable Variational Autoencoder\n",
      "  Variational Autoencoders (VAE) and their variants have been widely used in a\n",
      "variety of applications, such as dialog generation, image generation and\n",
      "disentangled representation learning. However, the existing VAE models have\n",
      "some limitations in different applications. For example, a VAE easily suffers\n",
      "from KL vanishing in language modeling and low reconstruction quality for\n",
      "disentangling. To address these issues, we propose a novel controllable\n",
      "variational autoencoder framework, ControlVAE, that combines a controller,\n",
      "inspired by automatic control theory, with the basic VAE to improve the\n",
      "performance of resulting generative models. Specifically, we design a new\n",
      "non-linear PI controller, a variant of the proportional-integral-derivative\n",
      "(PID) control, to automatically tune the hyperparameter (weight) added in the\n",
      "VAE objective using the output KL-divergence as feedback during model training.\n",
      "The framework is evaluated using three applications; namely, language modeling,\n",
      "disentangled representation learning, and image generation. The results show\n",
      "that ControlVAE can achieve better disentangling and reconstruction quality\n",
      "than the existing methods. For language modelling, it not only averts the\n",
      "KL-vanishing, but also improves the diversity of generated text. Finally, we\n",
      "also demonstrate that ControlVAE improves the reconstruction quality of\n",
      "generated images compared to the original VAE.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.02228 \n",
      "Title :PushNet: Efficient and Adaptive Neural Message Passing\n",
      "  Message passing neural networks have recently evolved into a state-of-the-art\n",
      "approach to representation learning on graphs. Existing methods perform\n",
      "synchronous message passing along all edges in multiple subsequent rounds and\n",
      "consequently suffer from various shortcomings: Propagation schemes are\n",
      "inflexible since they are restricted to $k$-hop neighborhoods and insensitive\n",
      "to actual demands of information propagation. Further, long-range dependencies\n",
      "cannot be modeled adequately and learned representations are based on\n",
      "correlations of fixed locality. These issues prevent existing methods from\n",
      "reaching their full potential in terms of prediction performance. Instead, we\n",
      "consider a novel asynchronous message passing approach where information is\n",
      "pushed only along the most relevant edges until convergence. Our proposed\n",
      "algorithm can equivalently be formulated as a single synchronous message\n",
      "passing iteration using a suitable neighborhood function, thus sharing the\n",
      "advantages of existing methods while addressing their central issues. The\n",
      "resulting neural network utilizes a node-adaptive receptive field derived from\n",
      "meaningful sparse node neighborhoods. In addition, by learning and combining\n",
      "node representations over differently sized neighborhoods, our model is able to\n",
      "capture correlations on multiple scales. We further propose variants of our\n",
      "base model with different inductive bias. Empirical results are provided for\n",
      "semi-supervised node classification on five real-world datasets following a\n",
      "rigorous evaluation protocol. We find that our models outperform competitors on\n",
      "all datasets in terms of accuracy with statistical significance. In some cases,\n",
      "our models additionally provide faster runtime.\n",
      "\n",
      "**Paper Id :2010.12438 \n",
      "Title :Transferable Graph Optimizers for ML Compilers\n",
      "  Most compilers for machine learning (ML) frameworks need to solve many\n",
      "correlated optimization problems to generate efficient machine code. Current ML\n",
      "compilers rely on heuristics based algorithms to solve these optimization\n",
      "problems one at a time. However, this approach is not only hard to maintain but\n",
      "often leads to sub-optimal solutions especially for newer model architectures.\n",
      "Existing learning based approaches in the literature are sample inefficient,\n",
      "tackle a single optimization problem, and do not generalize to unseen graphs\n",
      "making them infeasible to be deployed in practice. To address these\n",
      "limitations, we propose an end-to-end, transferable deep reinforcement learning\n",
      "method for computational graph optimization (GO), based on a scalable\n",
      "sequential attention mechanism over an inductive graph neural network. GO\n",
      "generates decisions on the entire graph rather than on each individual node\n",
      "autoregressively, drastically speeding up the search compared to prior methods.\n",
      "Moreover, we propose recurrent attention layers to jointly optimize dependent\n",
      "graph optimization tasks and demonstrate 33%-60% speedup on three graph\n",
      "optimization tasks compared to TensorFlow default optimization. On a diverse\n",
      "set of representative graphs consisting of up to 80,000 nodes, including\n",
      "Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21%\n",
      "improvement over human experts and 18% improvement over the prior state of the\n",
      "art with 15x faster convergence, on a device placement task evaluated in real\n",
      "systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.02372 \n",
      "Title :Dynamic Experience Replay\n",
      "  We present a novel technique called Dynamic Experience Replay (DER) that\n",
      "allows Reinforcement Learning (RL) algorithms to use experience replay samples\n",
      "not only from human demonstrations but also successful transitions generated by\n",
      "RL agents during training and therefore improve training efficiency. It can be\n",
      "combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and\n",
      "their distributed versions. We build upon Ape-X DDPG and demonstrate our\n",
      "approach on robotic tight-fitting joint assembly tasks, based on force/torque\n",
      "and Cartesian pose observations. In particular, we run experiments on two\n",
      "different tasks: peg-in-hole and lap-joint. In each case, we compare different\n",
      "replay buffer structures and how DER affects them. Our ablation studies show\n",
      "that Dynamic Experience Replay is a crucial ingredient that either largely\n",
      "shortens the training time in these challenging environments or solves the\n",
      "tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies\n",
      "learned purely in simulation can be deployed successfully on the real robot.\n",
      "The video presenting our experiments is available at\n",
      "https://sites.google.com/site/dynamicexperiencereplay\n",
      "\n",
      "**Paper Id :1909.10707 \n",
      "Title :Invariant Transform Experience Replay: Data Augmentation for Deep\n",
      "  Reinforcement Learning\n",
      "  Deep Reinforcement Learning (RL) is a promising approach for adaptive robot\n",
      "control, but its current application to robotics is currently hindered by high\n",
      "sample requirements. To alleviate this issue, we propose to exploit the\n",
      "symmetries present in robotic tasks. Intuitively, symmetries from observed\n",
      "trajectories define transformations that leave the space of feasible RL\n",
      "trajectories invariant and can be used to generate new feasible trajectories,\n",
      "which could be used for training. Based on this data augmentation idea, we\n",
      "formulate a general framework, called Invariant Transform Experience Replay\n",
      "that we present with two techniques: (i) Kaleidoscope Experience Replay\n",
      "exploits reflectional symmetries and (ii) Goal-augmented Experience Replay\n",
      "which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI\n",
      "Gym, our experimental results show significant increases in learning rates and\n",
      "success rates. Particularly, we attain a 13, 3, and 5 times speedup in the\n",
      "pushing, sliding, and pick-and-place tasks respectively in the multi-goal\n",
      "setting. Performance gains are also observed in similar tasks with obstacles\n",
      "and we successfully deployed a trained policy on a real Baxter robot. Our work\n",
      "demonstrates that invariant transformations on RL trajectories are a promising\n",
      "methodology to speed up learning in deep RL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.02389 \n",
      "Title :Comparing Rewinding and Fine-tuning in Neural Network Pruning\n",
      "  Many neural network pruning algorithms proceed in three steps: train the\n",
      "network to completion, remove unwanted structure to compress the network, and\n",
      "retrain the remaining structure to recover lost accuracy. The standard\n",
      "retraining technique, fine-tuning, trains the unpruned weights from their final\n",
      "trained values using a small fixed learning rate. In this paper, we compare\n",
      "fine-tuning to alternative retraining techniques. Weight rewinding (as proposed\n",
      "by Frankle et al., (2019)), rewinds unpruned weights to their values from\n",
      "earlier in training and retrains them from there using the original training\n",
      "schedule. Learning rate rewinding (which we propose) trains the unpruned\n",
      "weights from their final values using the same learning rate schedule as weight\n",
      "rewinding. Both rewinding techniques outperform fine-tuning, forming the basis\n",
      "of a network-agnostic pruning algorithm that matches the accuracy and\n",
      "compression ratios of several more network-specific state-of-the-art\n",
      "techniques.\n",
      "\n",
      "**Paper Id :2003.05856 \n",
      "Title :Online Fast Adaptation and Knowledge Accumulation: a New Approach to\n",
      "  Continual Learning\n",
      "  Continual learning studies agents that learn from streams of tasks without\n",
      "forgetting previous ones while adapting to new ones. Two recent\n",
      "continual-learning scenarios have opened new avenues of research. In\n",
      "meta-continual learning, the model is pre-trained to minimize catastrophic\n",
      "forgetting of previous tasks. In continual-meta learning, the aim is to train\n",
      "agents for faster remembering of previous tasks through adaptation. In their\n",
      "original formulations, both methods have limitations. We stand on their\n",
      "shoulders to propose a more general scenario, OSAKA, where an agent must\n",
      "quickly solve new (out-of-distribution) tasks, while also requiring fast\n",
      "remembering. We show that current continual learning, meta-learning,\n",
      "meta-continual learning, and continual-meta learning techniques fail in this\n",
      "new scenario. We propose Continual-MAML, an online extension of the popular\n",
      "MAML algorithm as a strong baseline for this scenario. We empirically show that\n",
      "Continual-MAML is better suited to the new scenario than the aforementioned\n",
      "methodologies, as well as standard continual learning and meta-learning\n",
      "approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.02822 \n",
      "Title :Feature Extraction for Hyperspectral Imagery: The Evolution from Shallow\n",
      "  to Deep (Overview and Toolbox)\n",
      "  Hyperspectral images provide detailed spectral information through hundreds\n",
      "of (narrow) spectral channels (also known as dimensionality or bands) with\n",
      "continuous spectral information that can accurately classify diverse materials\n",
      "of interest. The increased dimensionality of such data makes it possible to\n",
      "significantly improve data information content but provides a challenge to the\n",
      "conventional techniques (the so-called curse of dimensionality) for accurate\n",
      "analysis of hyperspectral images. Feature extraction, as a vibrant field of\n",
      "research in the hyperspectral community, evolved through decades of research to\n",
      "address this issue and extract informative features suitable for data\n",
      "representation and classification. The advances in feature extraction have been\n",
      "inspired by two fields of research, including the popularization of image and\n",
      "signal processing as well as machine (deep) learning, leading to two types of\n",
      "feature extraction approaches named shallow and deep techniques. This article\n",
      "outlines the advances in feature extraction approaches for hyperspectral\n",
      "imagery by providing a technical overview of the state-of-the-art techniques,\n",
      "providing useful entry points for researchers at different levels, including\n",
      "students, researchers, and senior researchers, willing to explore novel\n",
      "investigations on this challenging topic. In more detail, this paper provides a\n",
      "bird's eye view over shallow (both supervised and unsupervised) and deep\n",
      "feature extraction approaches specifically dedicated to the topic of\n",
      "hyperspectral feature extraction and its application on hyperspectral image\n",
      "classification. Additionally, this paper compares 15 advanced techniques with\n",
      "an emphasis on their methodological foundations in terms of classification\n",
      "accuracies. Furthermore, the codes and libraries are shared at\n",
      "https://github.com/BehnoodRasti/HyFTech-Hyperspectral-Shallow-Deep-Feature-Extraction-Toolbox.\n",
      "\n",
      "**Paper Id :2004.11204 \n",
      "Title :Classification using Hyperdimensional Computing: A Review\n",
      "  Hyperdimensional (HD) computing is built upon its unique data type referred\n",
      "to as hypervectors. The dimension of these hypervectors is typically in the\n",
      "range of tens of thousands. Proposed to solve cognitive tasks, HD computing\n",
      "aims at calculating similarity among its data. Data transformation is realized\n",
      "by three operations, including addition, multiplication and permutation. Its\n",
      "ultra-wide data representation introduces redundancy against noise. Since\n",
      "information is evenly distributed over every bit of the hypervectors, HD\n",
      "computing is inherently robust. Additionally, due to the nature of those three\n",
      "operations, HD computing leads to fast learning ability, high energy efficiency\n",
      "and acceptable accuracy in learning and classification tasks. This paper\n",
      "introduces the background of HD computing, and reviews the data representation,\n",
      "data transformation, and similarity measurement. The orthogonality in high\n",
      "dimensions presents opportunities for flexible computing. To balance the\n",
      "tradeoff between accuracy and efficiency, strategies include but are not\n",
      "limited to encoding, retraining, binarization and hardware acceleration.\n",
      "Evaluations indicate that HD computing shows great potential in addressing\n",
      "problems using data in the form of letters, signals and images. HD computing\n",
      "especially shows significant promise to replace machine learning algorithms as\n",
      "a light-weight classifier in the field of internet of things (IoTs).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.02911 \n",
      "Title :Towards a generalization of information theory for hierarchical\n",
      "  partitions\n",
      "  Complex systems often exhibit multiple levels of organization covering a wide\n",
      "range of physical scales, so the study of the hierarchical decomposition of\n",
      "their structure and function is frequently convenient. To better understand\n",
      "this phenomenon, we introduce a generalization of information theory that works\n",
      "with hierarchical partitions. We begin revisiting the recently introduced\n",
      "Hierarchical Mutual Information (HMI), and show that it can be written as a\n",
      "level by level summation of classical conditional mutual information terms.\n",
      "Then, we prove that the HMI is bounded from above by the corresponding\n",
      "hierarchical joint entropy. In this way, in analogy to the classical case, we\n",
      "derive hierarchical generalizations of many other classical\n",
      "information-theoretic quantities. In particular, we prove that, as opposed to\n",
      "its classical counterpart, the hierarchical generalization of the Variation of\n",
      "Information is not a metric distance, but it admits a transformation into one.\n",
      "Moreover, focusing on potential applications of the existing developments of\n",
      "the theory, we show how to adjust by chance the HMI. We also corroborate and\n",
      "analyze all the presented theoretical results with exhaustive numerical\n",
      "computations, and include an illustrative application example of the introduced\n",
      "formalism. Finally, we mention some open problems that should be eventually\n",
      "addressed for the proposed generalization of information theory to reach\n",
      "maturity.\n",
      "\n",
      "**Paper Id :1907.02177 \n",
      "Title :Adaptive Approximation and Generalization of Deep Neural Network with\n",
      "  Intrinsic Dimensionality\n",
      "  In this study, we prove that an intrinsic low dimensionality of covariates is\n",
      "the main factor that determines the performance of deep neural networks (DNNs).\n",
      "DNNs generally provide outstanding empirical performance. Hence, numerous\n",
      "studies have actively investigated the theoretical properties of DNNs to\n",
      "understand their underlying mechanisms. In particular, the behavior of DNNs in\n",
      "terms of high-dimensional data is one of the most critical questions. However,\n",
      "this issue has not been sufficiently investigated from the aspect of\n",
      "covariates, although high-dimensional data have practically low intrinsic\n",
      "dimensionality. In this study, we derive bounds for an approximation error and\n",
      "a generalization error regarding DNNs with intrinsically low dimensional\n",
      "covariates. We apply the notion of the Minkowski dimension and develop a novel\n",
      "proof technique. Consequently, we show that convergence rates of the errors by\n",
      "DNNs do not depend on the nominal high dimensionality of data, but on its lower\n",
      "intrinsic dimension. We further prove that the rate is optimal in the minimax\n",
      "sense. We identify an advantage of DNNs by showing that DNNs can handle a\n",
      "broader class of intrinsic low dimensional data than other adaptive estimators.\n",
      "Finally, we conduct a numerical simulation to validate the theoretical results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.03051 \n",
      "Title :Cost-Sensitive Portfolio Selection via Deep Reinforcement Learning\n",
      "  Portfolio Selection is an important real-world financial task and has\n",
      "attracted extensive attention in artificial intelligence communities. This\n",
      "task, however, has two main difficulties: (i) the non-stationary price series\n",
      "and complex asset correlations make the learning of feature representation very\n",
      "hard; (ii) the practicality principle in financial markets requires controlling\n",
      "both transaction and risk costs. Most existing methods adopt handcraft features\n",
      "and/or consider no constraints for the costs, which may make them perform\n",
      "unsatisfactorily and fail to control both costs in practice. In this paper, we\n",
      "propose a cost-sensitive portfolio selection method with deep reinforcement\n",
      "learning. Specifically, a novel two-stream portfolio policy network is devised\n",
      "to extract both price series patterns and asset correlations, while a new\n",
      "cost-sensitive reward function is developed to maximize the accumulated return\n",
      "and constrain both costs via reinforcement learning. We theoretically analyze\n",
      "the near-optimality of the proposed reward, which shows that the growth rate of\n",
      "the policy regarding this reward function can approach the theoretical optimum.\n",
      "We also empirically evaluate the proposed method on real-world datasets.\n",
      "Promising results demonstrate the effectiveness and superiority of the proposed\n",
      "method in terms of profitability, cost-sensitivity and representation\n",
      "abilities.\n",
      "\n",
      "**Paper Id :1911.02248 \n",
      "Title :MBCAL: Sample Efficient and Variance Reduced Reinforcement Learning for\n",
      "  Recommender Systems\n",
      "  In recommender systems such as news feed stream, it is essential to optimize\n",
      "the long-term utilities in the continuous user-system interaction processes.\n",
      "Previous works have proved the capability of reinforcement learning in this\n",
      "problem. However, there are many practical challenges to implement deep\n",
      "reinforcement learning in online systems, including low sample efficiency,\n",
      "uncontrollable risks, and excessive variances. To address these issues, we\n",
      "propose a novel reinforcement learning method, namely model-based\n",
      "counterfactual advantage learning (MBCAL). The proposed method takes advantage\n",
      "of the characteristics of recommender systems and draws ideas from the\n",
      "model-based reinforcement learning method for higher sample efficiency. It has\n",
      "two components: an environment model that predicts the instant user behavior\n",
      "one-by-one in an auto-regressive form, and a future advantage model that\n",
      "predicts the future utility. To alleviate the impact of excessive variance when\n",
      "learning the future advantage model, we employ counterfactual comparisons\n",
      "derived from the environment model. In consequence, the proposed method\n",
      "possesses high sample efficiency and significantly lower variance; Also, it is\n",
      "able to use existing user logs to avoid the risks of starting from scratch. In\n",
      "contrast to its capability, its implementation cost is relatively low, which\n",
      "fits well with practical systems. Theoretical analysis and elaborate\n",
      "experiments are presented. Results show that the proposed method transcends the\n",
      "other supervised learning and RL-based methods in both sample efficiency and\n",
      "asymptotic performances.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.03519 \n",
      "Title :Distilling portable Generative Adversarial Networks for Image\n",
      "  Translation\n",
      "  Despite Generative Adversarial Networks (GANs) have been widely used in\n",
      "various image-to-image translation tasks, they can be hardly applied on mobile\n",
      "devices due to their heavy computation and storage cost. Traditional network\n",
      "compression methods focus on visually recognition tasks, but never deal with\n",
      "generation tasks. Inspired by knowledge distillation, a student generator of\n",
      "fewer parameters is trained by inheriting the low-level and high-level\n",
      "information from the original heavy teacher generator. To promote the\n",
      "capability of student generator, we include a student discriminator to measure\n",
      "the distances between real images, and images generated by student and teacher\n",
      "generators. An adversarial learning process is therefore established to\n",
      "optimize student generator and student discriminator. Qualitative and\n",
      "quantitative analysis by conducting experiments on benchmark datasets\n",
      "demonstrate that the proposed method can learn portable generative models with\n",
      "strong performance.\n",
      "\n",
      "**Paper Id :1912.01899 \n",
      "Title :Distribution-induced Bidirectional Generative Adversarial Network for\n",
      "  Graph Representation Learning\n",
      "  Graph representation learning aims to encode all nodes of a graph into\n",
      "low-dimensional vectors that will serve as input of many compute vision tasks.\n",
      "However, most existing algorithms ignore the existence of inherent data\n",
      "distribution and even noises. This may significantly increase the phenomenon of\n",
      "over-fitting and deteriorate the testing accuracy. In this paper, we propose a\n",
      "Distribution-induced Bidirectional Generative Adversarial Network (named DBGAN)\n",
      "for graph representation learning. Instead of the widely used normal\n",
      "distribution assumption, the prior distribution of latent representation in our\n",
      "DBGAN is estimated in a structure-aware way, which implicitly bridges the graph\n",
      "and feature spaces by prototype learning. Thus discriminative and robust\n",
      "representations are generated for all nodes. Furthermore, to improve their\n",
      "generalization ability while preserving representation ability, the\n",
      "sample-level and distribution-level consistency is well balanced via a\n",
      "bidirectional adversarial learning framework. An extensive group of experiments\n",
      "are then carefully designed and presented, demonstrating that our DBGAN obtains\n",
      "remarkably more favorable trade-off between representation and robustness, and\n",
      "meanwhile is dimension-efficient, over currently available alternatives in\n",
      "various tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.03633 \n",
      "Title :AL2: Progressive Activation Loss for Learning General Representations in\n",
      "  Classification Neural Networks\n",
      "  The large capacity of neural networks enables them to learn complex\n",
      "functions. To avoid overfitting, networks however require a lot of training\n",
      "data that can be expensive and time-consuming to collect. A common practical\n",
      "approach to attenuate overfitting is the use of network regularization\n",
      "techniques. We propose a novel regularization method that progressively\n",
      "penalizes the magnitude of activations during training. The combined activation\n",
      "signals produced by all neurons in a given layer form the representation of the\n",
      "input image in that feature space. We propose to regularize this representation\n",
      "in the last feature layer before classification layers. Our method's effect on\n",
      "generalization is analyzed with label randomization tests and cumulative\n",
      "ablations. Experimental results show the advantages of our approach in\n",
      "comparison with commonly-used regularizers on standard benchmark datasets.\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.03695 \n",
      "Title :Progressive Growing of Neural ODEs\n",
      "  Neural Ordinary Differential Equations (NODEs) have proven to be a powerful\n",
      "modeling tool for approximating (interpolation) and forecasting (extrapolation)\n",
      "irregularly sampled time series data. However, their performance degrades\n",
      "substantially when applied to real-world data, especially long-term data with\n",
      "complex behaviors (e.g., long-term trend across years, mid-term seasonality\n",
      "across months, and short-term local variation across days). To address the\n",
      "modeling of such complex data with different behaviors at different frequencies\n",
      "(time spans), we propose a novel progressive learning paradigm of NODEs for\n",
      "long-term time series forecasting. Specifically, following the principle of\n",
      "curriculum learning, we gradually increase the complexity of data and network\n",
      "capacity as training progresses. Our experiments with both synthetic data and\n",
      "real traffic data (PeMS Bay Area traffic data) show that our training\n",
      "methodology consistently improves the performance of vanilla NODEs by over 64%.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.03776 \n",
      "Title :Nature-Inspired Optimization Algorithms: Challenges and Open Problems\n",
      "  Many problems in science and engineering can be formulated as optimization\n",
      "problems, subject to complex nonlinear constraints. The solutions of highly\n",
      "nonlinear problems usually require sophisticated optimization algorithms, and\n",
      "traditional algorithms may struggle to deal with such problems. A current trend\n",
      "is to use nature-inspired algorithms due to their flexibility and\n",
      "effectiveness. However, there are some key issues concerning nature-inspired\n",
      "computation and swarm intelligence. This paper provides an in-depth review of\n",
      "some recent nature-inspired algorithms with the emphasis on their search\n",
      "mechanisms and mathematical foundations. Some challenging issues are identified\n",
      "and five open problems are highlighted, concerning the analysis of algorithmic\n",
      "convergence and stability, parameter tuning, mathematical framework, role of\n",
      "benchmarking and scalability. These problems are discussed with the directions\n",
      "for future research.\n",
      "\n",
      "**Paper Id :1812.11794 \n",
      "Title :Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n",
      "  Challenges, Solutions and Applications\n",
      "  Reinforcement learning (RL) algorithms have been around for decades and\n",
      "employed to solve various sequential decision-making problems. These algorithms\n",
      "however have faced great challenges when dealing with high-dimensional\n",
      "environments. The recent development of deep learning has enabled RL methods to\n",
      "drive optimal policies for sophisticated and capable agents, which can perform\n",
      "efficiently in these challenging environments. This paper addresses an\n",
      "important aspect of deep RL related to situations that require multiple agents\n",
      "to communicate and cooperate to solve complex tasks. A survey of different\n",
      "approaches to problems related to multi-agent deep RL (MADRL) is presented,\n",
      "including non-stationarity, partial observability, continuous state and action\n",
      "spaces, multi-agent training schemes, multi-agent transfer learning. The merits\n",
      "and demerits of the reviewed methods will be analyzed and discussed, with their\n",
      "corresponding applications explored. It is envisaged that this review provides\n",
      "insights about various MADRL methods and can lead to future development of more\n",
      "robust and highly useful multi-agent learning methods for solving real-world\n",
      "problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.04108 \n",
      "Title :Stable Policy Optimization via Off-Policy Divergence Regularization\n",
      "  Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization\n",
      "(PPO) are among the most successful policy gradient approaches in deep\n",
      "reinforcement learning (RL). While these methods achieve state-of-the-art\n",
      "performance across a wide range of challenging tasks, there is room for\n",
      "improvement in the stabilization of the policy learning and how the off-policy\n",
      "data are used. In this paper we revisit the theoretical foundations of these\n",
      "algorithms and propose a new algorithm which stabilizes the policy improvement\n",
      "through a proximity term that constrains the discounted state-action visitation\n",
      "distribution induced by consecutive policies to be close to one another. This\n",
      "proximity term, expressed in terms of the divergence between the visitation\n",
      "distributions, is learned in an off-policy and adversarial manner. We\n",
      "empirically show that our proposed method can have a beneficial effect on\n",
      "stability and improve final performance in benchmark high-dimensional control\n",
      "tasks.\n",
      "\n",
      "**Paper Id :1911.01546 \n",
      "Title :Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy\n",
      "  While maximizing expected return is the goal in most reinforcement learning\n",
      "approaches, risk-sensitive objectives such as conditional value at risk (CVaR)\n",
      "are more suitable for many high-stakes applications. However, relatively little\n",
      "is known about how to explore to quickly learn policies with good CVaR. In this\n",
      "paper, we present the first algorithm for sample-efficient learning of\n",
      "CVaR-optimal policies in Markov decision processes based on the optimism in the\n",
      "face of uncertainty principle. This method relies on a novel optimistic version\n",
      "of the distributional Bellman operator that moves probability mass from the\n",
      "lower to the upper tail of the return distribution. We prove asymptotic\n",
      "convergence and optimism of this operator for the tabular policy evaluation\n",
      "case. We further demonstrate that our algorithm finds CVaR-optimal policies\n",
      "substantially faster than existing baselines in several simulated environments\n",
      "with discrete and continuous state spaces.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.04166 \n",
      "Title :Learning entropy production via neural networks\n",
      "  This Letter presents a neural estimator for entropy production, or NEEP, that\n",
      "estimates entropy production (EP) from trajectories of relevant variables\n",
      "without detailed information on the system dynamics. For steady state, we\n",
      "rigorously prove that the estimator, which can be built up from different\n",
      "choices of deep neural networks, provides stochastic EP by optimizing the\n",
      "objective function proposed here. We verify the NEEP with the stochastic\n",
      "processes of the bead-spring and discrete flashing ratchet models, and also\n",
      "demonstrate that our method is applicable to high-dimensional data and can\n",
      "provide coarse-grained EP for Markov systems with unobservable states.\n",
      "\n",
      "**Paper Id :1910.13496 \n",
      "Title :Asymptotically unbiased estimation of physical observables with neural\n",
      "  samplers\n",
      "  We propose a general framework for the estimation of observables with\n",
      "generative neural samplers focusing on modern deep generative neural networks\n",
      "that provide an exact sampling probability. In this framework, we present\n",
      "asymptotically unbiased estimators for generic observables, including those\n",
      "that explicitly depend on the partition function such as free energy or\n",
      "entropy, and derive corresponding variance estimators. We demonstrate their\n",
      "practical applicability by numerical experiments for the 2d Ising model which\n",
      "highlight the superiority over existing methods. Our approach greatly enhances\n",
      "the applicability of generative neural samplers to real-world physical systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.04296 \n",
      "Title :Propagating Asymptotic-Estimated Gradients for Low Bitwidth Quantized\n",
      "  Neural Networks\n",
      "  The quantized neural networks (QNNs) can be useful for neural network\n",
      "acceleration and compression, but during the training process they pose a\n",
      "challenge: how to propagate the gradient of loss function through the graph\n",
      "flow with a derivative of 0 almost everywhere. In response to this\n",
      "non-differentiable situation, we propose a novel Asymptotic-Quantized Estimator\n",
      "(AQE) to estimate the gradient. In particular, during back-propagation, the\n",
      "graph that relates inputs to output remains smoothness and differentiability.\n",
      "At the end of training, the weights and activations have been quantized to\n",
      "low-precision because of the asymptotic behaviour of AQE. Meanwhile, we propose\n",
      "a M-bit Inputs and N-bit Weights Network (MINW-Net) trained by AQE, a quantized\n",
      "neural network with 1-3 bits weights and activations. In the inference phase,\n",
      "we can use XNOR or SHIFT operations instead of convolution operations to\n",
      "accelerate the MINW-Net. Our experiments on CIFAR datasets demonstrate that our\n",
      "AQE is well defined, and the QNNs with AQE perform better than that with\n",
      "Straight-Through Estimator (STE). For example, in the case of the same ConvNet\n",
      "that has 1-bit weights and activations, our MINW-Net with AQE can achieve a\n",
      "prediction accuracy 1.5\\% higher than the Binarized Neural Network (BNN) with\n",
      "STE. The MINW-Net, which is trained from scratch by AQE, can achieve comparable\n",
      "classification accuracy as 32-bit counterparts on CIFAR test sets. Extensive\n",
      "experimental results on ImageNet dataset show great superiority of the proposed\n",
      "AQE and our MINW-Net achieves comparable results with other state-of-the-art\n",
      "QNNs.\n",
      "\n",
      "**Paper Id :2004.02396 \n",
      "Title :A Learning Framework for n-bit Quantized Neural Networks toward FPGAs\n",
      "  The quantized neural network (QNN) is an efficient approach for network\n",
      "compression and can be widely used in the implementation of FPGAs. This paper\n",
      "proposes a novel learning framework for n-bit QNNs, whose weights are\n",
      "constrained to the power of two. To solve the gradient vanishing problem, we\n",
      "propose a reconstructed gradient function for QNNs in back-propagation\n",
      "algorithm that can directly get the real gradient rather than estimating an\n",
      "approximate gradient of the expected loss. We also propose a novel QNN\n",
      "structure named n-BQ-NN, which uses shift operation to replace the multiply\n",
      "operation and is more suitable for the inference on FPGAs. Furthermore, we also\n",
      "design a shift vector processing element (SVPE) array to replace all 16-bit\n",
      "multiplications with SHIFT operations in convolution operation on FPGAs. We\n",
      "also carry out comparable experiments to evaluate our framework. The\n",
      "experimental results show that the quantized models of ResNet, DenseNet and\n",
      "AlexNet through our learning framework can achieve almost the same accuracies\n",
      "with the original full-precision models. Moreover, when using our learning\n",
      "framework to train our n-BQ-NN from scratch, it can achieve state-of-the-art\n",
      "results compared with typical low-precision QNNs. Experiments on Xilinx ZCU102\n",
      "platform show that our n-BQ-NN with our SVPE can execute 2.9 times faster than\n",
      "with the vector processing element (VPE) in inference. As the SHIFT operation\n",
      "in our SVPE array will not consume Digital Signal Processings (DSPs) resources\n",
      "on FPGAs, the experiments have shown that the use of SVPE array also reduces\n",
      "average energy consumption to 68.7% of the VPE array with 16-bit.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.04299 \n",
      "Title :Discovering Symmetry Invariants and Conserved Quantities by Interpreting\n",
      "  Siamese Neural Networks\n",
      "  In this paper, we introduce interpretable Siamese Neural Networks (SNN) for\n",
      "similarity detection to the field of theoretical physics. More precisely, we\n",
      "apply SNNs to events in special relativity, the transformation of\n",
      "electromagnetic fields, and the motion of particles in a central potential. In\n",
      "these examples, the SNNs learn to identify datapoints belonging to the same\n",
      "events, field configurations, or trajectory of motion. It turns out that in the\n",
      "process of learning which datapoints belong to the same event or field\n",
      "configuration, these SNNs also learn the relevant symmetry invariants and\n",
      "conserved quantities. These SNNs are highly interpretable, which enables us to\n",
      "reveal the symmetry invariants and conserved quantities without prior\n",
      "knowledge.\n",
      "\n",
      "**Paper Id :1904.06194 \n",
      "Title :Compressing deep neural networks by matrix product operators\n",
      "  A deep neural network is a parametrization of a multilayer mapping of signals\n",
      "in terms of many alternatively arranged linear and nonlinear transformations.\n",
      "The linear transformations, which are generally used in the fully connected as\n",
      "well as convolutional layers, contain most of the variational parameters that\n",
      "are trained and stored. Compressing a deep neural network to reduce its number\n",
      "of variational parameters but not its prediction power is an important but\n",
      "challenging problem toward the establishment of an optimized scheme in training\n",
      "efficiently these parameters and in lowering the risk of overfitting. Here we\n",
      "show that this problem can be effectively solved by representing linear\n",
      "transformations with matrix product operators (MPOs), which is a tensor network\n",
      "originally proposed in physics to characterize the short-range entanglement in\n",
      "one-dimensional quantum states. We have tested this approach in five typical\n",
      "neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\n",
      "widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\n",
      "representation indeed sets up a faithful and efficient mapping between input\n",
      "and output signals, which can keep or even improve the prediction accuracy with\n",
      "a dramatically reduced number of parameters. Our method greatly simplifies the\n",
      "representations in deep learning, and opens a possible route toward\n",
      "establishing a framework of modern neural networks which might be simpler and\n",
      "cheaper, but more efficient.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.04470 \n",
      "Title :Data Warehouse and Decision Support on Integrated Crop Big Data\n",
      "  In recent years, precision agriculture is becoming very popular. The\n",
      "introduction of modern information and communication technologies for\n",
      "collecting and processing Agricultural data revolutionise the agriculture\n",
      "practises. This has started a while ago (early 20th century) and it is driven\n",
      "by the low cost of collecting data about everything; from information on fields\n",
      "such as seed, soil, fertiliser, pest, to weather data, drones and satellites\n",
      "images. Specially, the agricultural data mining today is considered as Big Data\n",
      "application in terms of volume, variety, velocity and veracity. Hence it leads\n",
      "to challenges in processing vast amounts of complex and diverse information to\n",
      "extract useful knowledge for the farmer, agronomist, and other businesses. It\n",
      "is a key foundation to establishing a crop intelligence platform, which will\n",
      "enable efficient resource management and high quality agronomy decision making\n",
      "and recommendations. In this paper, we designed and implemented a continental\n",
      "level agricultural data warehouse (ADW). ADW is characterised by its (1)\n",
      "flexible schema; (2) data integration from real agricultural multi datasets;\n",
      "(3) data science and business intelligent support; (4) high performance; (5)\n",
      "high storage; (6) security; (7) governance and monitoring; (8) consistency,\n",
      "availability and partition tolerant; (9) cloud compatibility. We also evaluate\n",
      "the performance of ADW and present some complex queries to extract and return\n",
      "necessary knowledge about crop management.\n",
      "\n",
      "**Paper Id :2003.05043 \n",
      "Title :Crop Knowledge Discovery Based on Agricultural Big Data Integration\n",
      "  Nowadays, the agricultural data can be generated through various sources,\n",
      "such as: Internet of Thing (IoT), sensors, satellites, weather stations,\n",
      "robots, farm equipment, agricultural laboratories, farmers, government agencies\n",
      "and agribusinesses. The analysis of this big data enables farmers, companies\n",
      "and agronomists to extract high business and scientific knowledge, improving\n",
      "their operational processes and product quality. However, before analysing this\n",
      "data, different data sources need to be normalised, homogenised and integrated\n",
      "into a unified data representation. In this paper, we propose an agricultural\n",
      "data integration method using a constellation schema which is designed to be\n",
      "flexible enough to incorporate other datasets and big data models. We also\n",
      "apply some methods to extract knowledge with the view to improve crop yield;\n",
      "these include finding suitable quantities of soil properties, herbicides and\n",
      "insecticides for both increasing crop yield and protecting the environment.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.04960 \n",
      "Title :Curriculum Learning for Reinforcement Learning Domains: A Framework and\n",
      "  Survey\n",
      "  Reinforcement learning (RL) is a popular paradigm for addressing sequential\n",
      "decision tasks in which the agent has only limited environmental feedback.\n",
      "Despite many advances over the past three decades, learning in many domains\n",
      "still requires a large amount of interaction with the environment, which can be\n",
      "prohibitively expensive in realistic scenarios. To address this problem,\n",
      "transfer learning has been applied to reinforcement learning such that\n",
      "experience gained in one task can be leveraged when starting to learn the next,\n",
      "harder task. More recently, several lines of research have explored how tasks,\n",
      "or data samples themselves, can be sequenced into a curriculum for the purpose\n",
      "of learning a problem that may otherwise be too difficult to learn from\n",
      "scratch. In this article, we present a framework for curriculum learning (CL)\n",
      "in reinforcement learning, and use it to survey and classify existing CL\n",
      "methods in terms of their assumptions, capabilities, and goals. Finally, we use\n",
      "our framework to find open problems and suggest directions for future RL\n",
      "curriculum learning research.\n",
      "\n",
      "**Paper Id :2002.06306 \n",
      "Title :Jelly Bean World: A Testbed for Never-Ending Learning\n",
      "  Machine learning has shown growing success in recent years. However, current\n",
      "machine learning systems are highly specialized, trained for particular\n",
      "problems or domains, and typically on a single narrow dataset. Human learning,\n",
      "on the other hand, is highly general and adaptable. Never-ending learning is a\n",
      "machine learning paradigm that aims to bridge this gap, with the goal of\n",
      "encouraging researchers to design machine learning systems that can learn to\n",
      "perform a wider variety of inter-related tasks in more complex environments. To\n",
      "date, there is no environment or testbed to facilitate the development and\n",
      "evaluation of never-ending learning systems. To this end, we propose the Jelly\n",
      "Bean World testbed. The Jelly Bean World allows experimentation over\n",
      "two-dimensional grid worlds which are filled with items and in which agents can\n",
      "navigate. This testbed provides environments that are sufficiently complex and\n",
      "where more generally intelligent algorithms ought to perform better than\n",
      "current state-of-the-art reinforcement learning approaches. It does so by\n",
      "producing non-stationary environments and facilitating experimentation with\n",
      "multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope\n",
      "that this new freely-available software will prompt new research and interest\n",
      "in the development and evaluation of never-ending learning systems and more\n",
      "broadly, general intelligence systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.05043 \n",
      "Title :Crop Knowledge Discovery Based on Agricultural Big Data Integration\n",
      "  Nowadays, the agricultural data can be generated through various sources,\n",
      "such as: Internet of Thing (IoT), sensors, satellites, weather stations,\n",
      "robots, farm equipment, agricultural laboratories, farmers, government agencies\n",
      "and agribusinesses. The analysis of this big data enables farmers, companies\n",
      "and agronomists to extract high business and scientific knowledge, improving\n",
      "their operational processes and product quality. However, before analysing this\n",
      "data, different data sources need to be normalised, homogenised and integrated\n",
      "into a unified data representation. In this paper, we propose an agricultural\n",
      "data integration method using a constellation schema which is designed to be\n",
      "flexible enough to incorporate other datasets and big data models. We also\n",
      "apply some methods to extract knowledge with the view to improve crop yield;\n",
      "these include finding suitable quantities of soil properties, herbicides and\n",
      "insecticides for both increasing crop yield and protecting the environment.\n",
      "\n",
      "**Paper Id :2003.04470 \n",
      "Title :Data Warehouse and Decision Support on Integrated Crop Big Data\n",
      "  In recent years, precision agriculture is becoming very popular. The\n",
      "introduction of modern information and communication technologies for\n",
      "collecting and processing Agricultural data revolutionise the agriculture\n",
      "practises. This has started a while ago (early 20th century) and it is driven\n",
      "by the low cost of collecting data about everything; from information on fields\n",
      "such as seed, soil, fertiliser, pest, to weather data, drones and satellites\n",
      "images. Specially, the agricultural data mining today is considered as Big Data\n",
      "application in terms of volume, variety, velocity and veracity. Hence it leads\n",
      "to challenges in processing vast amounts of complex and diverse information to\n",
      "extract useful knowledge for the farmer, agronomist, and other businesses. It\n",
      "is a key foundation to establishing a crop intelligence platform, which will\n",
      "enable efficient resource management and high quality agronomy decision making\n",
      "and recommendations. In this paper, we designed and implemented a continental\n",
      "level agricultural data warehouse (ADW). ADW is characterised by its (1)\n",
      "flexible schema; (2) data integration from real agricultural multi datasets;\n",
      "(3) data science and business intelligent support; (4) high performance; (5)\n",
      "high storage; (6) security; (7) governance and monitoring; (8) consistency,\n",
      "availability and partition tolerant; (9) cloud compatibility. We also evaluate\n",
      "the performance of ADW and present some complex queries to extract and return\n",
      "necessary knowledge about crop management.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.05078 \n",
      "Title :Visual Grounding in Video for Unsupervised Word Translation\n",
      "  There are thousands of actively spoken languages on Earth, but a single\n",
      "visual world. Grounding in this visual world has the potential to bridge the\n",
      "gap between all these languages. Our goal is to use visual grounding to improve\n",
      "unsupervised word mapping between languages. The key idea is to establish a\n",
      "common visual representation between two languages by learning embeddings from\n",
      "unpaired instructional videos narrated in the native language. Given this\n",
      "shared embedding we demonstrate that (i) we can map words between the\n",
      "languages, particularly the 'visual' words; (ii) that the shared embedding\n",
      "provides a good initialization for existing unsupervised text-based word\n",
      "translation techniques, forming the basis for our proposed hybrid visual-text\n",
      "mapping algorithm, MUVE; and (iii) our approach achieves superior performance\n",
      "by addressing the shortcomings of text-based methods -- it is more robust,\n",
      "handles datasets with less commonality, and is applicable to low-resource\n",
      "languages. We apply these methods to translate words from English to French,\n",
      "Korean, and Japanese -- all without any parallel corpora and simply by watching\n",
      "many videos of people speaking while doing things.\n",
      "\n",
      "**Paper Id :2010.11727 \n",
      "Title :Vision-Based Layout Detection from Scientific Literature using Recurrent\n",
      "  Convolutional Neural Networks\n",
      "  We present an approach for adapting convolutional neural networks for object\n",
      "recognition and classification to scientific literature layout detection\n",
      "(SLLD), a shared subtask of several information extraction problems. Scientific\n",
      "publications contain multiple types of information sought by researchers in\n",
      "various disciplines, organized into an abstract, bibliography, and sections\n",
      "documenting related work, experimental methods, and results; however, there is\n",
      "no effective way to extract this information due to their diverse layout. In\n",
      "this paper, we present a novel approach to developing an end-to-end learning\n",
      "framework to segment and classify major regions of a scientific document. We\n",
      "consider scientific document layout analysis as an object detection task over\n",
      "digital images, without any additional text features that need to be added into\n",
      "the network during the training process. Our technical objective is to\n",
      "implement transfer learning via fine-tuning of pre-trained networks and thereby\n",
      "demonstrate that this deep learning architecture is suitable for tasks that\n",
      "lack very large document corpora for training ab initio. As part of the\n",
      "experimental test bed for empirical evaluation of this approach, we created a\n",
      "merged multi-corpus data set for scientific publication layout detection tasks.\n",
      "Our results show good improvement with fine-tuning of a pre-trained base\n",
      "network using this merged data set, compared to the baseline convolutional\n",
      "neural network architecture.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.05189 \n",
      "Title :Convolutional Kernel Networks for Graph-Structured Data\n",
      "  We introduce a family of multilayer graph kernels and establish new links\n",
      "between graph convolutional neural networks and kernel methods. Our approach\n",
      "generalizes convolutional kernel networks to graph-structured data, by\n",
      "representing graphs as a sequence of kernel feature maps, where each node\n",
      "carries information about local graph substructures. On the one hand, the\n",
      "kernel point of view offers an unsupervised, expressive, and easy-to-regularize\n",
      "data representation, which is useful when limited samples are available. On the\n",
      "other hand, our model can also be trained end-to-end on large-scale data,\n",
      "leading to new types of graph convolutional neural networks. We show that our\n",
      "method achieves competitive performance on several graph classification\n",
      "benchmarks, while offering simple model interpretation. Our code is freely\n",
      "available at https://github.com/claying/GCKN.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.05353 \n",
      "Title :Majorization Minimization Methods to Distributed Pose Graph Optimization\n",
      "  with Convergence Guarantees\n",
      "  In this paper, we consider the problem of distributed pose graph optimization\n",
      "(PGO) that has extensive applications in multi-robot simultaneous localization\n",
      "and mapping (SLAM). We propose majorization minimization methods to distributed\n",
      "PGO and show that our proposed methods are guaranteed to converge to\n",
      "first-order critical points under mild conditions. Furthermore, since our\n",
      "proposed methods rely a proximal operator of distributed PGO, the convergence\n",
      "rate can be significantly accelerated with Nesterov's method, and more\n",
      "importantly, the acceleration induces no compromise of theoretical guarantees.\n",
      "In addition, we also present accelerated majorization minimization methods to\n",
      "the distributed chordal initialization that have a quadratic convergence, which\n",
      "can be used to compute an initial guess for distributed PGO. The efficacy of\n",
      "this work is validated through applications on a number of 2D and 3D SLAM\n",
      "datasets and comparisons with existing state-of-the-art methods, which\n",
      "indicates that our proposed methods have faster convergence and result in\n",
      "better solutions to distributed PGO.\n",
      "\n",
      "**Paper Id :2006.14117 \n",
      "Title :Fast Learning of Graph Neural Networks with Guaranteed Generalizability:\n",
      "  One-hidden-layer Case\n",
      "  Although graph neural networks (GNNs) have made great progress recently on\n",
      "learning from graph-structured data in practice, their theoretical guarantee on\n",
      "generalizability remains elusive in the literature. In this paper, we provide a\n",
      "theoretically-grounded generalizability analysis of GNNs with one hidden layer\n",
      "for both regression and binary classification problems. Under the assumption\n",
      "that there exists a ground-truth GNN model (with zero generalization error),\n",
      "the objective of GNN learning is to estimate the ground-truth GNN parameters\n",
      "from the training data. To achieve this objective, we propose a learning\n",
      "algorithm that is built on tensor initialization and accelerated gradient\n",
      "descent. We then show that the proposed learning algorithm converges to the\n",
      "ground-truth GNN model for the regression problem, and to a model sufficiently\n",
      "close to the ground-truth for the binary classification problem. Moreover, for\n",
      "both cases, the convergence rate of the proposed learning algorithm is proven\n",
      "to be linear and faster than the vanilla gradient descent algorithm. We further\n",
      "explore the relationship between the sample complexity of GNNs and their\n",
      "underlying graph properties. Lastly, we provide numerical experiments to\n",
      "demonstrate the validity of our analysis and the effectiveness of the proposed\n",
      "learning algorithm for GNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.05856 \n",
      "Title :Online Fast Adaptation and Knowledge Accumulation: a New Approach to\n",
      "  Continual Learning\n",
      "  Continual learning studies agents that learn from streams of tasks without\n",
      "forgetting previous ones while adapting to new ones. Two recent\n",
      "continual-learning scenarios have opened new avenues of research. In\n",
      "meta-continual learning, the model is pre-trained to minimize catastrophic\n",
      "forgetting of previous tasks. In continual-meta learning, the aim is to train\n",
      "agents for faster remembering of previous tasks through adaptation. In their\n",
      "original formulations, both methods have limitations. We stand on their\n",
      "shoulders to propose a more general scenario, OSAKA, where an agent must\n",
      "quickly solve new (out-of-distribution) tasks, while also requiring fast\n",
      "remembering. We show that current continual learning, meta-learning,\n",
      "meta-continual learning, and continual-meta learning techniques fail in this\n",
      "new scenario. We propose Continual-MAML, an online extension of the popular\n",
      "MAML algorithm as a strong baseline for this scenario. We empirically show that\n",
      "Continual-MAML is better suited to the new scenario than the aforementioned\n",
      "methodologies, as well as standard continual learning and meta-learning\n",
      "approaches.\n",
      "\n",
      "**Paper Id :2002.08396 \n",
      "Title :Keep Doing What Worked: Behavioral Modelling Priors for Offline\n",
      "  Reinforcement Learning\n",
      "  Off-policy reinforcement learning algorithms promise to be applicable in\n",
      "settings where only a fixed data-set (batch) of environment interactions is\n",
      "available and no new experience can be acquired. This property makes these\n",
      "algorithms appealing for real world problems such as robot control. In\n",
      "practice, however, standard off-policy algorithms fail in the batch setting for\n",
      "continuous control. In this paper, we propose a simple solution to this\n",
      "problem. It admits the use of data generated by arbitrary behavior policies and\n",
      "uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias\n",
      "the RL policy towards actions that have previously been executed and are likely\n",
      "to be successful on the new task. Our method can be seen as an extension of\n",
      "recent work on batch-RL that enables stable learning from conflicting\n",
      "data-sources. We find improvements on competitive baselines in a variety of RL\n",
      "tasks -- including standard continuous control benchmarks and multi-task\n",
      "learning for simulated and real-world robots.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.06005 \n",
      "Title :Model Agnostic Multilevel Explanations\n",
      "  In recent years, post-hoc local instance-level and global dataset-level\n",
      "explainability of black-box models has received a lot of attention. Much less\n",
      "attention has been given to obtaining insights at intermediate or group levels,\n",
      "which is a need outlined in recent works that study the challenges in realizing\n",
      "the guidelines in the General Data Protection Regulation (GDPR). In this paper,\n",
      "we propose a meta-method that, given a typical local explainability method, can\n",
      "build a multilevel explanation tree. The leaves of this tree correspond to the\n",
      "local explanations, the root corresponds to the global explanation, and\n",
      "intermediate levels correspond to explanations for groups of data points that\n",
      "it automatically clusters. The method can also leverage side information, where\n",
      "users can specify points for which they may want the explanations to be\n",
      "similar. We argue that such a multilevel structure can also be an effective\n",
      "form of communication, where one could obtain few explanations that\n",
      "characterize the entire dataset by considering an appropriate level in our\n",
      "explanation tree. Explanations for novel test points can be cost-efficiently\n",
      "obtained by associating them with the closest training points. When the local\n",
      "explainability technique is generalized additive (viz. LIME, GAMs), we develop\n",
      "a fast approximate algorithm for building the multilevel tree and study its\n",
      "convergence behavior. We validate the effectiveness of the proposed technique\n",
      "based on two human studies -- one with experts and the other with non-expert\n",
      "users -- on real world datasets, and show that we produce high fidelity sparse\n",
      "explanations on several other public datasets.\n",
      "\n",
      "**Paper Id :2002.08247 \n",
      "Title :Learning Global Transparent Models Consistent with Local Contrastive\n",
      "  Explanations\n",
      "  There is a rich and growing literature on producing local\n",
      "contrastive/counterfactual explanations for black-box models (e.g. neural\n",
      "networks).\n",
      "  In these methods, for an input, an explanation is in the form of a contrast\n",
      "point differing in very few features from the original input and lying in a\n",
      "different class. Other works try to build globally interpretable models like\n",
      "decision trees and rule lists based on the data using actual labels or based on\n",
      "the black-box models predictions. Although these interpretable global models\n",
      "can be useful, they may not be consistent with local explanations from a\n",
      "specific black-box of choice. In this work, we explore the question: Can we\n",
      "produce a transparent global model that is simultaneously accurate and\n",
      "consistent with the local (contrastive) explanations of the black-box model? We\n",
      "introduce a natural local consistency metric that quantifies if the local\n",
      "explanations and predictions of the black-box model are also consistent with\n",
      "the proxy global transparent model. Based on a key insight we propose a novel\n",
      "method where we create custom boolean features from sparse local contrastive\n",
      "explanations of the black-box model and then train a globally transparent model\n",
      "on just these, and showcase empirically that such models have higher local\n",
      "consistency compared with other known strategies, while still being close in\n",
      "performance to models that are trained with access to the original data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.06221 \n",
      "Title :Semantic Pyramid for Image Generation\n",
      "  We present a novel GAN-based model that utilizes the space of deep features\n",
      "learned by a pre-trained classification model. Inspired by classical image\n",
      "pyramid representations, we construct our model as a Semantic Generation\n",
      "Pyramid -- a hierarchical framework which leverages the continuum of semantic\n",
      "information encapsulated in such deep features; this ranges from low level\n",
      "information contained in fine features to high level, semantic information\n",
      "contained in deeper features. More specifically, given a set of features\n",
      "extracted from a reference image, our model generates diverse image samples,\n",
      "each with matching features at each semantic level of the classification model.\n",
      "We demonstrate that our model results in a versatile and flexible framework\n",
      "that can be used in various classic and novel image generation tasks. These\n",
      "include: generating images with a controllable extent of semantic similarity to\n",
      "a reference image, and different manipulation tasks such as\n",
      "semantically-controlled inpainting and compositing; all achieved with the same\n",
      "model, with no further training.\n",
      "\n",
      "**Paper Id :1910.08978 \n",
      "Title :Attention Enriched Deep Learning Model for Breast Tumor Segmentation in\n",
      "  Ultrasound Images\n",
      "  Incorporating human domain knowledge for breast tumor diagnosis is\n",
      "challenging, since shape, boundary, curvature, intensity, or other common\n",
      "medical priors vary significantly across patients and cannot be employed. This\n",
      "work proposes a new approach for integrating visual saliency into a deep\n",
      "learning model for breast tumor segmentation in ultrasound images. Visual\n",
      "saliency refers to image maps containing regions that are more likely to\n",
      "attract radiologists visual attention. The proposed approach introduces\n",
      "attention blocks into a U-Net architecture, and learns feature representations\n",
      "that prioritize spatial regions with high saliency levels. The validation\n",
      "results demonstrate increased accuracy for tumor segmentation relative to\n",
      "models without salient attention layers. The approach achieved a Dice\n",
      "similarity coefficient of 90.5 percent on a dataset of 510 images. The salient\n",
      "attention model has potential to enhance accuracy and robustness in processing\n",
      "medical images of other organs, by providing a means to incorporate\n",
      "task-specific knowledge into deep learning architectures.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.06413 \n",
      "Title :Equivariant flow-based sampling for lattice gauge theory\n",
      "  We define a class of machine-learned flow-based sampling algorithms for\n",
      "lattice gauge theories that are gauge-invariant by construction. We demonstrate\n",
      "the application of this framework to U(1) gauge theory in two spacetime\n",
      "dimensions, and find that near critical points in parameter space the approach\n",
      "is orders of magnitude more efficient at sampling topological quantities than\n",
      "more traditional sampling procedures such as Hybrid Monte Carlo and Heat Bath.\n",
      "\n",
      "**Paper Id :1910.13496 \n",
      "Title :Asymptotically unbiased estimation of physical observables with neural\n",
      "  samplers\n",
      "  We propose a general framework for the estimation of observables with\n",
      "generative neural samplers focusing on modern deep generative neural networks\n",
      "that provide an exact sampling probability. In this framework, we present\n",
      "asymptotically unbiased estimators for generic observables, including those\n",
      "that explicitly depend on the partition function such as free energy or\n",
      "entropy, and derive corresponding variance estimators. We demonstrate their\n",
      "practical applicability by numerical experiments for the 2d Ising model which\n",
      "highlight the superiority over existing methods. Our approach greatly enhances\n",
      "the applicability of generative neural samplers to real-world physical systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.06516 \n",
      "Title :Deep Representation Learning of Electronic Health Records to Unlock\n",
      "  Patient Stratification at Scale\n",
      "  Deriving disease subtypes from electronic health records (EHRs) can guide\n",
      "next-generation personalized medicine. However, challenges in summarizing and\n",
      "representing patient data prevent widespread practice of scalable EHR-based\n",
      "stratification analysis. Here we present an unsupervised framework based on\n",
      "deep learning to process heterogeneous EHRs and derive patient representations\n",
      "that can efficiently and effectively enable patient stratification at scale. We\n",
      "considered EHRs of 1,608,741 patients from a diverse hospital cohort comprising\n",
      "of a total of 57,464 clinical concepts. We introduce a representation learning\n",
      "model based on word embeddings, convolutional neural networks, and autoencoders\n",
      "(i.e., ConvAE) to transform patient trajectories into low-dimensional latent\n",
      "vectors. We evaluated these representations as broadly enabling patient\n",
      "stratification by applying hierarchical clustering to different multi-disease\n",
      "and disease-specific patient cohorts. ConvAE significantly outperformed several\n",
      "baselines in a clustering task to identify patients with different complex\n",
      "conditions, with 2.61 entropy and 0.31 purity average scores. When applied to\n",
      "stratify patients within a certain condition, ConvAE led to various clinically\n",
      "relevant subtypes for different disorders, including type 2 diabetes,\n",
      "Parkinson's disease and Alzheimer's disease, largely related to comorbidities,\n",
      "disease progression, and symptom severity. With these results, we demonstrate\n",
      "that ConvAE can generate patient representations that lead to clinically\n",
      "meaningful insights. This scalable framework can help better understand varying\n",
      "etiologies in heterogeneous sub-populations and unlock patterns for EHR-based\n",
      "research in the realm of personalized medicine.\n",
      "\n",
      "**Paper Id :1912.03366 \n",
      "Title :Med2Meta: Learning Representations of Medical Concepts with\n",
      "  Meta-Embeddings\n",
      "  Distributed representations of medical concepts have been used to support\n",
      "downstream clinical tasks recently. Electronic Health Records (EHR) capture\n",
      "different aspects of patients' hospital encounters and serve as a rich source\n",
      "for augmenting clinical decision making by learning robust medical concept\n",
      "embeddings. However, the same medical concept can be recorded in different\n",
      "modalities (e.g., clinical notes, lab results)-with each capturing salient\n",
      "information unique to that modality-and a holistic representation calls for\n",
      "relevant feature ensemble from all information sources. We hypothesize that\n",
      "representations learned from heterogeneous data types would lead to performance\n",
      "enhancement on various clinical informatics and predictive modeling tasks. To\n",
      "this end, our proposed approach makes use of meta-embeddings, embeddings\n",
      "aggregated from learned embeddings. Firstly, modality-specific embeddings for\n",
      "each medical concept is learned with graph autoencoders. The ensemble of all\n",
      "the embeddings is then modeled as a meta-embedding learning problem to\n",
      "incorporate their correlating and complementary information through a joint\n",
      "reconstruction. Empirical results of our model on both quantitative and\n",
      "qualitative clinical evaluations have shown improvements over state-of-the-art\n",
      "embedding models, thus validating our hypothesis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.07070 \n",
      "Title :Merge-split Markov chain Monte Carlo for community detection\n",
      "  We present a Markov chain Monte Carlo scheme based on merges and splits of\n",
      "groups that is capable of efficiently sampling from the posterior distribution\n",
      "of network partitions, defined according to the stochastic block model (SBM).\n",
      "We demonstrate how schemes based on the move of single nodes between groups\n",
      "systematically fail at correctly sampling from the posterior distribution even\n",
      "on small networks, and how our merge-split approach behaves significantly\n",
      "better, and improves the mixing time of the Markov chain by several orders of\n",
      "magnitude in typical cases. We also show how the scheme can be\n",
      "straightforwardly extended to nested versions of the SBM, yielding\n",
      "asymptotically exact samples of hierarchical network partitions.\n",
      "\n",
      "**Paper Id :2001.05591 \n",
      "Title :Distributed, partially collapsed MCMC for Bayesian Nonparametrics\n",
      "  Bayesian nonparametric (BNP) models provide elegant methods for discovering\n",
      "underlying latent features within a data set, but inference in such models can\n",
      "be slow. We exploit the fact that completely random measures, which commonly\n",
      "used models like the Dirichlet process and the beta-Bernoulli process can be\n",
      "expressed as, are decomposable into independent sub-measures. We use this\n",
      "decomposition to partition the latent measure into a finite measure containing\n",
      "only instantiated components, and an infinite measure containing all other\n",
      "components. We then select different inference algorithms for the two\n",
      "components: uncollapsed samplers mix well on the finite measure, while\n",
      "collapsed samplers mix well on the infinite, sparsely occupied tail. The\n",
      "resulting hybrid algorithm can be applied to a wide class of models, and can be\n",
      "easily distributed to allow scalable inference without sacrificing asymptotic\n",
      "convergence guarantees.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.07406 \n",
      "Title :Neighborhood-based Pooling for Population-level Label Distribution\n",
      "  Learning\n",
      "  Supervised machine learning often requires human-annotated data. While\n",
      "annotator disagreement is typically interpreted as evidence of noise,\n",
      "population-level label distribution learning (PLDL) treats the collection of\n",
      "annotations for each data item as a sample of the opinions of a population of\n",
      "human annotators, among whom disagreement may be proper and expected, even with\n",
      "no noise present. From this perspective, a typical training set may contain a\n",
      "large number of very small-sized samples, one for each data item, none of\n",
      "which, by itself, is large enough to be considered representative of the\n",
      "underlying population's beliefs about that item. We propose an algorithmic\n",
      "framework and new statistical tests for PLDL that account for sampling size. We\n",
      "apply them to previously proposed methods for sharing labels across similar\n",
      "data items. We also propose new approaches for label sharing, which we call\n",
      "neighborhood-based pooling.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.07756 \n",
      "Title :Characterizing and Avoiding Problematic Global Optima of Variational\n",
      "  Autoencoders\n",
      "  Variational Auto-encoders (VAEs) are deep generative latent variable models\n",
      "consisting of two components: a generative model that captures a data\n",
      "distribution p(x) by transforming a distribution p(z) over latent space, and an\n",
      "inference model that infers likely latent codes for each data point (Kingma and\n",
      "Welling, 2013). Recent work shows that traditional training methods tend to\n",
      "yield solutions that violate modeling desiderata: (1) the learned generative\n",
      "model captures the observed data distribution but does so while ignoring the\n",
      "latent codes, resulting in codes that do not represent the data (e.g. van den\n",
      "Oord et al. (2017); Kim et al. (2018)); (2) the aggregate of the learned latent\n",
      "codes does not match the prior p(z). This mismatch means that the learned\n",
      "generative model will be unable to generate realistic data with samples from\n",
      "p(z)(e.g. Makhzani et al. (2015); Tomczak and Welling (2017)). In this paper,\n",
      "we demonstrate that both issues stem from the fact that the global optima of\n",
      "the VAE training objective often correspond to undesirable solutions. Our\n",
      "analysis builds on two observations: (1) the generative model is unidentifiable\n",
      "- there exist many generative models that explain the data equally well, each\n",
      "with different (and potentially unwanted) properties and (2) bias in the VAE\n",
      "objective - the VAE objective may prefer generative models that explain the\n",
      "data poorly but have posteriors that are easy to approximate. We present a\n",
      "novel inference method, LiBI, mitigating the problems identified in our\n",
      "analysis. On synthetic datasets, we show that LiBI can learn generative models\n",
      "that capture the data distribution and inference models that better satisfy\n",
      "modeling assumptions when traditional methods struggle to do so.\n",
      "\n",
      "**Paper Id :2002.07217 \n",
      "Title :Decision-Making with Auto-Encoding Variational Bayes\n",
      "  To make decisions based on a model fit with auto-encoding variational Bayes\n",
      "(AEVB), practitioners often let the variational distribution serve as a\n",
      "surrogate for the posterior distribution. This approach yields biased estimates\n",
      "of the expected risk, and therefore leads to poor decisions for two reasons.\n",
      "First, the model fit with AEVB may not equal the underlying data distribution.\n",
      "Second, the variational distribution may not equal the posterior distribution\n",
      "under the fitted model. We explore how fitting the variational distribution\n",
      "based on several objective functions other than the ELBO, while continuing to\n",
      "fit the generative model based on the ELBO, affects the quality of downstream\n",
      "decisions. For the probabilistic principal component analysis model, we\n",
      "investigate how importance sampling error, as well as the bias of the model\n",
      "parameter estimates, varies across several approximate posteriors when used as\n",
      "proposal distributions. Our theoretical results suggest that a posterior\n",
      "approximation distinct from the variational distribution should be used for\n",
      "making decisions. Motivated by these theoretical results, we propose learning\n",
      "several approximate proposals for the best model and combining them using\n",
      "multiple importance sampling for decision-making. In addition to toy examples,\n",
      "we present a full-fledged case study of single-cell RNA sequencing. In this\n",
      "challenging instance of multiple hypothesis testing, our proposed approach\n",
      "surpasses the current state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.07959 \n",
      "Title :Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks\n",
      "  (Extended Version)\n",
      "  Verifying real-world programs often requires inferring loop invariants with\n",
      "nonlinear constraints. This is especially true in programs that perform many\n",
      "numerical operations, such as control systems for avionics or industrial\n",
      "plants. Recently, data-driven methods for loop invariant inference have shown\n",
      "promise, especially on linear invariants. However, applying data-driven\n",
      "inference to nonlinear loop invariants is challenging due to the large numbers\n",
      "of and magnitudes of high-order terms, the potential for overfitting on a small\n",
      "number of samples, and the large space of possible inequality bounds.\n",
      "  In this paper, we introduce a new neural architecture for general SMT\n",
      "learning, the Gated Continuous Logic Network (G-CLN), and apply it to nonlinear\n",
      "loop invariant learning. G-CLNs extend the Continuous Logic Network (CLN)\n",
      "architecture with gating units and dropout, which allow the model to robustly\n",
      "learn general invariants over large numbers of terms. To address overfitting\n",
      "that arises from finite program sampling, we introduce fractional sampling---a\n",
      "sound relaxation of loop semantics to continuous functions that facilitates\n",
      "unbounded sampling on real domain. We additionally design a new CLN activation\n",
      "function, the Piecewise Biased Quadratic Unit (PBQU), for naturally learning\n",
      "tight inequality bounds.\n",
      "  We incorporate these methods into a nonlinear loop invariant inference system\n",
      "that can learn general nonlinear loop invariants. We evaluate our system on a\n",
      "benchmark of nonlinear loop invariants and show it solves 26 out of 27\n",
      "problems, 3 more than prior work, with an average runtime of 53.3 seconds. We\n",
      "further demonstrate the generic learning ability of G-CLNs by solving all 124\n",
      "problems in the linear Code2Inv benchmark. We also perform a quantitative\n",
      "stability evaluation and show G-CLNs have a convergence rate of $97.5\\%$ on\n",
      "quadratic problems, a $39.2\\%$ improvement over CLN models.\n",
      "\n",
      "**Paper Id :1907.06011 \n",
      "Title :Extracting Interpretable Physical Parameters from Spatiotemporal Systems\n",
      "  using Unsupervised Learning\n",
      "  Experimental data is often affected by uncontrolled variables that make\n",
      "analysis and interpretation difficult. For spatiotemporal systems, this problem\n",
      "is further exacerbated by their intricate dynamics. Modern machine learning\n",
      "methods are particularly well-suited for analyzing and modeling complex\n",
      "datasets, but to be effective in science, the result needs to be interpretable.\n",
      "We demonstrate an unsupervised learning technique for extracting interpretable\n",
      "physical parameters from noisy spatiotemporal data and for building a\n",
      "transferable model of the system. In particular, we implement a\n",
      "physics-informed architecture based on variational autoencoders that is\n",
      "designed for analyzing systems governed by partial differential equations\n",
      "(PDEs). The architecture is trained end-to-end and extracts latent parameters\n",
      "that parameterize the dynamics of a learned predictive model for the system. To\n",
      "test our method, we train our model on simulated data from a variety of PDEs\n",
      "with varying dynamical parameters that act as uncontrolled variables. Numerical\n",
      "experiments show that our method can accurately identify relevant parameters\n",
      "and extract them from raw and even noisy spatiotemporal data (tested with\n",
      "roughly 10% added noise). These extracted parameters correlate well (linearly\n",
      "with $R^2 > 0.95$) with the ground truth physical parameters used to generate\n",
      "the datasets. We then apply this method to nonlinear fiber propagation data,\n",
      "generated by an ab-initio simulation, to demonstrate its capabilities on a more\n",
      "realistic dataset. Our method for discovering interpretable latent parameters\n",
      "in spatiotemporal systems will allow us to better analyze and understand\n",
      "real-world phenomena and datasets, which often have unknown and uncontrolled\n",
      "variables that alter the system dynamics and cause varying behaviors that are\n",
      "difficult to disentangle.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.08225 \n",
      "Title :Detecting Replay Attacks Using Multi-Channel Audio: A Neural\n",
      "  Network-Based Method\n",
      "  With the rapidly growing number of security-sensitive systems that use voice\n",
      "as the primary input, it becomes increasingly important to address these\n",
      "systems' potential vulnerability to replay attacks. Previous efforts to address\n",
      "this concern have focused primarily on single-channel audio. In this paper, we\n",
      "introduce a novel neural network-based replay attack detection model that\n",
      "further leverages spatial information of multi-channel audio and is able to\n",
      "significantly improve the replay attack detection performance.\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.08394 \n",
      "Title :Convergence of Artificial Intelligence and High Performance Computing on\n",
      "  NSF-supported Cyberinfrastructure\n",
      "  Significant investments to upgrade and construct large-scale scientific\n",
      "facilities demand commensurate investments in R&D to design algorithms and\n",
      "computing approaches to enable scientific and engineering breakthroughs in the\n",
      "big data era. Innovative Artificial Intelligence (AI) applications have powered\n",
      "transformational solutions for big data challenges in industry and technology\n",
      "that now drive a multi-billion dollar industry, and which play an ever\n",
      "increasing role shaping human social patterns. As AI continues to evolve into a\n",
      "computing paradigm endowed with statistical and mathematical rigor, it has\n",
      "become apparent that single-GPU solutions for training, validation, and testing\n",
      "are no longer sufficient for computational grand challenges brought about by\n",
      "scientific facilities that produce data at a rate and volume that outstrip the\n",
      "computing capabilities of available cyberinfrastructure platforms. This\n",
      "realization has been driving the confluence of AI and high performance\n",
      "computing (HPC) to reduce time-to-insight, and to enable a systematic study of\n",
      "domain-inspired AI architectures and optimization schemes to enable data-driven\n",
      "discovery. In this article we present a summary of recent developments in this\n",
      "field, and describe specific advances that authors in this article are\n",
      "spearheading to accelerate and streamline the use of HPC platforms to design\n",
      "and apply accelerated AI algorithms in academia and industry.\n",
      "\n",
      "**Paper Id :2005.06540 \n",
      "Title :Deep Learning for Political Science\n",
      "  Political science, and social science in general, have traditionally been\n",
      "using computational methods to study areas such as voting behavior, policy\n",
      "making, international conflict, and international development. More recently,\n",
      "increasingly available quantities of data are being combined with improved\n",
      "algorithms and affordable computational resources to predict, learn, and\n",
      "discover new insights from data that is large in volume and variety. New\n",
      "developments in the areas of machine learning, deep learning, natural language\n",
      "processing (NLP), and, more generally, artificial intelligence (AI) are opening\n",
      "up new opportunities for testing theories and evaluating the impact of\n",
      "interventions and programs in a more dynamic and effective way. Applications\n",
      "using large volumes of structured and unstructured data are becoming common in\n",
      "government and industry, and increasingly also in social science research. This\n",
      "chapter offers an introduction to such methods drawing examples from political\n",
      "science. Focusing on the areas where the strengths of the methods coincide with\n",
      "challenges in these fields, the chapter first presents an introduction to AI\n",
      "and its core technology - machine learning, with its rapidly developing\n",
      "subfield of deep learning. The discussion of deep neural networks is\n",
      "illustrated with the NLP tasks that are relevant to political science. The\n",
      "latest advances in deep learning methods for NLP are also reviewed, together\n",
      "with their potential for improving information extraction and pattern\n",
      "recognition from political science texts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.08420 \n",
      "Title :Unsupervised Hierarchical Graph Representation Learning by Mutual\n",
      "  Information Maximization\n",
      "  Graph representation learning based on graph neural networks (GNNs) can\n",
      "greatly improve the performance of downstream tasks, such as node and graph\n",
      "classification. However, the general GNN models do not aggregate node\n",
      "information in a hierarchical manner, and can miss key higher-order structural\n",
      "features of many graphs. The hierarchical aggregation also enables the graph\n",
      "representations to be explainable. In addition, supervised graph representation\n",
      "learning requires labeled data, which is expensive and error-prone. To address\n",
      "these issues, we present an unsupervised graph representation learning method,\n",
      "Unsupervised Hierarchical Graph Representation (UHGR), which can generate\n",
      "hierarchical representations of graphs. Our method focuses on maximizing mutual\n",
      "information between \"local\" and high-level \"global\" representations, which\n",
      "enables us to learn the node embeddings and graph embeddings without any\n",
      "labeled data. To demonstrate the effectiveness of the proposed method, we\n",
      "perform the node and graph classification using the learned node and graph\n",
      "embeddings. The results show that the proposed method achieves comparable\n",
      "results to state-of-the-art supervised methods on several benchmarks. In\n",
      "addition, our visualization of hierarchical representations indicates that our\n",
      "method can capture meaningful and interpretable clusters.\n",
      "\n",
      "**Paper Id :2007.10467 \n",
      "Title :Second-Order Pooling for Graph Neural Networks\n",
      "  Graph neural networks have achieved great success in learning node\n",
      "representations for graph tasks such as node classification and link\n",
      "prediction. Graph representation learning requires graph pooling to obtain\n",
      "graph representations from node representations. It is challenging to develop\n",
      "graph pooling methods due to the variable sizes and isomorphic structures of\n",
      "graphs. In this work, we propose to use second-order pooling as graph pooling,\n",
      "which naturally solves the above challenges. In addition, compared to existing\n",
      "graph pooling methods, second-order pooling is able to use information from all\n",
      "nodes and collect second-order statistics, making it more powerful. We show\n",
      "that direct use of second-order pooling with graph neural networks leads to\n",
      "practical problems. To overcome these problems, we propose two novel global\n",
      "graph pooling methods based on second-order pooling; namely, bilinear mapping\n",
      "and attentional second-order pooling. In addition, we extend attentional\n",
      "second-order pooling to hierarchical graph pooling for more flexible use in\n",
      "GNNs. We perform thorough experiments on graph classification tasks to\n",
      "demonstrate the effectiveness and superiority of our proposed methods.\n",
      "Experimental results show that our methods improve the performance\n",
      "significantly and consistently.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.08767 \n",
      "Title :A Review of Computational Approaches for Evaluation of Rehabilitation\n",
      "  Exercises\n",
      "  Recent advances in data analytics and computer-aided diagnostics stimulate\n",
      "the vision of patient-centric precision healthcare, where treatment plans are\n",
      "customized based on the health records and needs of every patient. In physical\n",
      "rehabilitation, the progress in machine learning and the advent of affordable\n",
      "and reliable motion capture sensors have been conducive to the development of\n",
      "approaches for automated assessment of patient performance and progress toward\n",
      "functional recovery. The presented study reviews computational approaches for\n",
      "evaluating patient performance in rehabilitation programs using motion capture\n",
      "systems. Such approaches will play an important role in supplementing\n",
      "traditional rehabilitation assessment performed by trained clinicians, and in\n",
      "assisting patients participating in home-based rehabilitation. The reviewed\n",
      "computational methods for exercise evaluation are grouped into three main\n",
      "categories: discrete movement score, rule-based, and template-based approaches.\n",
      "The review places an emphasis on the application of machine learning methods\n",
      "for movement evaluation in rehabilitation. Related work in the literature on\n",
      "data representation, feature engineering, movement segmentation, and scoring\n",
      "functions is presented. The study also reviews existing sensors for capturing\n",
      "rehabilitation movements and provides an informative listing of pertinent\n",
      "benchmark datasets. The significance of this paper is in being the first to\n",
      "provide a comprehensive review of computational methods for evaluation of\n",
      "patient performance in rehabilitation programs.\n",
      "\n",
      "**Paper Id :1901.10435 \n",
      "Title :A Deep Learning Framework for Assessing Physical Rehabilitation\n",
      "  Exercises\n",
      "  Computer-aided assessment of physical rehabilitation entails evaluation of\n",
      "patient performance in completing prescribed rehabilitation exercises, based on\n",
      "processing movement data captured with a sensory system. Despite the essential\n",
      "role of rehabilitation assessment toward improved patient outcomes and reduced\n",
      "healthcare costs, existing approaches lack versatility, robustness, and\n",
      "practical relevance. In this paper, we propose a deep learning-based framework\n",
      "for automated assessment of the quality of physical rehabilitation exercises.\n",
      "The main components of the framework are metrics for quantifying movement\n",
      "performance, scoring functions for mapping the performance metrics into\n",
      "numerical scores of movement quality, and deep neural network models for\n",
      "generating quality scores of input movements via supervised learning. The\n",
      "proposed performance metric is defined based on the log-likelihood of a\n",
      "Gaussian mixture model, and encodes low-dimensional data representation\n",
      "obtained with a deep autoencoder network. The proposed deep spatio-temporal\n",
      "neural network arranges data into temporal pyramids, and exploits the spatial\n",
      "characteristics of human movements by using sub-networks to process joint\n",
      "displacements of individual body parts. The presented framework is validated\n",
      "using a dataset of ten rehabilitation exercises. The significance of this work\n",
      "is that it is the first that implements deep neural networks for assessment of\n",
      "rehabilitation performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.08839 \n",
      "Title :Monotonic Value Function Factorisation for Deep Multi-Agent\n",
      "  Reinforcement Learning\n",
      "  In many real-world settings, a team of agents must coordinate its behaviour\n",
      "while acting in a decentralised fashion. At the same time, it is often possible\n",
      "to train the agents in a centralised fashion where global state information is\n",
      "available and communication constraints are lifted. Learning joint\n",
      "action-values conditioned on extra state information is an attractive way to\n",
      "exploit centralised learning, but the best strategy for then extracting\n",
      "decentralised policies is unclear. Our solution is QMIX, a novel value-based\n",
      "method that can train decentralised policies in a centralised end-to-end\n",
      "fashion. QMIX employs a mixing network that estimates joint action-values as a\n",
      "monotonic combination of per-agent values. We structurally enforce that the\n",
      "joint-action value is monotonic in the per-agent values, through the use of\n",
      "non-negative weights in the mixing network, which guarantees consistency\n",
      "between the centralised and decentralised policies. To evaluate the performance\n",
      "of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new\n",
      "benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a\n",
      "challenging set of SMAC scenarios and show that it significantly outperforms\n",
      "existing multi-agent reinforcement learning methods.\n",
      "\n",
      "**Paper Id :1901.03887 \n",
      "Title :Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement\n",
      "  Learning through Memory-driven Communication\n",
      "  Deep reinforcement learning algorithms have recently been used to train\n",
      "multiple interacting agents in a centralised manner whilst keeping their\n",
      "execution decentralised. When the agents can only acquire partial observations\n",
      "and are faced with tasks requiring coordination and synchronisation skills,\n",
      "inter-agent communication plays an essential role. In this work, we propose a\n",
      "framework for multi-agent training using deep deterministic policy gradients\n",
      "that enables concurrent, end-to-end learning of an explicit communication\n",
      "protocol through a memory device. During training, the agents learn to perform\n",
      "read and write operations enabling them to infer a shared representation of the\n",
      "world. We empirically demonstrate that concurrent learning of the communication\n",
      "device and individual policies can improve inter-agent coordination and\n",
      "performance in small-scale systems. Our experimental results show that the\n",
      "proposed method achieves superior performance in scenarios with up to six\n",
      "agents. We illustrate how different communication patterns can emerge on six\n",
      "different tasks of increasing complexity. Furthermore, we study the effects of\n",
      "corrupting the communication channel, provide a visualisation of the\n",
      "time-varying memory content as the underlying task is being solved and validate\n",
      "the building blocks of the proposed memory device through ablation studies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09148 \n",
      "Title :Event-based Asynchronous Sparse Convolutional Networks\n",
      "  Event cameras are bio-inspired sensors that respond to per-pixel brightness\n",
      "changes in the form of asynchronous and sparse \"events\". Recently, pattern\n",
      "recognition algorithms, such as learning-based methods, have made significant\n",
      "progress with event cameras by converting events into synchronous dense,\n",
      "image-like representations and applying traditional machine learning methods\n",
      "developed for standard cameras. However, these approaches discard the spatial\n",
      "and temporal sparsity inherent in event data at the cost of higher\n",
      "computational complexity and latency. In this work, we present a general\n",
      "framework for converting models trained on synchronous image-like event\n",
      "representations into asynchronous models with identical output, thus directly\n",
      "leveraging the intrinsic asynchronous and sparse nature of the event data. We\n",
      "show both theoretically and experimentally that this drastically reduces the\n",
      "computational complexity and latency of high-capacity, synchronous neural\n",
      "networks without sacrificing accuracy. In addition, our framework has several\n",
      "desirable characteristics: (i) it exploits spatio-temporal sparsity of events\n",
      "explicitly, (ii) it is agnostic to the event representation, network\n",
      "architecture, and task, and (iii) it does not require any train-time change,\n",
      "since it is compatible with the standard neural networks' training process. We\n",
      "thoroughly validate the proposed framework on two computer vision tasks: object\n",
      "detection and object recognition. In these tasks, we reduce the computational\n",
      "complexity up to 20 times with respect to high-latency neural networks. At the\n",
      "same time, we outperform state-of-the-art asynchronous approaches up to 24% in\n",
      "prediction accuracy.\n",
      "\n",
      "**Paper Id :2010.12455 \n",
      "Title :Primal-Dual Mesh Convolutional Neural Networks\n",
      "  Recent works in geometric deep learning have introduced neural networks that\n",
      "allow performing inference tasks on three-dimensional geometric data by\n",
      "defining convolution, and sometimes pooling, operations on triangle meshes.\n",
      "These methods, however, either consider the input mesh as a graph, and do not\n",
      "exploit specific geometric properties of meshes for feature aggregation and\n",
      "downsampling, or are specialized for meshes, but rely on a rigid definition of\n",
      "convolution that does not properly capture the local topology of the mesh. We\n",
      "propose a method that combines the advantages of both types of approaches,\n",
      "while addressing their limitations: we extend a primal-dual framework drawn\n",
      "from the graph-neural-network literature to triangle meshes, and define\n",
      "convolutions on two types of graphs constructed from an input mesh. Our method\n",
      "takes features for both edges and faces of a 3D mesh as input and dynamically\n",
      "aggregates them using an attention mechanism. At the same time, we introduce a\n",
      "pooling operation with a precise geometric interpretation, that allows handling\n",
      "variations in the mesh connectivity by clustering mesh faces in a task-driven\n",
      "fashion. We provide theoretical insights of our approach using tools from the\n",
      "mesh-simplification literature. In addition, we validate experimentally our\n",
      "method in the tasks of shape classification and shape segmentation, where we\n",
      "obtain comparable or superior performance to the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09284 \n",
      "Title :Acoustic Scene Classification with Squeeze-Excitation Residual Networks\n",
      "  Acoustic scene classification (ASC) is a problem related to the field of\n",
      "machine listening whose objective is to classify/tag an audio clip in a\n",
      "predefined label describing a scene location (e. g. park, airport, etc.). Many\n",
      "state-of-the-art solutions to ASC incorporate data augmentation techniques and\n",
      "model ensembles. However, considerable improvements can also be achieved only\n",
      "by modifying the architecture of convolutional neural networks (CNNs). In this\n",
      "work we propose two novel squeeze-excitation blocks to improve the accuracy of\n",
      "a CNN-based ASC framework based on residual learning. The main idea of\n",
      "squeeze-excitation blocks is to learn spatial and channel-wise feature maps\n",
      "independently instead of jointly as standard CNNs do. This is usually achieved\n",
      "by some global grouping operators, linear operators and a final calibration\n",
      "between the input of the block and its obtained relationships. The behavior of\n",
      "the block that implements such operators and, therefore, the entire neural\n",
      "network, can be modified depending on the input to the block, the established\n",
      "residual configurations and the selected non-linear activations. The analysis\n",
      "has been carried out using the TAU Urban Acoustic Scenes 2019 dataset\n",
      "(https://zenodo.org/record/2589280) presented in the 2019 edition of the DCASE\n",
      "challenge. All configurations discussed in this document exceed the performance\n",
      "of the baseline proposed by the DCASE organization by 13\\% percentage points.\n",
      "In turn, the novel configurations proposed in this paper outperform the\n",
      "residual configurations proposed in previous works.\n",
      "\n",
      "**Paper Id :2001.04692 \n",
      "Title :Unsupervised Domain Adaptation for Mobile Semantic Segmentation based on\n",
      "  Cycle Consistency and Feature Alignment\n",
      "  The supervised training of deep networks for semantic segmentation requires a\n",
      "huge amount of labeled real world data. To solve this issue, a commonly\n",
      "exploited workaround is to use synthetic data for training, but deep networks\n",
      "show a critical performance drop when analyzing data with slightly different\n",
      "statistical properties with respect to the training set. In this work, we\n",
      "propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the\n",
      "domain shift issue between real world and synthetic representations. An\n",
      "adversarial model, based on the cycle consistency framework, performs the\n",
      "mapping between the synthetic and real domain. The data is then fed to a\n",
      "MobileNet-v2 architecture that performs the semantic segmentation task. An\n",
      "additional couple of discriminators, working at the feature level of the\n",
      "MobileNet-v2, allows to better align the features of the two domain\n",
      "distributions and to further improve the performance. Finally, the consistency\n",
      "of the semantic maps is exploited. After an initial supervised training on\n",
      "synthetic data, the whole UDA architecture is trained end-to-end considering\n",
      "all its components at once. Experimental results show how the proposed strategy\n",
      "is able to obtain impressive performance in adapting a segmentation network\n",
      "trained on synthetic data to real world scenarios. The usage of the lightweight\n",
      "MobileNet-v2 architecture allows its deployment on devices with limited\n",
      "computational resources as the ones employed in autonomous vehicles.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09432 \n",
      "Title :Distinguishing Cell Phenotype Using Cell Epigenotype\n",
      "  The relationship between microscopic observations and macroscopic behavior is\n",
      "a fundamental open question in biophysical systems. Here, we develop a unified\n",
      "approach that---in contrast with existing methods---predicts cell type from\n",
      "macromolecular data even when accounting for the scale of human tissue\n",
      "diversity and limitations in the available data. We achieve these benefits by\n",
      "applying a k-nearest-neighbors algorithm after projecting our data onto the\n",
      "eigenvectors of the correlation matrix inferred from many observations of gene\n",
      "expression or chromatin conformation. Our approach identifies variations in\n",
      "epigenotype that impact cell type, thereby supporting the cell type attractor\n",
      "hypothesis and representing the first step toward model-independent control\n",
      "strategies in biological systems.\n",
      "\n",
      "**Paper Id :2002.05909 \n",
      "Title :Deep reconstruction of strange attractors from time series\n",
      "  Experimental measurements of physical systems often have a limited number of\n",
      "independent channels, causing essential dynamical variables to remain\n",
      "unobserved. However, many popular methods for unsupervised inference of latent\n",
      "dynamics from experimental data implicitly assume that the measurements have\n",
      "higher intrinsic dimensionality than the underlying system---making coordinate\n",
      "identification a dimensionality reduction problem. Here, we study the opposite\n",
      "limit, in which hidden governing coordinates must be inferred from only a\n",
      "low-dimensional time series of measurements. Inspired by classical analysis\n",
      "techniques for partial observations of chaotic attractors, we introduce a\n",
      "general embedding technique for univariate and multivariate time series,\n",
      "consisting of an autoencoder trained with a novel latent-space loss function.\n",
      "We show that our technique reconstructs the strange attractors of synthetic and\n",
      "real-world systems better than existing techniques, and that it creates\n",
      "consistent, predictive representations of even stochastic systems. We conclude\n",
      "by using our technique to discover dynamical attractors in diverse systems such\n",
      "as patient electrocardiograms, household electricity usage, neural spiking, and\n",
      "eruptions of the Old Faithful geyser---demonstrating diverse applications of\n",
      "our technique for exploratory data analysis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09504 \n",
      "Title :Ellipsoidal Subspace Support Vector Data Description\n",
      "  In this paper, we propose a novel method for transforming data into a\n",
      "low-dimensional space optimized for one-class classification. The proposed\n",
      "method iteratively transforms data into a new subspace optimized for\n",
      "ellipsoidal encapsulation of target class data. We provide both linear and\n",
      "non-linear formulations for the proposed method. The method takes into account\n",
      "the covariance of the data in the subspace; hence, it yields a more generalized\n",
      "solution as compared to Subspace Support Vector Data Description for a\n",
      "hypersphere. We propose different regularization terms expressing the class\n",
      "variance in the projected space. We compare the results with classic and\n",
      "recently proposed one-class classification methods and achieve better results\n",
      "in the majority of cases. The proposed method is also noticed to converge much\n",
      "faster than recently proposed Subspace Support Vector Data Description.\n",
      "\n",
      "**Paper Id :1904.07698 \n",
      "Title :Multimodal Subspace Support Vector Data Description\n",
      "  In this paper, we propose a novel method for projecting data from multiple\n",
      "modalities to a new subspace optimized for one-class classification. The\n",
      "proposed method iteratively transforms the data from the original feature space\n",
      "of each modality to a new common feature space along with finding a joint\n",
      "compact description of data coming from all the modalities. For data in each\n",
      "modality, we define a separate transformation to map the data from the\n",
      "corresponding feature space to the new optimized subspace by exploiting the\n",
      "available information from the class of interest only. We also propose\n",
      "different regularization strategies for the proposed method and provide both\n",
      "linear and non-linear formulations. The proposed Multimodal Subspace Support\n",
      "Vector Data Description outperforms all the competing methods using data from a\n",
      "single modality or fusing data from all modalities in four out of five\n",
      "datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09540 \n",
      "Title :Distributed Reinforcement Learning for Cooperative Multi-Robot Object\n",
      "  Manipulation\n",
      "  We consider solving a cooperative multi-robot object manipulation task using\n",
      "reinforcement learning (RL). We propose two distributed multi-agent RL\n",
      "approaches: distributed approximate RL (DA-RL), where each agent applies\n",
      "Q-learning with individual reward functions; and game-theoretic RL (GT-RL),\n",
      "where the agents update their Q-values based on the Nash equilibrium of a\n",
      "bimatrix Q-value game. We validate the proposed approaches in the setting of\n",
      "cooperative object manipulation with two simulated robot arms. Although we\n",
      "focus on a small system of two agents in this paper, both DA-RL and GT-RL apply\n",
      "to general multi-agent systems, and are expected to scale well to large\n",
      "systems.\n",
      "\n",
      "**Paper Id :2010.13032 \n",
      "Title :Byzantine Resilient Distributed Multi-Task Learning\n",
      "  Distributed multi-task learning provides significant advantages in\n",
      "multi-agent networks with heterogeneous data sources where agents aim to learn\n",
      "distinct but correlated models simultaneously. However, distributed algorithms\n",
      "for learning relatedness among tasks are not resilient in the presence of\n",
      "Byzantine agents. In this paper, we present an approach for Byzantine resilient\n",
      "distributed multi-task learning. We propose an efficient online weight\n",
      "assignment rule by measuring the accumulated loss using an agent's data and its\n",
      "neighbors' models. A small accumulated loss indicates a large similarity\n",
      "between the two tasks. In order to ensure the Byzantine resilience of the\n",
      "aggregation at a normal agent, we introduce a step for filtering out larger\n",
      "losses. We analyze the approach for convex models and show that normal agents\n",
      "converge resiliently towards their true targets. Further, an agent's learning\n",
      "performance using the proposed weight assignment rule is guaranteed to be at\n",
      "least as good as in the non-cooperative case as measured by the expected\n",
      "regret. Finally, we demonstrate the approach using three case studies,\n",
      "including regression and classification problems, and show that our method\n",
      "exhibits good empirical performance for non-convex models, such as\n",
      "convolutional neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09773 \n",
      "Title :HDF: Hybrid Deep Features for Scene Image Representation\n",
      "  Nowadays it is prevalent to take features extracted from pre-trained deep\n",
      "learning models as image representations which have achieved promising\n",
      "classification performance. Existing methods usually consider either\n",
      "object-based features or scene-based features only. However, both types of\n",
      "features are important for complex images like scene images, as they can\n",
      "complement each other. In this paper, we propose a novel type of features --\n",
      "hybrid deep features, for scene images. Specifically, we exploit both\n",
      "object-based and scene-based features at two levels: part image level (i.e.,\n",
      "parts of an image) and whole image level (i.e., a whole image), which produces\n",
      "a total number of four types of deep features. Regarding the part image level,\n",
      "we also propose two new slicing techniques to extract part based features.\n",
      "Finally, we aggregate these four types of deep features via the concatenation\n",
      "operator. We demonstrate the effectiveness of our hybrid deep features on three\n",
      "commonly used scene datasets (MIT-67, Scene-15, and Event-8), in terms of the\n",
      "scene image classification task. Extensive comparisons show that our introduced\n",
      "features can produce state-of-the-art classification accuracies which are more\n",
      "consistent and stable than the results of existing features across all\n",
      "datasets.\n",
      "\n",
      "**Paper Id :2006.11223 \n",
      "Title :Unified Representation Learning for Efficient Medical Image Analysis\n",
      "  Medical image analysis typically includes several tasks such as image\n",
      "enhancement, detection, segmentation, and classification. These tasks are often\n",
      "implemented through separate machine learning methods, or recently through deep\n",
      "learning methods. We propose a novel multitask deep learning-based approach,\n",
      "called unified representation (U-Rep), that can be used to simultaneously\n",
      "perform several medical image analysis tasks. U-Rep is modality-specific and\n",
      "takes into consideration inter-task relationships. The proposed U-Rep can be\n",
      "trained using unlabeled data or limited amounts of labeled data. The trained\n",
      "U-Rep is then shared to simultaneously learn key tasks in medical image\n",
      "analysis, such as segmentation, classification and visual assessment. We also\n",
      "show that pre-processing operations, such as noise reduction and image\n",
      "enhancement, can be learned while constructing U-Rep. Our experimental results,\n",
      "on two medical image datasets, show that U-Rep improves generalization, and\n",
      "decreases resource utilization and training time while preventing unnecessary\n",
      "repetitions of building task-specific models in isolation. We believe that the\n",
      "proposed method (U-Rep) would tread a path toward promising future research in\n",
      "medical image analysis, especially for tasks with unlabeled data or limited\n",
      "amounts of labeled data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09831 \n",
      "Title :Prior Knowledge Driven Label Embedding for Slot Filling in Natural\n",
      "  Language Understanding\n",
      "  Traditional slot filling in natural language understanding (NLU) predicts a\n",
      "one-hot vector for each word. This form of label representation lacks semantic\n",
      "correlation modelling, which leads to severe data sparsity problem, especially\n",
      "when adapting an NLU model to a new domain. To address this issue, a novel\n",
      "label embedding based slot filling framework is proposed in this paper. Here,\n",
      "distributed label embedding is constructed for each slot using prior knowledge.\n",
      "Three encoding methods are investigated to incorporate different kinds of prior\n",
      "knowledge about slots: atomic concepts, slot descriptions, and slot exemplars.\n",
      "The proposed label embeddings tend to share text patterns and reuses data with\n",
      "different slot labels. This makes it useful for adaptive NLU with limited data.\n",
      "Also, since label embedding is independent of NLU model, it is compatible with\n",
      "almost all deep learning based slot filling models. The proposed approaches are\n",
      "evaluated on three datasets. Experiments on single domain and domain adaptation\n",
      "tasks show that label embedding achieves significant performance improvement\n",
      "over traditional one-hot label representation as well as advanced zero-shot\n",
      "approaches.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "**Paper Id :2007.02040 \n",
      "Title :Discount Factor as a Regularizer in Reinforcement Learning\n",
      "  Specifying a Reinforcement Learning (RL) task involves choosing a suitable\n",
      "planning horizon, which is typically modeled by a discount factor. It is known\n",
      "that applying RL algorithms with a lower discount factor can act as a\n",
      "regularizer, improving performance in the limited data regime. Yet the exact\n",
      "nature of this regularizer has not been investigated. In this work, we fill in\n",
      "this gap. For several Temporal-Difference (TD) learning methods, we show an\n",
      "explicit equivalence between using a reduced discount factor and adding an\n",
      "explicit regularization term to the algorithm's loss. Motivated by the\n",
      "equivalence, we empirically study this technique compared to standard $L_2$\n",
      "regularization by extensive experiments in discrete and continuous domains,\n",
      "using tabular and functional representations. Our experiments suggest the\n",
      "regularization effectiveness is strongly related to properties of the available\n",
      "data, such as size, distribution, and mixing rate.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.10038 \n",
      "Title :Robust Hypergraph Clustering via Convex Relaxation of Truncated MLE\n",
      "  We study hypergraph clustering in the weighted $d$-uniform hypergraph\n",
      "stochastic block model ($d$\\textsf{-WHSBM}), where each edge consisting of $d$\n",
      "nodes from the same community has higher expected weight than the edges\n",
      "consisting of nodes from different communities. We propose a new hypergraph\n",
      "clustering algorithm, called \\textsf{CRTMLE}, and provide its performance\n",
      "guarantee under the $d$\\textsf{-WHSBM} for general parameter regimes. We show\n",
      "that the proposed method achieves the order-wise optimal or the best existing\n",
      "results for approximately balanced community sizes. Moreover, our results\n",
      "settle the first recovery guarantees for growing number of clusters of\n",
      "unbalanced sizes. Involving theoretical analysis and empirical results, we\n",
      "demonstrate the robustness of our algorithm against the unbalancedness of\n",
      "community sizes or the presence of outlier nodes.\n",
      "\n",
      "**Paper Id :2002.06524 \n",
      "Title :Tensor denoising and completion based on ordinal observations\n",
      "  Higher-order tensors arise frequently in applications such as neuroimaging,\n",
      "recommendation system, social network analysis, and psychological studies. We\n",
      "consider the problem of low-rank tensor estimation from possibly incomplete,\n",
      "ordinal-valued observations. Two related problems are studied, one on tensor\n",
      "denoising and another on tensor completion. We propose a multi-linear\n",
      "cumulative link model, develop a rank-constrained M-estimator, and obtain\n",
      "theoretical accuracy guarantees. Our mean squared error bound enjoys a faster\n",
      "convergence rate than previous results, and we show that the proposed estimator\n",
      "is minimax optimal under the class of low-rank models. Furthermore, the\n",
      "procedure developed serves as an efficient completion method which guarantees\n",
      "consistent recovery of an order-$K$ $(d,\\ldots,d)$-dimensional low-rank tensor\n",
      "using only $\\tilde{\\mathcal{O}}(Kd)$ noisy, quantized observations. We\n",
      "demonstrate the outperformance of our approach over previous methods on the\n",
      "tasks of clustering and collaborative filtering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.10540 \n",
      "Title :Data-driven models and computational tools for neurolinguistics: a\n",
      "  language technology perspective\n",
      "  In this paper, our focus is the connection and influence of language\n",
      "technologies on the research in neurolinguistics. We present a review of brain\n",
      "imaging-based neurolinguistic studies with a focus on the natural language\n",
      "representations, such as word embeddings and pre-trained language models.\n",
      "Mutual enrichment of neurolinguistics and language technologies leads to\n",
      "development of brain-aware natural language representations. The importance of\n",
      "this research area is emphasized by medical applications.\n",
      "\n",
      "**Paper Id :1902.02181 \n",
      "Title :Attention in Natural Language Processing\n",
      "  Attention is an increasingly popular mechanism used in a wide range of neural\n",
      "architectures. The mechanism itself has been realized in a variety of formats.\n",
      "However, because of the fast-paced advances in this domain, a systematic\n",
      "overview of attention is still missing. In this article, we define a unified\n",
      "model for attention architectures in natural language processing, with a focus\n",
      "on those designed to work with vector representations of the textual data. We\n",
      "propose a taxonomy of attention models according to four dimensions: the\n",
      "representation of the input, the compatibility function, the distribution\n",
      "function, and the multiplicity of the input and/or output. We present the\n",
      "examples of how prior information can be exploited in attention models and\n",
      "discuss ongoing research efforts and open challenges in the area, providing the\n",
      "first extensive categorization of the vast body of literature in this exciting\n",
      "domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.11003 \n",
      "Title :Learn to Schedule (LEASCH): A Deep reinforcement learning approach for\n",
      "  radio resource scheduling in the 5G MAC layer\n",
      "  Network management tools are usually inherited from one generation to\n",
      "another. This was successful since these tools have been kept in check and\n",
      "updated regularly to fit new networking goals and service requirements.\n",
      "Unfortunately, new networking services will render this approach obsolete and\n",
      "handcrafting new tools or upgrading the current ones may lead to complicated\n",
      "systems that will be extremely difficult to maintain and improve. Fortunately,\n",
      "recent advances in AI have provided new promising tools that can help solving\n",
      "many network management problems. Following this interesting trend, the current\n",
      "article presents LEASCH, a deep reinforcement learning model able to solve the\n",
      "radio resource scheduling problem in the MAC layer of 5G networks. LEASCH is\n",
      "developed and trained in a sand-box and then deployed in a 5G network. The\n",
      "experimental results validate the effectiveness of LEASCH compared to\n",
      "conventional baseline methods in many key performance indicators.\n",
      "\n",
      "**Paper Id :1812.11794 \n",
      "Title :Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n",
      "  Challenges, Solutions and Applications\n",
      "  Reinforcement learning (RL) algorithms have been around for decades and\n",
      "employed to solve various sequential decision-making problems. These algorithms\n",
      "however have faced great challenges when dealing with high-dimensional\n",
      "environments. The recent development of deep learning has enabled RL methods to\n",
      "drive optimal policies for sophisticated and capable agents, which can perform\n",
      "efficiently in these challenging environments. This paper addresses an\n",
      "important aspect of deep RL related to situations that require multiple agents\n",
      "to communicate and cooperate to solve complex tasks. A survey of different\n",
      "approaches to problems related to multi-agent deep RL (MADRL) is presented,\n",
      "including non-stationarity, partial observability, continuous state and action\n",
      "spaces, multi-agent training schemes, multi-agent transfer learning. The merits\n",
      "and demerits of the reviewed methods will be analyzed and discussed, with their\n",
      "corresponding applications explored. It is envisaged that this review provides\n",
      "insights about various MADRL methods and can lead to future development of more\n",
      "robust and highly useful multi-agent learning methods for solving real-world\n",
      "problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.11617 \n",
      "Title :Covid-19: Automatic detection from X-Ray images utilizing Transfer\n",
      "  Learning with Convolutional Neural Networks\n",
      "  In this study, a dataset of X-Ray images from patients with common pneumonia,\n",
      "Covid-19, and normal incidents was utilized for the automatic detection of the\n",
      "Coronavirus. The aim of the study is to evaluate the performance of\n",
      "state-of-the-art Convolutional Neural Network architectures proposed over\n",
      "recent years for medical image classification. Specifically, the procedure\n",
      "called transfer learning was adopted. With transfer learning, the detection of\n",
      "various abnormalities in small medical image datasets is an achievable target,\n",
      "often yielding remarkable results. The dataset utilized in this experiment is a\n",
      "collection of 1427 X-Ray images. 224 images with confirmed Covid-19, 700 images\n",
      "with confirmed common pneumonia, and 504 images of normal conditions are\n",
      "included. The data was collected from the available X-Ray images on public\n",
      "medical repositories. With transfer learning, an overall accuracy of 97.82% in\n",
      "the detection of Covid-19 is achieved.\n",
      "\n",
      "**Paper Id :2003.13145 \n",
      "Title :Can AI help in screening Viral and COVID-19 pneumonia?\n",
      "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\n",
      "caused thousands of causalities and infected several millions of people\n",
      "worldwide. Any technological tool enabling rapid screening of the COVID-19\n",
      "infection with high accuracy can be crucially helpful to healthcare\n",
      "professionals. The main clinical tool currently in use for the diagnosis of\n",
      "COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\n",
      "is expensive, less-sensitive and requires specialized medical personnel. X-ray\n",
      "imaging is an easily accessible tool that can be an excellent alternative in\n",
      "the COVID-19 diagnosis. This research was taken to investigate the utility of\n",
      "artificial intelligence (AI) in the rapid and accurate detection of COVID-19\n",
      "from chest X-ray images. The aim of this paper is to propose a robust technique\n",
      "for automatic detection of COVID-19 pneumonia from digital chest X-ray images\n",
      "applying pre-trained deep-learning algorithms while maximizing the detection\n",
      "accuracy. A public database was created by the authors combining several public\n",
      "databases and also by collecting images from recently published articles. The\n",
      "database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\n",
      "normal chest X-ray images. Transfer learning technique was used with the help\n",
      "of image augmentation to train and validate several pre-trained deep\n",
      "Convolutional Neural Networks (CNNs). The networks were trained to classify two\n",
      "different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\n",
      "COVID-19 pneumonia with and without image augmentation. The classification\n",
      "accuracy, precision, sensitivity, and specificity for both the schemes were\n",
      "99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\n",
      "respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.11644 \n",
      "Title :Multi-Label Text Classification using Attention-based Graph Neural\n",
      "  Network\n",
      "  In Multi-Label Text Classification (MLTC), one sample can belong to more than\n",
      "one class. It is observed that most MLTC tasks, there are dependencies or\n",
      "correlations among labels. Existing methods tend to ignore the relationship\n",
      "among labels. In this paper, a graph attention network-based model is proposed\n",
      "to capture the attentive dependency structure among the labels. The graph\n",
      "attention network uses a feature matrix and a correlation matrix to capture and\n",
      "explore the crucial dependencies between the labels and generate classifiers\n",
      "for the task. The generated classifiers are applied to sentence feature vectors\n",
      "obtained from the text feature extraction network (BiLSTM) to enable end-to-end\n",
      "training. Attention allows the system to assign different weights to neighbor\n",
      "nodes per label, thus allowing it to learn the dependencies among labels\n",
      "implicitly. The results of the proposed model are validated on five real-world\n",
      "MLTC datasets. The proposed model achieves similar or better performance\n",
      "compared to the previous state-of-the-art models.\n",
      "\n",
      "**Paper Id :2003.08420 \n",
      "Title :Unsupervised Hierarchical Graph Representation Learning by Mutual\n",
      "  Information Maximization\n",
      "  Graph representation learning based on graph neural networks (GNNs) can\n",
      "greatly improve the performance of downstream tasks, such as node and graph\n",
      "classification. However, the general GNN models do not aggregate node\n",
      "information in a hierarchical manner, and can miss key higher-order structural\n",
      "features of many graphs. The hierarchical aggregation also enables the graph\n",
      "representations to be explainable. In addition, supervised graph representation\n",
      "learning requires labeled data, which is expensive and error-prone. To address\n",
      "these issues, we present an unsupervised graph representation learning method,\n",
      "Unsupervised Hierarchical Graph Representation (UHGR), which can generate\n",
      "hierarchical representations of graphs. Our method focuses on maximizing mutual\n",
      "information between \"local\" and high-level \"global\" representations, which\n",
      "enables us to learn the node embeddings and graph embeddings without any\n",
      "labeled data. To demonstrate the effectiveness of the proposed method, we\n",
      "perform the node and graph classification using the learned node and graph\n",
      "embeddings. The results show that the proposed method achieves comparable\n",
      "results to state-of-the-art supervised methods on several benchmarks. In\n",
      "addition, our visualization of hierarchical representations indicates that our\n",
      "method can capture meaningful and interpretable clusters.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.11804 \n",
      "Title :Active Learning Approach to Optimization of Experimental Control\n",
      "  In this work we present a general machine learning based scheme to optimize\n",
      "experimental control. The method utilizes the neural network to learn the\n",
      "relation between the control parameters and the control goal, with which the\n",
      "optimal control parameters can be obtained. The main challenge of this approach\n",
      "is that the labeled data obtained from experiments are not abundant. The\n",
      "central idea of our scheme is to use the active learning to overcome this\n",
      "difficulty. As a demonstration example, we apply our method to control\n",
      "evaporative cooling experiments in cold atoms. We have first tested our method\n",
      "with simulated data and then applied our method to real experiments. We\n",
      "demonstrate that our method can successfully reach the best performance within\n",
      "hundreds of experimental runs. Our method does not require knowledge of the\n",
      "experimental system as a prior and is universal for experimental control in\n",
      "different systems.\n",
      "\n",
      "**Paper Id :2001.08092 \n",
      "Title :Local Policy Optimization for Trajectory-Centric Reinforcement Learning\n",
      "  The goal of this paper is to present a method for simultaneous trajectory and\n",
      "local stabilizing policy optimization to generate local policies for\n",
      "trajectory-centric model-based reinforcement learning (MBRL). This is motivated\n",
      "by the fact that global policy optimization for non-linear systems could be a\n",
      "very challenging problem both algorithmically and numerically. However, a lot\n",
      "of robotic manipulation tasks are trajectory-centric, and thus do not require a\n",
      "global model or policy. Due to inaccuracies in the learned model estimates, an\n",
      "open-loop trajectory optimization process mostly results in very poor\n",
      "performance when used on the real system. Motivated by these problems, we try\n",
      "to formulate the problem of trajectory optimization and local policy synthesis\n",
      "as a single optimization problem. It is then solved simultaneously as an\n",
      "instance of nonlinear programming. We provide some results for analysis as well\n",
      "as achieved performance of the proposed technique under some simplifying\n",
      "assumptions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12168 \n",
      "Title :Adversarial System Variant Approximation to Quantify Process Model\n",
      "  Generalization\n",
      "  In process mining, process models are extracted from event logs using process\n",
      "discovery algorithms and are commonly assessed using multiple quality\n",
      "dimensions. While the metrics that measure the relationship of an extracted\n",
      "process model to its event log are well-studied, quantifying the level by which\n",
      "a process model can describe the unobserved behavior of its underlying system\n",
      "falls short in the literature. In this paper, a novel deep learning-based\n",
      "methodology called Adversarial System Variant Approximation (AVATAR) is\n",
      "proposed to overcome this issue. Sequence Generative Adversarial Networks are\n",
      "trained on the variants contained in an event log with the intention to\n",
      "approximate the underlying variant distribution of the system behavior.\n",
      "Unobserved realistic variants are sampled either directly from the Sequence\n",
      "Generative Adversarial Network or by leveraging the Metropolis-Hastings\n",
      "algorithm. The degree by which a process model relates to its underlying\n",
      "unknown system behavior is then quantified based on the realistic observed and\n",
      "estimated unobserved variants using established process model quality metrics.\n",
      "Significant performance improvements in revealing realistic unobserved variants\n",
      "are demonstrated in a controlled experiment on 15 ground truth systems.\n",
      "Additionally, the proposed methodology is experimentally tested and evaluated\n",
      "to quantify the generalization of 60 discovered process models with respect to\n",
      "their systems.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12175 \n",
      "Title :Incremental Learning Algorithm for Sound Event Detection\n",
      "  This paper presents a new learning strategy for the Sound Event Detection\n",
      "(SED) system to tackle the issues of i) knowledge migration from a pre-trained\n",
      "model to a new target model and ii) learning new sound events without\n",
      "forgetting the previously learned ones without re-training from scratch. In\n",
      "order to migrate the previously learned knowledge from the source model to the\n",
      "target one, a neural adapter is employed on the top of the source model. The\n",
      "source model and the target model are merged via this neural adapter layer. The\n",
      "neural adapter layer facilitates the target model to learn new sound events\n",
      "with minimal training data and maintaining the performance of the previously\n",
      "learned sound events similar to the source model. Our extensive analysis on the\n",
      "DCASE16 and US-SED dataset reveals the effectiveness of the proposed method in\n",
      "transferring knowledge between source and target models without introducing any\n",
      "performance degradation on the previously learned sound events while obtaining\n",
      "a competitive detection performance on the newly learned sound events.\n",
      "\n",
      "**Paper Id :1910.05878 \n",
      "Title :Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces\n",
      "  Transfer learning makes use of data or knowledge in one problem to help solve\n",
      "a different, yet related, problem. It is particularly useful in brain-computer\n",
      "interfaces (BCIs), for coping with variations among different subjects and/or\n",
      "tasks. This paper considers offline unsupervised cross-subject\n",
      "electroencephalogram (EEG) classification, i.e., we have labeled EEG trials\n",
      "from one or more source subjects, but only unlabeled EEG trials from the target\n",
      "subject. We propose a novel manifold embedded knowledge transfer (MEKT)\n",
      "approach, which first aligns the covariance matrices of the EEG trials in the\n",
      "Riemannian manifold, extracts features in the tangent space, and then performs\n",
      "domain adaptation by minimizing the joint probability distribution shift\n",
      "between the source and the target domains, while preserving their geometric\n",
      "structures. MEKT can cope with one or multiple source domains, and can be\n",
      "computed efficiently. We also propose a domain transferability estimation (DTE)\n",
      "approach to identify the most beneficial source domains, in case there are a\n",
      "large number of source domains. Experiments on four EEG datasets from two\n",
      "different BCI paradigms demonstrated that MEKT outperformed several\n",
      "state-of-the-art transfer learning approaches, and DTE can reduce more than\n",
      "half of the computational cost when the number of source subjects is large,\n",
      "with little sacrifice of classification accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12198 \n",
      "Title :Sorting Big Data by Revealed Preference with Application to College\n",
      "  Ranking\n",
      "  When ranking big data observations such as colleges in the United States,\n",
      "diverse consumers reveal heterogeneous preferences. The objective of this paper\n",
      "is to sort out a linear ordering for these observations and to recommend\n",
      "strategies to improve their relative positions in the ranking. A properly\n",
      "sorted solution could help consumers make the right choices, and governments\n",
      "make wise policy decisions. Previous researchers have applied exogenous\n",
      "weighting or multivariate regression approaches to sort big data objects,\n",
      "ignoring their variety and variability. By recognizing the diversity and\n",
      "heterogeneity among both the observations and the consumers, we instead apply\n",
      "endogenous weighting to these contradictory revealed preferences. The outcome\n",
      "is a consistent steady-state solution to the counterbalance equilibrium within\n",
      "these contradictions. The solution takes into consideration the spillover\n",
      "effects of multiple-step interactions among the observations. When information\n",
      "from data is efficiently revealed in preferences, the revealed preferences\n",
      "greatly reduce the volume of the required data in the sorting process. The\n",
      "employed approach can be applied in many other areas, such as sports team\n",
      "ranking, academic journal ranking, voting, and real effective exchange rates.\n",
      "\n",
      "**Paper Id :2004.13277 \n",
      "Title :Detecting multi-timescale consumption patterns from receipt data: A\n",
      "  non-negative tensor factorization approach\n",
      "  Understanding consumer behavior is an important task, not only for developing\n",
      "marketing strategies but also for the management of economic policies.\n",
      "Detecting consumption patterns, however, is a high-dimensional problem in which\n",
      "various factors that would affect consumers' behavior need to be considered,\n",
      "such as consumers' demographics, circadian rhythm, seasonal cycles, etc. Here,\n",
      "we develop a method to extract multi-timescale expenditure patterns of\n",
      "consumers from a large dataset of scanned receipts. We use a non-negative\n",
      "tensor factorization (NTF) to detect intra- and inter-week consumption patterns\n",
      "at one time. The proposed method allows us to characterize consumers based on\n",
      "their consumption patterns that are correlated over different timescales.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12319 \n",
      "Title :Boolean learning under noise-perturbations in hardware neural networks\n",
      "  A high efficiency hardware integration of neural networks benefits from\n",
      "realizing nonlinearity, network connectivity and learning fully in a physical\n",
      "substrate. Multiple systems have recently implemented some or all of these\n",
      "operations, yet the focus was placed on addressing technological challenges.\n",
      "Fundamental questions regarding learning in hardware neural networks remain\n",
      "largely unexplored. Noise in particular is unavoidable in such architectures,\n",
      "and here we investigate its interaction with a learning algorithm using an\n",
      "opto-electronic recurrent neural network. We find that noise strongly modifies\n",
      "the system's path during convergence, and surprisingly fully decorrelates the\n",
      "final readout weight matrices. This highlights the importance of understanding\n",
      "architecture, noise and learning algorithm as interacting players, and\n",
      "therefore identifies the need for mathematical tools for noisy, analogue system\n",
      "optimization.\n",
      "\n",
      "**Paper Id :2006.14619 \n",
      "Title :Recurrent Quantum Neural Networks\n",
      "  Recurrent neural networks are the foundation of many sequence-to-sequence\n",
      "models in machine learning, such as machine translation and speech synthesis.\n",
      "In contrast, applied quantum computing is in its infancy. Nevertheless there\n",
      "already exist quantum machine learning models such as variational quantum\n",
      "eigensolvers which have been used successfully e.g. in the context of energy\n",
      "minimization tasks. In this work we construct a quantum recurrent neural\n",
      "network (QRNN) with demonstrable performance on non-trivial tasks such as\n",
      "sequence learning and integer digit classification. The QRNN cell is built from\n",
      "parametrized quantum neurons, which, in conjunction with amplitude\n",
      "amplification, create a nonlinear activation of polynomials of its inputs and\n",
      "cell state, and allow the extraction of a probability distribution over\n",
      "predicted classes at each step. To study the model's performance, we provide an\n",
      "implementation in pytorch, which allows the relatively efficient optimization\n",
      "of parametrized quantum circuits with thousands of parameters. We establish a\n",
      "QRNN training setup by benchmarking optimization hyperparameters, and analyse\n",
      "suitable network topologies for simple memorisation and sequence prediction\n",
      "tasks from Elman's seminal paper (1990) on temporal structure learning. We then\n",
      "proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN\n",
      "each image pixel-by-pixel; and by utilising modern data augmentation as\n",
      "preprocessing step. Finally, we analyse to what extent the unitary nature of\n",
      "the network counteracts the vanishing gradient problem that plagues many\n",
      "existing quantum classifiers and classical RNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12386 \n",
      "Title :A New Gene Selection Algorithm using Fuzzy-Rough Set Theory for Tumor\n",
      "  Classification\n",
      "  In statistics and machine learning, feature selection is the process of\n",
      "picking a subset of relevant attributes for utilizing in a predictive model.\n",
      "Recently, rough set-based feature selection techniques, that employ feature\n",
      "dependency to perform selection process, have been drawn attention.\n",
      "Classification of tumors based on gene expression is utilized to diagnose\n",
      "proper treatment and prognosis of the disease in bioinformatics applications.\n",
      "Microarray gene expression data includes superfluous feature genes of high\n",
      "dimensionality and smaller training instances. Since exact supervised\n",
      "classification of gene expression instances in such high-dimensional problems\n",
      "is very complex, the selection of appropriate genes is a crucial task for tumor\n",
      "classification. In this study, we present a new technique for gene selection\n",
      "using a discernibility matrix of fuzzy-rough sets. The proposed technique takes\n",
      "into account the similarity of those instances that have the same and different\n",
      "class labels to improve the gene selection results, while the state-of-the art\n",
      "previous approaches only address the similarity of instances with different\n",
      "class labels. To meet that requirement, we extend the Johnson reducer technique\n",
      "into the fuzzy case. Experimental results demonstrate that this technique\n",
      "provides better efficiency compared to the state-of-the-art approaches.\n",
      "\n",
      "**Paper Id :1911.01220 \n",
      "Title :Learning-based estimation of dielectric properties and tissue density in\n",
      "  head models for personalized radio-frequency dosimetry\n",
      "  Radio-frequency dosimetry is an important process in human safety and for\n",
      "compliance of related products. Recently, computational human models generated\n",
      "from medical images have often been used for such assessment, especially to\n",
      "consider the inter-variability of subjects. However, the common procedure to\n",
      "develop personalized models is time consuming because it involves excessive\n",
      "segmentation of several components that represent different biological tissues,\n",
      "which limits the inter-variability assessment of radiation safety based on\n",
      "personalized dosimetry. Deep learning methods have been shown to be a powerful\n",
      "approach for pattern recognition and signal analysis. Convolutional neural\n",
      "networks with deep architecture are proven robust for feature extraction and\n",
      "image mapping in several biomedical applications. In this study, we develop a\n",
      "learning-based approach for fast and accurate estimation of the dielectric\n",
      "properties and density of tissues directly from magnetic resonance images in a\n",
      "single shot. The smooth distribution of the dielectric properties in head\n",
      "models, which is realized using a process without tissue segmentation, improves\n",
      "the smoothness of the specific absorption rate (SAR) distribution compared with\n",
      "that in the commonly used procedure. The estimated SAR distributions, as well\n",
      "as that averaged over 10-g of tissue in a cubic shape, are found to be highly\n",
      "consistent with those computed using the conventional methods that employ\n",
      "segmentation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12635 \n",
      "Title :The impossibility of low rank representations for triangle-rich complex\n",
      "  networks\n",
      "  The study of complex networks is a significant development in modern science,\n",
      "and has enriched the social sciences, biology, physics, and computer science.\n",
      "Models and algorithms for such networks are pervasive in our society, and\n",
      "impact human behavior via social networks, search engines, and recommender\n",
      "systems to name a few. A widely used algorithmic technique for modeling such\n",
      "complex networks is to construct a low-dimensional Euclidean embedding of the\n",
      "vertices of the network, where proximity of vertices is interpreted as the\n",
      "likelihood of an edge. Contrary to the common view, we argue that such graph\n",
      "embeddings do not}capture salient properties of complex networks. The two\n",
      "properties we focus on are low degree and large clustering coefficients, which\n",
      "have been widely established to be empirically true for real-world networks. We\n",
      "mathematically prove that any embedding (that uses dot products to measure\n",
      "similarity) that can successfully create these two properties must have rank\n",
      "nearly linear in the number of vertices. Among other implications, this\n",
      "establishes that popular embedding techniques such as Singular Value\n",
      "Decomposition and node2vec fail to capture significant structural aspects of\n",
      "real-world complex networks. Furthermore, we empirically study a number of\n",
      "different embedding techniques based on dot product, and show that they all\n",
      "fail to capture the triangle structure.\n",
      "\n",
      "**Paper Id :2004.01293 \n",
      "Title :Motif-Based Spectral Clustering of Weighted Directed Networks\n",
      "  Clustering is an essential technique for network analysis, with applications\n",
      "in a diverse range of fields. Although spectral clustering is a popular and\n",
      "effective method, it fails to consider higher-order structure and can perform\n",
      "poorly on directed networks. One approach is to capture and cluster\n",
      "higher-order structures using motif adjacency matrices. However, current\n",
      "formulations fail to take edge weights into account, and thus are somewhat\n",
      "limited when weight is a key component of the network under study.\n",
      "  We address these shortcomings by exploring motif-based weighted spectral\n",
      "clustering methods. We present new and computationally useful matrix formulae\n",
      "for motif adjacency matrices on weighted networks, which can be used to\n",
      "construct efficient algorithms for any anchored or non-anchored motif on three\n",
      "nodes. In a very sparse regime, our proposed method can handle graphs with a\n",
      "million nodes and tens of millions of edges. We further use our framework to\n",
      "construct a motif-based approach for clustering bipartite networks.\n",
      "  We provide comprehensive experimental results, demonstrating (i) the\n",
      "scalability of our approach, (ii) advantages of higher-order clustering on\n",
      "synthetic examples, and (iii) the effectiveness of our techniques on a variety\n",
      "of real world data sets; and compare against several techniques from the\n",
      "literature. We conclude that motif-based spectral clustering is a valuable tool\n",
      "for analysis of directed and bipartite weighted networks, which is also\n",
      "scalable and easy to implement.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12796 \n",
      "Title :Correlated daily time series and forecasting in the M4 competition\n",
      "  We participated in the M4 competition for time series forecasting and\n",
      "describe here our methods for forecasting daily time series. We used an\n",
      "ensemble of five statistical forecasting methods and a method that we refer to\n",
      "as the correlator. Our retrospective analysis using the ground truth values\n",
      "published by the M4 organisers after the competition demonstrates that the\n",
      "correlator was responsible for most of our gains over the naive constant\n",
      "forecasting method. We identify data leakage as one reason for its success,\n",
      "partly due to test data selected from different time intervals, and partly due\n",
      "to quality issues in the original time series. We suggest that future\n",
      "forecasting competitions should provide actual dates for the time series so\n",
      "that some of those leakages could be avoided by the participants.\n",
      "\n",
      "**Paper Id :2008.04222 \n",
      "Title :Accuracy of neural networks for the simulation of chaotic dynamics:\n",
      "  precision of training data vs precision of the algorithm\n",
      "  We explore the influence of precision of the data and the algorithm for the\n",
      "simulation of chaotic dynamics by neural networks techniques. For this purpose,\n",
      "we simulate the Lorenz system with different precisions using three different\n",
      "neural network techniques adapted to time series, namely reservoir computing\n",
      "(using ESN), LSTM and TCN, for both short and long time predictions, and assess\n",
      "their efficiency and accuracy. Our results show that the ESN network is better\n",
      "at predicting accurately the dynamics of the system, and that in all cases the\n",
      "precision of the algorithm is more important than the precision of the training\n",
      "data for the accuracy of the predictions. This result gives support to the idea\n",
      "that neural networks can perform time-series predictions in many practical\n",
      "applications for which data are necessarily of limited precision, in line with\n",
      "recent results. It also suggests that for a given set of data the reliability\n",
      "of the predictions can be significantly improved by using a network with higher\n",
      "precision than the one of the data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.12853 \n",
      "Title :Optimising HEP parameter fits via Monte Carlo weight derivative\n",
      "  regression\n",
      "  HEP event selection is traditionally considered a binary classification\n",
      "problem, involving the dichotomous categories of signal and background. In\n",
      "distribution fits for particle masses or couplings, however, signal events are\n",
      "not all equivalent, as the signal differential cross section has different\n",
      "sensitivities to the measured parameter in different regions of phase space. In\n",
      "this paper, I describe a mathematical framework for the evaluation and\n",
      "optimization of HEP parameter fits, where this sensitivity is defined on an\n",
      "event-by-event basis, and for MC events it is modeled in terms of their MC\n",
      "weight derivatives with respect to the measured parameter. Minimising the\n",
      "statistical error on a measurement implies the need to resolve (i.e. separate)\n",
      "events with different sensitivities, which ultimately represents a\n",
      "non-dichotomous classification problem. Since MC weight derivatives are not\n",
      "available for real data, the practical strategy I suggest consists in training\n",
      "a regressor of weight derivatives against MC events, and then using it as an\n",
      "optimal partitioning variable for 1-dimensional fits of data events. This\n",
      "CHEP2019 paper is an extension of the study presented at CHEP2018: in\n",
      "particular, event-by-event sensitivities allow the exact computation of the\n",
      "\"FIP\" ratio between the Fisher information obtained from an analysis and the\n",
      "maximum information that could possibly be obtained with an ideal detector.\n",
      "Using this expression, I discuss the relationship between FIP and two metrics\n",
      "commonly used in Meteorology (Brier score and MSE), and the importance of\n",
      "\"sharpness\" both in HEP and in that domain. I finally point out that HEP\n",
      "distribution fits should be optimized and evaluated using probabilistic metrics\n",
      "(like FIP or MSE), whereas ranking metrics (like AUC) or threshold metrics\n",
      "(like accuracy) are of limited relevance for these specific problems.\n",
      "\n",
      "**Paper Id :1910.09358 \n",
      "Title :A Decision-Theoretic Approach for Model Interpretability in Bayesian\n",
      "  Framework\n",
      "  A salient approach to interpretable machine learning is to restrict modeling\n",
      "to simple models. In the Bayesian framework, this can be pursued by restricting\n",
      "the model structure and prior to favor interpretable models. Fundamentally,\n",
      "however, interpretability is about users' preferences, not the data generation\n",
      "mechanism; it is more natural to formulate interpretability as a utility\n",
      "function. In this work, we propose an interpretability utility, which\n",
      "explicates the trade-off between explanation fidelity and interpretability in\n",
      "the Bayesian framework. The method consists of two steps. First, a reference\n",
      "model, possibly a black-box Bayesian predictive model which does not compromise\n",
      "accuracy, is fitted to the training data. Second, a proxy model from an\n",
      "interpretable model family that best mimics the predictive behaviour of the\n",
      "reference model is found by optimizing the interpretability utility function.\n",
      "The approach is model agnostic -- neither the interpretable model nor the\n",
      "reference model are restricted to a certain class of models -- and the\n",
      "optimization problem can be solved using standard tools. Through experiments on\n",
      "real-word data sets, using decision trees as interpretable models and Bayesian\n",
      "additive regression models as reference models, we show that for the same level\n",
      "of interpretability, our approach generates more accurate models than the\n",
      "alternative of restricting the prior. We also propose a systematic way to\n",
      "measure stability of interpretabile models constructed by different\n",
      "interpretability approaches and show that our proposed approach generates more\n",
      "stable models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13145 \n",
      "Title :Can AI help in screening Viral and COVID-19 pneumonia?\n",
      "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\n",
      "caused thousands of causalities and infected several millions of people\n",
      "worldwide. Any technological tool enabling rapid screening of the COVID-19\n",
      "infection with high accuracy can be crucially helpful to healthcare\n",
      "professionals. The main clinical tool currently in use for the diagnosis of\n",
      "COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\n",
      "is expensive, less-sensitive and requires specialized medical personnel. X-ray\n",
      "imaging is an easily accessible tool that can be an excellent alternative in\n",
      "the COVID-19 diagnosis. This research was taken to investigate the utility of\n",
      "artificial intelligence (AI) in the rapid and accurate detection of COVID-19\n",
      "from chest X-ray images. The aim of this paper is to propose a robust technique\n",
      "for automatic detection of COVID-19 pneumonia from digital chest X-ray images\n",
      "applying pre-trained deep-learning algorithms while maximizing the detection\n",
      "accuracy. A public database was created by the authors combining several public\n",
      "databases and also by collecting images from recently published articles. The\n",
      "database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\n",
      "normal chest X-ray images. Transfer learning technique was used with the help\n",
      "of image augmentation to train and validate several pre-trained deep\n",
      "Convolutional Neural Networks (CNNs). The networks were trained to classify two\n",
      "different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\n",
      "COVID-19 pneumonia with and without image augmentation. The classification\n",
      "accuracy, precision, sensitivity, and specificity for both the schemes were\n",
      "99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\n",
      "respectively.\n",
      "\n",
      "**Paper Id :2006.12229 \n",
      "Title :Improving performance of CNN to predict likelihood of COVID-19 using\n",
      "  chest X-ray images with preprocessing algorithms\n",
      "  As the rapid spread of coronavirus disease (COVID-19) worldwide, chest X-ray\n",
      "radiography has also been used to detect COVID-19 infected pneumonia and assess\n",
      "its severity or monitor its prognosis in the hospitals due to its low cost, low\n",
      "radiation dose, and wide accessibility. However, how to more accurately and\n",
      "efficiently detect COVID-19 infected pneumonia and distinguish it from other\n",
      "community-acquired pneumonia remains a challenge. In order to address this\n",
      "challenge, we in this study develop and test a new computer-aided diagnosis\n",
      "(CAD) scheme. It includes several image pre-processing algorithms to remove\n",
      "diaphragms, normalize image contrast-to-noise ratio, and generate three input\n",
      "images, then links to a transfer learning based convolutional neural network (a\n",
      "VGG16 based CNN model) to classify chest X-ray images into three classes of\n",
      "COVID-19 infected pneumonia, other community-acquired pneumonia and normal\n",
      "(non-pneumonia) cases. To this purpose, a publicly available dataset of 8,474\n",
      "chest X-ray images is used, which includes 415 confirmed COVID-19 infected\n",
      "pneumonia, 5,179 community-acquired pneumonia, and 2,880 non-pneumonia cases.\n",
      "The dataset is divided into two subsets with 90% and 10% of images in each\n",
      "subset to train and test the CNN-based CAD scheme. The testing results achieve\n",
      "94.0% of overall accuracy in classifying three classes and 98.6% accuracy in\n",
      "detecting Covid-19 infected cases. Thus, the study demonstrates the feasibility\n",
      "of developing a CAD scheme of chest X-ray images and providing radiologists\n",
      "useful decision-making supporting tools in detecting and diagnosis of COVID-19\n",
      "infected pneumonia.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13300 \n",
      "Title :Weighted Random Search for CNN Hyperparameter Optimization\n",
      "  Nearly all model algorithms used in machine learning use two different sets\n",
      "of parameters: the training parameters and the meta-parameters\n",
      "(hyperparameters). While the training parameters are learned during the\n",
      "training phase, the values of the hyperparameters have to be specified before\n",
      "learning starts. For a given dataset, we would like to find the optimal\n",
      "combination of hyperparameter values, in a reasonable amount of time. This is a\n",
      "challenging task because of its computational complexity. In previous work\n",
      "[11], we introduced the Weighted Random Search (WRS) method, a combination of\n",
      "Random Search (RS) and probabilistic greedy heuristic. In the current paper, we\n",
      "compare the WRS method with several state-of-the art hyperparameter\n",
      "optimization methods with respect to Convolutional Neural Network (CNN)\n",
      "hyperparameter optimization. The criterion is the classification accuracy\n",
      "achieved within the same number of tested combinations of hyperparameter\n",
      "values. According to our experiments, the WRS algorithm outperforms the other\n",
      "methods.\n",
      "\n",
      "**Paper Id :2010.05609 \n",
      "Title :Load What You Need: Smaller Versions of Multilingual BERT\n",
      "  Pre-trained Transformer-based models are achieving state-of-the-art results\n",
      "on a variety of Natural Language Processing data sets. However, the size of\n",
      "these models is often a drawback for their deployment in real production\n",
      "applications. In the case of multilingual models, most of the parameters are\n",
      "located in the embeddings layer. Therefore, reducing the vocabulary size should\n",
      "have an important impact on the total number of parameters. In this paper, we\n",
      "propose to generate smaller models that handle fewer number of languages\n",
      "according to the targeted corpora. We present an evaluation of smaller versions\n",
      "of multilingual BERT on the XNLI data set, but we believe that this method may\n",
      "be applied to other multilingual transformers. The obtained results confirm\n",
      "that we can generate smaller models that keep comparable results, while\n",
      "reducing up to 45% of the total number of parameters. We compared our models\n",
      "with DistilmBERT (a distilled version of multilingual BERT) and showed that\n",
      "unlike language reduction, distillation induced a 1.7% to 6% drop in the\n",
      "overall accuracy on the XNLI data set. The presented models and code are\n",
      "publicly available.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13376 \n",
      "Title :End-to-End Evaluation of Federated Learning and Split Learning for\n",
      "  Internet of Things\n",
      "  This work is the first attempt to evaluate and compare felderated learning\n",
      "(FL) and split neural networks (SplitNN) in real-world IoT settings in terms of\n",
      "learning performance and device implementation overhead. We consider a variety\n",
      "of datasets, different model architectures, multiple clients, and various\n",
      "performance metrics. For learning performance, which is specified by the model\n",
      "accuracy and convergence speed metrics, we empirically evaluate both FL and\n",
      "SplitNN under different types of data distributions such as imbalanced and\n",
      "non-independent and identically distributed (non-IID) data. We show that the\n",
      "learning performance of SplitNN is better than FL under an imbalanced data\n",
      "distribution, but worse than FL under an extreme non-IID data distribution. For\n",
      "implementation overhead, we end-to-end mount both FL and SplitNN on Raspberry\n",
      "Pis, and comprehensively evaluate overheads including training time,\n",
      "communication overhead under the real LAN setting, power consumption and memory\n",
      "usage. Our key observations are that under IoT scenario where the communication\n",
      "traffic is the main concern, the FL appears to perform better over SplitNN\n",
      "because FL has the significantly lower communication overhead compared with\n",
      "SplitNN, which empirically corroborate previous statistical analysis. In\n",
      "addition, we reveal several unrecognized limitations about SplitNN, forming the\n",
      "basis for future research.\n",
      "\n",
      "**Paper Id :2005.12379 \n",
      "Title :Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias?\n",
      "  An Empirical Study on Model Fairness\n",
      "  Machine learning models are increasingly being used in important\n",
      "decision-making software such as approving bank loans, recommending criminal\n",
      "sentencing, hiring employees, and so on. It is important to ensure the fairness\n",
      "of these models so that no discrimination is made based on protected attribute\n",
      "(e.g., race, sex, age) while decision making. Algorithms have been developed to\n",
      "measure unfairness and mitigate them to a certain extent. In this paper, we\n",
      "have focused on the empirical evaluation of fairness and mitigations on\n",
      "real-world machine learning models. We have created a benchmark of 40 top-rated\n",
      "models from Kaggle used for 5 different tasks, and then using a comprehensive\n",
      "set of fairness metrics, evaluated their fairness. Then, we have applied 7\n",
      "mitigation techniques on these models and analyzed the fairness, mitigation\n",
      "results, and impacts on performance. We have found that some model optimization\n",
      "techniques result in inducing unfairness in the models. On the other hand,\n",
      "although there are some fairness control mechanisms in machine learning\n",
      "libraries, they are not documented. The mitigation algorithm also exhibit\n",
      "common patterns such as mitigation in the post-processing is often costly (in\n",
      "terms of performance) and mitigation in the pre-processing stage is preferred\n",
      "in most cases. We have also presented different trade-off choices of fairness\n",
      "mitigation decisions. Our study suggests future research directions to reduce\n",
      "the gap between theoretical fairness aware algorithms and the software\n",
      "engineering methods to leverage them in practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13379 \n",
      "Title :Global Attention based Graph Convolutional Neural Networks for Improved\n",
      "  Materials Property Prediction\n",
      "  Machine learning (ML) methods have gained increasing popularity in exploring\n",
      "and developing new materials. More specifically, graph neural network (GNN) has\n",
      "been applied in predicting material properties. In this work, we develop a\n",
      "novel model, GATGNN, for predicting inorganic material properties based on\n",
      "graph neural networks composed of multiple graph-attention layers (GAT) and a\n",
      "global attention layer. Through the application of the GAT layers, our model\n",
      "can efficiently learn the complex bonds shared among the atoms within each\n",
      "atom's local neighborhood. Subsequently, the global attention layer provides\n",
      "the weight coefficients of each atom in the inorganic crystal material which\n",
      "are used to considerably improve our model's performance. Notably, with the\n",
      "development of our GATGNN model, we show that our method is able to both\n",
      "outperform the previous models' predictions and provide insight into the\n",
      "crystallization of the material.\n",
      "\n",
      "**Paper Id :2004.05707 \n",
      "Title :VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification\n",
      "  Much progress has been made recently on text classification with methods\n",
      "based on neural networks. In particular, models using attention mechanism such\n",
      "as BERT have shown to have the capability of capturing the contextual\n",
      "information within a sentence or document. However, their ability of capturing\n",
      "the global information about the vocabulary of a language is more limited. This\n",
      "latter is the strength of Graph Convolutional Networks (GCN). In this paper, we\n",
      "propose VGCN-BERT model which combines the capability of BERT with a Vocabulary\n",
      "Graph Convolutional Network (VGCN). Local information and global information\n",
      "interact through different layers of BERT, allowing them to influence mutually\n",
      "and to build together a final representation for classification. In our\n",
      "experiments on several text classification datasets, our approach outperforms\n",
      "BERT and GCN alone, and achieve higher effectiveness than that reported in\n",
      "previous studies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13418 \n",
      "Title :Machine Learning Enabled Discovery of Application Dependent Design\n",
      "  Principles for Two-dimensional Materials\n",
      "  The large-scale search for high-performing candidate 2D materials is limited\n",
      "to calculating a few simple descriptors, usually with first-principles density\n",
      "functional theory calculations. In this work, we alleviate this issue by\n",
      "extending and generalizing crystal graph convolutional neural networks to\n",
      "systems with planar periodicity, and train an ensemble of models to predict\n",
      "thermodynamic, mechanical, and electronic properties. To demonstrate the\n",
      "utility of this approach, we carry out a screening of nearly 45,000 structures\n",
      "for two largely disjoint applications: namely, mechanically robust composites\n",
      "and photovoltaics. An analysis of the uncertainty associated with our methods\n",
      "indicates the ensemble of neural networks is well-calibrated and has errors\n",
      "comparable with those from accurate first-principles density functional theory\n",
      "calculations. The ensemble of models allows us to gauge the confidence of our\n",
      "predictions, and to find the candidates most likely to exhibit effective\n",
      "performance in their applications. Since the datasets used in our screening\n",
      "were combinatorically generated, we are also able to investigate, using an\n",
      "innovative method, structural and compositional design principles that impact\n",
      "the properties of the structures surveyed and which can act as a generative\n",
      "model basis for future material discovery through reverse engineering. Our\n",
      "approach allowed us to recover some well-accepted design principles: for\n",
      "instance, we find that hybrid organic-inorganic perovskites with lead and tin\n",
      "tend to be good candidates for solar cell applications.\n",
      "\n",
      "**Paper Id :2003.01504 \n",
      "Title :Towards Novel Insights in Lattice Field Theory with Explainable Machine\n",
      "  Learning\n",
      "  Machine learning has the potential to aid our understanding of phase\n",
      "structures in lattice quantum field theories through the statistical analysis\n",
      "of Monte Carlo samples. Available algorithms, in particular those based on deep\n",
      "learning, often demonstrate remarkable performance in the search for previously\n",
      "unidentified features, but tend to lack transparency if applied naively. To\n",
      "address these shortcomings, we propose representation learning in combination\n",
      "with interpretability methods as a framework for the identification of\n",
      "observables. More specifically, we investigate action parameter regression as a\n",
      "pretext task while using layer-wise relevance propagation (LRP) to identify the\n",
      "most important observables depending on the location in the phase diagram. The\n",
      "approach is put to work in the context of a scalar Yukawa model in (2+1)d.\n",
      "First, we investigate a multilayer perceptron to determine an importance\n",
      "hierarchy of several predefined, standard observables. The method is then\n",
      "applied directly to the raw field configurations using a convolutional network,\n",
      "demonstrating the ability to reconstruct all order parameters from the learned\n",
      "filter weights. Based on our results, we argue that due to its broad\n",
      "applicability, attribution methods such as LRP could prove a useful and\n",
      "versatile tool in our search for new physical insights. In the case of the\n",
      "Yukawa model, it facilitates the construction of an observable that\n",
      "characterises the symmetric phase.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13425 \n",
      "Title :Predicting Elastic Properties of Materials from Electronic Charge\n",
      "  Density Using 3D Deep Convolutional Neural Networks\n",
      "  Materials representation plays a key role in machine learning based\n",
      "prediction of materials properties and new materials discovery. Currently both\n",
      "graph and 3D voxel representation methods are based on the heterogeneous\n",
      "elements of the crystal structures. Here, we propose to use electronic charge\n",
      "density (ECD) as a generic unified 3D descriptor for materials property\n",
      "prediction with the advantage of possessing close relation with the physical\n",
      "and chemical properties of materials. We developed an ECD based 3D\n",
      "convolutional neural networks (CNNs) for predicting elastic properties of\n",
      "materials, in which CNNs can learn effective hierarchical features with\n",
      "multiple convolving and pooling operations. Extensive benchmark experiments\n",
      "over 2,170 Fm-3m face-centered-cubic (FCC) materials show that our ECD based\n",
      "CNNs can achieve good performance for elasticity prediction. Especially, our\n",
      "CNN models based on the fusion of elemental Magpie features and ECD descriptors\n",
      "achieved the best 5-fold cross-validation performance. More importantly, we\n",
      "showed that our ECD based CNN models can achieve significantly better\n",
      "extrapolation performance when evaluated over non-redundant datasets where\n",
      "there are few neighbor training samples around test samples. As additional\n",
      "validation, we evaluated the predictive performance of our models on 329\n",
      "materials of space group Fm-3m by comparing to DFT calculated values, which\n",
      "shows better prediction power of our model for bulk modulus than shear modulus.\n",
      "Due to the unified representation power of ECD, it is expected that our ECD\n",
      "based CNN approach can also be applied to predict other physical and chemical\n",
      "properties of crystalline materials.\n",
      "\n",
      "**Paper Id :2008.06415 \n",
      "Title :Orbital Graph Convolutional Neural Network for Material Property\n",
      "  Prediction\n",
      "  Material representations that are compatible with machine learning models\n",
      "play a key role in developing models that exhibit high accuracy for property\n",
      "prediction. Atomic orbital interactions are one of the important factors that\n",
      "govern the properties of crystalline materials, from which the local chemical\n",
      "environments of atoms is inferred. Therefore, to develop robust machine\n",
      "learningmodels for material properties prediction, it is imperative to include\n",
      "features representing such chemical attributes. Here, we propose the Orbital\n",
      "Graph Convolutional Neural Network (OGCNN), a crystal graph convolutional\n",
      "neural network framework that includes atomic orbital interaction features that\n",
      "learns material properties in a robust way. In addition, we embedded an\n",
      "encoder-decoder network into the OGCNN enabling it to learn important features\n",
      "among basic atomic (elemental features), orbital-orbital interactions, and\n",
      "topological features. We examined the performance of this model on a broad\n",
      "range of crystalline material data to predict different properties. We\n",
      "benchmarked the performance of the OGCNN model with that of: 1) the crystal\n",
      "graph convolutional neural network (CGCNN), 2) other state-of-the-art\n",
      "descriptors for material representations including Many-body Tensor\n",
      "Representation (MBTR) and the Smooth Overlap of Atomic Positions (SOAP), and 3)\n",
      "other conventional regression machine learning algorithms where different\n",
      "crystal featurization methods have been used. We find that OGCNN significantly\n",
      "outperforms them. The OGCNN model with high predictive accuracy can be used to\n",
      "discover new materials among the immense phase and compound spaces of materials\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13428 \n",
      "Title :Cost-effective search for lower-error region in material parameter space\n",
      "  using multifidelity Gaussian process modeling\n",
      "  Information regarding precipitate shapes is critical for estimating material\n",
      "parameters. Hence, we considered estimating a region of material parameter\n",
      "space in which a computational model produces precipitates having shapes\n",
      "similar to those observed in the experimental images. This region, called the\n",
      "lower-error region (LER), reflects intrinsic information of the material\n",
      "contained in the precipitate shapes. However, the computational cost of LER\n",
      "estimation can be high because the accurate computation of the model is\n",
      "required many times to better explore parameters. To overcome this difficulty,\n",
      "we used a Gaussian-process-based multifidelity modeling, in which training data\n",
      "can be sampled from multiple computations with different accuracy levels\n",
      "(fidelity). Lower-fidelity samples may have lower accuracy, but the\n",
      "computational cost is lower than that for higher-fidelity samples. Our proposed\n",
      "sampling procedure iteratively determines the most cost-effective pair of a\n",
      "point and a fidelity level for enhancing the accuracy of LER estimation. We\n",
      "demonstrated the efficiency of our method through estimation of the interface\n",
      "energy and lattice mismatch between MgZn2 and {\\alpha}-Mg phases in an Mg-based\n",
      "alloy. The results showed that the sampling cost required to obtain accurate\n",
      "LER estimation could be drastically reduced.\n",
      "\n",
      "**Paper Id :1911.01220 \n",
      "Title :Learning-based estimation of dielectric properties and tissue density in\n",
      "  head models for personalized radio-frequency dosimetry\n",
      "  Radio-frequency dosimetry is an important process in human safety and for\n",
      "compliance of related products. Recently, computational human models generated\n",
      "from medical images have often been used for such assessment, especially to\n",
      "consider the inter-variability of subjects. However, the common procedure to\n",
      "develop personalized models is time consuming because it involves excessive\n",
      "segmentation of several components that represent different biological tissues,\n",
      "which limits the inter-variability assessment of radiation safety based on\n",
      "personalized dosimetry. Deep learning methods have been shown to be a powerful\n",
      "approach for pattern recognition and signal analysis. Convolutional neural\n",
      "networks with deep architecture are proven robust for feature extraction and\n",
      "image mapping in several biomedical applications. In this study, we develop a\n",
      "learning-based approach for fast and accurate estimation of the dielectric\n",
      "properties and density of tissues directly from magnetic resonance images in a\n",
      "single shot. The smooth distribution of the dielectric properties in head\n",
      "models, which is realized using a process without tissue segmentation, improves\n",
      "the smoothness of the specific absorption rate (SAR) distribution compared with\n",
      "that in the commonly used procedure. The estimated SAR distributions, as well\n",
      "as that averaged over 10-g of tissue in a cubic shape, are found to be highly\n",
      "consistent with those computed using the conventional methods that employ\n",
      "segmentation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.13834 \n",
      "Title :Label-Efficient Learning on Point Clouds using Approximate Convex\n",
      "  Decompositions\n",
      "  The problems of shape classification and part segmentation from 3D point\n",
      "clouds have garnered increasing attention in the last few years. Both of these\n",
      "problems, however, suffer from relatively small training sets, creating the\n",
      "need for statistically efficient methods to learn 3D shape representations. In\n",
      "this paper, we investigate the use of Approximate Convex Decompositions (ACD)\n",
      "as a self-supervisory signal for label-efficient learning of point cloud\n",
      "representations. We show that using ACD to approximate ground truth\n",
      "segmentation provides excellent self-supervision for learning 3D point cloud\n",
      "representations that are highly effective on downstream tasks. We report\n",
      "improvements over the state-of-the-art for unsupervised representation learning\n",
      "on the ModelNet40 shape classification dataset and significant gains in\n",
      "few-shot part segmentation on the ShapeNetPart dataset.Code available at\n",
      "https://github.com/matheusgadelha/PointCloudLearningACD\n",
      "\n",
      "**Paper Id :1911.11237 \n",
      "Title :Learning to Learn Words from Visual Scenes\n",
      "  Language acquisition is the process of learning words from the surrounding\n",
      "scene. We introduce a meta-learning framework that learns how to learn word\n",
      "representations from unconstrained scenes. We leverage the natural\n",
      "compositional structure of language to create training episodes that cause a\n",
      "meta-learner to learn strong policies for language acquisition. Experiments on\n",
      "two datasets show that our approach is able to more rapidly acquire novel words\n",
      "as well as more robustly generalize to unseen compositions, significantly\n",
      "outperforming established baselines. A key advantage of our approach is that it\n",
      "is data efficient, allowing representations to be learned from scratch without\n",
      "language pre-training. Visualizations and analysis suggest visual information\n",
      "helps our approach learn a rich cross-modal representation from minimal\n",
      "examples. Project webpage is available at https://expert.cs.columbia.edu/\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.14058 \n",
      "Title :MTL-NAS: Task-Agnostic Neural Architecture Search towards\n",
      "  General-Purpose Multi-Task Learning\n",
      "  We propose to incorporate neural architecture search (NAS) into\n",
      "general-purpose multi-task learning (GP-MTL). Existing NAS methods typically\n",
      "define different search spaces according to different tasks. In order to adapt\n",
      "to different task combinations (i.e., task sets), we disentangle the GP-MTL\n",
      "networks into single-task backbones (optionally encode the task priors), and a\n",
      "hierarchical and layerwise features sharing/fusing scheme across them. This\n",
      "enables us to design a novel and general task-agnostic search space, which\n",
      "inserts cross-task edges (i.e., feature fusion connections) into fixed\n",
      "single-task network backbones. Moreover, we also propose a novel single-shot\n",
      "gradient-based search algorithm that closes the performance gap between the\n",
      "searched architectures and the final evaluation architecture. This is realized\n",
      "with a minimum entropy regularization on the architecture weights during the\n",
      "search phase, which makes the architecture weights converge to near-discrete\n",
      "values and therefore achieves a single model. As a result, our searched model\n",
      "can be directly used for evaluation without (re-)training from scratch. We\n",
      "perform extensive experiments using different single-task backbones on various\n",
      "task sets, demonstrating the promising performance obtained by exploiting the\n",
      "hierarchical and layerwise features, as well as the desirable generalizability\n",
      "to different i) task sets and ii) single-task backbones. The code of our paper\n",
      "is available at https://github.com/bhpfelix/MTLNAS.\n",
      "\n",
      "**Paper Id :2002.10306 \n",
      "Title :Adaptive Propagation Graph Convolutional Network\n",
      "  Graph convolutional networks (GCNs) are a family of neural network models\n",
      "that perform inference on graph data by interleaving vertex-wise operations and\n",
      "message-passing exchanges across nodes. Concerning the latter, two key\n",
      "questions arise: (i) how to design a differentiable exchange protocol (e.g., a\n",
      "1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize\n",
      "the trade-off in complexity with respect to the local updates. In this paper,\n",
      "we show that state-of-the-art results can be achieved by adapting the number of\n",
      "communication steps independently at every node. In particular, we endow each\n",
      "node with a halting unit (inspired by Graves' adaptive computation time) that\n",
      "after every exchange decides whether to continue communicating or not. We show\n",
      "that the proposed adaptive propagation GCN (AP-GCN) achieves superior or\n",
      "similar results to the best proposed models so far on a number of benchmarks,\n",
      "while requiring a small overhead in terms of additional parameters. We also\n",
      "investigate a regularization term to enforce an explicit trade-off between\n",
      "communication and accuracy. The code for the AP-GCN experiments is released as\n",
      "an open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2003.14358 \n",
      "Title :Controlling Rayleigh-B\\'enard convection via Reinforcement Learning\n",
      "  Thermal convection is ubiquitous in nature as well as in many industrial\n",
      "applications. The identification of effective control strategies to, e.g.,\n",
      "suppress or enhance the convective heat exchange under fixed external thermal\n",
      "gradients is an outstanding fundamental and technological issue. In this work,\n",
      "we explore a novel approach, based on a state-of-the-art Reinforcement Learning\n",
      "(RL) algorithm, which is capable of significantly reducing the heat transport\n",
      "in a two-dimensional Rayleigh-B\\'enard system by applying small temperature\n",
      "fluctuations to the lower boundary of the system. By using numerical\n",
      "simulations, we show that our RL-based control is able to stabilize the\n",
      "conductive regime and bring the onset of convection up to a Rayleigh number\n",
      "$Ra_c \\approx 3 \\cdot 10^4$, whereas in the uncontrolled case it holds\n",
      "$Ra_{c}=1708$. Additionally, for $Ra > 3 \\cdot 10^4$, our approach outperforms\n",
      "other state-of-the-art control algorithms reducing the heat flux by a factor of\n",
      "about $2.5$. In the last part of the manuscript, we address theoretical limits\n",
      "connected to controlling an unstable and chaotic dynamics as the one considered\n",
      "here. We show that controllability is hindered by observability and/or\n",
      "capabilities of actuating actions, which can be quantified in terms of\n",
      "characteristic time delays. When these delays become comparable with the\n",
      "Lyapunov time of the system, control becomes impossible.\n",
      "\n",
      "**Paper Id :2001.01520 \n",
      "Title :Combining data assimilation and machine learning to emulate a dynamical\n",
      "  model from sparse and noisy observations: a case study with the Lorenz 96\n",
      "  model\n",
      "  A novel method, based on the combination of data assimilation and machine\n",
      "learning is introduced. The new hybrid approach is designed for a two-fold\n",
      "scope: (i) emulating hidden, possibly chaotic, dynamics and (ii) predicting\n",
      "their future states. The method consists in applying iteratively a data\n",
      "assimilation step, here an ensemble Kalman filter, and a neural network. Data\n",
      "assimilation is used to optimally combine a surrogate model with sparse noisy\n",
      "data. The output analysis is spatially complete and is used as a training set\n",
      "by the neural network to update the surrogate model. The two steps are then\n",
      "repeated iteratively. Numerical experiments have been carried out using the\n",
      "chaotic 40-variables Lorenz 96 model, proving both convergence and statistical\n",
      "skill of the proposed hybrid approach. The surrogate model shows short-term\n",
      "forecast skill up to two Lyapunov times, the retrieval of positive Lyapunov\n",
      "exponents as well as the more energetic frequencies of the power density\n",
      "spectrum. The sensitivity of the method to critical setup parameters is also\n",
      "presented: the forecast skill decreases smoothly with increased observational\n",
      "noise but drops abruptly if less than half of the model domain is observed. The\n",
      "successful synergy between data assimilation and machine learning, proven here\n",
      "with a low-dimensional system, encourages further investigation of such hybrids\n",
      "with more sophisticated dynamics.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.00588 \n",
      "Title :Better Sign Language Translation with STMC-Transformer\n",
      "  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\n",
      "system to extract sign language glosses from videos. Then, a translation system\n",
      "generates spoken language translations from the sign language glosses. This\n",
      "paper focuses on the translation system and introduces the STMC-Transformer\n",
      "which improves on the current state-of-the-art by over 5 and 7 BLEU\n",
      "respectively on gloss-to-text and video-to-text translation of the\n",
      "PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\n",
      "of over 16 BLEU.\n",
      "  We also demonstrate the problem in current methods that rely on gloss\n",
      "supervision. The video-to-text translation of our STMC-Transformer outperforms\n",
      "translation of GT glosses. This contradicts previous claims that GT gloss\n",
      "translation acts as an upper bound for SLT performance and reveals that glosses\n",
      "are an inefficient representation of sign language. For future SLT research, we\n",
      "therefore suggest an end-to-end training of the recognition and translation\n",
      "models, or using a different sign language annotation scheme.\n",
      "\n",
      "**Paper Id :2007.16011 \n",
      "Title :Neural Machine Translation model for University Email Application\n",
      "  Machine translation has many applications such as news translation, email\n",
      "translation, official letter translation etc. Commercial translators, e.g.\n",
      "Google Translation lags in regional vocabulary and are unable to learn the\n",
      "bilingual text in the source and target languages within the input. In this\n",
      "paper, a regional vocabulary-based application-oriented Neural Machine\n",
      "Translation (NMT) model is proposed over the data set of emails used at the\n",
      "University for communication over a period of three years. A state-of-the-art\n",
      "Sequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is\n",
      "compared with Google Translate using Gated Recurrent Unit Recurrent Neural\n",
      "Network machine translation model with attention decoder. The low BLEU score of\n",
      "Google Translation in comparison to our model indicates that the application\n",
      "based regional models are better. The low BLEU score of EN -> ML of our model\n",
      "and Google Translation indicates that the Malay Language has complex language\n",
      "features corresponding to English.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.00959 \n",
      "Title :Neural network based country wise risk prediction of COVID-19\n",
      "  The recent worldwide outbreak of the novel coronavirus (COVID-19) has opened\n",
      "up new challenges to the research community. Artificial intelligence (AI)\n",
      "driven methods can be useful to predict the parameters, risks, and effects of\n",
      "such an epidemic. Such predictions can be helpful to control and prevent the\n",
      "spread of such diseases. The main challenges of applying AI is the small volume\n",
      "of data and the uncertain nature. Here, we propose a shallow long short-term\n",
      "memory (LSTM) based neural network to predict the risk category of a country.\n",
      "We have used a Bayesian optimization framework to optimize and automatically\n",
      "design country-specific networks. The results show that the proposed pipeline\n",
      "outperforms state-of-the-art methods for data of 180 countries and can be a\n",
      "useful tool for such risk categorization. We have also experimented with the\n",
      "trend data and weather data combined for the prediction. The outcome shows that\n",
      "the weather does not have a significant role. The tool can be used to predict\n",
      "long-duration outbreak of such an epidemic such that we can take preventive\n",
      "steps earlier\n",
      "\n",
      "**Paper Id :1905.10891 \n",
      "Title :A hybrid model for predicting human physical activity status from\n",
      "  lifelogging data\n",
      "  One trend in the recent healthcare transformations is people are encouraged\n",
      "to monitor and manage their health based on their daily diets and physical\n",
      "activity habits. However, much attention of the use of operational research and\n",
      "analytical models in healthcare has been paid to the systematic level such as\n",
      "country or regional policy making or organisational issues. This paper proposes\n",
      "a model concerned with healthcare analytics at the individual level, which can\n",
      "predict human physical activity status from sequential lifelogging data\n",
      "collected from wearable sensors. The model has a two-stage hybrid structure (in\n",
      "short, MOGP-HMM) -- a multi-objective genetic programming (MOGP) algorithm in\n",
      "the first stage to reduce the dimensions of lifelogging data and a hidden\n",
      "Markov model (HMM) in the second stage for activity status prediction over\n",
      "time. It can be used as a decision support tool to provide real-time\n",
      "monitoring, statistical analysis and personalized advice to individuals,\n",
      "encouraging positive attitudes towards healthy lifestyles. We validate the\n",
      "model with the real data collected from a group of participants in the UK, and\n",
      "compare it with other popular two-stage hybrid models. Our experimental results\n",
      "show that the MOGP-HMM can achieve comparable performance. To the best of our\n",
      "knowledge, this is the very first study that uses the MOGP in the hybrid\n",
      "two-stage structure for individuals' activity status prediction. It fits\n",
      "seamlessly with the current trend in the UK healthcare transformation of\n",
      "patient empowerment as well as contributing to a strategic development for more\n",
      "efficient and cost-effective provision of healthcare.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.01275 \n",
      "Title :AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough\n",
      "  Samples via an App\n",
      "  Background: The inability to test at scale has become humanity's Achille's\n",
      "heel in the ongoing war against the COVID-19 pandemic. A scalable screening\n",
      "tool would be a game changer. Building on the prior work on cough-based\n",
      "diagnosis of respiratory diseases, we propose, develop and test an Artificial\n",
      "Intelligence (AI)-powered screening solution for COVID-19 infection that is\n",
      "deployable via a smartphone app. The app, named AI4COVID-19 records and sends\n",
      "three 3-second cough sounds to an AI engine running in the cloud, and returns a\n",
      "result within two minutes. Methods: Cough is a symptom of over thirty\n",
      "non-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19\n",
      "infection by cough alone an extremely challenging multidisciplinary problem. We\n",
      "address this problem by investigating the distinctness of pathomorphological\n",
      "alterations in the respiratory system induced by COVID-19 infection when\n",
      "compared to other respiratory infections. To overcome the COVID-19 cough\n",
      "training data shortage we exploit transfer learning. To reduce the misdiagnosis\n",
      "risk stemming from the complex dimensionality of the problem, we leverage a\n",
      "multi-pronged mediator centered risk-averse AI architecture. Results: Results\n",
      "show AI4COVID-19 can distinguish among COVID-19 coughs and several types of\n",
      "non-COVID-19 coughs. The accuracy is promising enough to encourage a\n",
      "large-scale collection of labeled cough data to gauge the generalization\n",
      "capability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool.\n",
      "Instead, it offers a screening tool deployable anytime, anywhere, by anyone. It\n",
      "can also be a clinical decision assistance tool used to channel\n",
      "clinical-testing and treatment to those who need it the most, thereby saving\n",
      "more lives.\n",
      "\n",
      "**Paper Id :2003.13145 \n",
      "Title :Can AI help in screening Viral and COVID-19 pneumonia?\n",
      "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\n",
      "caused thousands of causalities and infected several millions of people\n",
      "worldwide. Any technological tool enabling rapid screening of the COVID-19\n",
      "infection with high accuracy can be crucially helpful to healthcare\n",
      "professionals. The main clinical tool currently in use for the diagnosis of\n",
      "COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\n",
      "is expensive, less-sensitive and requires specialized medical personnel. X-ray\n",
      "imaging is an easily accessible tool that can be an excellent alternative in\n",
      "the COVID-19 diagnosis. This research was taken to investigate the utility of\n",
      "artificial intelligence (AI) in the rapid and accurate detection of COVID-19\n",
      "from chest X-ray images. The aim of this paper is to propose a robust technique\n",
      "for automatic detection of COVID-19 pneumonia from digital chest X-ray images\n",
      "applying pre-trained deep-learning algorithms while maximizing the detection\n",
      "accuracy. A public database was created by the authors combining several public\n",
      "databases and also by collecting images from recently published articles. The\n",
      "database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\n",
      "normal chest X-ray images. Transfer learning technique was used with the help\n",
      "of image augmentation to train and validate several pre-trained deep\n",
      "Convolutional Neural Networks (CNNs). The networks were trained to classify two\n",
      "different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\n",
      "COVID-19 pneumonia with and without image augmentation. The classification\n",
      "accuracy, precision, sensitivity, and specificity for both the schemes were\n",
      "99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\n",
      "respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.01293 \n",
      "Title :Motif-Based Spectral Clustering of Weighted Directed Networks\n",
      "  Clustering is an essential technique for network analysis, with applications\n",
      "in a diverse range of fields. Although spectral clustering is a popular and\n",
      "effective method, it fails to consider higher-order structure and can perform\n",
      "poorly on directed networks. One approach is to capture and cluster\n",
      "higher-order structures using motif adjacency matrices. However, current\n",
      "formulations fail to take edge weights into account, and thus are somewhat\n",
      "limited when weight is a key component of the network under study.\n",
      "  We address these shortcomings by exploring motif-based weighted spectral\n",
      "clustering methods. We present new and computationally useful matrix formulae\n",
      "for motif adjacency matrices on weighted networks, which can be used to\n",
      "construct efficient algorithms for any anchored or non-anchored motif on three\n",
      "nodes. In a very sparse regime, our proposed method can handle graphs with a\n",
      "million nodes and tens of millions of edges. We further use our framework to\n",
      "construct a motif-based approach for clustering bipartite networks.\n",
      "  We provide comprehensive experimental results, demonstrating (i) the\n",
      "scalability of our approach, (ii) advantages of higher-order clustering on\n",
      "synthetic examples, and (iii) the effectiveness of our techniques on a variety\n",
      "of real world data sets; and compare against several techniques from the\n",
      "literature. We conclude that motif-based spectral clustering is a valuable tool\n",
      "for analysis of directed and bipartite weighted networks, which is also\n",
      "scalable and easy to implement.\n",
      "\n",
      "**Paper Id :2004.04704 \n",
      "Title :Heuristics for Link Prediction in Multiplex Networks\n",
      "  Link prediction, or the inference of future or missing connections between\n",
      "entities, is a well-studied problem in network analysis. A multitude of\n",
      "heuristics exist for link prediction in ordinary networks with a single type of\n",
      "connection. However, link prediction in multiplex networks, or networks with\n",
      "multiple types of connections, is not a well understood problem. We propose a\n",
      "novel general framework and three families of heuristics for multiplex network\n",
      "link prediction that are simple, interpretable, and take advantage of the rich\n",
      "connection type correlation structure that exists in many real world networks.\n",
      "We further derive a theoretical threshold for determining when to use a\n",
      "different connection type based on the number of links that overlap with an\n",
      "Erdos-Renyi random graph. Through experiments with simulated and real world\n",
      "scientific collaboration, transportation and global trade networks, we\n",
      "demonstrate that the proposed heuristics show increased performance with the\n",
      "richness of connection type correlation structure and significantly outperform\n",
      "their baseline heuristics for ordinary networks with a single connection type.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.01546 \n",
      "Title :Temporarily-Aware Context Modelling using Generative Adversarial\n",
      "  Networks for Speech Activity Detection\n",
      "  This paper presents a novel framework for Speech Activity Detection (SAD).\n",
      "Inspired by the recent success of multi-task learning approaches in the speech\n",
      "processing domain, we propose a novel joint learning framework for SAD. We\n",
      "utilise generative adversarial networks to automatically learn a loss function\n",
      "for joint prediction of the frame-wise speech/ non-speech classifications\n",
      "together with the next audio segment. In order to exploit the temporal\n",
      "relationships within the input signal, we propose a temporal discriminator\n",
      "which aims to ensure that the predicted signal is temporally consistent. We\n",
      "evaluate the proposed framework on multiple public benchmarks, including NIST\n",
      "OpenSAT' 17, AMI Meeting and HAVIC, where we demonstrate its capability to\n",
      "outperform state-of-the-art SAD approaches. Furthermore, our cross-database\n",
      "evaluations demonstrate the robustness of the proposed approach across\n",
      "different languages, accents, and acoustic environments.\n",
      "\n",
      "**Paper Id :2002.08801 \n",
      "Title :Guiding attention in Sequence-to-sequence models for Dialogue Act\n",
      "  prediction\n",
      "  The task of predicting dialog acts (DA) based on conversational dialog is a\n",
      "key component in the development of conversational agents. Accurately\n",
      "predicting DAs requires a precise modeling of both the conversation and the\n",
      "global tag dependencies. We leverage seq2seq approaches widely adopted in\n",
      "Neural Machine Translation (NMT) to improve the modelling of tag sequentiality.\n",
      "Seq2seq models are known to learn complex global dependencies while currently\n",
      "proposed approaches using linear conditional random fields (CRF) only model\n",
      "local tag dependencies. In this work, we introduce a seq2seq model tailored for\n",
      "DA classification using: a hierarchical encoder, a novel guided attention\n",
      "mechanism and beam search applied to both training and inference. Compared to\n",
      "the state of the art our model does not require handcrafted features and is\n",
      "trained end-to-end. Furthermore, the proposed approach achieves an unmatched\n",
      "accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on\n",
      "MRDA.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.01857 \n",
      "Title :Weighted Fisher Discriminant Analysis in the Input and Feature Spaces\n",
      "  Fisher Discriminant Analysis (FDA) is a subspace learning method which\n",
      "minimizes and maximizes the intra- and inter-class scatters of data,\n",
      "respectively. Although, in FDA, all the pairs of classes are treated the same\n",
      "way, some classes are closer than the others. Weighted FDA assigns weights to\n",
      "the pairs of classes to address this shortcoming of FDA. In this paper, we\n",
      "propose a cosine-weighted FDA as well as an automatically weighted FDA in which\n",
      "weights are found automatically. We also propose a weighted FDA in the feature\n",
      "space to establish a weighted kernel FDA for both existing and newly proposed\n",
      "weights. Our experiments on the ORL face recognition dataset show the\n",
      "effectiveness of the proposed weighting schemes.\n",
      "\n",
      "**Paper Id :2004.04674 \n",
      "Title :Fisher Discriminant Triplet and Contrastive Losses for Training Siamese\n",
      "  Networks\n",
      "  Siamese neural network is a very powerful architecture for both feature\n",
      "extraction and metric learning. It usually consists of several networks that\n",
      "share weights. The Siamese concept is topology-agnostic and can use any neural\n",
      "network as its backbone. The two most popular loss functions for training these\n",
      "networks are the triplet and contrastive loss functions. In this paper, we\n",
      "propose two novel loss functions, named Fisher Discriminant Triplet (FDT) and\n",
      "Fisher Discriminant Contrastive (FDC). The former uses anchor-neighbor-distant\n",
      "triplets while the latter utilizes pairs of anchor-neighbor and anchor-distant\n",
      "samples. The FDT and FDC loss functions are designed based on the statistical\n",
      "formulation of the Fisher Discriminant Analysis (FDA), which is a linear\n",
      "subspace learning method. Our experiments on the MNIST and two challenging and\n",
      "publicly available histopathology datasets show the effectiveness of the\n",
      "proposed loss functions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.01864 \n",
      "Title :Theoretical Insights into the Use of Structural Similarity Index In\n",
      "  Generative Models and Inferential Autoencoders\n",
      "  Generative models and inferential autoencoders mostly make use of $\\ell_2$\n",
      "norm in their optimization objectives. In order to generate perceptually better\n",
      "images, this short paper theoretically discusses how to use Structural\n",
      "Similarity Index (SSIM) in generative models and inferential autoencoders. We\n",
      "first review SSIM, SSIM distance metrics, and SSIM kernel. We show that the\n",
      "SSIM kernel is a universal kernel and thus can be used in unconditional and\n",
      "conditional generated moment matching networks. Then, we explain how to use\n",
      "SSIM distance in variational and adversarial autoencoders and unconditional and\n",
      "conditional Generative Adversarial Networks (GANs). Finally, we propose to use\n",
      "SSIM distance rather than $\\ell_2$ norm in least squares GAN.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.01893 \n",
      "Title :ForecastTB An R Package as a Test-Bench for Time Series Forecasting\n",
      "  Application of Wind Speed and Solar Radiation Modeling\n",
      "  This paper introduces an R package ForecastTB that can be used to compare the\n",
      "accuracy of different forecasting methods as related to the characteristics of\n",
      "a time series dataset. The ForecastTB is a plug-and-play structured module, and\n",
      "several forecasting methods can be included with simple instructions. The\n",
      "proposed test-bench is not limited to the default forecasting and error metric\n",
      "functions, and users are able to append, remove, or choose the desired methods\n",
      "as per requirements. Besides, several plotting functions and statistical\n",
      "performance metrics are provided to visualize the comparative performance and\n",
      "accuracy of different forecasting methods. Furthermore, this paper presents\n",
      "real application examples with natural time series datasets (i.e., wind speed\n",
      "and solar radiation) to exhibit the features of the ForecastTB package to\n",
      "evaluate forecasting comparison analysis as affected by the characteristics of\n",
      "a dataset. Modeling results indicated the applicability and robustness of the\n",
      "proposed R package ForecastTB for time series forecasting.\n",
      "\n",
      "**Paper Id :1903.02787 \n",
      "Title :GRATIS: GeneRAting TIme Series with diverse and controllable\n",
      "  characteristics\n",
      "  The explosion of time series data in recent years has brought a flourish of\n",
      "new time series analysis methods, for forecasting, clustering, classification\n",
      "and other tasks. The evaluation of these new methods requires either collecting\n",
      "or simulating a diverse set of time series benchmarking data to enable reliable\n",
      "comparisons against alternative approaches. We propose GeneRAting TIme Series\n",
      "with diverse and controllable characteristics, named GRATIS, with the use of\n",
      "mixture autoregressive (MAR) models. We simulate sets of time series using MAR\n",
      "models and investigate the diversity and coverage of the generated time series\n",
      "in a time series feature space. By tuning the parameters of the MAR models,\n",
      "GRATIS is also able to efficiently generate new time series with controllable\n",
      "features. In general, as a costless surrogate to the traditional data\n",
      "collection approach, GRATIS can be used as an evaluation tool for tasks such as\n",
      "time series forecasting and classification. We illustrate the usefulness of our\n",
      "time series generation process through a time series forecasting application.\n",
      "\n",
      "\n",
      "*********\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Paper Id :2004.01899 \n",
      "Title :A Generic Graph-based Neural Architecture Encoding Scheme for\n",
      "  Predictor-based NAS\n",
      "  This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme,\n",
      "a.k.a. GATES, to improve the predictor-based neural architecture search.\n",
      "Specifically, different from existing graph-based schemes, GATES models the\n",
      "operations as the transformation of the propagating information, which mimics\n",
      "the actual data processing of neural architecture. GATES is a more reasonable\n",
      "modeling of the neural architectures, and can encode architectures from both\n",
      "the \"operation on node\" and \"operation on edge\" cell search spaces\n",
      "consistently. Experimental results on various search spaces confirm GATES's\n",
      "effectiveness in improving the performance predictor. Furthermore, equipped\n",
      "with the improved performance predictor, the sample efficiency of the\n",
      "predictor-based neural architecture search (NAS) flow is boosted. Codes are\n",
      "available at https://github.com/walkerning/aw_nas.\n",
      "\n",
      "**Paper Id :2003.13379 \n",
      "Title :Global Attention based Graph Convolutional Neural Networks for Improved\n",
      "  Materials Property Prediction\n",
      "  Machine learning (ML) methods have gained increasing popularity in exploring\n",
      "and developing new materials. More specifically, graph neural network (GNN) has\n",
      "been applied in predicting material properties. In this work, we develop a\n",
      "novel model, GATGNN, for predicting inorganic material properties based on\n",
      "graph neural networks composed of multiple graph-attention layers (GAT) and a\n",
      "global attention layer. Through the application of the GAT layers, our model\n",
      "can efficiently learn the complex bonds shared among the atoms within each\n",
      "atom's local neighborhood. Subsequently, the global attention layer provides\n",
      "the weight coefficients of each atom in the inorganic crystal material which\n",
      "are used to considerably improve our model's performance. Notably, with the\n",
      "development of our GATGNN model, we show that our method is able to both\n",
      "outperform the previous models' predictions and provide insight into the\n",
      "crystallization of the material.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.02137 \n",
      "Title :Anomaly Detection and Prototype Selection Using Polyhedron Curvature\n",
      "  We propose a novel approach to anomaly detection called Curvature Anomaly\n",
      "Detection (CAD) and Kernel CAD based on the idea of polyhedron curvature. Using\n",
      "the nearest neighbors for a point, we consider every data point as the vertex\n",
      "of a polyhedron where the more anomalous point has more curvature. We also\n",
      "propose inverse CAD (iCAD) and Kernel iCAD for instance ranking and prototype\n",
      "selection by looking at CAD from an opposite perspective. We define the concept\n",
      "of anomaly landscape and anomaly path and we demonstrate an application for it\n",
      "which is image denoising. The proposed methods are straightforward and easy to\n",
      "implement. Our experiments on different benchmarks show that the proposed\n",
      "methods are effective for anomaly detection and prototype selection.\n",
      "\n",
      "**Paper Id :2002.01793 \n",
      "Title :Proximity Preserving Binary Code using Signed Graph-Cut\n",
      "  We introduce a binary embedding framework, called Proximity Preserving Code\n",
      "(PPC), which learns similarity and dissimilarity between data points to create\n",
      "a compact and affinity-preserving binary code. This code can be used to apply\n",
      "fast and memory-efficient approximation to nearest-neighbor searches. Our\n",
      "framework is flexible, enabling different proximity definitions between data\n",
      "points. In contrast to previous methods that extract binary codes based on\n",
      "unsigned graph partitioning, our system models the attractive and repulsive\n",
      "forces in the data by incorporating positive and negative graph weights. The\n",
      "proposed framework is shown to boil down to finding the minimal cut of a signed\n",
      "graph, a problem known to be NP-hard. We offer an efficient approximation and\n",
      "achieve superior results by constructing the code bit after bit. We show that\n",
      "the proposed approximation is superior to the commonly used spectral methods\n",
      "with respect to both accuracy and complexity. Thus, it is useful for many other\n",
      "problems that can be translated into signed graph cut.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.02396 \n",
      "Title :A Learning Framework for n-bit Quantized Neural Networks toward FPGAs\n",
      "  The quantized neural network (QNN) is an efficient approach for network\n",
      "compression and can be widely used in the implementation of FPGAs. This paper\n",
      "proposes a novel learning framework for n-bit QNNs, whose weights are\n",
      "constrained to the power of two. To solve the gradient vanishing problem, we\n",
      "propose a reconstructed gradient function for QNNs in back-propagation\n",
      "algorithm that can directly get the real gradient rather than estimating an\n",
      "approximate gradient of the expected loss. We also propose a novel QNN\n",
      "structure named n-BQ-NN, which uses shift operation to replace the multiply\n",
      "operation and is more suitable for the inference on FPGAs. Furthermore, we also\n",
      "design a shift vector processing element (SVPE) array to replace all 16-bit\n",
      "multiplications with SHIFT operations in convolution operation on FPGAs. We\n",
      "also carry out comparable experiments to evaluate our framework. The\n",
      "experimental results show that the quantized models of ResNet, DenseNet and\n",
      "AlexNet through our learning framework can achieve almost the same accuracies\n",
      "with the original full-precision models. Moreover, when using our learning\n",
      "framework to train our n-BQ-NN from scratch, it can achieve state-of-the-art\n",
      "results compared with typical low-precision QNNs. Experiments on Xilinx ZCU102\n",
      "platform show that our n-BQ-NN with our SVPE can execute 2.9 times faster than\n",
      "with the vector processing element (VPE) in inference. As the SHIFT operation\n",
      "in our SVPE array will not consume Digital Signal Processings (DSPs) resources\n",
      "on FPGAs, the experiments have shown that the use of SVPE array also reduces\n",
      "average energy consumption to 68.7% of the VPE array with 16-bit.\n",
      "\n",
      "**Paper Id :2003.04296 \n",
      "Title :Propagating Asymptotic-Estimated Gradients for Low Bitwidth Quantized\n",
      "  Neural Networks\n",
      "  The quantized neural networks (QNNs) can be useful for neural network\n",
      "acceleration and compression, but during the training process they pose a\n",
      "challenge: how to propagate the gradient of loss function through the graph\n",
      "flow with a derivative of 0 almost everywhere. In response to this\n",
      "non-differentiable situation, we propose a novel Asymptotic-Quantized Estimator\n",
      "(AQE) to estimate the gradient. In particular, during back-propagation, the\n",
      "graph that relates inputs to output remains smoothness and differentiability.\n",
      "At the end of training, the weights and activations have been quantized to\n",
      "low-precision because of the asymptotic behaviour of AQE. Meanwhile, we propose\n",
      "a M-bit Inputs and N-bit Weights Network (MINW-Net) trained by AQE, a quantized\n",
      "neural network with 1-3 bits weights and activations. In the inference phase,\n",
      "we can use XNOR or SHIFT operations instead of convolution operations to\n",
      "accelerate the MINW-Net. Our experiments on CIFAR datasets demonstrate that our\n",
      "AQE is well defined, and the QNNs with AQE perform better than that with\n",
      "Straight-Through Estimator (STE). For example, in the case of the same ConvNet\n",
      "that has 1-bit weights and activations, our MINW-Net with AQE can achieve a\n",
      "prediction accuracy 1.5\\% higher than the Binarized Neural Network (BNN) with\n",
      "STE. The MINW-Net, which is trained from scratch by AQE, can achieve comparable\n",
      "classification accuracy as 32-bit counterparts on CIFAR test sets. Extensive\n",
      "experimental results on ImageNet dataset show great superiority of the proposed\n",
      "AQE and our MINW-Net achieves comparable results with other state-of-the-art\n",
      "QNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.02956 \n",
      "Title :Deblurring using Analysis-Synthesis Networks Pair\n",
      "  Blind image deblurring remains a challenging problem for modern artificial\n",
      "neural networks. Unlike other image restoration problems, deblurring networks\n",
      "fail behind the performance of existing deblurring algorithms in case of\n",
      "uniform and 3D blur models. This follows from the diverse and profound effect\n",
      "that the unknown blur-kernel has on the deblurring operator.\n",
      "  We propose a new architecture which breaks the deblurring network into an\n",
      "analysis network which estimates the blur, and a synthesis network that uses\n",
      "this kernel to deblur the image. Unlike existing deblurring networks, this\n",
      "design allows us to explicitly incorporate the blur-kernel in the network's\n",
      "training. In addition, we introduce new cross-correlation layers that allow\n",
      "better blur estimations, as well as unique components that allow the estimate\n",
      "blur to control the action of the synthesis deblurring action.\n",
      "  Evaluating the new approach over established benchmark datasets shows its\n",
      "ability to achieve state-of-the-art deblurring accuracy on various tests, as\n",
      "well as offer a major speedup in runtime.\n",
      "\n",
      "**Paper Id :2007.13866 \n",
      "Title :se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n",
      "  Residuals in Synthetic Domains\n",
      "  Tracking the 6D pose of objects in video sequences is important for robot\n",
      "manipulation. This task, however, introduces multiple challenges: (i) robot\n",
      "manipulation involves significant occlusions; (ii) data and annotations are\n",
      "troublesome and difficult to collect for 6D poses, which complicates machine\n",
      "learning solutions, and (iii) incremental error drift often accumulates in long\n",
      "term tracking to necessitate re-initialization of the object's pose. This work\n",
      "proposes a data-driven optimization approach for long-term, 6D pose tracking.\n",
      "It aims to identify the optimal relative pose given the current RGB-D\n",
      "observation and a synthetic image conditioned on the previous best estimate and\n",
      "the object's model. The key contribution in this context is a novel neural\n",
      "network architecture, which appropriately disentangles the feature encoding to\n",
      "help reduce domain shift, and an effective 3D orientation representation via\n",
      "Lie Algebra. Consequently, even when the network is trained only with synthetic\n",
      "data can work effectively over real images. Comprehensive experiments over\n",
      "benchmarks - existing ones as well as a new dataset with significant occlusions\n",
      "related to object manipulation - show that the proposed approach achieves\n",
      "consistently robust estimates and outperforms alternatives, even though they\n",
      "have been trained with real images. The approach is also the most\n",
      "computationally efficient among the alternatives and achieves a tracking\n",
      "frequency of 90.9Hz.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.03019 \n",
      "Title :Disentangled Sticky Hierarchical Dirichlet Process Hidden Markov Model\n",
      "  The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) has been\n",
      "used widely as a natural Bayesian nonparametric extension of the classical\n",
      "Hidden Markov Model for learning from sequential and time-series data. A sticky\n",
      "extension of the HDP-HMM has been proposed to strengthen the self-persistence\n",
      "probability in the HDP-HMM. However, the sticky HDP-HMM entangles the strength\n",
      "of the self-persistence prior and transition prior together, limiting its\n",
      "expressiveness. Here, we propose a more general model: the disentangled sticky\n",
      "HDP-HMM (DS-HDP-HMM). We develop novel Gibbs sampling algorithms for efficient\n",
      "inference in this model. We show that the disentangled sticky HDP-HMM\n",
      "outperforms the sticky HDP-HMM and HDP-HMM on both synthetic and real data, and\n",
      "apply the new approach to analyze neural data and segment behavioral video\n",
      "data.\n",
      "\n",
      "**Paper Id :1910.05744 \n",
      "Title :Powering Hidden Markov Model by Neural Network based Generative Models\n",
      "  Hidden Markov model (HMM) has been successfully used for sequential data\n",
      "modeling problems. In this work, we propose to power the modeling capacity of\n",
      "HMM by bringing in neural network based generative models. The proposed model\n",
      "is termed as GenHMM. In the proposed GenHMM, each HMM hidden state is\n",
      "associated with a neural network based generative model that has tractability\n",
      "of exact likelihood and provides efficient likelihood computation. A generative\n",
      "model in GenHMM consists of mixture of generators that are realized by flow\n",
      "models. A learning algorithm for GenHMM is proposed in expectation-maximization\n",
      "framework. The convergence of the learning GenHMM is analyzed. We demonstrate\n",
      "the efficiency of GenHMM by classification tasks on practical sequential data.\n",
      "Code available at https://github.com/FirstHandScientist/genhmm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.03267 \n",
      "Title :Guided Dialog Policy Learning without Adversarial Learning in the Loop\n",
      "  Reinforcement Learning (RL) methods have emerged as a popular choice for\n",
      "training an efficient and effective dialogue policy. However, these methods\n",
      "suffer from sparse and unstable reward signals returned by a user simulator\n",
      "only when a dialogue finishes. Besides, the reward signal is manually designed\n",
      "by human experts, which requires domain knowledge. Recently, a number of\n",
      "adversarial learning methods have been proposed to learn the reward function\n",
      "together with the dialogue policy. However, to alternatively update the\n",
      "dialogue policy and the reward model on the fly, we are limited to\n",
      "policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the\n",
      "alternating training of a dialogue agent and the reward model can easily get\n",
      "stuck in local optima or result in mode collapse. To overcome the listed\n",
      "issues, we propose to decompose the adversarial training into two steps. First,\n",
      "we train the discriminator with an auxiliary dialogue generator and then\n",
      "incorporate a derived reward model into a common RL method to guide the\n",
      "dialogue policy learning. This approach is applicable to both on-policy and\n",
      "off-policy RL methods. Based on our extensive experimentation, we can conclude\n",
      "the proposed method: (1) achieves a remarkable task success rate using both\n",
      "on-policy and off-policy RL methods; and (2) has the potential to transfer\n",
      "knowledge from existing domains to a new domain.\n",
      "\n",
      "**Paper Id :2009.09781 \n",
      "Title :Rethinking Supervised Learning and Reinforcement Learning in\n",
      "  Task-Oriented Dialogue Systems\n",
      "  Dialogue policy learning for task-oriented dialogue systems has enjoyed great\n",
      "progress recently mostly through employing reinforcement learning methods.\n",
      "However, these approaches have become very sophisticated. It is time to\n",
      "re-evaluate it. Are we really making progress developing dialogue agents only\n",
      "based on reinforcement learning? We demonstrate how (1)~traditional supervised\n",
      "learning together with (2)~a simulator-free adversarial learning method can be\n",
      "used to achieve performance comparable to state-of-the-art RL-based methods.\n",
      "First, we introduce a simple dialogue action decoder to predict the appropriate\n",
      "actions. Then, the traditional multi-label classification solution for dialogue\n",
      "policy learning is extended by adding dense layers to improve the dialogue\n",
      "agent performance. Finally, we employ the Gumbel-Softmax estimator to\n",
      "alternatively train the dialogue agent and the dialogue reward model without\n",
      "using reinforcement learning. Based on our extensive experimentation, we can\n",
      "conclude the proposed methods can achieve more stable and higher performance\n",
      "with fewer efforts, such as the domain knowledge required to design a user\n",
      "simulator and the intractable parameter tuning in reinforcement learning. Our\n",
      "main goal is not to beat reinforcement learning with supervised learning, but\n",
      "to demonstrate the value of rethinking the role of reinforcement learning and\n",
      "supervised learning in optimizing task-oriented dialogue systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04491 \n",
      "Title :Multi-Granularity Canonical Appearance Pooling for Remote Sensing Scene\n",
      "  Classification\n",
      "  Recognising remote sensing scene images remains challenging due to large\n",
      "visual-semantic discrepancies. These mainly arise due to the lack of detailed\n",
      "annotations that can be employed to align pixel-level representations with\n",
      "high-level semantic labels. As the tagging process is labour-intensive and\n",
      "subjective, we hereby propose a novel Multi-Granularity Canonical Appearance\n",
      "Pooling (MG-CAP) to automatically capture the latent ontological structure of\n",
      "remote sensing datasets. We design a granular framework that allows\n",
      "progressively cropping the input image to learn multi-grained features. For\n",
      "each specific granularity, we discover the canonical appearance from a set of\n",
      "pre-defined transformations and learn the corresponding CNN features through a\n",
      "maxout-based Siamese style architecture. Then, we replace the standard CNN\n",
      "features with Gaussian covariance matrices and adopt the proper matrix\n",
      "normalisations for improving the discriminative power of features. Besides, we\n",
      "provide a stable solution for training the eigenvalue-decomposition function\n",
      "(EIG) in a GPU and demonstrate the corresponding back-propagation using matrix\n",
      "calculus. Extensive experiments have shown that our framework can achieve\n",
      "promising results in public remote sensing scene datasets.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04573 \n",
      "Title :Backprojection for Training Feedforward Neural Networks in the Input and\n",
      "  Feature Spaces\n",
      "  After the tremendous development of neural networks trained by\n",
      "backpropagation, it is a good time to develop other algorithms for training\n",
      "neural networks to gain more insights into networks. In this paper, we propose\n",
      "a new algorithm for training feedforward neural networks which is fairly faster\n",
      "than backpropagation. This method is based on projection and reconstruction\n",
      "where, at every layer, the projected data and reconstructed labels are forced\n",
      "to be similar and the weights are tuned accordingly layer by layer. The\n",
      "proposed algorithm can be used for both input and feature spaces, named as\n",
      "backprojection and kernel backprojection, respectively. This algorithm gives an\n",
      "insight to networks with a projection-based perspective. The experiments on\n",
      "synthetic datasets show the effectiveness of the proposed method.\n",
      "\n",
      "**Paper Id :1912.09132 \n",
      "Title :Mean field theory for deep dropout networks: digging up gradient\n",
      "  backpropagation deeply\n",
      "  In recent years, the mean field theory has been applied to the study of\n",
      "neural networks and has achieved a great deal of success. The theory has been\n",
      "applied to various neural network structures, including CNNs, RNNs, Residual\n",
      "networks, and Batch normalization. Inevitably, recent work has also covered the\n",
      "use of dropout. The mean field theory shows that the existence of depth scales\n",
      "that limit the maximum depth of signal propagation and gradient\n",
      "backpropagation. However, the gradient backpropagation is derived under the\n",
      "gradient independence assumption that weights used during feed forward are\n",
      "drawn independently from the ones used in backpropagation. This is not how\n",
      "neural networks are trained in a real setting. Instead, the same weights used\n",
      "in a feed-forward step needs to be carried over to its corresponding\n",
      "backpropagation. Using this realistic condition, we perform theoretical\n",
      "computation on linear dropout networks and a series of experiments on dropout\n",
      "networks. Our empirical results show an interesting phenomenon that the length\n",
      "gradients can backpropagate for a single input and a pair of inputs are\n",
      "governed by the same depth scale. Besides, we study the relationship between\n",
      "variance and mean of statistical metrics of the gradient and shown an emergence\n",
      "of universality. Finally, we investigate the maximum trainable length for deep\n",
      "dropout networks through a series of experiments using MNIST and CIFAR10 and\n",
      "provide a more precise empirical formula that describes the trainable length\n",
      "than original work.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04674 \n",
      "Title :Fisher Discriminant Triplet and Contrastive Losses for Training Siamese\n",
      "  Networks\n",
      "  Siamese neural network is a very powerful architecture for both feature\n",
      "extraction and metric learning. It usually consists of several networks that\n",
      "share weights. The Siamese concept is topology-agnostic and can use any neural\n",
      "network as its backbone. The two most popular loss functions for training these\n",
      "networks are the triplet and contrastive loss functions. In this paper, we\n",
      "propose two novel loss functions, named Fisher Discriminant Triplet (FDT) and\n",
      "Fisher Discriminant Contrastive (FDC). The former uses anchor-neighbor-distant\n",
      "triplets while the latter utilizes pairs of anchor-neighbor and anchor-distant\n",
      "samples. The FDT and FDC loss functions are designed based on the statistical\n",
      "formulation of the Fisher Discriminant Analysis (FDA), which is a linear\n",
      "subspace learning method. Our experiments on the MNIST and two challenging and\n",
      "publicly available histopathology datasets show the effectiveness of the\n",
      "proposed loss functions.\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04704 \n",
      "Title :Heuristics for Link Prediction in Multiplex Networks\n",
      "  Link prediction, or the inference of future or missing connections between\n",
      "entities, is a well-studied problem in network analysis. A multitude of\n",
      "heuristics exist for link prediction in ordinary networks with a single type of\n",
      "connection. However, link prediction in multiplex networks, or networks with\n",
      "multiple types of connections, is not a well understood problem. We propose a\n",
      "novel general framework and three families of heuristics for multiplex network\n",
      "link prediction that are simple, interpretable, and take advantage of the rich\n",
      "connection type correlation structure that exists in many real world networks.\n",
      "We further derive a theoretical threshold for determining when to use a\n",
      "different connection type based on the number of links that overlap with an\n",
      "Erdos-Renyi random graph. Through experiments with simulated and real world\n",
      "scientific collaboration, transportation and global trade networks, we\n",
      "demonstrate that the proposed heuristics show increased performance with the\n",
      "richness of connection type correlation structure and significantly outperform\n",
      "their baseline heuristics for ordinary networks with a single connection type.\n",
      "\n",
      "**Paper Id :2004.01293 \n",
      "Title :Motif-Based Spectral Clustering of Weighted Directed Networks\n",
      "  Clustering is an essential technique for network analysis, with applications\n",
      "in a diverse range of fields. Although spectral clustering is a popular and\n",
      "effective method, it fails to consider higher-order structure and can perform\n",
      "poorly on directed networks. One approach is to capture and cluster\n",
      "higher-order structures using motif adjacency matrices. However, current\n",
      "formulations fail to take edge weights into account, and thus are somewhat\n",
      "limited when weight is a key component of the network under study.\n",
      "  We address these shortcomings by exploring motif-based weighted spectral\n",
      "clustering methods. We present new and computationally useful matrix formulae\n",
      "for motif adjacency matrices on weighted networks, which can be used to\n",
      "construct efficient algorithms for any anchored or non-anchored motif on three\n",
      "nodes. In a very sparse regime, our proposed method can handle graphs with a\n",
      "million nodes and tens of millions of edges. We further use our framework to\n",
      "construct a motif-based approach for clustering bipartite networks.\n",
      "  We provide comprehensive experimental results, demonstrating (i) the\n",
      "scalability of our approach, (ii) advantages of higher-order clustering on\n",
      "synthetic examples, and (iii) the effectiveness of our techniques on a variety\n",
      "of real world data sets; and compare against several techniques from the\n",
      "literature. We conclude that motif-based spectral clustering is a valuable tool\n",
      "for analysis of directed and bipartite weighted networks, which is also\n",
      "scalable and easy to implement.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04743 \n",
      "Title :Topological Quantum Compiling with Reinforcement Learning\n",
      "  Quantum compiling, a process that decomposes the quantum algorithm into a\n",
      "series of hardware-compatible commands or elementary gates, is of fundamental\n",
      "importance for quantum computing. We introduce an efficient algorithm based on\n",
      "deep reinforcement learning that compiles an arbitrary single-qubit gate into a\n",
      "sequence of elementary gates from a finite universal set. It generates\n",
      "near-optimal gate sequences with given accuracy and is generally applicable to\n",
      "various scenarios, independent of the hardware-feasible universal set and free\n",
      "from using ancillary qubits. For concreteness, we apply this algorithm to the\n",
      "case of topological compiling of Fibonacci anyons and obtain near-optimal\n",
      "braiding sequences for arbitrary single-qubit unitaries. Our algorithm may\n",
      "carry over to other challenging quantum discrete problems, thus opening up a\n",
      "new avenue for intriguing applications of deep learning in quantum physics.\n",
      "\n",
      "**Paper Id :1810.11922 \n",
      "Title :The Expressive Power of Parameterized Quantum Circuits\n",
      "  Parameterized quantum circuits (PQCs) have been broadly used as a hybrid\n",
      "quantum-classical machine learning scheme to accomplish generative tasks.\n",
      "However, whether PQCs have better expressive power than classical generative\n",
      "neural networks, such as restricted or deep Boltzmann machines, remains an open\n",
      "issue. In this paper, we prove that PQCs with a simple structure already\n",
      "outperform any classical neural network for generative tasks, unless the\n",
      "polynomial hierarchy collapses. Our proof builds on known results from tensor\n",
      "networks and quantum circuits (in particular, instantaneous quantum polynomial\n",
      "circuits). In addition, PQCs equipped with ancillary qubits for post-selection\n",
      "have even stronger expressive power than those without post-selection. We\n",
      "employ them as an application for Bayesian learning, since it is possible to\n",
      "learn prior probabilities rather than assuming they are known. We expect that\n",
      "it will find many more applications in semi-supervised learning where prior\n",
      "distributions are normally assumed to be unknown. Lastly, we conduct several\n",
      "numerical experiments using the Rigetti Forest platform to demonstrate the\n",
      "performance of the proposed Bayesian quantum circuit.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04787 \n",
      "Title :An End-to-End Learning Approach for Trajectory Prediction in Pedestrian\n",
      "  Zones\n",
      "  This paper aims to explore the problem of trajectory prediction in\n",
      "heterogeneous pedestrian zones, where social dynamics representation is a big\n",
      "challenge. Proposed is an end-to-end learning framework for prediction accuracy\n",
      "improvement based on an attention mechanism to learn social interaction from\n",
      "multi-factor inputs.\n",
      "\n",
      "**Paper Id :2007.05314 \n",
      "Title :ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection\n",
      "  In this paper, we introduce ID-Conditioned Auto-Encoder for unsupervised\n",
      "anomaly detection. Our method is an adaptation of the Class-Conditioned\n",
      "Auto-Encoder (C2AE) designed for the open-set recognition. Assuming that\n",
      "non-anomalous samples constitute of distinct IDs, we apply Conditioned\n",
      "Auto-Encoder with labels provided by these IDs. Opposed to C2AE, our approach\n",
      "omits the classification subtask and reduces the learning process to the single\n",
      "run. We simplify the learning process further by fixing a constant vector as\n",
      "the target for non-matching labels. We apply our method in the context of\n",
      "sounds for machine condition monitoring. We evaluate our method on the ToyADMOS\n",
      "and MIMII datasets from the DCASE 2020 Challenge Task 2. We conduct an ablation\n",
      "study to indicate which steps of our method influences results the most.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04872 \n",
      "Title :Full Law Identification In Graphical Models Of Missing Data:\n",
      "  Completeness Results\n",
      "  Missing data has the potential to affect analyses conducted in all fields of\n",
      "scientific study, including healthcare, economics, and the social sciences.\n",
      "Several approaches to unbiased inference in the presence of non-ignorable\n",
      "missingness rely on the specification of the target distribution and its\n",
      "missingness process as a probability distribution that factorizes with respect\n",
      "to a directed acyclic graph. In this paper, we address the longstanding\n",
      "question of the characterization of models that are identifiable within this\n",
      "class of missing data distributions. We provide the first completeness result\n",
      "in this field of study -- necessary and sufficient graphical conditions under\n",
      "which, the full data distribution can be recovered from the observed data\n",
      "distribution. We then simultaneously address issues that may arise due to the\n",
      "presence of both missing data and unmeasured confounding, by extending these\n",
      "graphical conditions and proofs of completeness, to settings where some\n",
      "variables are not just missing, but completely unobserved.\n",
      "\n",
      "**Paper Id :1902.00450 \n",
      "Title :Time Series Deconfounder: Estimating Treatment Effects over Time in the\n",
      "  Presence of Hidden Confounders\n",
      "  The estimation of treatment effects is a pervasive problem in medicine.\n",
      "Existing methods for estimating treatment effects from longitudinal\n",
      "observational data assume that there are no hidden confounders, an assumption\n",
      "that is not testable in practice and, if it does not hold, leads to biased\n",
      "estimates. In this paper, we develop the Time Series Deconfounder, a method\n",
      "that leverages the assignment of multiple treatments over time to enable the\n",
      "estimation of treatment effects in the presence of multi-cause hidden\n",
      "confounders. The Time Series Deconfounder uses a novel recurrent neural network\n",
      "architecture with multitask output to build a factor model over time and infer\n",
      "latent variables that render the assigned treatments conditionally independent;\n",
      "then, it performs causal inference using these latent variables that act as\n",
      "substitutes for the multi-cause unobserved confounders. We provide a\n",
      "theoretical analysis for obtaining unbiased causal effects of time-varying\n",
      "exposures using the Time Series Deconfounder. Using both simulated and real\n",
      "data we show the effectiveness of our method in deconfounding the estimation of\n",
      "treatment responses over time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.05273 \n",
      "Title :Safe Multi-Agent Interaction through Robust Control Barrier Functions\n",
      "  with Learned Uncertainties\n",
      "  Robots operating in real world settings must navigate and maintain safety\n",
      "while interacting with many heterogeneous agents and obstacles. Multi-Agent\n",
      "Control Barrier Functions (CBF) have emerged as a computationally efficient\n",
      "tool to guarantee safety in multi-agent environments, but they assume perfect\n",
      "knowledge of both the robot dynamics and other agents' dynamics. While\n",
      "knowledge of the robot's dynamics might be reasonably well known, the\n",
      "heterogeneity of agents in real-world environments means there will always be\n",
      "considerable uncertainty in our prediction of other agents' dynamics. This work\n",
      "aims to learn high-confidence bounds for these dynamic uncertainties using\n",
      "Matrix-Variate Gaussian Process models, and incorporates them into a robust\n",
      "multi-agent CBF framework. We transform the resulting min-max robust CBF into a\n",
      "quadratic program, which can be efficiently solved in real time. We verify via\n",
      "simulation results that the nominal multi-agent CBF is often violated during\n",
      "agent interactions, whereas our robust formulation maintains safety with a much\n",
      "higher probability and adapts to learned uncertainties\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.05707 \n",
      "Title :VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification\n",
      "  Much progress has been made recently on text classification with methods\n",
      "based on neural networks. In particular, models using attention mechanism such\n",
      "as BERT have shown to have the capability of capturing the contextual\n",
      "information within a sentence or document. However, their ability of capturing\n",
      "the global information about the vocabulary of a language is more limited. This\n",
      "latter is the strength of Graph Convolutional Networks (GCN). In this paper, we\n",
      "propose VGCN-BERT model which combines the capability of BERT with a Vocabulary\n",
      "Graph Convolutional Network (VGCN). Local information and global information\n",
      "interact through different layers of BERT, allowing them to influence mutually\n",
      "and to build together a final representation for classification. In our\n",
      "experiments on several text classification datasets, our approach outperforms\n",
      "BERT and GCN alone, and achieve higher effectiveness than that reported in\n",
      "previous studies.\n",
      "\n",
      "**Paper Id :2003.13379 \n",
      "Title :Global Attention based Graph Convolutional Neural Networks for Improved\n",
      "  Materials Property Prediction\n",
      "  Machine learning (ML) methods have gained increasing popularity in exploring\n",
      "and developing new materials. More specifically, graph neural network (GNN) has\n",
      "been applied in predicting material properties. In this work, we develop a\n",
      "novel model, GATGNN, for predicting inorganic material properties based on\n",
      "graph neural networks composed of multiple graph-attention layers (GAT) and a\n",
      "global attention layer. Through the application of the GAT layers, our model\n",
      "can efficiently learn the complex bonds shared among the atoms within each\n",
      "atom's local neighborhood. Subsequently, the global attention layer provides\n",
      "the weight coefficients of each atom in the inorganic crystal material which\n",
      "are used to considerably improve our model's performance. Notably, with the\n",
      "development of our GATGNN model, we show that our method is able to both\n",
      "outperform the previous models' predictions and provide insight into the\n",
      "crystallization of the material.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.05988 \n",
      "Title :ControlVAE: Controllable Variational Autoencoder\n",
      "  Variational Autoencoders (VAE) and their variants have been widely used in a\n",
      "variety of applications, such as dialog generation, image generation and\n",
      "disentangled representation learning. However, the existing VAE models have\n",
      "some limitations in different applications. For example, a VAE easily suffers\n",
      "from KL vanishing in language modeling and low reconstruction quality for\n",
      "disentangling. To address these issues, we propose a novel controllable\n",
      "variational autoencoder framework, ControlVAE, that combines a controller,\n",
      "inspired by automatic control theory, with the basic VAE to improve the\n",
      "performance of resulting generative models. Specifically, we design a new\n",
      "non-linear PI controller, a variant of the proportional-integral-derivative\n",
      "(PID) control, to automatically tune the hyperparameter (weight) added in the\n",
      "VAE objective using the output KL-divergence as feedback during model training.\n",
      "The framework is evaluated using three applications; namely, language modeling,\n",
      "disentangled representation learning, and image generation. The results show\n",
      "that ControlVAE can achieve better disentangling and reconstruction quality\n",
      "than the existing methods. For language modelling, it not only averts the\n",
      "KL-vanishing, but also improves the diversity of generated text. Finally, we\n",
      "also demonstrate that ControlVAE improves the reconstruction quality of\n",
      "generated images compared to the original VAE.\n",
      "\n",
      "**Paper Id :2004.03267 \n",
      "Title :Guided Dialog Policy Learning without Adversarial Learning in the Loop\n",
      "  Reinforcement Learning (RL) methods have emerged as a popular choice for\n",
      "training an efficient and effective dialogue policy. However, these methods\n",
      "suffer from sparse and unstable reward signals returned by a user simulator\n",
      "only when a dialogue finishes. Besides, the reward signal is manually designed\n",
      "by human experts, which requires domain knowledge. Recently, a number of\n",
      "adversarial learning methods have been proposed to learn the reward function\n",
      "together with the dialogue policy. However, to alternatively update the\n",
      "dialogue policy and the reward model on the fly, we are limited to\n",
      "policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the\n",
      "alternating training of a dialogue agent and the reward model can easily get\n",
      "stuck in local optima or result in mode collapse. To overcome the listed\n",
      "issues, we propose to decompose the adversarial training into two steps. First,\n",
      "we train the discriminator with an auxiliary dialogue generator and then\n",
      "incorporate a derived reward model into a common RL method to guide the\n",
      "dialogue policy learning. This approach is applicable to both on-policy and\n",
      "off-policy RL methods. Based on our extensive experimentation, we can conclude\n",
      "the proposed method: (1) achieves a remarkable task success rate using both\n",
      "on-policy and off-policy RL methods; and (2) has the potential to transfer\n",
      "knowledge from existing domains to a new domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.06286 \n",
      "Title :Transfer Learning for EEG-Based Brain-Computer Interfaces: A Review of\n",
      "  Progress Made Since 2016\n",
      "  A brain-computer interface (BCI) enables a user to communicate with a\n",
      "computer directly using brain signals. The most common non-invasive BCI\n",
      "modality, electroencephalogram (EEG), is sensitive to noise/artifact and\n",
      "suffers between-subject/within-subject non-stationarity. Therefore, it is\n",
      "difficult to build a generic pattern recognition model in an EEG-based BCI\n",
      "system that is optimal for different subjects, during different sessions, for\n",
      "different devices and tasks. Usually, a calibration session is needed to\n",
      "collect some training data for a new subject, which is time-consuming and user\n",
      "unfriendly. Transfer learning (TL), which utilizes data or knowledge from\n",
      "similar or relevant subjects/sessions/devices/tasks to facilitate learning for\n",
      "a new subject/session/device/task, is frequently used to reduce the amount of\n",
      "calibration effort. This paper reviews journal publications on TL approaches in\n",
      "EEG-based BCIs in the last few years, i.e., since 2016. Six paradigms and\n",
      "applications -- motor imagery, event-related potentials, steady-state visual\n",
      "evoked potentials, affective BCIs, regression problems, and adversarial attacks\n",
      "-- are considered. For each paradigm/application, we group the TL approaches\n",
      "into cross-subject/session, cross-device, and cross-task settings and review\n",
      "them separately. Observations and conclusions are made at the end of the paper,\n",
      "which may point to future research directions.\n",
      "\n",
      "**Paper Id :1910.05878 \n",
      "Title :Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces\n",
      "  Transfer learning makes use of data or knowledge in one problem to help solve\n",
      "a different, yet related, problem. It is particularly useful in brain-computer\n",
      "interfaces (BCIs), for coping with variations among different subjects and/or\n",
      "tasks. This paper considers offline unsupervised cross-subject\n",
      "electroencephalogram (EEG) classification, i.e., we have labeled EEG trials\n",
      "from one or more source subjects, but only unlabeled EEG trials from the target\n",
      "subject. We propose a novel manifold embedded knowledge transfer (MEKT)\n",
      "approach, which first aligns the covariance matrices of the EEG trials in the\n",
      "Riemannian manifold, extracts features in the tangent space, and then performs\n",
      "domain adaptation by minimizing the joint probability distribution shift\n",
      "between the source and the target domains, while preserving their geometric\n",
      "structures. MEKT can cope with one or multiple source domains, and can be\n",
      "computed efficiently. We also propose a domain transferability estimation (DTE)\n",
      "approach to identify the most beneficial source domains, in case there are a\n",
      "large number of source domains. Experiments on four EEG datasets from two\n",
      "different BCI paradigms demonstrated that MEKT outperformed several\n",
      "state-of-the-art transfer learning approaches, and DTE can reduce more than\n",
      "half of the computational cost when the number of source subjects is large,\n",
      "with little sacrifice of classification accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.06874 \n",
      "Title :Understanding Aesthetic Evaluation using Deep Learning\n",
      "  A bottleneck in any evolutionary art system is aesthetic evaluation. Many\n",
      "different methods have been proposed to automate the evaluation of aesthetics,\n",
      "including measures of symmetry, coherence, complexity, contrast and grouping.\n",
      "The interactive genetic algorithm (IGA) relies on human-in-the-loop, subjective\n",
      "evaluation of aesthetics, but limits possibilities for large search due to user\n",
      "fatigue and small population sizes. In this paper we look at how recent\n",
      "advances in deep learning can assist in automating personal aesthetic\n",
      "judgement. Using a leading artist's computer art dataset, we use dimensionality\n",
      "reduction methods to visualise both genotype and phenotype space in order to\n",
      "support the exploration of new territory in any generative system.\n",
      "Convolutional Neural Networks trained on the user's prior aesthetic evaluations\n",
      "are used to suggest new possibilities similar or between known high quality\n",
      "genotype-phenotype mappings.\n",
      "\n",
      "**Paper Id :1909.11655 \n",
      "Title :Augmenting Genetic Algorithms with Deep Neural Networks for Exploring\n",
      "  the Chemical Space\n",
      "  Challenges in natural sciences can often be phrased as optimization problems.\n",
      "Machine learning techniques have recently been applied to solve such problems.\n",
      "One example in chemistry is the design of tailor-made organic materials and\n",
      "molecules, which requires efficient methods to explore the chemical space. We\n",
      "present a genetic algorithm (GA) that is enhanced with a neural network (DNN)\n",
      "based discriminator model to improve the diversity of generated molecules and\n",
      "at the same time steer the GA. We show that our algorithm outperforms other\n",
      "generative models in optimization tasks. We furthermore present a way to\n",
      "increase interpretability of genetic algorithms, which helped us to derive\n",
      "design principles.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.07210 \n",
      "Title :On Box-Cox Transformation for Image Normality and Pattern Classification\n",
      "  A unique member of the power transformation family is known as the Box-Cox\n",
      "transformation. The latter can be seen as a mathematical operation that leads\n",
      "to finding the optimum lambda ({\\lambda}) value that maximizes the\n",
      "log-likelihood function to transform a data to a normal distribution and to\n",
      "reduce heteroscedasticity. In data analytics, a normality assumption underlies\n",
      "a variety of statistical test models. This technique, however, is best known in\n",
      "statistical analysis to handle one-dimensional data. Herein, this paper\n",
      "revolves around the utility of such a tool as a pre-processing step to\n",
      "transform two-dimensional data, namely, digital images and to study its effect.\n",
      "Moreover, to reduce time complexity, it suffices to estimate the parameter\n",
      "lambda in real-time for large two-dimensional matrices by merely considering\n",
      "their probability density function as a statistical inference of the underlying\n",
      "data distribution. We compare the effect of this light-weight Box-Cox\n",
      "transformation with well-established state-of-the-art low light image\n",
      "enhancement techniques. We also demonstrate the effectiveness of our approach\n",
      "through several test-bed data sets for generic improvement of visual appearance\n",
      "of images and for ameliorating the performance of a colour pattern\n",
      "classification algorithm as an example application. Results with and without\n",
      "the proposed approach, are compared using the AlexNet (transfer deep learning)\n",
      "pretrained model. To the best of our knowledge, this is the first time that the\n",
      "Box-Cox transformation is extended to digital images by exploiting histogram\n",
      "transformation.\n",
      "\n",
      "**Paper Id :1905.09314 \n",
      "Title :Kernel Wasserstein Distance\n",
      "  The Wasserstein distance is a powerful metric based on the theory of optimal\n",
      "transport. It gives a natural measure of the distance between two distributions\n",
      "with a wide range of applications. In contrast to a number of the common\n",
      "divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is\n",
      "(weakly) continuous, and thus ideal for analyzing corrupted data. To date,\n",
      "however, no kernel methods for dealing with nonlinear data have been proposed\n",
      "via the Wasserstein distance. In this work, we develop a novel method to\n",
      "compute the L2-Wasserstein distance in a kernel space implemented using the\n",
      "kernel trick. The latter is a general method in machine learning employed to\n",
      "handle data in a nonlinear manner. We evaluate the proposed approach in\n",
      "identifying computerized tomography (CT) slices with dental artifacts in head\n",
      "and neck cancer, performing unsupervised hierarchical clustering on the\n",
      "resulting Wasserstein distance matrix that is computed on imaging texture\n",
      "features extracted from each CT slice. Our experiments show that the kernel\n",
      "approach outperforms classical non-kernel approaches in identifying CT slices\n",
      "with artifacts.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.07633 \n",
      "Title :A Methodology for Creating Question Answering Corpora Using Inverse Data\n",
      "  Annotation\n",
      "  In this paper, we introduce a novel methodology to efficiently construct a\n",
      "corpus for question answering over structured data. For this, we introduce an\n",
      "intermediate representation that is based on the logical query plan in a\n",
      "database called Operation Trees (OT). This representation allows us to invert\n",
      "the annotation process without losing flexibility in the types of queries that\n",
      "we generate. Furthermore, it allows for fine-grained alignment of query tokens\n",
      "to OT operations. In our method, we randomly generate OTs from a context-free\n",
      "grammar. Afterwards, annotators have to write the appropriate natural language\n",
      "question that is represented by the OT. Finally, the annotators assign the\n",
      "tokens to the OT operations. We apply the method to create a new corpus OTTA\n",
      "(Operation Trees and Token Assignment), a large semantic parsing corpus for\n",
      "evaluating natural language interfaces to databases. We compare OTTA to Spider\n",
      "and LC-QuaD 2.0 and show that our methodology more than triples the annotation\n",
      "speed while maintaining the complexity of the queries. Finally, we train a\n",
      "state-of-the-art semantic parsing model on our data and show that our corpus is\n",
      "a challenging dataset and that the token alignment can be leveraged to increase\n",
      "the performance significantly.\n",
      "\n",
      "**Paper Id :2010.05609 \n",
      "Title :Load What You Need: Smaller Versions of Multilingual BERT\n",
      "  Pre-trained Transformer-based models are achieving state-of-the-art results\n",
      "on a variety of Natural Language Processing data sets. However, the size of\n",
      "these models is often a drawback for their deployment in real production\n",
      "applications. In the case of multilingual models, most of the parameters are\n",
      "located in the embeddings layer. Therefore, reducing the vocabulary size should\n",
      "have an important impact on the total number of parameters. In this paper, we\n",
      "propose to generate smaller models that handle fewer number of languages\n",
      "according to the targeted corpora. We present an evaluation of smaller versions\n",
      "of multilingual BERT on the XNLI data set, but we believe that this method may\n",
      "be applied to other multilingual transformers. The obtained results confirm\n",
      "that we can generate smaller models that keep comparable results, while\n",
      "reducing up to 45% of the total number of parameters. We compared our models\n",
      "with DistilmBERT (a distilled version of multilingual BERT) and showed that\n",
      "unlike language reduction, distillation induced a 1.7% to 6% drop in the\n",
      "overall accuracy on the XNLI data set. The presented models and code are\n",
      "publicly available.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.08301 \n",
      "Title :Belief Propagation for Maximum Coverage on Weighted Bipartite Graph and\n",
      "  Application to Text Summarization\n",
      "  We study text summarization from the viewpoint of maximum coverage problem.\n",
      "In graph theory, the task of text summarization is regarded as maximum coverage\n",
      "problem on bipartite graph with weighted nodes. In recent study,\n",
      "belief-propagation based algorithm for maximum coverage on unweighted graph was\n",
      "proposed using the idea of statistical mechanics. We generalize it to weighted\n",
      "graph for text summarization. Then we apply our algorithm to weighted biregular\n",
      "random graph for verification of maximum coverage performance. We also apply it\n",
      "to bipartite graph representing real document in open text dataset, and check\n",
      "the performance of text summarization. As a result, our algorithm exhibits\n",
      "better performance than greedy-type algorithm in some setting of text\n",
      "summarization.\n",
      "\n",
      "**Paper Id :2003.10038 \n",
      "Title :Robust Hypergraph Clustering via Convex Relaxation of Truncated MLE\n",
      "  We study hypergraph clustering in the weighted $d$-uniform hypergraph\n",
      "stochastic block model ($d$\\textsf{-WHSBM}), where each edge consisting of $d$\n",
      "nodes from the same community has higher expected weight than the edges\n",
      "consisting of nodes from different communities. We propose a new hypergraph\n",
      "clustering algorithm, called \\textsf{CRTMLE}, and provide its performance\n",
      "guarantee under the $d$\\textsf{-WHSBM} for general parameter regimes. We show\n",
      "that the proposed method achieves the order-wise optimal or the best existing\n",
      "results for approximately balanced community sizes. Moreover, our results\n",
      "settle the first recovery guarantees for growing number of clusters of\n",
      "unbalanced sizes. Involving theoretical analysis and empirical results, we\n",
      "demonstrate the robustness of our algorithm against the unbalancedness of\n",
      "community sizes or the presence of outlier nodes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.08930 \n",
      "Title :Space of Functions Computed by Deep-Layered Machines\n",
      "  We study the space of functions computed by random-layered machines,\n",
      "including deep neural networks and Boolean circuits. Investigating the\n",
      "distribution of Boolean functions computed on the recurrent and layer-dependent\n",
      "architectures, we find that it is the same in both models. Depending on the\n",
      "initial conditions and computing elements used, we characterize the space of\n",
      "functions computed at the large depth limit and show that the macroscopic\n",
      "entropy of Boolean functions is either monotonically increasing or decreasing\n",
      "with the growing depth.\n",
      "\n",
      "**Paper Id :1808.00408 \n",
      "Title :Geometry of energy landscapes and the optimizability of deep neural\n",
      "  networks\n",
      "  Deep neural networks are workhorse models in machine learning with multiple\n",
      "layers of non-linear functions composed in series. Their loss function is\n",
      "highly non-convex, yet empirically even gradient descent minimisation is\n",
      "sufficient to arrive at accurate and predictive models. It is hitherto unknown\n",
      "why are deep neural networks easily optimizable. We analyze the energy\n",
      "landscape of a spin glass model of deep neural networks using random matrix\n",
      "theory and algebraic geometry. We analytically show that the multilayered\n",
      "structure holds the key to optimizability: Fixing the number of parameters and\n",
      "increasing network depth, the number of stationary points in the loss function\n",
      "decreases, minima become more clustered in parameter space, and the tradeoff\n",
      "between the depth and width of minima becomes less severe. Our analytical\n",
      "results are numerically verified through comparison with neural networks\n",
      "trained on a set of classical benchmark datasets. Our model uncovers generic\n",
      "design principles of machine learning models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.09044 \n",
      "Title :Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike\n",
      "  Common Sense\n",
      "  Recent progress in deep learning is essentially based on a \"big data for\n",
      "small tasks\" paradigm, under which massive amounts of data are used to train a\n",
      "classifier for a single narrow task. In this paper, we call for a shift that\n",
      "flips this paradigm upside down. Specifically, we propose a \"small data for big\n",
      "tasks\" paradigm, wherein a single artificial intelligence (AI) system is\n",
      "challenged to develop \"common sense\", enabling it to solve a wide range of\n",
      "tasks with little training data. We illustrate the potential power of this new\n",
      "paradigm by reviewing models of common sense that synthesize recent\n",
      "breakthroughs in both machine and human vision. We identify functionality,\n",
      "physics, intent, causality, and utility (FPICU) as the five core domains of\n",
      "cognitive AI with humanlike common sense. When taken as a unified concept,\n",
      "FPICU is concerned with the questions of \"why\" and \"how\", beyond the dominant\n",
      "\"what\" and \"where\" framework for understanding vision. They are invisible in\n",
      "terms of pixels but nevertheless drive the creation, maintenance, and\n",
      "development of visual scenes. We therefore coin them the \"dark matter\" of\n",
      "vision. Just as our universe cannot be understood by merely studying observable\n",
      "matter, we argue that vision cannot be understood without studying FPICU. We\n",
      "demonstrate the power of this perspective to develop cognitive AI systems with\n",
      "humanlike common sense by showing how to observe and apply FPICU with little\n",
      "training data to solve a wide range of challenging tasks, including tool use,\n",
      "planning, utility inference, and social learning. In summary, we argue that the\n",
      "next generation of AI must embrace \"dark\" humanlike common sense for solving\n",
      "novel tasks.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.09169 \n",
      "Title :Pose Manipulation with Identity Preservation\n",
      "  This paper describes a new model which generates images in novel poses e.g.\n",
      "by altering face expression and orientation, from just a few instances of a\n",
      "human subject. Unlike previous approaches which require large datasets of a\n",
      "specific person for training, our approach may start from a scarce set of\n",
      "images, even from a single image. To this end, we introduce Character Adaptive\n",
      "Identity Normalization GAN (CainGAN) which uses spatial characteristic features\n",
      "extracted by an embedder and combined across source images. The identity\n",
      "information is propagated throughout the network by applying conditional\n",
      "normalization. After extensive adversarial training, CainGAN receives figures\n",
      "of faces from a certain individual and produces new ones while preserving the\n",
      "person's identity. Experimental results show that the quality of generated\n",
      "images scales with the size of the input set used during inference.\n",
      "Furthermore, quantitative measurements indicate that CainGAN performs better\n",
      "compared to other methods when training data is limited.\n",
      "\n",
      "**Paper Id :2004.10734 \n",
      "Title :Red-GAN: Attacking class imbalance via conditioned generation. Yet\n",
      "  another medical imaging perspective\n",
      "  Exploiting learning algorithms under scarce data regimes is a limitation and\n",
      "a reality of the medical imaging field. In an attempt to mitigate the problem,\n",
      "we propose a data augmentation protocol based on generative adversarial\n",
      "networks. We condition the networks at a pixel-level (segmentation mask) and at\n",
      "a global-level information (acquisition environment or lesion type). Such\n",
      "conditioning provides immediate access to the image-label pairs while\n",
      "controlling global class specific appearance of the synthesized images. To\n",
      "stimulate synthesis of the features relevant for the segmentation task, an\n",
      "additional passive player in a form of segmentor is introduced into the\n",
      "adversarial game. We validate the approach on two medical datasets: BraTS,\n",
      "ISIC. By controlling the class distribution through injection of synthetic\n",
      "images into the training set we achieve control over the accuracy levels of the\n",
      "datasets' classes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.09610 \n",
      "Title :Deep variational network for rapid 4D flow MRI reconstruction\n",
      "  Phase-contrast magnetic resonance imaging (MRI) provides time-resolved\n",
      "quantification of blood flow dynamics that can aid clinical diagnosis. Long in\n",
      "vivo scan times due to repeated three-dimensional (3D) volume sampling over\n",
      "cardiac phases and breathing cycles necessitate accelerated imaging techniques\n",
      "that leverage data correlations. Standard compressed sensing reconstruction\n",
      "methods require tuning of hyperparameters and are computationally expensive,\n",
      "which diminishes the potential reduction of examination times. We propose an\n",
      "efficient model-based deep neural reconstruction network and evaluate its\n",
      "performance on clinical aortic flow data. The network is shown to reconstruct\n",
      "undersampled 4D flow MRI data in under a minute on standard consumer hardware.\n",
      "Remarkably, the relatively low amounts of tunable parameters allowed the\n",
      "network to be trained on images from 11 reference scans while generalizing well\n",
      "to retrospective and prospective undersampled data for various acceleration\n",
      "factors and anatomies.\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.10734 \n",
      "Title :Red-GAN: Attacking class imbalance via conditioned generation. Yet\n",
      "  another medical imaging perspective\n",
      "  Exploiting learning algorithms under scarce data regimes is a limitation and\n",
      "a reality of the medical imaging field. In an attempt to mitigate the problem,\n",
      "we propose a data augmentation protocol based on generative adversarial\n",
      "networks. We condition the networks at a pixel-level (segmentation mask) and at\n",
      "a global-level information (acquisition environment or lesion type). Such\n",
      "conditioning provides immediate access to the image-label pairs while\n",
      "controlling global class specific appearance of the synthesized images. To\n",
      "stimulate synthesis of the features relevant for the segmentation task, an\n",
      "additional passive player in a form of segmentor is introduced into the\n",
      "adversarial game. We validate the approach on two medical datasets: BraTS,\n",
      "ISIC. By controlling the class distribution through injection of synthetic\n",
      "images into the training set we achieve control over the accuracy levels of the\n",
      "datasets' classes.\n",
      "\n",
      "**Paper Id :2004.09169 \n",
      "Title :Pose Manipulation with Identity Preservation\n",
      "  This paper describes a new model which generates images in novel poses e.g.\n",
      "by altering face expression and orientation, from just a few instances of a\n",
      "human subject. Unlike previous approaches which require large datasets of a\n",
      "specific person for training, our approach may start from a scarce set of\n",
      "images, even from a single image. To this end, we introduce Character Adaptive\n",
      "Identity Normalization GAN (CainGAN) which uses spatial characteristic features\n",
      "extracted by an embedder and combined across source images. The identity\n",
      "information is propagated throughout the network by applying conditional\n",
      "normalization. After extensive adversarial training, CainGAN receives figures\n",
      "of faces from a certain individual and produces new ones while preserving the\n",
      "person's identity. Experimental results show that the quality of generated\n",
      "images scales with the size of the input set used during inference.\n",
      "Furthermore, quantitative measurements indicate that CainGAN performs better\n",
      "compared to other methods when training data is limited.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11123 \n",
      "Title :Imputation of missing sub-hourly precipitation data in a large sensor\n",
      "  network: a machine learning approach\n",
      "  Precipitation data collected at sub-hourly resolution represents specific\n",
      "challenges for missing data recovery by being largely stochastic in nature and\n",
      "highly unbalanced in the duration of rain vs non-rain. Here we present a\n",
      "two-step analysis utilising current machine learning techniques for imputing\n",
      "precipitation data sampled at 30-minute intervals by devolving the task into\n",
      "(a) the classification of rain or non-rain samples, and (b) regressing the\n",
      "absolute values of predicted rain samples. Investigating 37 weather stations in\n",
      "the UK, this machine learning process produces more accurate predictions for\n",
      "recovering precipitation data than an established surface fitting technique\n",
      "utilising neighbouring rain gauges. Increasing available features for the\n",
      "training of machine learning algorithms increases performance with the\n",
      "integration of weather data at the target site with externally sourced rain\n",
      "gauges providing the highest performance. This method informs machine learning\n",
      "models by utilising information in concurrently collected environmental data to\n",
      "make accurate predictions of missing rain data. Capturing complex non-linear\n",
      "relationships from weakly correlated variables is critical for data recovery at\n",
      "sub-hourly resolutions. Such pipelines for data recovery can be developed and\n",
      "deployed for highly automated and near instantaneous imputation of missing\n",
      "values in ongoing datasets at high temporal resolutions.\n",
      "\n",
      "**Paper Id :2006.07686 \n",
      "Title :Predictive modeling approaches in laser-based material processing\n",
      "  Predictive modelling represents an emerging field that combines existing and\n",
      "novel methodologies aimed to rapidly understand physical mechanisms and\n",
      "concurrently develop new materials, processes and structures. In the current\n",
      "study, previously-unexplored predictive modelling in a key-enabled technology,\n",
      "the laser-based manufacturing, aims to automate and forecast the effect of\n",
      "laser processing on material structures. The focus is centred on the\n",
      "performance of representative statistical and machine learning algorithms in\n",
      "predicting the outcome of laser processing on a range of materials. Results on\n",
      "experimental data showed that predictive models were able to satisfactorily\n",
      "learn the mapping between the laser input variables and the observed material\n",
      "structure. These results are further integrated with simulation data aiming to\n",
      "elucidate the multiscale physical processes upon laser-material interaction. As\n",
      "a consequence, we augmented the adjusted simulated data to the experimental and\n",
      "substantially improved the predictive performance, due to the availability of\n",
      "increased number of sampling points. In parallel, a metric to identify and\n",
      "quantify the regions with high predictive uncertainty, is presented, revealing\n",
      "that high uncertainty occurs around the transition boundaries. Our results can\n",
      "set the basis for a systematic methodology towards reducing material design,\n",
      "testing and production cost via the replacement of expensive trial-and-error\n",
      "based manufacturing procedure with a precise pre-fabrication predictive tool.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11204 \n",
      "Title :Classification using Hyperdimensional Computing: A Review\n",
      "  Hyperdimensional (HD) computing is built upon its unique data type referred\n",
      "to as hypervectors. The dimension of these hypervectors is typically in the\n",
      "range of tens of thousands. Proposed to solve cognitive tasks, HD computing\n",
      "aims at calculating similarity among its data. Data transformation is realized\n",
      "by three operations, including addition, multiplication and permutation. Its\n",
      "ultra-wide data representation introduces redundancy against noise. Since\n",
      "information is evenly distributed over every bit of the hypervectors, HD\n",
      "computing is inherently robust. Additionally, due to the nature of those three\n",
      "operations, HD computing leads to fast learning ability, high energy efficiency\n",
      "and acceptable accuracy in learning and classification tasks. This paper\n",
      "introduces the background of HD computing, and reviews the data representation,\n",
      "data transformation, and similarity measurement. The orthogonality in high\n",
      "dimensions presents opportunities for flexible computing. To balance the\n",
      "tradeoff between accuracy and efficiency, strategies include but are not\n",
      "limited to encoding, retraining, binarization and hardware acceleration.\n",
      "Evaluations indicate that HD computing shows great potential in addressing\n",
      "problems using data in the form of letters, signals and images. HD computing\n",
      "especially shows significant promise to replace machine learning algorithms as\n",
      "a light-weight classifier in the field of internet of things (IoTs).\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11238 \n",
      "Title :Learning Constrained Dynamics with Gauss Principle adhering Gaussian\n",
      "  Processes\n",
      "  The identification of the constrained dynamics of mechanical systems is often\n",
      "challenging. Learning methods promise to ease an analytical analysis, but\n",
      "require considerable amounts of data for training. We propose to combine\n",
      "insights from analytical mechanics with Gaussian process regression to improve\n",
      "the model's data efficiency and constraint integrity. The result is a Gaussian\n",
      "process model that incorporates a priori constraint knowledge such that its\n",
      "predictions adhere to Gauss' principle of least constraint. In return,\n",
      "predictions of the system's acceleration naturally respect potentially\n",
      "non-ideal (non-)holonomic equality constraints. As corollary results, our model\n",
      "enables to infer the acceleration of the unconstrained system from data of the\n",
      "constrained system and enables knowledge transfer between differing constraint\n",
      "configurations.\n",
      "\n",
      "**Paper Id :2001.08861 \n",
      "Title :Encoding Physical Constraints in Differentiable Newton-Euler Algorithm\n",
      "  The recursive Newton-Euler Algorithm (RNEA) is a popular technique for\n",
      "computing the dynamics of robots. RNEA can be framed as a differentiable\n",
      "computational graph, enabling the dynamics parameters of the robot to be\n",
      "learned from data via modern auto-differentiation toolboxes. However, the\n",
      "dynamics parameters learned in this manner can be physically implausible. In\n",
      "this work, we incorporate physical constraints in the learning by adding\n",
      "structure to the learned parameters. This results in a framework that can learn\n",
      "physically plausible dynamics via gradient descent, improving the training\n",
      "speed as well as generalization of the learned dynamics models. We evaluate our\n",
      "method on real-time inverse dynamics control tasks on a 7 degree of freedom\n",
      "robot arm, both in simulation and on the real robot. Our experiments study a\n",
      "spectrum of structure added to the parameters of the differentiable RNEA\n",
      "algorithm, and compare their performance and generalization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11483 \n",
      "Title :Spatiotemporal data analysis with chronological networks\n",
      "  The amount and size of spatiotemporal data sets from different domains have\n",
      "been rapidly increasing in the last years, which demands the development of\n",
      "robust and fast methods to analyze and extract information from them. In this\n",
      "paper, we propose a network-based model for spatiotemporal data analysis called\n",
      "chronnet. It consists of dividing a geometrical space into grid cells\n",
      "represented by nodes connected chronologically. The main goal of this model is\n",
      "to represent consecutive recurrent events between cells with strong links in\n",
      "the network. This representation permits the use of network science and\n",
      "graphing mining tools to extract information from spatiotemporal data. The\n",
      "chronnet construction process is fast, which makes it suitable for large data\n",
      "sets. In this paper, we describe how to use our model considering artificial\n",
      "and real data. For this purpose, we propose an artificial spatiotemporal data\n",
      "set generator to show how chronnets capture not just simple statistics, but\n",
      "also frequent patterns, spatial changes, outliers, and spatiotemporal clusters.\n",
      "Additionally, we analyze a real-world data set composed of global fire\n",
      "detections, in which we describe the frequency of fire events, outlier fire\n",
      "detections, and the seasonal activity, using a single chronnet.\n",
      "\n",
      "**Paper Id :1903.07789 \n",
      "Title :Predicting Citywide Crowd Flows in Irregular Regions Using Multi-View\n",
      "  Graph Convolutional Networks\n",
      "  Being able to predict the crowd flows in each and every part of a city,\n",
      "especially in irregular regions, is strategically important for traffic\n",
      "control, risk assessment, and public safety. However, it is very challenging\n",
      "because of interactions and spatial correlations between different regions. In\n",
      "addition, it is affected by many factors: i) multiple temporal correlations\n",
      "among different time intervals: closeness, period, trend; ii) complex external\n",
      "influential factors: weather, events; iii) meta features: time of the day, day\n",
      "of the week, and so on. In this paper, we formulate crowd flow forecasting in\n",
      "irregular regions as a spatio-temporal graph (STG) prediction problem in which\n",
      "each node represents a region with time-varying flows. By extending graph\n",
      "convolution to handle the spatial information, we propose using spatial graph\n",
      "convolution to build a multi-view graph convolutional network (MVGCN) for the\n",
      "crowd flow forecasting problem, where different views can capture different\n",
      "factors as mentioned above. We evaluate MVGCN using four real-world datasets\n",
      "(taxicabs and bikes) and extensive experimental results show that our approach\n",
      "outperforms the adaptations of state-of-the-art methods. And we have developed\n",
      "a crowd flow forecasting system for irregular regions that can now be used\n",
      "internally.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11587 \n",
      "Title :Concept Drift Detection via Equal Intensity k-means Space Partitioning\n",
      "  Data stream poses additional challenges to statistical classification tasks\n",
      "because distributions of the training and target samples may differ as time\n",
      "passes. Such distribution change in streaming data is called concept drift.\n",
      "Numerous histogram-based distribution change detection methods have been\n",
      "proposed to detect drift. Most histograms are developed on grid-based or\n",
      "tree-based space partitioning algorithms which makes the space partitions\n",
      "arbitrary, unexplainable, and may cause drift blind-spots. There is a need to\n",
      "improve the drift detection accuracy for histogram-based methods with the\n",
      "unsupervised setting. To address this problem, we propose a cluster-based\n",
      "histogram, called equal intensity k-means space partitioning (EI-kMeans). In\n",
      "addition, a heuristic method to improve the sensitivity of drift detection is\n",
      "introduced. The fundamental idea of improving the sensitivity is to minimize\n",
      "the risk of creating partitions in distribution offset regions. Pearson's\n",
      "chi-square test is used as the statistical hypothesis test so that the test\n",
      "statistics remain independent of the sample distribution. The number of bins\n",
      "and their shapes, which strongly influence the ability to detect drift, are\n",
      "determined dynamically from the sample based on an asymptotic constraint in the\n",
      "chi-square test. Accordingly, three algorithms are developed to implement\n",
      "concept drift detection, including a greedy centroids initialization algorithm,\n",
      "a cluster amplify-shrink algorithm, and a drift detection algorithm. For drift\n",
      "adaptation, we recommend retraining the learner if a drift is detected. The\n",
      "results of experiments on synthetic and real-world datasets demonstrate the\n",
      "advantages of EI-kMeans and show its efficacy in detecting concept drift.\n",
      "\n",
      "**Paper Id :1904.10900 \n",
      "Title :Learning big Gaussian Bayesian networks: partition, estimation, and\n",
      "  fusion\n",
      "  Structure learning of Bayesian networks has always been a challenging\n",
      "problem. Nowadays, massive-size networks with thousands or more of nodes but\n",
      "fewer samples frequently appear in many areas. We develop a divide-and-conquer\n",
      "framework, called partition-estimation-fusion (PEF), for structure learning of\n",
      "such big networks. The proposed method first partitions nodes into clusters,\n",
      "then learns a subgraph on each cluster of nodes, and finally fuses all learned\n",
      "subgraphs into one Bayesian network. The PEF method is designed in a flexible\n",
      "way so that any structure learning method may be used in the second step to\n",
      "learn a subgraph structure as either a DAG or a CPDAG. In the clustering step,\n",
      "we adapt the hierarchical clustering method to automatically choose a proper\n",
      "number of clusters. In the fusion step, we propose a novel hybrid method that\n",
      "sequentially add edges between subgraphs. Extensive numerical experiments\n",
      "demonstrate the competitive performance of our PEF method, in terms of both\n",
      "speed and accuracy compared to existing methods. Our method can improve the\n",
      "accuracy of structure learning by 20% or more, while reducing running time up\n",
      "to two orders-of-magnitude.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11676 \n",
      "Title :Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray\n",
      "  images using fine-tuned deep neural networks\n",
      "  The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that\n",
      "resembles pneumonia. The current diagnostic procedure of COVID-19 follows\n",
      "reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which\n",
      "however is less sensitive to identify the virus at the initial stage. Hence, a\n",
      "more robust and alternate diagnosis technique is desirable. Recently, with the\n",
      "release of publicly available datasets of corona positive patients comprising\n",
      "of computed tomography (CT) and chest X-ray (CXR) imaging; scientists,\n",
      "researchers and healthcare experts are contributing for faster and automated\n",
      "diagnosis of COVID-19 by identifying pulmonary infections using deep learning\n",
      "approaches to achieve better cure and treatment. These datasets have limited\n",
      "samples concerned with the positive COVID-19 cases, which raise the challenge\n",
      "for unbiased learning. Following from this context, this article presents the\n",
      "random oversampling and weighted class loss function approach for unbiased\n",
      "fine-tuned learning (transfer learning) in various state-of-the-art deep\n",
      "learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2,\n",
      "DenseNet169, and NASNetLarge to perform binary classification (as normal and\n",
      "COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia,\n",
      "and normal case) of posteroanterior CXR images. Accuracy, precision, recall,\n",
      "loss, and area under the curve (AUC) are utilized to evaluate the performance\n",
      "of the models. Considering the experimental results, the performance of each\n",
      "model is scenario dependent; however, NASNetLarge displayed better scores in\n",
      "contrast to other architectures, which is further compared with other recently\n",
      "proposed approaches. This article also added the visual explanation to\n",
      "illustrate the basis of model classification and perception of COVID-19 in CXR\n",
      "images.\n",
      "\n",
      "**Paper Id :2005.03227 \n",
      "Title :Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent\n",
      "  Multi-View Representation Learning\n",
      "  Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread\n",
      "rapidly across the world. Due to the large number of affected patients and\n",
      "heavy labor for doctors, computer-aided diagnosis with machine learning\n",
      "algorithm is urgently needed, and could largely reduce the efforts of\n",
      "clinicians and accelerate the diagnosis process. Chest computed tomography (CT)\n",
      "has been recognized as an informative tool for diagnosis of the disease. In\n",
      "this study, we propose to conduct the diagnosis of COVID-19 with a series of\n",
      "features extracted from CT images. To fully explore multiple features\n",
      "describing CT images from different views, a unified latent representation is\n",
      "learned which can completely encode information from different aspects of\n",
      "features and is endowed with promising class structure for separability.\n",
      "Specifically, the completeness is guaranteed with a group of backward neural\n",
      "networks (each for one type of features), while by using class labels the\n",
      "representation is enforced to be compact within COVID-19/community-acquired\n",
      "pneumonia (CAP) and also a large margin is guaranteed between different types\n",
      "of pneumonia. In this way, our model can well avoid overfitting compared to the\n",
      "case of directly projecting highdimensional features into classes. Extensive\n",
      "experimental results show that the proposed method outperforms all comparison\n",
      "methods, and rather stable performances are observed when varying the numbers\n",
      "of training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11706 \n",
      "Title :Target specific mining of COVID-19 scholarly articles using one-class\n",
      "  approach\n",
      "  In recent years, several research articles have been published in the field\n",
      "of corona-virus caused diseases like severe acute respiratory syndrome (SARS),\n",
      "middle east respiratory syndrome (MERS) and COVID-19. In the presence of\n",
      "numerous research articles, extracting best-suited articles is time-consuming\n",
      "and manually impractical. The objective of this paper is to extract the\n",
      "activity and trends of corona-virus related research articles using machine\n",
      "learning approaches. The COVID-19 open research dataset (CORD-19) is used for\n",
      "experiments, whereas several target-tasks along with explanations are defined\n",
      "for classification, based on domain knowledge. Clustering techniques are used\n",
      "to create the different clusters of available articles, and later the task\n",
      "assignment is performed using parallel one-class support vector machines\n",
      "(OCSVMs). Experiments with original and reduced features validate the\n",
      "performance of the approach. It is evident that the k-means clustering\n",
      "algorithm, followed by parallel OCSVMs, outperforms other methods for both\n",
      "original and reduced feature space.\n",
      "\n",
      "**Paper Id :2003.11617 \n",
      "Title :Covid-19: Automatic detection from X-Ray images utilizing Transfer\n",
      "  Learning with Convolutional Neural Networks\n",
      "  In this study, a dataset of X-Ray images from patients with common pneumonia,\n",
      "Covid-19, and normal incidents was utilized for the automatic detection of the\n",
      "Coronavirus. The aim of the study is to evaluate the performance of\n",
      "state-of-the-art Convolutional Neural Network architectures proposed over\n",
      "recent years for medical image classification. Specifically, the procedure\n",
      "called transfer learning was adopted. With transfer learning, the detection of\n",
      "various abnormalities in small medical image datasets is an achievable target,\n",
      "often yielding remarkable results. The dataset utilized in this experiment is a\n",
      "collection of 1427 X-Ray images. 224 images with confirmed Covid-19, 700 images\n",
      "with confirmed common pneumonia, and 504 images of normal conditions are\n",
      "included. The data was collected from the available X-Ray images on public\n",
      "medical repositories. With transfer learning, an overall accuracy of 97.82% in\n",
      "the detection of Covid-19 is achieved.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11714 \n",
      "Title :Residual Energy-Based Models for Text Generation\n",
      "  Text generation is ubiquitous in many NLP tasks, from summarization, to\n",
      "dialogue and machine translation. The dominant parametric approach is based on\n",
      "locally normalized models which predict one word at a time. While these work\n",
      "remarkably well, they are plagued by exposure bias due to the greedy nature of\n",
      "the generation process. In this work, we investigate un-normalized energy-based\n",
      "models (EBMs) which operate not at the token but at the sequence level. In\n",
      "order to make training tractable, we first work in the residual of a pretrained\n",
      "locally normalized language model and second we train using noise contrastive\n",
      "estimation. Furthermore, since the EBM works at the sequence level, we can\n",
      "leverage pretrained bi-directional contextual representations, such as BERT and\n",
      "RoBERTa. Our experiments on two large language modeling datasets show that\n",
      "residual EBMs yield lower perplexity compared to locally normalized baselines.\n",
      "Moreover, generation via importance sampling is very efficient and of higher\n",
      "quality than the baseline models according to human evaluation.\n",
      "\n",
      "**Paper Id :2002.08801 \n",
      "Title :Guiding attention in Sequence-to-sequence models for Dialogue Act\n",
      "  prediction\n",
      "  The task of predicting dialog acts (DA) based on conversational dialog is a\n",
      "key component in the development of conversational agents. Accurately\n",
      "predicting DAs requires a precise modeling of both the conversation and the\n",
      "global tag dependencies. We leverage seq2seq approaches widely adopted in\n",
      "Neural Machine Translation (NMT) to improve the modelling of tag sequentiality.\n",
      "Seq2seq models are known to learn complex global dependencies while currently\n",
      "proposed approaches using linear conditional random fields (CRF) only model\n",
      "local tag dependencies. In this work, we introduce a seq2seq model tailored for\n",
      "DA classification using: a hierarchical encoder, a novel guided attention\n",
      "mechanism and beam search applied to both training and inference. Compared to\n",
      "the state of the art our model does not require handcrafted features and is\n",
      "trained end-to-end. Furthermore, the proposed approach achieves an unmatched\n",
      "accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on\n",
      "MRDA.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.11848 \n",
      "Title :Deep learning for smart fish farming: applications, opportunities and\n",
      "  challenges\n",
      "  With the rapid emergence of deep learning (DL) technology, it has been\n",
      "successfully used in various fields including aquaculture. This change can\n",
      "create new opportunities and a series of challenges for information and data\n",
      "processing in smart fish farming. This paper focuses on the applications of DL\n",
      "in aquaculture, including live fish identification, species classification,\n",
      "behavioral analysis, feeding decision-making, size or biomass estimation, water\n",
      "quality prediction. In addition, the technical details of DL methods applied to\n",
      "smart fish farming are also analyzed, including data, algorithms, computing\n",
      "power, and performance. The results of this review show that the most\n",
      "significant contribution of DL is the ability to automatically extract\n",
      "features. However, challenges still exist; DL is still in an era of weak\n",
      "artificial intelligence. A large number of labeled data are needed for\n",
      "training, which has become a bottleneck restricting further DL applications in\n",
      "aquaculture. Nevertheless, DL still offers breakthroughs in the handling of\n",
      "complex data in aquaculture. In brief, our purpose is to provide researchers\n",
      "and practitioners with a better understanding of the current state of the art\n",
      "of DL in aquaculture, which can provide strong support for the implementation\n",
      "of smart fish farming.\n",
      "\n",
      "**Paper Id :2007.13290 \n",
      "Title :Deep Learning Methods for Solving Linear Inverse Problems: Research\n",
      "  Directions and Paradigms\n",
      "  The linear inverse problem is fundamental to the development of various\n",
      "scientific areas. Innumerable attempts have been carried out to solve different\n",
      "variants of the linear inverse problem in different applications. Nowadays, the\n",
      "rapid development of deep learning provides a fresh perspective for solving the\n",
      "linear inverse problem, which has various well-designed network architectures\n",
      "results in state-of-the-art performance in many applications. In this paper, we\n",
      "present a comprehensive survey of the recent progress in the development of\n",
      "deep learning for solving various linear inverse problems. We review how deep\n",
      "learning methods are used in solving different linear inverse problems, and\n",
      "explore the structured neural network architectures that incorporate knowledge\n",
      "used in traditional methods. Furthermore, we identify open challenges and\n",
      "potential future directions along this research line.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.12076 \n",
      "Title :Quantum machine learning and quantum biomimetics: A perspective\n",
      "  Quantum machine learning has emerged as an exciting and promising paradigm\n",
      "inside quantum technologies. It may permit, on the one hand, to carry out more\n",
      "efficient machine learning calculations by means of quantum devices, while, on\n",
      "the other hand, to employ machine learning techniques to better control quantum\n",
      "systems. Inside quantum machine learning, quantum reinforcement learning aims\n",
      "at developing \"intelligent\" quantum agents that may interact with the outer\n",
      "world and adapt to it, with the strategy of achieving some final goal. Another\n",
      "paradigm inside quantum machine learning is that of quantum autoencoders, which\n",
      "may allow one for employing fewer resources in a quantum device via a training\n",
      "process. Moreover, the field of quantum biomimetics aims at establishing\n",
      "analogies between biological and quantum systems, to look for previously\n",
      "inadvertent connections that may enable useful applications. Two recent\n",
      "examples are the concepts of quantum artificial life, as well as of quantum\n",
      "memristors. In this Perspective, we give an overview of these topics,\n",
      "describing the related research carried out by the scientific community.\n",
      "\n",
      "**Paper Id :1904.10797 \n",
      "Title :Machine learning for long-distance quantum communication\n",
      "  Machine learning can help us in solving problems in the context big data\n",
      "analysis and classification, as well as in playing complex games such as Go.\n",
      "But can it also be used to find novel protocols and algorithms for applications\n",
      "such as large-scale quantum communication? Here we show that machine learning\n",
      "can be used to identify central quantum protocols, including teleportation,\n",
      "entanglement purification and the quantum repeater. These schemes are of\n",
      "importance in long-distance quantum communication, and their discovery has\n",
      "shaped the field of quantum information processing. However, the usefulness of\n",
      "learning agents goes beyond the mere re-production of known protocols; the same\n",
      "approach allows one to find improved solutions to long-distance communication\n",
      "problems, in particular when dealing with asymmetric situations where channel\n",
      "noise and segment distance are non-uniform. Our findings are based on the use\n",
      "of projective simulation, a model of a learning agent that combines\n",
      "reinforcement learning and decision making in a physically motivated framework.\n",
      "The learning agent is provided with a universal gate set, and the desired task\n",
      "is specified via a reward scheme. From a technical perspective, the learning\n",
      "agent has to deal with stochastic environments and reactions. We utilize an\n",
      "idea reminiscent of hierarchical skill acquisition, where solutions to\n",
      "sub-problems are learned and re-used in the overall scheme. This is of\n",
      "particular importance in the development of long-distance communication\n",
      "schemes, and opens the way for using machine learning in the design and\n",
      "implementation of quantum networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.12157 \n",
      "Title :A Bayesian machine scientist to aid in the solution of challenging\n",
      "  scientific problems\n",
      "  Closed-form, interpretable mathematical models have been instrumental for\n",
      "advancing our understanding of the world; with the data revolution, we may now\n",
      "be in a position to uncover new such models for many systems from physics to\n",
      "the social sciences. However, to deal with increasing amounts of data, we need\n",
      "\"machine scientists\" that are able to extract these models automatically from\n",
      "data. Here, we introduce a Bayesian machine scientist, which establishes the\n",
      "plausibility of models using explicit approximations to the exact marginal\n",
      "posterior over models and establishes its prior expectations about models by\n",
      "learning from a large empirical corpus of mathematical expressions. It explores\n",
      "the space of models using Markov chain Monte Carlo. We show that this approach\n",
      "uncovers accurate models for synthetic and real data and provides out-of-sample\n",
      "predictions that are more accurate than those of existing approaches and of\n",
      "other nonparametric methods.\n",
      "\n",
      "**Paper Id :1909.04305 \n",
      "Title :Inverse Ising inference from high-temperature re-weighting of\n",
      "  observations\n",
      "  Maximum Likelihood Estimation (MLE) is the bread and butter of system\n",
      "inference for stochastic systems. In some generality, MLE will converge to the\n",
      "correct model in the infinite data limit. In the context of physical approaches\n",
      "to system inference, such as Boltzmann machines, MLE requires the arduous\n",
      "computation of partition functions summing over all configurations, both\n",
      "observed and unobserved. We present here a conceptually and computationally\n",
      "transparent data-driven approach to system inference that is based on the\n",
      "simple question: How should the Boltzmann weights of observed configurations be\n",
      "modified to make the probability distribution of observed configurations close\n",
      "to a flat distribution? This algorithm gives accurate inference by using only\n",
      "observed configurations for systems with a large number of degrees of freedom\n",
      "where other approaches are intractable.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.12700 \n",
      "Title :In-Vehicle Object Detection in the Wild for Driverless Vehicles\n",
      "  In-vehicle human object identification plays an important role in\n",
      "vision-based automated vehicle driving systems while objects such as\n",
      "pedestrians and vehicles on roads or streets are the primary targets to protect\n",
      "from driverless vehicles. A challenge is the difficulty to detect objects in\n",
      "moving under the wild conditions, while illumination and image quality could\n",
      "drastically vary. In this work, to address this challenge, we exploit Deep\n",
      "Convolutional Generative Adversarial Networks (DCGANs) with Single Shot\n",
      "Detector (SSD) to handle with the wild conditions. In our work, a GAN was\n",
      "trained with low-quality images to handle with the challenges arising from the\n",
      "wild conditions in smart cities, while a cascaded SSD is employed as the object\n",
      "detector to perform with the GAN. We used tested our approach under wild\n",
      "conditions using taxi driver videos on London street in both daylight and night\n",
      "times, and the tests from in-vehicle videos demonstrate that this strategy can\n",
      "drastically achieve a better detection rate under the wild conditions.\n",
      "\n",
      "**Paper Id :1905.07817 \n",
      "Title :Spatio-Temporal Adversarial Learning for Detecting Unseen Falls\n",
      "  Fall detection is an important problem from both the health and machine\n",
      "learning perspective. A fall can lead to severe injuries, long term impairments\n",
      "or even death in some cases. In terms of machine learning, it presents a\n",
      "severely class imbalance problem with very few or no training data for falls\n",
      "owing to the fact that falls occur rarely. In this paper, we take an alternate\n",
      "philosophy to detect falls in the absence of their training data, by training\n",
      "the classifier on only the normal activities (that are available in abundance)\n",
      "and identifying a fall as an anomaly. To realize such a classifier, we use an\n",
      "adversarial learning framework, which comprises of a spatio-temporal\n",
      "autoencoder for reconstructing input video frames and a spatio-temporal\n",
      "convolution network to discriminate them against original video frames. 3D\n",
      "convolutions are used to learn spatial and temporal features from the input\n",
      "video frames. The adversarial learning of the spatio-temporal autoencoder will\n",
      "enable reconstructing the normal activities of daily living efficiently; thus,\n",
      "rendering detecting unseen falls plausible within this framework. We tested the\n",
      "performance of the proposed framework on camera sensing modalities that may\n",
      "preserve an individual's privacy (fully or partially), such as thermal and\n",
      "depth camera. Our results on three publicly available datasets show that the\n",
      "proposed spatio-temporal adversarial framework performed better than other\n",
      "baseline frame based (or spatial) adversarial learning methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.12733 \n",
      "Title :Personalized Recommendation of PoIs to People with Autism\n",
      "  The suggestion of Points of Interest to people with Autism Spectrum Disorder\n",
      "(ASD) challenges recommender systems research because these users' perception\n",
      "of places is influenced by idiosyncratic sensory aversions which can mine their\n",
      "experience by causing stress and anxiety. Therefore, managing individual\n",
      "preferences is not enough to provide these people with suitable\n",
      "recommendations. In order to address this issue, we propose a Top-N\n",
      "recommendation model that combines the user's idiosyncratic aversions with\n",
      "her/his preferences in a personalized way to suggest the most compatible and\n",
      "likable Points of Interest for her/him. We are interested in finding a\n",
      "user-specific balance of compatibility and interest within a recommendation\n",
      "model that integrates heterogeneous evaluation criteria to appropriately take\n",
      "these aspects into account. We tested our model on both ASD and \"neurotypical\"\n",
      "people. The evaluation results show that, on both groups, our model outperforms\n",
      "in accuracy and ranking capability the recommender systems based on item\n",
      "compatibility, on user preferences, or which integrate these two aspects by\n",
      "means of a uniform evaluation model.\n",
      "\n",
      "**Paper Id :2008.11432 \n",
      "Title :Time-Aware Music Recommender Systems: Modeling the Evolution of Implicit\n",
      "  User Preferences and User Listening Habits in A Collaborative Filtering\n",
      "  Approach\n",
      "  Online streaming services have become the most popular way of listening to\n",
      "music. The majority of these services are endowed with recommendation\n",
      "mechanisms that help users to discover songs and artists that may interest them\n",
      "from the vast amount of music available. However, many are not reliable as they\n",
      "may not take into account contextual aspects or the ever-evolving user\n",
      "behavior. Therefore, it is necessary to develop systems that consider these\n",
      "aspects. In the field of music, time is one of the most important factors\n",
      "influencing user preferences and managing its effects, and is the motivation\n",
      "behind the work presented in this paper. Here, the temporal information\n",
      "regarding when songs are played is examined. The purpose is to model both the\n",
      "evolution of user preferences in the form of evolving implicit ratings and user\n",
      "listening behavior. In the collaborative filtering method proposed in this\n",
      "work, daily listening habits are captured in order to characterize users and\n",
      "provide them with more reliable recommendations. The results of the validation\n",
      "prove that this approach outperforms other methods in generating both\n",
      "context-aware and context-free recommendations\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.12814 \n",
      "Title :Why should we add early exits to neural networks?\n",
      "  Deep neural networks are generally designed as a stack of differentiable\n",
      "layers, in which a prediction is obtained only after running the full stack.\n",
      "Recently, some contributions have proposed techniques to endow the networks\n",
      "with early exits, allowing to obtain predictions at intermediate points of the\n",
      "stack. These multi-output networks have a number of advantages, including: (i)\n",
      "significant reductions of the inference time, (ii) reduced tendency to\n",
      "overfitting and vanishing gradients, and (iii) capability of being distributed\n",
      "over multi-tier computation platforms. In addition, they connect to the wider\n",
      "themes of biological plausibility and layered cognitive reasoning. In this\n",
      "paper, we provide a comprehensive introduction to this family of neural\n",
      "networks, by describing in a unified fashion the way these architectures can be\n",
      "designed, trained, and actually deployed in time-constrained scenarios. We also\n",
      "describe in-depth their application scenarios in 5G and Fog computing\n",
      "environments, as long as some of the open research questions connected to them.\n",
      "\n",
      "**Paper Id :1902.02181 \n",
      "Title :Attention in Natural Language Processing\n",
      "  Attention is an increasingly popular mechanism used in a wide range of neural\n",
      "architectures. The mechanism itself has been realized in a variety of formats.\n",
      "However, because of the fast-paced advances in this domain, a systematic\n",
      "overview of attention is still missing. In this article, we define a unified\n",
      "model for attention architectures in natural language processing, with a focus\n",
      "on those designed to work with vector representations of the textual data. We\n",
      "propose a taxonomy of attention models according to four dimensions: the\n",
      "representation of the input, the compatibility function, the distribution\n",
      "function, and the multiplicity of the input and/or output. We present the\n",
      "examples of how prior information can be exploited in attention models and\n",
      "discuss ongoing research efforts and open challenges in the area, providing the\n",
      "first extensive categorization of the vast body of literature in this exciting\n",
      "domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.13167 \n",
      "Title :Energy-based models for atomic-resolution protein conformations\n",
      "  We propose an energy-based model (EBM) of protein conformations that operates\n",
      "at atomic scale. The model is trained solely on crystallized protein data. By\n",
      "contrast, existing approaches for scoring conformations use energy functions\n",
      "that incorporate knowledge of physical principles and features that are the\n",
      "complex product of several decades of research and tuning. To evaluate the\n",
      "model, we benchmark on the rotamer recovery task, the problem of predicting the\n",
      "conformation of a side chain from its context within a protein structure, which\n",
      "has been used to evaluate energy functions for protein design. The model\n",
      "achieves performance close to that of the Rosetta energy function, a\n",
      "state-of-the-art method widely used in protein structure prediction and design.\n",
      "An investigation of the model's outputs and hidden representations finds that\n",
      "it captures physicochemical properties relevant to protein energy.\n",
      "\n",
      "**Paper Id :2011.04640 \n",
      "Title :Scaling Hidden Markov Language Models\n",
      "  The hidden Markov model (HMM) is a fundamental tool for sequence modeling\n",
      "that cleanly separates the hidden state from the emission structure. However,\n",
      "this separation makes it difficult to fit HMMs to large datasets in modern NLP,\n",
      "and they have fallen out of use due to very poor performance compared to fully\n",
      "observed models. This work revisits the challenge of scaling HMMs to language\n",
      "modeling datasets, taking ideas from recent approaches to neural modeling. We\n",
      "propose methods for scaling HMMs to massive state spaces while maintaining\n",
      "efficient exact inference, a compact parameterization, and effective\n",
      "regularization. Experiments show that this approach leads to models that are\n",
      "more accurate than previous HMM and n-gram-based methods, making progress\n",
      "towards the performance of state-of-the-art neural models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.13277 \n",
      "Title :Detecting multi-timescale consumption patterns from receipt data: A\n",
      "  non-negative tensor factorization approach\n",
      "  Understanding consumer behavior is an important task, not only for developing\n",
      "marketing strategies but also for the management of economic policies.\n",
      "Detecting consumption patterns, however, is a high-dimensional problem in which\n",
      "various factors that would affect consumers' behavior need to be considered,\n",
      "such as consumers' demographics, circadian rhythm, seasonal cycles, etc. Here,\n",
      "we develop a method to extract multi-timescale expenditure patterns of\n",
      "consumers from a large dataset of scanned receipts. We use a non-negative\n",
      "tensor factorization (NTF) to detect intra- and inter-week consumption patterns\n",
      "at one time. The proposed method allows us to characterize consumers based on\n",
      "their consumption patterns that are correlated over different timescales.\n",
      "\n",
      "**Paper Id :1903.07789 \n",
      "Title :Predicting Citywide Crowd Flows in Irregular Regions Using Multi-View\n",
      "  Graph Convolutional Networks\n",
      "  Being able to predict the crowd flows in each and every part of a city,\n",
      "especially in irregular regions, is strategically important for traffic\n",
      "control, risk assessment, and public safety. However, it is very challenging\n",
      "because of interactions and spatial correlations between different regions. In\n",
      "addition, it is affected by many factors: i) multiple temporal correlations\n",
      "among different time intervals: closeness, period, trend; ii) complex external\n",
      "influential factors: weather, events; iii) meta features: time of the day, day\n",
      "of the week, and so on. In this paper, we formulate crowd flow forecasting in\n",
      "irregular regions as a spatio-temporal graph (STG) prediction problem in which\n",
      "each node represents a region with time-varying flows. By extending graph\n",
      "convolution to handle the spatial information, we propose using spatial graph\n",
      "convolution to build a multi-view graph convolutional network (MVGCN) for the\n",
      "crowd flow forecasting problem, where different views can capture different\n",
      "factors as mentioned above. We evaluate MVGCN using four real-world datasets\n",
      "(taxicabs and bikes) and extensive experimental results show that our approach\n",
      "outperforms the adaptations of state-of-the-art methods. And we have developed\n",
      "a crowd flow forecasting system for irregular regions that can now be used\n",
      "internally.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.13408 \n",
      "Title :Time Series Forecasting With Deep Learning: A Survey\n",
      "  Numerous deep learning architectures have been developed to accommodate the\n",
      "diversity of time series datasets across different domains. In this article, we\n",
      "survey common encoder and decoder designs used in both one-step-ahead and\n",
      "multi-horizon time series forecasting -- describing how temporal information is\n",
      "incorporated into predictions by each model. Next, we highlight recent\n",
      "developments in hybrid deep learning models, which combine well-studied\n",
      "statistical models with neural network components to improve pure methods in\n",
      "either category. Lastly, we outline some ways in which deep learning can also\n",
      "facilitate decision support with time series data.\n",
      "\n",
      "**Paper Id :1901.08096 \n",
      "Title :Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps\n",
      "  for Time Series Prediction\n",
      "  Despite the recent popularity of deep generative state space models, few\n",
      "comparisons have been made between network architectures and the inference\n",
      "steps of the Bayesian filtering framework -- with most models simultaneously\n",
      "approximating both state transition and update steps with a single recurrent\n",
      "neural network (RNN). In this paper, we introduce the Recurrent Neural Filter\n",
      "(RNF), a novel recurrent autoencoder architecture that learns distinct\n",
      "representations for each Bayesian filtering step, captured by a series of\n",
      "encoders and decoders. Testing this on three real-world time series datasets,\n",
      "we demonstrate that the decoupled representations learnt not only improve the\n",
      "accuracy of one-step-ahead forecasts while providing realistic uncertainty\n",
      "estimates, but also facilitate multistep prediction through the separation of\n",
      "encoder stages.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.14171 \n",
      "Title :SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic\n",
      "  Question Answering and Spatial Semantic Lifting\n",
      "  Learning knowledge graph (KG) embeddings is an emerging technique for a\n",
      "variety of downstream tasks such as summarization, link prediction, information\n",
      "retrieval, and question answering. However, most existing KG embedding models\n",
      "neglect space and, therefore, do not perform well when applied to (geo)spatial\n",
      "data and tasks. For those models that consider space, most of them primarily\n",
      "rely on some notions of distance. These models suffer from higher computational\n",
      "complexity during training while still losing information beyond the relative\n",
      "distance between entities. In this work, we propose a location-aware KG\n",
      "embedding model called SE-KGE. It directly encodes spatial information such as\n",
      "point coordinates or bounding boxes of geographic entities into the KG\n",
      "embedding space. The resulting model is capable of handling different types of\n",
      "spatial reasoning. We also construct a geographic knowledge graph as well as a\n",
      "set of geographic query-answer pairs called DBGeo to evaluate the performance\n",
      "of SE-KGE in comparison to multiple baselines. Evaluation results show that\n",
      "SE-KGE outperforms these baselines on the DBGeo dataset for geographic logic\n",
      "query answering task. This demonstrates the effectiveness of our\n",
      "spatially-explicit model and the importance of considering the scale of\n",
      "different geographic entities. Finally, we introduce a novel downstream task\n",
      "called spatial semantic lifting which links an arbitrary location in the study\n",
      "area to entities in the KG via some relations. Evaluation on DBGeo shows that\n",
      "our model outperforms the baseline by a substantial margin.\n",
      "\n",
      "**Paper Id :2005.01690 \n",
      "Title :Learning Geo-Contextual Embeddings for Commuting Flow Prediction\n",
      "  Predicting commuting flows based on infrastructure and land-use information\n",
      "is critical for urban planning and public policy development. However, it is a\n",
      "challenging task given the complex patterns of commuting flows. Conventional\n",
      "models, such as gravity model, are mainly derived from physics principles and\n",
      "limited by their predictive power in real-world scenarios where many factors\n",
      "need to be considered. Meanwhile, most existing machine learning-based methods\n",
      "ignore the spatial correlations and fail to model the influence of nearby\n",
      "regions. To address these issues, we propose Geo-contextual Multitask Embedding\n",
      "Learner (GMEL), a model that captures the spatial correlations from geographic\n",
      "contextual information for commuting flow prediction. Specifically, we first\n",
      "construct a geo-adjacency network containing the geographic contextual\n",
      "information. Then, an attention mechanism is proposed based on the framework of\n",
      "graph attention network (GAT) to capture the spatial correlations and encode\n",
      "geographic contextual information to embedding space. Two separate GATs are\n",
      "used to model supply and demand characteristics. A multitask learning framework\n",
      "is used to introduce stronger restrictions and enhance the effectiveness of the\n",
      "embedding representation. Finally, a gradient boosting machine is trained based\n",
      "on the learned embeddings to predict commuting flows. We evaluate our model\n",
      "using real-world datasets from New York City and the experimental results\n",
      "demonstrate the effectiveness of our proposal against the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2004.14764 \n",
      "Title :Hierarchical clustering of bipartite data sets based on the statistical\n",
      "  significance of coincidences\n",
      "  When some 'entities' are related by the 'features' they share they are\n",
      "amenable to a bipartite network representation. Plant-pollinator ecological\n",
      "communities, co-authorship of scientific papers, customers and purchases, or\n",
      "answers in a poll, are but a few examples. Analyzing clustering of such\n",
      "entities in the network is a useful tool with applications in many fields, like\n",
      "internet technology, recommender systems, or detection of diseases. The\n",
      "algorithms most widely applied to find clusters in bipartite networks are\n",
      "variants of modularity optimization. Here we provide an hierarchical clustering\n",
      "algorithm based on a dissimilarity between entities that quantifies the\n",
      "probability that the features shared by two entities is due to mere chance. The\n",
      "algorithm performance is $O(n^2)$ when applied to a set of n entities, and its\n",
      "outcome is a dendrogram exhibiting the connections of those entities. Through\n",
      "the introduction of a 'susceptibility' measure we can provide an 'optimal'\n",
      "choice for the clustering as well as quantify its quality. The dendrogram\n",
      "reveals further useful structural information though -- like the existence of\n",
      "sub-clusters within clusters or of nodes that do not fit in any cluster. We\n",
      "illustrate the algorithm by applying it first to a set of synthetic networks,\n",
      "and then to a selection of examples. We also illustrate how to transform our\n",
      "algorithm into a valid alternative for one-mode networks as well, and show that\n",
      "it performs at least as well as the standard, modularity-based algorithms --\n",
      "with a higher numerical performance. We provide an implementation of the\n",
      "algorithm in Python freely accessible from GitHub.\n",
      "\n",
      "**Paper Id :2004.01293 \n",
      "Title :Motif-Based Spectral Clustering of Weighted Directed Networks\n",
      "  Clustering is an essential technique for network analysis, with applications\n",
      "in a diverse range of fields. Although spectral clustering is a popular and\n",
      "effective method, it fails to consider higher-order structure and can perform\n",
      "poorly on directed networks. One approach is to capture and cluster\n",
      "higher-order structures using motif adjacency matrices. However, current\n",
      "formulations fail to take edge weights into account, and thus are somewhat\n",
      "limited when weight is a key component of the network under study.\n",
      "  We address these shortcomings by exploring motif-based weighted spectral\n",
      "clustering methods. We present new and computationally useful matrix formulae\n",
      "for motif adjacency matrices on weighted networks, which can be used to\n",
      "construct efficient algorithms for any anchored or non-anchored motif on three\n",
      "nodes. In a very sparse regime, our proposed method can handle graphs with a\n",
      "million nodes and tens of millions of edges. We further use our framework to\n",
      "construct a motif-based approach for clustering bipartite networks.\n",
      "  We provide comprehensive experimental results, demonstrating (i) the\n",
      "scalability of our approach, (ii) advantages of higher-order clustering on\n",
      "synthetic examples, and (iii) the effectiveness of our techniques on a variety\n",
      "of real world data sets; and compare against several techniques from the\n",
      "literature. We conclude that motif-based spectral clustering is a valuable tool\n",
      "for analysis of directed and bipartite weighted networks, which is also\n",
      "scalable and easy to implement.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.00220 \n",
      "Title :Automatic Catalog of RRLyrae from $\\sim$ 14 million VVV Light Curves:\n",
      "  How far can we go with traditional machine-learning?\n",
      "  The creation of a 3D map of the bulge using RRLyrae (RRL) is one of the main\n",
      "goals of the VVV(X) surveys. The overwhelming number of sources under analysis\n",
      "request the use of automatic procedures. In this context, previous works\n",
      "introduced the use of Machine Learning (ML) methods for the variable star\n",
      "classification. Our goal is the development and analysis of an automatic\n",
      "procedure, based on ML, for the identification of RRLs in the VVV Survey. This\n",
      "procedure will be use to generate reliable catalogs integrated over several\n",
      "tiles in the survey. After the reconstruction of light-curves, we extract a set\n",
      "of period and intensity-based features. We use for the first time a new subset\n",
      "of pseudo color features. We discuss all the appropriate steps needed to define\n",
      "our automatic pipeline: selection of quality measures; sampling procedures;\n",
      "classifier setup and model selection. As final result, we construct an ensemble\n",
      "classifier with an average Recall of 0.48 and average Precision of 0.86 over 15\n",
      "tiles. We also make available our processed datasets and a catalog of candidate\n",
      "RRLs. Perhaps most interestingly, from a classification perspective based on\n",
      "photometric broad-band data, is that our results indicate that Color is an\n",
      "informative feature type of the RRL that should be considered for automatic\n",
      "classification methods via ML. We also argue that Recall and Precision in both\n",
      "tables and curves are high quality metrics for this highly imbalanced problem.\n",
      "Furthermore, we show for our VVV data-set that to have good estimates it is\n",
      "important to use the original distribution more than reduced samples with an\n",
      "artificial balance. Finally, we show that the use of ensemble classifiers helps\n",
      "resolve the crucial model selection step, and that most errors in the\n",
      "identification of RRLs are related to low quality observations of some sources\n",
      "or to the difficulty to resolve the RRL-C type given the date.\n",
      "\n",
      "**Paper Id :1911.08508 \n",
      "Title :Parameters Estimation for the Cosmic Microwave Background with Bayesian\n",
      "  Neural Networks\n",
      "  In this paper, we present the first study that compares different models of\n",
      "Bayesian Neural Networks (BNNs) to predict the posterior distribution of the\n",
      "cosmological parameters directly from the Cosmic Microwave Background\n",
      "temperature and polarization maps. We focus our analysis on four different\n",
      "methods to sample the weights of the network during training: Dropout,\n",
      "DropConnect, Reparameterization Trick (RT), and Flipout. We find out that\n",
      "Flipout outperforms all other methods regardless of the architecture used, and\n",
      "provides tighter constraints for the cosmological parameters. Moreover we\n",
      "compare with MCMC posterior analysis obtaining comparable error correlation\n",
      "among parameters, with BNNs being orders of magnitude faster in inference,\n",
      "although less accurate. Thanks to the speed of the inference process with BNNs,\n",
      "the posterior distribution, outcome of the neural network, can be used as the\n",
      "initial proposal for the Markov Chain. We show that this combined approach\n",
      "increases the acceptance rate in the Metropolis-Hasting algorithm and\n",
      "accelerates the convergence of the MCMC, while reaching the same final\n",
      "accuracy. In the second part of the paper, we present a guide to the training\n",
      "and calibration of a successful multi-channel BNN for the CMB temperature and\n",
      "polarization map. We show how tuning the regularization parameter for the\n",
      "standard deviation of the approximate posterior on the weights in Flipout and\n",
      "RT we can produce unbiased and reliable uncertainty estimates, i.e., the\n",
      "regularizer acts like a hyperparameter analogous to the dropout rate in\n",
      "Dropout. Finally, we show how polarization, when combined with the temperature\n",
      "in a unique multi-channel tensor fed to a single BNN, helps to break\n",
      "degeneracies among parameters and provides stringent constraints.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.00544 \n",
      "Title :Variational Quantum Eigensolver for Frustrated Quantum Systems\n",
      "  Hybrid quantum-classical algorithms have been proposed as a potentially\n",
      "viable application of quantum computers. A particular example - the variational\n",
      "quantum eigensolver, or VQE - is designed to determine a global minimum in an\n",
      "energy landscape specified by a quantum Hamiltonian, which makes it appealing\n",
      "for the needs of quantum chemistry. Experimental realizations have been\n",
      "reported in recent years and theoretical estimates of its efficiency are a\n",
      "subject of intense effort. Here we consider the performance of the VQE\n",
      "technique for a Hubbard-like model describing a one-dimensional chain of\n",
      "fermions with competing nearest- and next-nearest-neighbor interactions. We\n",
      "find that recovering the VQE solution allows one to obtain the correlation\n",
      "function of the ground state consistent with the exact result. We also study\n",
      "the barren plateau phenomenon for the Hamiltonian in question and find that the\n",
      "severity of this effect depends on the encoding of fermions to qubits. Our\n",
      "results are consistent with the current knowledge about the barren plateaus in\n",
      "quantum optimization.\n",
      "\n",
      "**Paper Id :2006.13222 \n",
      "Title :Certified variational quantum algorithms for eigenstate preparation\n",
      "  Solutions to many-body problem instances often involve an intractable number\n",
      "of degrees of freedom and admit no known approximations in general form. In\n",
      "practice, representing quantum-mechanical states of a given Hamiltonian using\n",
      "available numerical methods, in particular those based on variational Monte\n",
      "Carlo simulations, become exponentially more challenging with increasing system\n",
      "size. Recently quantum algorithms implemented as variational models have been\n",
      "proposed to accelerate such simulations. The variational ansatz states are\n",
      "characterized by a polynomial number of parameters devised in a way to minimize\n",
      "the expectation value of a given Hamiltonian, which is emulated by local\n",
      "measurements. In this study, we develop a means to certify the termination of\n",
      "variational algorithms. We demonstrate our approach by applying it to three\n",
      "models: the transverse field Ising model, the model of one-dimensional spinless\n",
      "fermions with competing interactions, and the Schwinger model of quantum\n",
      "electrodynamics. By means of comparison, we observe that our approach shows\n",
      "better performance near critical points in these models. We hence take a\n",
      "further step to improve the applicability and to certify the results of\n",
      "variational quantum simulators.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.00813 \n",
      "Title :Social Biases in NLP Models as Barriers for Persons with Disabilities\n",
      "  Building equitable and inclusive NLP technologies demands consideration of\n",
      "whether and how social attitudes are represented in ML models. In particular,\n",
      "representations encoded in models often inadvertently perpetuate undesirable\n",
      "social biases from the data on which they are trained. In this paper, we\n",
      "present evidence of such undesirable biases towards mentions of disability in\n",
      "two different English language models: toxicity prediction and sentiment\n",
      "analysis. Next, we demonstrate that the neural embeddings that are the critical\n",
      "first step in most NLP pipelines similarly contain undesirable biases towards\n",
      "mentions of disability. We end by highlighting topical biases in the discourse\n",
      "about disability which may contribute to the observed model biases; for\n",
      "instance, gun violence, homelessness, and drug addiction are over-represented\n",
      "in texts discussing mental illness.\n",
      "\n",
      "**Paper Id :2005.06618 \n",
      "Title :Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective\n",
      "  Learning\n",
      "  Human society had a long history of suffering from cognitive biases leading\n",
      "to social prejudices and mass injustice. The prevalent existence of cognitive\n",
      "biases in large volumes of historical data can pose a threat of being\n",
      "manifested as unethical and seemingly inhuman predictions as outputs of AI\n",
      "systems trained on such data. To alleviate this problem, we propose a\n",
      "bias-aware multi-objective learning framework that given a set of identity\n",
      "attributes (e.g. gender, ethnicity etc.) and a subset of sensitive categories\n",
      "of the possible classes of prediction outputs, learns to reduce the frequency\n",
      "of predicting certain combinations of them, e.g. predicting stereotypes such as\n",
      "`most blacks use abusive language', or `fear is a virtue of women'. Our\n",
      "experiments conducted on an emotion prediction task with balanced class priors\n",
      "shows that a set of baseline bias-agnostic models exhibit cognitive biases with\n",
      "respect to gender, such as women are prone to be afraid whereas men are more\n",
      "prone to be angry. In contrast, our proposed bias-aware multi-objective\n",
      "learning methodology is shown to reduce such biases in the predictied emotions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.00866 \n",
      "Title :Minerva: A Portable Machine Learning Microservice Framework for\n",
      "  Traditional Enterprise SaaS Applications\n",
      "  In traditional SaaS enterprise applications, microservices are an essential\n",
      "ingredient to deploy machine learning (ML) models successfully. In general,\n",
      "microservices result in efficiencies in software service design, development,\n",
      "and delivery. As they become ubiquitous in the redesign of monolithic software,\n",
      "with the addition of machine learning, the traditional applications are also\n",
      "becoming increasingly intelligent. Here, we propose a portable ML microservice\n",
      "framework Minerva (microservices container for applied ML) as an efficient way\n",
      "to modularize and deploy intelligent microservices in traditional legacy SaaS\n",
      "applications suite, especially in the enterprise domain. We identify and\n",
      "discuss the needs, challenges and architecture to incorporate ML microservices\n",
      "in such applications. Minervas design for optimal integration with legacy\n",
      "applications using microservices architecture leveraging lightweight\n",
      "infrastructure accelerates deploying ML models in such applications.\n",
      "\n",
      "**Paper Id :1907.08349 \n",
      "Title :Convergence of Edge Computing and Deep Learning: A Comprehensive Survey\n",
      "  Ubiquitous sensors and smart devices from factories and communities are\n",
      "generating massive amounts of data, and ever-increasing computing power is\n",
      "driving the core of computation and services from the cloud to the edge of the\n",
      "network. As an important enabler broadly changing people's lives, from face\n",
      "recognition to ambitious smart factories and cities, developments of artificial\n",
      "intelligence (especially deep learning, DL) based applications and services are\n",
      "thriving. However, due to efficiency and latency issues, the current cloud\n",
      "computing service architecture hinders the vision of \"providing artificial\n",
      "intelligence for every person and every organization at everywhere\". Thus,\n",
      "unleashing DL services using resources at the network edge near the data\n",
      "sources has emerged as a desirable solution. Therefore, edge intelligence,\n",
      "aiming to facilitate the deployment of DL services by edge computing, has\n",
      "received significant attention. In addition, DL, as the representative\n",
      "technique of artificial intelligence, can be integrated into edge computing\n",
      "frameworks to build intelligent edge for dynamic, adaptive edge maintenance and\n",
      "management. With regard to mutually beneficial edge intelligence and\n",
      "intelligent edge, this paper introduces and discusses: 1) the application\n",
      "scenarios of both; 2) the practical implementation methods and enabling\n",
      "technologies, namely DL training and inference in the customized edge computing\n",
      "framework; 3) challenges and future trends of more pervasive and fine-grained\n",
      "intelligence. We believe that by consolidating information scattered across the\n",
      "communication, networking, and DL areas, this survey can help readers to\n",
      "understand the connections between enabling technologies while promoting\n",
      "further discussions on the fusion of edge intelligence and intelligent edge,\n",
      "i.e., Edge DL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.01214 \n",
      "Title :Graph Homomorphism Convolution\n",
      "  In this paper, we study the graph classification problem from the graph\n",
      "homomorphism perspective. We consider the homomorphisms from $F$ to $G$, where\n",
      "$G$ is a graph of interest (e.g. molecules or social networks) and $F$ belongs\n",
      "to some family of graphs (e.g. paths or non-isomorphic trees). We show that\n",
      "graph homomorphism numbers provide a natural invariant (isomorphism invariant\n",
      "and $\\mathcal{F}$-invariant) embedding maps which can be used for graph\n",
      "classification. Viewing the expressive power of a graph classifier by the\n",
      "$\\mathcal{F}$-indistinguishable concept, we prove the universality property of\n",
      "graph homomorphism vectors in approximating $\\mathcal{F}$-invariant functions.\n",
      "In practice, by choosing $\\mathcal{F}$ whose elements have bounded tree-width,\n",
      "we show that the homomorphism method is efficient compared with other methods.\n",
      "\n",
      "**Paper Id :2003.10038 \n",
      "Title :Robust Hypergraph Clustering via Convex Relaxation of Truncated MLE\n",
      "  We study hypergraph clustering in the weighted $d$-uniform hypergraph\n",
      "stochastic block model ($d$\\textsf{-WHSBM}), where each edge consisting of $d$\n",
      "nodes from the same community has higher expected weight than the edges\n",
      "consisting of nodes from different communities. We propose a new hypergraph\n",
      "clustering algorithm, called \\textsf{CRTMLE}, and provide its performance\n",
      "guarantee under the $d$\\textsf{-WHSBM} for general parameter regimes. We show\n",
      "that the proposed method achieves the order-wise optimal or the best existing\n",
      "results for approximately balanced community sizes. Moreover, our results\n",
      "settle the first recovery guarantees for growing number of clusters of\n",
      "unbalanced sizes. Involving theoretical analysis and empirical results, we\n",
      "demonstrate the robustness of our algorithm against the unbalancedness of\n",
      "community sizes or the presence of outlier nodes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.01690 \n",
      "Title :Learning Geo-Contextual Embeddings for Commuting Flow Prediction\n",
      "  Predicting commuting flows based on infrastructure and land-use information\n",
      "is critical for urban planning and public policy development. However, it is a\n",
      "challenging task given the complex patterns of commuting flows. Conventional\n",
      "models, such as gravity model, are mainly derived from physics principles and\n",
      "limited by their predictive power in real-world scenarios where many factors\n",
      "need to be considered. Meanwhile, most existing machine learning-based methods\n",
      "ignore the spatial correlations and fail to model the influence of nearby\n",
      "regions. To address these issues, we propose Geo-contextual Multitask Embedding\n",
      "Learner (GMEL), a model that captures the spatial correlations from geographic\n",
      "contextual information for commuting flow prediction. Specifically, we first\n",
      "construct a geo-adjacency network containing the geographic contextual\n",
      "information. Then, an attention mechanism is proposed based on the framework of\n",
      "graph attention network (GAT) to capture the spatial correlations and encode\n",
      "geographic contextual information to embedding space. Two separate GATs are\n",
      "used to model supply and demand characteristics. A multitask learning framework\n",
      "is used to introduce stronger restrictions and enhance the effectiveness of the\n",
      "embedding representation. Finally, a gradient boosting machine is trained based\n",
      "on the learned embeddings to predict commuting flows. We evaluate our model\n",
      "using real-world datasets from New York City and the experimental results\n",
      "demonstrate the effectiveness of our proposal against the state of the art.\n",
      "\n",
      "**Paper Id :2005.08598 \n",
      "Title :Sequential Recommender via Time-aware Attentive Memory Network\n",
      "  Recommendation systems aim to assist users to discover most preferred\n",
      "contents from an ever-growing corpus of items. Although recommenders have been\n",
      "greatly improved by deep learning, they still faces several challenges: (1)\n",
      "Behaviors are much more complex than words in sentences, so traditional\n",
      "attentive and recurrent models may fail in capturing the temporal dynamics of\n",
      "user preferences. (2) The preferences of users are multiple and evolving, so it\n",
      "is difficult to integrate long-term memory and short-term intent.\n",
      "  In this paper, we propose a temporal gating methodology to improve attention\n",
      "mechanism and recurrent units, so that temporal information can be considered\n",
      "in both information filtering and state transition. Additionally, we propose a\n",
      "Multi-hop Time-aware Attentive Memory network (MTAM) to integrate long-term and\n",
      "short-term preferences. We use the proposed time-aware GRU network to learn the\n",
      "short-term intent and maintain prior records in user memory. We treat the\n",
      "short-term intent as a query and design a multi-hop memory reading operation\n",
      "via the proposed time-aware attention to generate user representation based on\n",
      "the current intent and long-term memory. Our approach is scalable for candidate\n",
      "retrieval tasks and can be viewed as a non-linear generalization of latent\n",
      "factorization for dot-product based Top-K recommendation. Finally, we conduct\n",
      "extensive experiments on six benchmark datasets and the experimental results\n",
      "demonstrate the effectiveness of our MTAM and temporal gating methodology.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.01697 \n",
      "Title :Setting up experimental Bell test with reinforcement learning\n",
      "  Finding optical setups producing measurement results with a targeted\n",
      "probability distribution is hard as a priori the number of possible\n",
      "experimental implementations grows exponentially with the number of modes and\n",
      "the number of devices. To tackle this complexity, we introduce a method\n",
      "combining reinforcement learning and simulated annealing enabling the automated\n",
      "design of optical experiments producing results with the desired probability\n",
      "distributions. We illustrate the relevance of our method by applying it to a\n",
      "probability distribution favouring high violations of the Bell-CHSH inequality.\n",
      "As a result, we propose new unintuitive experiments leading to higher Bell-CHSH\n",
      "inequality violations than the best currently known setups. Our method might\n",
      "positively impact the usefulness of photonic experiments for device-independent\n",
      "quantum information processing.\n",
      "\n",
      "**Paper Id :1902.04057 \n",
      "Title :Deep autoregressive models for the efficient variational simulation of\n",
      "  many-body quantum systems\n",
      "  Artificial Neural Networks were recently shown to be an efficient\n",
      "representation of highly-entangled many-body quantum states. In practical\n",
      "applications, neural-network states inherit numerical schemes used in\n",
      "Variational Monte Carlo, most notably the use of Markov-Chain Monte-Carlo\n",
      "(MCMC) sampling to estimate quantum expectations. The local stochastic sampling\n",
      "in MCMC caps the potential advantages of neural networks in two ways: (i) Its\n",
      "intrinsic computational cost sets stringent practical limits on the width and\n",
      "depth of the networks, and therefore limits their expressive capacity; (ii) Its\n",
      "difficulty in generating precise and uncorrelated samples can result in\n",
      "estimations of observables that are very far from their true value. Inspired by\n",
      "the state-of-the-art generative models used in machine learning, we propose a\n",
      "specialized Neural Network architecture that supports efficient and exact\n",
      "sampling, completely circumventing the need for Markov Chain sampling. We\n",
      "demonstrate our approach for two-dimensional interacting spin models,\n",
      "showcasing the ability to obtain accurate results on larger system sizes than\n",
      "those currently accessible to neural-network quantum states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.01797 \n",
      "Title :3D Printed Brain-Controlled Robot-Arm Prosthetic via Embedded Deep\n",
      "  Learning from sEMG Sensors\n",
      "  In this paper, we present our work on developing robot arm prosthetic via\n",
      "deep learning. Our work proposes to use transfer learning techniques applied to\n",
      "the Google Inception model to retrain the final layer for surface\n",
      "electromyography (sEMG) classification. Data have been collected using the\n",
      "Thalmic Labs Myo Armband and used to generate graph images comprised of 8\n",
      "subplots per image containing sEMG data captured from 40 data points per\n",
      "sensor, corresponding to the array of 8 sEMG sensors in the armband. Data\n",
      "captured were then classified into four categories (Fist, Thumbs Up, Open Hand,\n",
      "Rest) via using a deep learning model, Inception-v3, with transfer learning to\n",
      "train the model for accurate prediction of each on real-time input of new data.\n",
      "This trained model was then downloaded to the ARM processor based embedding\n",
      "system to enable the brain-controlled robot-arm prosthetic manufactured from\n",
      "our 3D printer. Testing of the functionality of the method, a robotic arm was\n",
      "produced using a 3D printer and off-the-shelf hardware to control it. SSH\n",
      "communication protocols are employed to execute python files hosted on an\n",
      "embedded Raspberry Pi with ARM processors to trigger movement on the robot arm\n",
      "of the predicted gesture.\n",
      "\n",
      "**Paper Id :2008.07092 \n",
      "Title :Understanding Brain Dynamics for Color Perception using Wearable EEG\n",
      "  headband\n",
      "  The perception of color is an important cognitive feature of the human brain.\n",
      "The variety of colors that impinge upon the human eye can trigger changes in\n",
      "brain activity which can be captured using electroencephalography (EEG). In\n",
      "this work, we have designed a multiclass classification model to detect the\n",
      "primary colors from the features of raw EEG signals. In contrast to previous\n",
      "research, our method employs spectral power features, statistical features as\n",
      "well as correlation features from the signal band power obtained from\n",
      "continuous Morlet wavelet transform instead of raw EEG, for the classification\n",
      "task. We have applied dimensionality reduction techniques such as Forward\n",
      "Feature Selection and Stacked Autoencoders to reduce the dimension of data\n",
      "eventually increasing the model's efficiency. Our proposed methodology using\n",
      "Forward Selection and Random Forest Classifier gave the best overall accuracy\n",
      "of 80.6\\% for intra-subject classification. Our approach shows promise in\n",
      "developing techniques for cognitive tasks using color cues such as controlling\n",
      "Internet of Thing (IoT) devices by looking at primary colors for individuals\n",
      "with restricted motor abilities.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.01819 \n",
      "Title :Neural Subdivision\n",
      "  This paper introduces Neural Subdivision, a novel framework for data-driven\n",
      "coarse-to-fine geometry modeling. During inference, our method takes a coarse\n",
      "triangle mesh as input and recursively subdivides it to a finer geometry by\n",
      "applying the fixed topological updates of Loop Subdivision, but predicting\n",
      "vertex positions using a neural network conditioned on the local geometry of a\n",
      "patch. This approach enables us to learn complex non-linear subdivision\n",
      "schemes, beyond simple linear averaging used in classical techniques. One of\n",
      "our key contributions is a novel self-supervised training setup that only\n",
      "requires a set of high-resolution meshes for learning network weights. For any\n",
      "training shape, we stochastically generate diverse low-resolution\n",
      "discretizations of coarse counterparts, while maintaining a bijective mapping\n",
      "that prescribes the exact target position of every new vertex during the\n",
      "subdivision process. This leads to a very efficient and accurate loss function\n",
      "for conditional mesh generation, and enables us to train a method that\n",
      "generalizes across discretizations and favors preserving the manifold structure\n",
      "of the output. During training we optimize for the same set of network weights\n",
      "across all local mesh patches, thus providing an architecture that is not\n",
      "constrained to a specific input mesh, fixed genus, or category. Our network\n",
      "encodes patch geometry in a local frame in a rotation- and\n",
      "translation-invariant manner. Jointly, these design choices enable our method\n",
      "to generalize well, and we demonstrate that even when trained on a single\n",
      "high-resolution mesh our method generates reasonable subdivisions for novel\n",
      "shapes.\n",
      "\n",
      "**Paper Id :2010.12455 \n",
      "Title :Primal-Dual Mesh Convolutional Neural Networks\n",
      "  Recent works in geometric deep learning have introduced neural networks that\n",
      "allow performing inference tasks on three-dimensional geometric data by\n",
      "defining convolution, and sometimes pooling, operations on triangle meshes.\n",
      "These methods, however, either consider the input mesh as a graph, and do not\n",
      "exploit specific geometric properties of meshes for feature aggregation and\n",
      "downsampling, or are specialized for meshes, but rely on a rigid definition of\n",
      "convolution that does not properly capture the local topology of the mesh. We\n",
      "propose a method that combines the advantages of both types of approaches,\n",
      "while addressing their limitations: we extend a primal-dual framework drawn\n",
      "from the graph-neural-network literature to triangle meshes, and define\n",
      "convolutions on two types of graphs constructed from an input mesh. Our method\n",
      "takes features for both edges and faces of a 3D mesh as input and dynamically\n",
      "aggregates them using an attention mechanism. At the same time, we introduce a\n",
      "pooling operation with a precise geometric interpretation, that allows handling\n",
      "variations in the mesh connectivity by clustering mesh faces in a task-driven\n",
      "fashion. We provide theoretical insights of our approach using tools from the\n",
      "mesh-simplification literature. In addition, we validate experimentally our\n",
      "method in the tasks of shape classification and shape segmentation, where we\n",
      "obtain comparable or superior performance to the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.01988 \n",
      "Title :One-step regression and classification with crosspoint resistive memory\n",
      "  arrays\n",
      "  Machine learning has been getting a large attention in the recent years, as a\n",
      "tool to process big data generated by ubiquitous sensors in our daily life.\n",
      "High speed, low energy computing machines are in demand to enable real-time\n",
      "artificial intelligence at the edge, i.e., without the support of a remote\n",
      "frame server in the cloud. Such requirements challenge the complementary\n",
      "metal-oxide-semiconductor (CMOS) technology, which is limited by the Moore's\n",
      "law approaching its end and the communication bottleneck in conventional\n",
      "computing architecture. Novel computing concepts, architectures and devices are\n",
      "thus strongly needed to accelerate data-intensive applications. Here we show a\n",
      "crosspoint resistive memory circuit with feedback configuration can execute\n",
      "linear regression and logistic regression in just one step by computing the\n",
      "pseudoinverse matrix of the data within the memory. The most elementary\n",
      "learning operation, that is the regression of a sequence of data and the\n",
      "classification of a set of data, can thus be executed in one single\n",
      "computational step by the novel technology. One-step learning is further\n",
      "supported by simulations of the prediction of the cost of a house in Boston and\n",
      "the training of a 2-layer neural network for MNIST digit recognition. The\n",
      "results are all obtained in one computational step, thanks to the physical,\n",
      "parallel, and analog computing within the crosspoint array.\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.02426 \n",
      "Title :Communication-Efficient Distributed Stochastic AUC Maximization with\n",
      "  Deep Neural Networks\n",
      "  In this paper, we study distributed algorithms for large-scale AUC\n",
      "maximization with a deep neural network as a predictive model. Although\n",
      "distributed learning techniques have been investigated extensively in deep\n",
      "learning, they are not directly applicable to stochastic AUC maximization with\n",
      "deep neural networks due to its striking differences from standard loss\n",
      "minimization problems (e.g., cross-entropy). Towards addressing this challenge,\n",
      "we propose and analyze a communication-efficient distributed optimization\n",
      "algorithm based on a {\\it non-convex concave} reformulation of the AUC\n",
      "maximization, in which the communication of both the primal variable and the\n",
      "dual variable between each worker and the parameter server only occurs after\n",
      "multiple steps of gradient-based updates in each worker. Compared with the\n",
      "naive parallel version of an existing algorithm that computes stochastic\n",
      "gradients at individual machines and averages them for updating the model\n",
      "parameters, our algorithm requires a much less number of communication rounds\n",
      "and still achieves a linear speedup in theory. To the best of our knowledge,\n",
      "this is the \\textbf{first} work that solves the {\\it non-convex concave\n",
      "min-max} problem for AUC maximization with deep neural networks in a\n",
      "communication-efficient distributed manner while still maintaining the linear\n",
      "speedup property in theory. Our experiments on several benchmark datasets show\n",
      "the effectiveness of our algorithm and also confirm our theory.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.02552 \n",
      "Title :Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder\n",
      "  Whereas adversarial training is employed as the main defence strategy against\n",
      "specific adversarial samples, it has limited generalization capability and\n",
      "incurs excessive time complexity. In this paper, we propose an attack-agnostic\n",
      "defence framework to enhance the intrinsic robustness of neural networks,\n",
      "without jeopardizing the ability of generalizing clean samples. Our Feature\n",
      "Pyramid Decoder (FPD) framework applies to all block-based convolutional neural\n",
      "networks (CNNs). It implants denoising and image restoration modules into a\n",
      "targeted CNN, and it also constraints the Lipschitz constant of the\n",
      "classification layer. Moreover, we propose a two-phase strategy to train the\n",
      "FPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\n",
      "multi-task and self-supervised learning. Evaluated against a variety of\n",
      "white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\n",
      "sufficient robustness against general adversarial samples on MNIST, SVHN and\n",
      "CALTECH. In addition, if we further conduct adversarial training, the\n",
      "FPD-enhanced CNNs perform better than their non-enhanced versions.\n",
      "\n",
      "**Paper Id :1811.01443 \n",
      "Title :SSCNets: Robustifying DNNs using Secure Selective Convolutional Filters\n",
      "  In this paper, we introduce a novel technique based on the Secure Selective\n",
      "Convolutional (SSC) techniques in the training loop that increases the\n",
      "robustness of a given DNN by allowing it to learn the data distribution based\n",
      "on the important edges in the input image. We validate our technique on\n",
      "Convolutional DNNs against the state-of-the-art attacks from the open-source\n",
      "Cleverhans library using the MNIST, the CIFAR-10, and the CIFAR-100 datasets.\n",
      "Our experimental results show that the attack success rate, as well as the\n",
      "imperceptibility of the adversarial images, can be significantly reduced by\n",
      "adding effective pre-processing functions, i.e., Sobel filtering.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.02587 \n",
      "Title :Modeling nanoconfinement effects using active learning\n",
      "  Predicting the spatial configuration of gas molecules in nanopores of shale\n",
      "formations is crucial for fluid flow forecasting and hydrocarbon reserves\n",
      "estimation. The key challenge in these tight formations is that the majority of\n",
      "the pore sizes are less than 50 nm. At this scale, the fluid properties are\n",
      "affected by nanoconfinement effects due to the increased fluid-solid\n",
      "interactions. For instance, gas adsorption to the pore walls could account for\n",
      "up to 85% of the total hydrocarbon volume in a tight reservoir. Although there\n",
      "are analytical solutions that describe this phenomenon for simple geometries,\n",
      "they are not suitable for describing realistic pores, where surface roughness\n",
      "and geometric anisotropy play important roles. To describe these, molecular\n",
      "dynamics (MD) simulations are used since they consider fluid-solid and\n",
      "fluid-fluid interactions at the molecular level. However, MD simulations are\n",
      "computationally expensive, and are not able to simulate scales larger than a\n",
      "few connected nanopores. We present a method for building and training\n",
      "physics-based deep learning surrogate models to carry out fast and accurate\n",
      "predictions of molecular configurations of gas inside nanopores. Since training\n",
      "deep learning models requires extensive databases that are computationally\n",
      "expensive to create, we employ active learning (AL). AL reduces the overhead of\n",
      "creating comprehensive sets of high-fidelity data by determining where the\n",
      "model uncertainty is greatest, and running simulations on the fly to minimize\n",
      "it. The proposed workflow enables nanoconfinement effects to be rigorously\n",
      "considered at the mesoscale where complex connected sets of nanopores control\n",
      "key applications such as hydrocarbon recovery and CO2 sequestration.\n",
      "\n",
      "**Paper Id :2003.13418 \n",
      "Title :Machine Learning Enabled Discovery of Application Dependent Design\n",
      "  Principles for Two-dimensional Materials\n",
      "  The large-scale search for high-performing candidate 2D materials is limited\n",
      "to calculating a few simple descriptors, usually with first-principles density\n",
      "functional theory calculations. In this work, we alleviate this issue by\n",
      "extending and generalizing crystal graph convolutional neural networks to\n",
      "systems with planar periodicity, and train an ensemble of models to predict\n",
      "thermodynamic, mechanical, and electronic properties. To demonstrate the\n",
      "utility of this approach, we carry out a screening of nearly 45,000 structures\n",
      "for two largely disjoint applications: namely, mechanically robust composites\n",
      "and photovoltaics. An analysis of the uncertainty associated with our methods\n",
      "indicates the ensemble of neural networks is well-calibrated and has errors\n",
      "comparable with those from accurate first-principles density functional theory\n",
      "calculations. The ensemble of models allows us to gauge the confidence of our\n",
      "predictions, and to find the candidates most likely to exhibit effective\n",
      "performance in their applications. Since the datasets used in our screening\n",
      "were combinatorically generated, we are also able to investigate, using an\n",
      "innovative method, structural and compositional design principles that impact\n",
      "the properties of the structures surveyed and which can act as a generative\n",
      "model basis for future material discovery through reverse engineering. Our\n",
      "approach allowed us to recover some well-accepted design principles: for\n",
      "instance, we find that hybrid organic-inorganic perovskites with lead and tin\n",
      "tend to be good candidates for solar cell applications.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.02595 \n",
      "Title :Approaches and Applications of Early Classification of Time Series: A\n",
      "  Review\n",
      "  Early classification of time series has been extensively studied for\n",
      "minimizing class prediction delay in time-sensitive applications such as\n",
      "healthcare and finance. A primary task of an early classification approach is\n",
      "to classify an incomplete time series as soon as possible with some desired\n",
      "level of accuracy. Recent years have witnessed several approaches for early\n",
      "classification of time series. As most of the approaches have solved the early\n",
      "classification problem with different aspects, it becomes very important to\n",
      "make a thorough review of the existing solutions to know the current status of\n",
      "the area. These solutions have demonstrated reasonable performance in a wide\n",
      "range of applications including human activity recognition, gene expression\n",
      "based health diagnostic, industrial monitoring, and so on. In this paper, we\n",
      "present a systematic review of current literature on early classification\n",
      "approaches for both univariate and multivariate time series. We divide various\n",
      "existing approaches into four exclusive categories based on their proposed\n",
      "solution strategies. The four categories include prefix based, shapelet based,\n",
      "model based, and miscellaneous approaches. The authors also discuss the\n",
      "applications of early classification in many areas including industrial\n",
      "monitoring, intelligent transportation, and medical. Finally, we provide a\n",
      "quick summary of the current literature with future research directions.\n",
      "\n",
      "**Paper Id :2007.14622 \n",
      "Title :Approaches to Fraud Detection on Credit Card Transactions Using\n",
      "  Artificial Intelligence Methods\n",
      "  Credit card fraud is an ongoing problem for almost all industries in the\n",
      "world, and it raises millions of dollars to the global economy each year.\n",
      "Therefore, there is a number of research either completed or proceeding in\n",
      "order to detect these kinds of frauds in the industry. These researches\n",
      "generally use rule-based or novel artificial intelligence approaches to find\n",
      "eligible solutions. The ultimate goal of this paper is to summarize\n",
      "state-of-the-art approaches to fraud detection using artificial intelligence\n",
      "and machine learning techniques. While summarizing, we will categorize the\n",
      "common problems such as imbalanced dataset, real time working scenarios, and\n",
      "feature engineering challenges that almost all research works encounter, and\n",
      "identify general approaches to solve them. The imbalanced dataset problem\n",
      "occurs because the number of legitimate transactions is much higher than the\n",
      "fraudulent ones whereas applying the right feature engineering is substantial\n",
      "as the features obtained from the industries are limited, and applying feature\n",
      "engineering methods and reforming the dataset is crucial. Also, adapting the\n",
      "detection system to real time scenarios is a challenge since the number of\n",
      "credit card transactions in a limited time period is very high. In addition, we\n",
      "will discuss how evaluation metrics and machine learning methods differentiate\n",
      "among each research.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.03227 \n",
      "Title :Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent\n",
      "  Multi-View Representation Learning\n",
      "  Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread\n",
      "rapidly across the world. Due to the large number of affected patients and\n",
      "heavy labor for doctors, computer-aided diagnosis with machine learning\n",
      "algorithm is urgently needed, and could largely reduce the efforts of\n",
      "clinicians and accelerate the diagnosis process. Chest computed tomography (CT)\n",
      "has been recognized as an informative tool for diagnosis of the disease. In\n",
      "this study, we propose to conduct the diagnosis of COVID-19 with a series of\n",
      "features extracted from CT images. To fully explore multiple features\n",
      "describing CT images from different views, a unified latent representation is\n",
      "learned which can completely encode information from different aspects of\n",
      "features and is endowed with promising class structure for separability.\n",
      "Specifically, the completeness is guaranteed with a group of backward neural\n",
      "networks (each for one type of features), while by using class labels the\n",
      "representation is enforced to be compact within COVID-19/community-acquired\n",
      "pneumonia (CAP) and also a large margin is guaranteed between different types\n",
      "of pneumonia. In this way, our model can well avoid overfitting compared to the\n",
      "case of directly projecting highdimensional features into classes. Extensive\n",
      "experimental results show that the proposed method outperforms all comparison\n",
      "methods, and rather stable performances are observed when varying the numbers\n",
      "of training data.\n",
      "\n",
      "**Paper Id :2004.11676 \n",
      "Title :Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray\n",
      "  images using fine-tuned deep neural networks\n",
      "  The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that\n",
      "resembles pneumonia. The current diagnostic procedure of COVID-19 follows\n",
      "reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which\n",
      "however is less sensitive to identify the virus at the initial stage. Hence, a\n",
      "more robust and alternate diagnosis technique is desirable. Recently, with the\n",
      "release of publicly available datasets of corona positive patients comprising\n",
      "of computed tomography (CT) and chest X-ray (CXR) imaging; scientists,\n",
      "researchers and healthcare experts are contributing for faster and automated\n",
      "diagnosis of COVID-19 by identifying pulmonary infections using deep learning\n",
      "approaches to achieve better cure and treatment. These datasets have limited\n",
      "samples concerned with the positive COVID-19 cases, which raise the challenge\n",
      "for unbiased learning. Following from this context, this article presents the\n",
      "random oversampling and weighted class loss function approach for unbiased\n",
      "fine-tuned learning (transfer learning) in various state-of-the-art deep\n",
      "learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2,\n",
      "DenseNet169, and NASNetLarge to perform binary classification (as normal and\n",
      "COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia,\n",
      "and normal case) of posteroanterior CXR images. Accuracy, precision, recall,\n",
      "loss, and area under the curve (AUC) are utilized to evaluate the performance\n",
      "of the models. Considering the experimental results, the performance of each\n",
      "model is scenario dependent; however, NASNetLarge displayed better scores in\n",
      "contrast to other architectures, which is further compared with other recently\n",
      "proposed approaches. This article also added the visual explanation to\n",
      "illustrate the basis of model classification and perception of COVID-19 in CXR\n",
      "images.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.03355 \n",
      "Title :Quantum correlation alignment for unsupervised domain adaptation\n",
      "  Correlation alignment (CORAL), a representative domain adaptation (DA)\n",
      "algorithm, decorrelates and aligns a labelled source domain dataset to an\n",
      "unlabelled target domain dataset to minimize the domain shift such that a\n",
      "classifier can be applied to predict the target domain labels. In this paper,\n",
      "we implement the CORAL on quantum devices by two different methods. One method\n",
      "utilizes quantum basic linear algebra subroutines (QBLAS) to implement the\n",
      "CORAL with exponential speedup in the number and dimension of the given data\n",
      "samples. The other method is achieved through a variational hybrid\n",
      "quantum-classical procedure. In addition, the numerical experiments of the\n",
      "CORAL with three different types of data sets, namely the synthetic data, the\n",
      "synthetic-Iris data, the handwritten digit data, are presented to evaluate the\n",
      "performance of our work. The simulation results prove that the variational\n",
      "quantum correlation alignment algorithm (VQCORAL) can achieve competitive\n",
      "performance compared with the classical CORAL.\n",
      "\n",
      "**Paper Id :2002.06239 \n",
      "Title :Boosted Locality Sensitive Hashing: Discriminative Binary Codes for\n",
      "  Source Separation\n",
      "  Speech enhancement tasks have seen significant improvements with the advance\n",
      "of deep learning technology, but with the cost of increased computational\n",
      "complexity. In this study, we propose an adaptive boosting approach to learning\n",
      "locality sensitive hash codes, which represent audio spectra efficiently. We\n",
      "use the learned hash codes for single-channel speech denoising tasks as an\n",
      "alternative to a complex machine learning model, particularly to address the\n",
      "resource-constrained environments. Our adaptive boosting algorithm learns\n",
      "simple logistic regressors as the weak learners. Once trained, their binary\n",
      "classification results transform each spectrum of test noisy speech into a bit\n",
      "string. Simple bitwise operations calculate Hamming distance to find the\n",
      "K-nearest matching frames in the dictionary of training noisy speech spectra,\n",
      "whose associated ideal binary masks are averaged to estimate the denoising mask\n",
      "for that test mixture. Our proposed learning algorithm differs from AdaBoost in\n",
      "the sense that the projections are trained to minimize the distances between\n",
      "the self-similarity matrix of the hash codes and that of the original spectra,\n",
      "rather than the misclassification rate. We evaluate our discriminative hash\n",
      "codes on the TIMIT corpus with various noise types, and show comparative\n",
      "performance to deep learning methods in terms of denoising performance and\n",
      "complexity.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.03405 \n",
      "Title :Joint Prediction and Time Estimation of COVID-19 Developing Severe\n",
      "  Symptoms using Chest CT Scan\n",
      "  With the rapidly worldwide spread of Coronavirus disease (COVID-19), it is of\n",
      "great importance to conduct early diagnosis of COVID-19 and predict the time\n",
      "that patients might convert to the severe stage, for designing effective\n",
      "treatment plan and reducing the clinicians' workloads. In this study, we\n",
      "propose a joint classification and regression method to determine whether the\n",
      "patient would develop severe symptoms in the later time, and if yes, predict\n",
      "the possible conversion time that the patient would spend to convert to the\n",
      "severe stage. To do this, the proposed method takes into account 1) the weight\n",
      "for each sample to reduce the outliers' influence and explore the problem of\n",
      "imbalance classification, and 2) the weight for each feature via a sparsity\n",
      "regularization term to remove the redundant features of high-dimensional data\n",
      "and learn the shared information across the classification task and the\n",
      "regression task. To our knowledge, this study is the first work to predict the\n",
      "disease progression and the conversion time, which could help clinicians to\n",
      "deal with the potential severe cases in time or even save the patients' lives.\n",
      "Experimental analysis was conducted on a real data set from two hospitals with\n",
      "422 chest computed tomography (CT) scans, where 52 cases were converted to\n",
      "severe on average 5.64 days and 34 cases were severe at admission. Results show\n",
      "that our method achieves the best classification (e.g., 85.91% of accuracy) and\n",
      "regression (e.g., 0.462 of the correlation coefficient) performance, compared\n",
      "to all comparison methods. Moreover, our proposed method yields 76.97% of\n",
      "accuracy for predicting the severe cases, 0.524 of the correlation coefficient,\n",
      "and 0.55 days difference for the converted time.\n",
      "\n",
      "**Paper Id :2005.03227 \n",
      "Title :Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent\n",
      "  Multi-View Representation Learning\n",
      "  Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread\n",
      "rapidly across the world. Due to the large number of affected patients and\n",
      "heavy labor for doctors, computer-aided diagnosis with machine learning\n",
      "algorithm is urgently needed, and could largely reduce the efforts of\n",
      "clinicians and accelerate the diagnosis process. Chest computed tomography (CT)\n",
      "has been recognized as an informative tool for diagnosis of the disease. In\n",
      "this study, we propose to conduct the diagnosis of COVID-19 with a series of\n",
      "features extracted from CT images. To fully explore multiple features\n",
      "describing CT images from different views, a unified latent representation is\n",
      "learned which can completely encode information from different aspects of\n",
      "features and is endowed with promising class structure for separability.\n",
      "Specifically, the completeness is guaranteed with a group of backward neural\n",
      "networks (each for one type of features), while by using class labels the\n",
      "representation is enforced to be compact within COVID-19/community-acquired\n",
      "pneumonia (CAP) and also a large margin is guaranteed between different types\n",
      "of pneumonia. In this way, our model can well avoid overfitting compared to the\n",
      "case of directly projecting highdimensional features into classes. Extensive\n",
      "experimental results show that the proposed method outperforms all comparison\n",
      "methods, and rather stable performances are observed when varying the numbers\n",
      "of training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.03812 \n",
      "Title :Comparative Analysis of Word Embeddings for Capturing Word Similarities\n",
      "  Distributed language representation has become the most widely used technique\n",
      "for language representation in various natural language processing tasks. Most\n",
      "of the natural language processing models that are based on deep learning\n",
      "techniques use already pre-trained distributed word representations, commonly\n",
      "called word embeddings. Determining the most qualitative word embeddings is of\n",
      "crucial importance for such models. However, selecting the appropriate word\n",
      "embeddings is a perplexing task since the projected embedding space is not\n",
      "intuitive to humans. In this paper, we explore different approaches for\n",
      "creating distributed word representations. We perform an intrinsic evaluation\n",
      "of several state-of-the-art word embedding methods. Their performance on\n",
      "capturing word similarities is analysed with existing benchmark datasets for\n",
      "word pairs similarities. The research in this paper conducts a correlation\n",
      "analysis between ground truth word similarities and similarities obtained by\n",
      "different word embedding methods.\n",
      "\n",
      "**Paper Id :2010.05609 \n",
      "Title :Load What You Need: Smaller Versions of Multilingual BERT\n",
      "  Pre-trained Transformer-based models are achieving state-of-the-art results\n",
      "on a variety of Natural Language Processing data sets. However, the size of\n",
      "these models is often a drawback for their deployment in real production\n",
      "applications. In the case of multilingual models, most of the parameters are\n",
      "located in the embeddings layer. Therefore, reducing the vocabulary size should\n",
      "have an important impact on the total number of parameters. In this paper, we\n",
      "propose to generate smaller models that handle fewer number of languages\n",
      "according to the targeted corpora. We present an evaluation of smaller versions\n",
      "of multilingual BERT on the XNLI data set, but we believe that this method may\n",
      "be applied to other multilingual transformers. The obtained results confirm\n",
      "that we can generate smaller models that keep comparable results, while\n",
      "reducing up to 45% of the total number of parameters. We compared our models\n",
      "with DistilmBERT (a distilled version of multilingual BERT) and showed that\n",
      "unlike language reduction, distillation induced a 1.7% to 6% drop in the\n",
      "overall accuracy on the XNLI data set. The presented models and code are\n",
      "publicly available.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.03945 \n",
      "Title :Inferring Vector Magnetic Fields from Stokes Profiles of GST/NIRIS Using\n",
      "  a Convolutional Neural Network\n",
      "  We propose a new machine learning approach to Stokes inversion based on a\n",
      "convolutional neural network (CNN) and the Milne-Eddington (ME) method. The\n",
      "Stokes measurements used in this study were taken by the Near InfraRed Imaging\n",
      "Spectropolarimeter (NIRIS) on the 1.6 m Goode Solar Telescope (GST) at the Big\n",
      "Bear Solar Observatory. By learning the latent patterns in the training data\n",
      "prepared by the physics-based ME tool, the proposed CNN method is able to infer\n",
      "vector magnetic fields from the Stokes profiles of GST/NIRIS. Experimental\n",
      "results show that our CNN method produces smoother and cleaner magnetic maps\n",
      "than the widely used ME method. Furthermore, the CNN method is 4~6 times faster\n",
      "than the ME method, and is able to produce vector magnetic fields in near\n",
      "real-time, which is essential to space weather forecasting. Specifically, it\n",
      "takes ~50 seconds for the CNN method to process an image of 720 x 720 pixels\n",
      "comprising Stokes profiles of GST/NIRIS. Finally, the CNN-inferred results are\n",
      "highly correlated to the ME-calculated results and are closer to the ME's\n",
      "results with the Pearson product-moment correlation coefficient (PPMCC) being\n",
      "closer to 1 on average than those from other machine learning algorithms such\n",
      "as multiple support vector regression and multilayer perceptrons (MLP). In\n",
      "particular, the CNN method outperforms the current best machine learning method\n",
      "(MLP) by 2.6% on average in PPMCC according to our experimental study. Thus,\n",
      "the proposed physics-assisted deep learning-based CNN tool can be considered as\n",
      "an alternative, efficient method for Stokes inversion for high resolution\n",
      "polarimetric observations obtained by GST/NIRIS.\n",
      "\n",
      "**Paper Id :2008.12080 \n",
      "Title :Identifying and Tracking Solar Magnetic Flux Elements with Deep Learning\n",
      "  Deep learning has drawn a lot of interest in recent years due to its\n",
      "effectiveness in processing big and complex observational data gathered from\n",
      "diverse instruments. Here we propose a new deep learning method, called\n",
      "SolarUnet, to identify and track solar magnetic flux elements or features in\n",
      "observed vector magnetograms based on the Southwest Automatic Magnetic\n",
      "Identification Suite (SWAMIS). Our method consists of a data pre-processing\n",
      "component that prepares training data from the SWAMIS tool, a deep learning\n",
      "model implemented as a U-shaped convolutional neural network for fast and\n",
      "accurate image segmentation, and a post-processing component that prepares\n",
      "tracking results. SolarUnet is applied to data from the 1.6 meter Goode Solar\n",
      "Telescope at the Big Bear Solar Observatory. When compared to the widely used\n",
      "SWAMIS tool, SolarUnet is faster while agreeing mostly with SWAMIS on feature\n",
      "size and flux distributions, and complementing SWAMIS in tracking long-lifetime\n",
      "features. Thus, the proposed physics-guided deep learning-based tool can be\n",
      "considered as an alternative method for solar magnetic tracking.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04118 \n",
      "Title :Beyond Accuracy: Behavioral Testing of NLP models with CheckList\n",
      "  Although measuring held-out accuracy has been the primary approach to\n",
      "evaluate generalization, it often overestimates the performance of NLP models,\n",
      "while alternative approaches for evaluating models either focus on individual\n",
      "tasks or on specific behaviors. Inspired by principles of behavioral testing in\n",
      "software engineering, we introduce CheckList, a task-agnostic methodology for\n",
      "testing NLP models. CheckList includes a matrix of general linguistic\n",
      "capabilities and test types that facilitate comprehensive test ideation, as\n",
      "well as a software tool to generate a large and diverse number of test cases\n",
      "quickly. We illustrate the utility of CheckList with tests for three tasks,\n",
      "identifying critical failures in both commercial and state-of-art models. In a\n",
      "user study, a team responsible for a commercial sentiment analysis model found\n",
      "new and actionable bugs in an extensively tested model. In another user study,\n",
      "NLP practitioners with CheckList created twice as many tests, and found almost\n",
      "three times as many bugs as users without it.\n",
      "\n",
      "**Paper Id :1911.01225 \n",
      "Title :Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale\n",
      "  Service Environment\n",
      "  Root cause analysis in a large-scale production environment is challenging\n",
      "due to the complexity of services running across global data centers. Due to\n",
      "the distributed nature of a large-scale system, the various hardware, software,\n",
      "and tooling logs are often maintained separately, making it difficult to review\n",
      "the logs jointly for understanding production issues. Another challenge in\n",
      "reviewing the logs for identifying issues is the scale - there could easily be\n",
      "millions of entities, each described by hundreds of features. In this paper we\n",
      "present a fast dimensional analysis framework that automates the root cause\n",
      "analysis on structured logs with improved scalability.\n",
      "  We first explore item-sets, i.e. combinations of feature values, that could\n",
      "identify groups of samples with sufficient support for the target failures\n",
      "using the Apriori algorithm and a subsequent improvement, FP-Growth. These\n",
      "algorithms were designed for frequent item-set mining and association rule\n",
      "learning over transactional databases. After applying them on structured logs,\n",
      "we select the item-sets that are most unique to the target failures based on\n",
      "lift. We propose pre-processing steps with the use of a large-scale real-time\n",
      "database and post-processing techniques and parallelism to further speed up the\n",
      "analysis and improve interpretability, and demonstrate that such optimization\n",
      "is necessary for handling large-scale production datasets. We have successfully\n",
      "rolled out this approach for root cause investigation purposes in a large-scale\n",
      "infrastructure. We also present the setup and results from multiple production\n",
      "use cases in this paper.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04167 \n",
      "Title :Continuous Learning in a Single-Incremental-Task Scenario with Spike\n",
      "  Features\n",
      "  Deep Neural Networks (DNNs) have two key deficiencies, their dependence on\n",
      "high precision computing and their inability to perform sequential learning,\n",
      "that is, when a DNN is trained on a first task and the same DNN is trained on\n",
      "the next task it forgets the first task. This phenomenon of forgetting previous\n",
      "tasks is also referred to as catastrophic forgetting. On the other hand a\n",
      "mammalian brain outperforms DNNs in terms of energy efficiency and the ability\n",
      "to learn sequentially without catastrophically forgetting. Here, we use\n",
      "bio-inspired Spike Timing Dependent Plasticity (STDP)in the feature extraction\n",
      "layers of the network with instantaneous neurons to extract meaningful\n",
      "features. In the classification sections of the network we use a modified\n",
      "synaptic intelligence that we refer to as cost per synapse metric as a\n",
      "regularizer to immunize the network against catastrophic forgetting in a\n",
      "Single-Incremental-Task scenario (SIT). In this study, we use MNIST handwritten\n",
      "digits dataset that was divided into five sub-tasks.\n",
      "\n",
      "**Paper Id :1912.09132 \n",
      "Title :Mean field theory for deep dropout networks: digging up gradient\n",
      "  backpropagation deeply\n",
      "  In recent years, the mean field theory has been applied to the study of\n",
      "neural networks and has achieved a great deal of success. The theory has been\n",
      "applied to various neural network structures, including CNNs, RNNs, Residual\n",
      "networks, and Batch normalization. Inevitably, recent work has also covered the\n",
      "use of dropout. The mean field theory shows that the existence of depth scales\n",
      "that limit the maximum depth of signal propagation and gradient\n",
      "backpropagation. However, the gradient backpropagation is derived under the\n",
      "gradient independence assumption that weights used during feed forward are\n",
      "drawn independently from the ones used in backpropagation. This is not how\n",
      "neural networks are trained in a real setting. Instead, the same weights used\n",
      "in a feed-forward step needs to be carried over to its corresponding\n",
      "backpropagation. Using this realistic condition, we perform theoretical\n",
      "computation on linear dropout networks and a series of experiments on dropout\n",
      "networks. Our empirical results show an interesting phenomenon that the length\n",
      "gradients can backpropagate for a single input and a pair of inputs are\n",
      "governed by the same depth scale. Besides, we study the relationship between\n",
      "variance and mean of statistical metrics of the gradient and shown an emergence\n",
      "of universality. Finally, we investigate the maximum trainable length for deep\n",
      "dropout networks through a series of experiments using MNIST and CIFAR10 and\n",
      "provide a more precise empirical formula that describes the trainable length\n",
      "than original work.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04518 \n",
      "Title :What Was Written vs. Who Read It: News Media Profiling Using Text\n",
      "  Analysis and Social Media Context\n",
      "  Predicting the political bias and the factuality of reporting of entire news\n",
      "outlets are critical elements of media profiling, which is an understudied but\n",
      "an increasingly important research direction. The present level of\n",
      "proliferation of fake, biased, and propagandistic content online, has made it\n",
      "impossible to fact-check every single suspicious claim, either manually or\n",
      "automatically. Alternatively, we can profile entire news outlets and look for\n",
      "those that are likely to publish fake or biased content. This approach makes it\n",
      "possible to detect likely \"fake news\" the moment they are published, by simply\n",
      "checking the reliability of their source. From a practical perspective,\n",
      "political bias and factuality of reporting have a linguistic aspect but also a\n",
      "social context. Here, we study the impact of both, namely (i) what was written\n",
      "(i.e., what was published by the target medium, and how it describes itself on\n",
      "Twitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium\n",
      "on Facebook, Twitter, and YouTube). We further study (iii) what was written\n",
      "about the target medium on Wikipedia. The evaluation results show that what was\n",
      "written matters most, and that putting all information sources together yields\n",
      "huge improvements over the current state-of-the-art.\n",
      "\n",
      "**Paper Id :2005.06058 \n",
      "Title :That is a Known Lie: Detecting Previously Fact-Checked Claims\n",
      "  The recent proliferation of \"fake news\" has triggered a number of responses,\n",
      "most notably the emergence of several manual fact-checking initiatives. As a\n",
      "result and over time, a large number of fact-checked claims have been\n",
      "accumulated, which increases the likelihood that a new claim in social media or\n",
      "a new statement by a politician might have already been fact-checked by some\n",
      "trusted fact-checking organization, as viral claims often come back after a\n",
      "while in social media, and politicians like to repeat their favorite\n",
      "statements, true or false, over and over again. As manual fact-checking is very\n",
      "time-consuming (and fully automatic fact-checking has credibility issues), it\n",
      "is important to try to save this effort and to avoid wasting time on claims\n",
      "that have already been fact-checked. Interestingly, despite the importance of\n",
      "the task, it has been largely ignored by the research community so far. Here,\n",
      "we aim to bridge this gap. In particular, we formulate the task and we discuss\n",
      "how it relates to, but also differs from, previous work. We further create a\n",
      "specialized dataset, which we release to the research community. Finally, we\n",
      "present learning-to-rank experiments that demonstrate sizable improvements over\n",
      "state-of-the-art retrieval and textual similarity approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04726 \n",
      "Title :Knowledge Graph semantic enhancement of input data for improving AI\n",
      "  Intelligent systems designed using machine learning algorithms require a\n",
      "large number of labeled data. Background knowledge provides complementary, real\n",
      "world factual information that can augment the limited labeled data to train a\n",
      "machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for\n",
      "many practical applications, it is convenient and useful to organize this\n",
      "background knowledge in the form of a graph. Recent academic research and\n",
      "implemented industrial intelligent systems have shown promising performance for\n",
      "machine learning algorithms that combine training data with a knowledge graph.\n",
      "In this article, we discuss the use of relevant KGs to enhance input data for\n",
      "two applications that use machine learning -- recommendation and community\n",
      "detection. The KG improves both accuracy and explainability.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04755 \n",
      "Title :BayesRace: Learning to race autonomously using prior experience\n",
      "  Autonomous race cars require perception, estimation, planning, and control\n",
      "modules which work together asynchronously while driving at the limit of a\n",
      "vehicle's handling capability. A fundamental challenge encountered in designing\n",
      "these software components lies in predicting the vehicle's future state (e.g.\n",
      "position, orientation, and speed) with high accuracy. The root cause is the\n",
      "difficulty in identifying vehicle model parameters that capture the effects of\n",
      "lateral tire slip. We present a model-based planning and control framework for\n",
      "autonomous racing that significantly reduces the effort required in system\n",
      "identification and control design. Our approach alleviates the gap induced by\n",
      "simulation-based controller design by learning from on-board sensor\n",
      "measurements. A major focus of this work is empirical, thus, we demonstrate our\n",
      "contributions by experiments on validated 1:43 and 1:10 scale autonomous racing\n",
      "simulations.\n",
      "\n",
      "**Paper Id :1909.02119 \n",
      "Title :Inductive-bias-driven Reinforcement Learning For Efficient Schedules in\n",
      "  Heterogeneous Clusters\n",
      "  The problem of scheduling of workloads onto heterogeneous processors (e.g.,\n",
      "CPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\n",
      "system schedulers rely on application/system-specific heuristics that have to\n",
      "be built on a case-by-case basis. Recent work has demonstrated ML techniques\n",
      "for automating the heuristic search by using black-box approaches which require\n",
      "significant training data and time, which make them challenging to use in\n",
      "practice. This paper presents Symphony, a scheduling framework that addresses\n",
      "the challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n",
      "(RL) model for scheduling, which inherently models the resource dependencies\n",
      "identified from the system architecture; and (ii) a sampling-based technique to\n",
      "compute the gradients of a Bayesian model without performing full probabilistic\n",
      "inference. Together, these techniques reduce both the amount of training data\n",
      "and the time required to produce scheduling policies that significantly\n",
      "outperform black-box approaches by up to 2.2x.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04816 \n",
      "Title :Leveraging Monolingual Data with Self-Supervision for Multilingual\n",
      "  Neural Machine Translation\n",
      "  Over the last few years two promising research directions in low-resource\n",
      "neural machine translation (NMT) have emerged. The first focuses on utilizing\n",
      "high-resource languages to improve the quality of low-resource languages via\n",
      "multilingual NMT. The second direction employs monolingual data with\n",
      "self-supervision to pre-train translation models, followed by fine-tuning on\n",
      "small amounts of supervised data. In this work, we join these two lines of\n",
      "research and demonstrate the efficacy of monolingual data with self-supervision\n",
      "in multilingual NMT. We offer three major results: (i) Using monolingual data\n",
      "significantly boosts the translation quality of low-resource languages in\n",
      "multilingual models. (ii) Self-supervision improves zero-shot translation\n",
      "quality in multilingual models. (iii) Leveraging monolingual data with\n",
      "self-supervision provides a viable path towards adding new languages to\n",
      "multilingual models, getting up to 33 BLEU on ro-en translation without any\n",
      "parallel data or back-translation.\n",
      "\n",
      "**Paper Id :1911.09812 \n",
      "Title :Zero-Resource Cross-Lingual Named Entity Recognition\n",
      "  Recently, neural methods have achieved state-of-the-art (SOTA) results in\n",
      "Named Entity Recognition (NER) tasks for many languages without the need for\n",
      "manually crafted features. However, these models still require manually\n",
      "annotated training data, which is not available for many languages. In this\n",
      "paper, we propose an unsupervised cross-lingual NER model that can transfer NER\n",
      "knowledge from one language to another in a completely unsupervised way without\n",
      "relying on any bilingual dictionary or parallel data. Our model achieves this\n",
      "through word-level adversarial learning and augmented fine-tuning with\n",
      "parameter sharing and feature augmentation. Experiments on five different\n",
      "languages demonstrate the effectiveness of our approach, outperforming existing\n",
      "models by a good margin and setting a new SOTA for each language pair.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04879 \n",
      "Title :Incorporating structured assumptions with probabilistic graphical models\n",
      "  in fMRI data analysis\n",
      "  With the wide adoption of functional magnetic resonance imaging (fMRI) by\n",
      "cognitive neuroscience researchers, large volumes of brain imaging data have\n",
      "been accumulated in recent years. Aggregating these data to derive scientific\n",
      "insights often faces the challenge that fMRI data are high-dimensional,\n",
      "heterogeneous across people, and noisy. These challenges demand the development\n",
      "of computational tools that are tailored both for the neuroscience questions\n",
      "and for the properties of the data. We review a few recently developed\n",
      "algorithms in various domains of fMRI research: fMRI in naturalistic tasks,\n",
      "analyzing full-brain functional connectivity, pattern classification, inferring\n",
      "representational similarity and modeling structured residuals. These algorithms\n",
      "all tackle the challenges in fMRI similarly: they start by making clear\n",
      "statements of assumptions about neural data and existing domain knowledge,\n",
      "incorporating those assumptions and domain knowledge into probabilistic\n",
      "graphical models, and using those models to estimate properties of interest or\n",
      "latent structures in the data. Such approaches can avoid erroneous findings,\n",
      "reduce the impact of noise, better utilize known properties of the data, and\n",
      "better aggregate data across groups of subjects. With these successful cases,\n",
      "we advocate wider adoption of explicit model construction in cognitive\n",
      "neuroscience. Although we focus on fMRI, the principle illustrated here is\n",
      "generally applicable to brain data of other modalities.\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04912 \n",
      "Title :Maximizing Information Gain in Partially Observable Environments via\n",
      "  Prediction Reward\n",
      "  Information gathering in a partially observable environment can be formulated\n",
      "as a reinforcement learning (RL), problem where the reward depends on the\n",
      "agent's uncertainty. For example, the reward can be the negative entropy of the\n",
      "agent's belief over an unknown (or hidden) variable. Typically, the rewards of\n",
      "an RL agent are defined as a function of the state-action pairs and not as a\n",
      "function of the belief of the agent; this hinders the direct application of\n",
      "deep RL methods for such tasks. This paper tackles the challenge of using\n",
      "belief-based rewards for a deep RL agent, by offering a simple insight that\n",
      "maximizing any convex function of the belief of the agent can be approximated\n",
      "by instead maximizing a prediction reward: a reward based on prediction\n",
      "accuracy. In particular, we derive the exact error between negative entropy and\n",
      "the expected prediction reward. This insight provides theoretical motivation\n",
      "for several fields using prediction rewards---namely visual attention, question\n",
      "answering systems, and intrinsic motivation---and highlights their connection\n",
      "to the usually distinct fields of active perception, active sensing, and sensor\n",
      "placement. Based on this insight we present deep anticipatory networks (DANs),\n",
      "which enables an agent to take actions to reduce its uncertainty without\n",
      "performing explicit belief inference. We present two applications of DANs:\n",
      "building a sensor selection system for tracking people in a shopping mall and\n",
      "learning discrete models of attention on fashion MNIST and MNIST digit\n",
      "classification.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.04949 \n",
      "Title :Designing for Human Rights in AI\n",
      "  In the age of big data, companies and governments are increasingly using\n",
      "algorithms to inform hiring decisions, employee management, policing, credit\n",
      "scoring, insurance pricing, and many more aspects of our lives. AI systems can\n",
      "help us make evidence-driven, efficient decisions, but can also confront us\n",
      "with unjustified, discriminatory decisions wrongly assumed to be accurate\n",
      "because they are made automatically and quantitatively. It is becoming evident\n",
      "that these technological developments are consequential to people's fundamental\n",
      "human rights. Despite increasing attention to these urgent challenges in recent\n",
      "years, technical solutions to these complex socio-ethical problems are often\n",
      "developed without empirical study of societal context and the critical input of\n",
      "societal stakeholders who are impacted by the technology. On the other hand,\n",
      "calls for more ethically- and socially-aware AI often fail to provide answers\n",
      "for how to proceed beyond stressing the importance of transparency,\n",
      "explainability, and fairness. Bridging these socio-technical gaps and the deep\n",
      "divide between abstract value language and design requirements is essential to\n",
      "facilitate nuanced, context-dependent design choices that will support moral\n",
      "and social values. In this paper, we bridge this divide through the framework\n",
      "of Design for Values, drawing on methodologies of Value Sensitive Design and\n",
      "Participatory Design to present a roadmap for proactively engaging societal\n",
      "stakeholders to translate fundamental human rights into context-dependent\n",
      "design requirements through a structured, inclusive, and transparent process.\n",
      "\n",
      "**Paper Id :1908.00176 \n",
      "Title :FairSight: Visual Analytics for Fairness in Decision Making\n",
      "  Data-driven decision making related to individuals has become increasingly\n",
      "pervasive, but the issue concerning the potential discrimination has been\n",
      "raised by recent studies. In response, researchers have made efforts to propose\n",
      "and implement fairness measures and algorithms, but those efforts have not been\n",
      "translated to the real-world practice of data-driven decision making. As such,\n",
      "there is still an urgent need to create a viable tool to facilitate fair\n",
      "decision making. We propose FairSight, a visual analytic system to address this\n",
      "need; it is designed to achieve different notions of fairness in ranking\n",
      "decisions through identifying the required actions -- understanding, measuring,\n",
      "diagnosing and mitigating biases -- that together lead to fairer decision\n",
      "making. Through a case study and user study, we demonstrate that the proposed\n",
      "visual analytic and diagnostic modules in the system are effective in\n",
      "understanding the fairness-aware decision pipeline and obtaining more fair\n",
      "outcomes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "**Paper Id :2002.00011 \n",
      "Title :Age-Conditioned Synthesis of Pediatric Computed Tomography with\n",
      "  Auxiliary Classifier Generative Adversarial Networks\n",
      "  Deep learning is a popular and powerful tool in computed tomography (CT)\n",
      "image processing such as organ segmentation, but its requirement of large\n",
      "training datasets remains a challenge. Even though there is a large anatomical\n",
      "variability for children during their growth, the training datasets for\n",
      "pediatric CT scans are especially hard to obtain due to risks of radiation to\n",
      "children. In this paper, we propose a method to conditionally synthesize\n",
      "realistic pediatric CT images using a new auxiliary classifier generative\n",
      "adversarial network (ACGAN) architecture by taking age information into\n",
      "account. The proposed network generated age-conditioned high-resolution CT\n",
      "images to enrich pediatric training datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.05618 \n",
      "Title :Machine Learning Guided Discovery of Gigantic Magnetocaloric Effect in\n",
      "  HoB$_{2}$ Near Hydrogen Liquefaction Temperature\n",
      "  Magnetic refrigeration exploits the magnetocaloric effect which is the\n",
      "entropy change upon application and removal of magnetic fields in materials,\n",
      "providing an alternate path for refrigeration other than the conventional gas\n",
      "cycles. While intensive research has uncovered a vast number of magnetic\n",
      "materials which exhibits large magnetocaloric effect, these properties for a\n",
      "large number of compounds still remain unknown. To explore new functional\n",
      "materials in this unknown space, machine learning is used as a guide for\n",
      "selecting materials which could exhibit large magnetocaloric effect. By this\n",
      "approach, HoB$_{2}$ is singled out, synthesized and its magnetocaloric\n",
      "properties are evaluated, leading to the experimental discovery of gigantic\n",
      "magnetic entropy change 40.1 J kg$^{-1}$ K$^{-1}$ (0.35 J cm$^{-3}$ K$^{-1}$)\n",
      "for a field change of 5 T in the vicinity of a ferromagnetic second-order phase\n",
      "transition with a Curie temperature of 15 K. This is the highest value reported\n",
      "so far, to our knowledge, near the hydrogen liquefaction temperature thus it is\n",
      "a highly suitable material for hydrogen liquefaction and low temperature\n",
      "magnetic cooling applications.\n",
      "\n",
      "**Paper Id :2006.01247 \n",
      "Title :Wavelet Scattering Networks for Atomistic Systems with Extrapolation of\n",
      "  Material Properties\n",
      "  The dream of machine learning in materials science is for a model to learn\n",
      "the underlying physics of an atomic system, allowing it to move beyond\n",
      "interpolation of the training set to the prediction of properties that were not\n",
      "present in the original training data. In addition to advances in machine\n",
      "learning architectures and training techniques, achieving this ambitious goal\n",
      "requires a method to convert a 3D atomic system into a feature representation\n",
      "that preserves rotational and translational symmetry, smoothness under small\n",
      "perturbations, and invariance under re-ordering. The atomic orbital wavelet\n",
      "scattering transform preserves these symmetries by construction, and has\n",
      "achieved great success as a featurization method for machine learning energy\n",
      "prediction. Both in small molecules and in the bulk amorphous\n",
      "$\\text{Li}_{\\alpha}\\text{Si}$ system, machine learning models using wavelet\n",
      "scattering coefficients as features have demonstrated a comparable accuracy to\n",
      "Density Functional Theory at a small fraction of the computational cost. In\n",
      "this work, we test the generalizability of our $\\text{Li}_{\\alpha}\\text{Si}$\n",
      "energy predictor to properties that were not included in the training set, such\n",
      "as elastic constants and migration barriers. We demonstrate that statistical\n",
      "feature selection methods can reduce over-fitting and lead to remarkable\n",
      "accuracy in these extrapolation tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.05652 \n",
      "Title :Very High Resolution Land Cover Mapping of Urban Areas at Global Scale\n",
      "  with Convolutional Neural Networks\n",
      "  This paper describes a methodology to produce a 7-classes land cover map of\n",
      "urban areas from very high resolution images and limited noisy labeled data.\n",
      "The objective is to make a segmentation map of a large area (a french\n",
      "department) with the following classes: asphalt, bare soil, building,\n",
      "grassland, mineral material (permeable artificialized areas), forest and water\n",
      "from 20cm aerial images and Digital Height Model. We created a training dataset\n",
      "on a few areas of interest aggregating databases, semi-automatic\n",
      "classification, and manual annotation to get a complete ground truth in each\n",
      "class. A comparative study of different encoder-decoder architectures (U-Net,\n",
      "U-Net with Resnet encoders, Deeplab v3+) is presented with different loss\n",
      "functions. The final product is a highly valuable land cover map computed from\n",
      "model predictions stitched together, binarized, and refined before\n",
      "vectorization.\n",
      "\n",
      "**Paper Id :2006.15969 \n",
      "Title :Interpretation of 3D CNNs for Brain MRI Data Classification\n",
      "  Deep learning shows high potential for many medical image analysis tasks.\n",
      "Neural networks can work with full-size data without extensive preprocessing\n",
      "and feature generation and, thus, information loss. Recent work has shown that\n",
      "the morphological difference in specific brain regions can be found on MRI with\n",
      "the means of Convolution Neural Networks (CNN). However, interpretation of the\n",
      "existing models is based on a region of interest and can not be extended to\n",
      "voxel-wise image interpretation on a whole image. In the current work, we\n",
      "consider the classification task on a large-scale open-source dataset of young\n",
      "healthy subjects -- an exploration of brain differences between men and women.\n",
      "In this paper, we extend the previous findings in gender differences from\n",
      "diffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D\n",
      "CNN interpretation comparing the results of three interpretation methods:\n",
      "Meaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute\n",
      "with the open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.05823 \n",
      "Title :Perturbing Inputs to Prevent Model Stealing\n",
      "  We show how perturbing inputs to machine learning services (ML-service)\n",
      "deployed in the cloud can protect against model stealing attacks. In our\n",
      "formulation, there is an ML-service that receives inputs from users and returns\n",
      "the output of the model. There is an attacker that is interested in learning\n",
      "the parameters of the ML-service. We use the linear and logistic regression\n",
      "models to illustrate how strategically adding noise to the inputs fundamentally\n",
      "alters the attacker's estimation problem. We show that even with infinite\n",
      "samples, the attacker would not be able to recover the true model parameters.\n",
      "We focus on characterizing the trade-off between the error in the attacker's\n",
      "estimate of the parameters with the error in the ML-service's output.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.05854 \n",
      "Title :Prta: A System to Support the Analysis of Propaganda Techniques in the\n",
      "  News\n",
      "  Recent events, such as the 2016 US Presidential Campaign, Brexit and the\n",
      "COVID-19 \"infodemic\", have brought into the spotlight the dangers of online\n",
      "disinformation. There has been a lot of research focusing on fact-checking and\n",
      "disinformation detection. However, little attention has been paid to the\n",
      "specific rhetorical and psychological techniques used to convey propaganda\n",
      "messages. Revealing the use of such techniques can help promote media literacy\n",
      "and critical thinking, and eventually contribute to limiting the impact of\n",
      "\"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion\n",
      "Techniques Analyzer) allows users to explore the articles crawled on a regular\n",
      "basis by highlighting the spans in which propaganda techniques occur and to\n",
      "compare them on the basis of their use of propaganda techniques. The system\n",
      "further reports statistics about the use of such techniques, overall and over\n",
      "time, or according to filtering criteria specified by the user based on time\n",
      "interval, keywords, and/or political orientation of the media. Moreover, it\n",
      "allows users to analyze any text or URL through a dedicated interface or via an\n",
      "API. The system is available online: https://www.tanbih.org/prta\n",
      "\n",
      "**Paper Id :2007.08024 \n",
      "Title :A Survey on Computational Propaganda Detection\n",
      "  Propaganda campaigns aim at influencing people's mindset with the purpose of\n",
      "advancing a specific agenda. They exploit the anonymity of the Internet, the\n",
      "micro-profiling ability of social networks, and the ease of automatically\n",
      "creating and managing coordinated networks of accounts, to reach millions of\n",
      "social network users with persuasive messages, specifically targeted to topics\n",
      "each individual user is sensitive to, and ultimately influencing the outcome on\n",
      "a targeted issue. In this survey, we review the state of the art on\n",
      "computational propaganda detection from the perspective of Natural Language\n",
      "Processing and Network Analysis, arguing about the need for combined efforts\n",
      "between these communities. We further discuss current challenges and future\n",
      "research directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.05930 \n",
      "Title :Localized convolutional neural networks for geospatial wind forecasting\n",
      "  Convolutional Neural Networks (CNN) possess many positive qualities when it\n",
      "comes to spatial raster data. Translation invariance enables CNNs to detect\n",
      "features regardless of their position in the scene. However, in some domains,\n",
      "like geospatial, not all locations are exactly equal. In this work, we propose\n",
      "localized convolutional neural networks that enable convolutional architectures\n",
      "to learn local features in addition to the global ones. We investigate their\n",
      "instantiations in the form of learnable inputs, local weights, and a more\n",
      "general form. They can be added to any convolutional layers, easily end-to-end\n",
      "trained, introduce minimal additional complexity, and let CNNs retain most of\n",
      "their benefits to the extent that they are needed. In this work we address\n",
      "spatio-temporal prediction: test the effectiveness of our methods on a\n",
      "synthetic benchmark dataset and tackle three real-world wind prediction\n",
      "datasets. For one of them, we propose a method to spatially order the unordered\n",
      "data. We compare the recent state-of-the-art spatio-temporal prediction models\n",
      "on the same data. Models that use convolutional layers can be and are extended\n",
      "with our localizations. In all these cases our extensions improve the results,\n",
      "and thus often the state-of-the-art. We share all the code at a public\n",
      "repository.\n",
      "\n",
      "**Paper Id :2008.08072 \n",
      "Title :AssembleNet++: Assembling Modality Representations via Attention\n",
      "  Connections\n",
      "  We create a family of powerful video models which are able to: (i) learn\n",
      "interactions between semantic object information and raw appearance and motion\n",
      "features, and (ii) deploy attention in order to better learn the importance of\n",
      "features at each convolutional block of the network. A new network component\n",
      "named peer-attention is introduced, which dynamically learns the attention\n",
      "weights using another block or input modality. Even without pre-training, our\n",
      "models outperform the previous work on standard public activity recognition\n",
      "datasets with continuous videos, establishing new state-of-the-art. We also\n",
      "confirm that our findings of having neural connections from the object modality\n",
      "and the use of peer-attention is generally applicable for different existing\n",
      "architectures, improving their performances. We name our model explicitly as\n",
      "AssembleNet++. The code will be available at:\n",
      "https://sites.google.com/corp/view/assemblenet/\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06058 \n",
      "Title :That is a Known Lie: Detecting Previously Fact-Checked Claims\n",
      "  The recent proliferation of \"fake news\" has triggered a number of responses,\n",
      "most notably the emergence of several manual fact-checking initiatives. As a\n",
      "result and over time, a large number of fact-checked claims have been\n",
      "accumulated, which increases the likelihood that a new claim in social media or\n",
      "a new statement by a politician might have already been fact-checked by some\n",
      "trusted fact-checking organization, as viral claims often come back after a\n",
      "while in social media, and politicians like to repeat their favorite\n",
      "statements, true or false, over and over again. As manual fact-checking is very\n",
      "time-consuming (and fully automatic fact-checking has credibility issues), it\n",
      "is important to try to save this effort and to avoid wasting time on claims\n",
      "that have already been fact-checked. Interestingly, despite the importance of\n",
      "the task, it has been largely ignored by the research community so far. Here,\n",
      "we aim to bridge this gap. In particular, we formulate the task and we discuss\n",
      "how it relates to, but also differs from, previous work. We further create a\n",
      "specialized dataset, which we release to the research community. Finally, we\n",
      "present learning-to-rank experiments that demonstrate sizable improvements over\n",
      "state-of-the-art retrieval and textual similarity approaches.\n",
      "\n",
      "**Paper Id :2009.02931 \n",
      "Title :Team Alex at CLEF CheckThat! 2020: Identifying Check-Worthy Tweets With\n",
      "  Transformer Models\n",
      "  While misinformation and disinformation have been thriving in social media\n",
      "for years, with the emergence of the COVID-19 pandemic, the political and the\n",
      "health misinformation merged, thus elevating the problem to a whole new level\n",
      "and giving rise to the first global infodemic. The fight against this infodemic\n",
      "has many aspects, with fact-checking and debunking false and misleading claims\n",
      "being among the most important ones. Unfortunately, manual fact-checking is\n",
      "time-consuming and automatic fact-checking is resource-intense, which means\n",
      "that we need to pre-filter the input social media posts and to throw out those\n",
      "that do not appear to be check-worthy. With this in mind, here we propose a\n",
      "model for detecting check-worthy tweets about COVID-19, which combines deep\n",
      "contextualized text representations with modeling the social context of the\n",
      "tweet. We further describe a number of additional experiments and comparisons,\n",
      "which we believe should be useful for future research as they provide some\n",
      "indication about what techniques are effective for the task. Our official\n",
      "submission to the English version of CLEF-2020 CheckThat! Task 1, system\n",
      "Team_Alex, was ranked second with a MAP score of 0.8034, which is almost tied\n",
      "with the wining system, lagging behind by just 0.003 MAP points absolute.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06540 \n",
      "Title :Deep Learning for Political Science\n",
      "  Political science, and social science in general, have traditionally been\n",
      "using computational methods to study areas such as voting behavior, policy\n",
      "making, international conflict, and international development. More recently,\n",
      "increasingly available quantities of data are being combined with improved\n",
      "algorithms and affordable computational resources to predict, learn, and\n",
      "discover new insights from data that is large in volume and variety. New\n",
      "developments in the areas of machine learning, deep learning, natural language\n",
      "processing (NLP), and, more generally, artificial intelligence (AI) are opening\n",
      "up new opportunities for testing theories and evaluating the impact of\n",
      "interventions and programs in a more dynamic and effective way. Applications\n",
      "using large volumes of structured and unstructured data are becoming common in\n",
      "government and industry, and increasingly also in social science research. This\n",
      "chapter offers an introduction to such methods drawing examples from political\n",
      "science. Focusing on the areas where the strengths of the methods coincide with\n",
      "challenges in these fields, the chapter first presents an introduction to AI\n",
      "and its core technology - machine learning, with its rapidly developing\n",
      "subfield of deep learning. The discussion of deep neural networks is\n",
      "illustrated with the NLP tasks that are relevant to political science. The\n",
      "latest advances in deep learning methods for NLP are also reviewed, together\n",
      "with their potential for improving information extraction and pattern\n",
      "recognition from political science texts.\n",
      "\n",
      "**Paper Id :1905.08883 \n",
      "Title :Explainable Machine Learning for Scientific Insights and Discoveries\n",
      "  Machine learning methods have been remarkably successful for a wide range of\n",
      "application areas in the extraction of essential information from data. An\n",
      "exciting and relatively recent development is the uptake of machine learning in\n",
      "the natural sciences, where the major goal is to obtain novel scientific\n",
      "insights and discoveries from observational or simulated data. A prerequisite\n",
      "for obtaining a scientific outcome is domain knowledge, which is needed to gain\n",
      "explainability, but also to enhance scientific consistency. In this article we\n",
      "review explainable machine learning in view of applications in the natural\n",
      "sciences and discuss three core elements which we identified as relevant in\n",
      "this context: transparency, interpretability, and explainability. With respect\n",
      "to these core elements, we provide a survey of recent scientific works that\n",
      "incorporate machine learning and the way that explainable machine learning is\n",
      "used in combination with domain knowledge from the application areas.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06587 \n",
      "Title :Entity-Enriched Neural Models for Clinical Question Answering\n",
      "  We explore state-of-the-art neural models for question answering on\n",
      "electronic medical records and improve their ability to generalize better on\n",
      "previously unseen (paraphrased) questions at test time. We enable this by\n",
      "learning to predict logical forms as an auxiliary task along with the main task\n",
      "of answer span detection. The predicted logical forms also serve as a rationale\n",
      "for the answer. Further, we also incorporate medical entity information in\n",
      "these models via the ERNIE architecture. We train our models on the large-scale\n",
      "emrQA dataset and observe that our multi-task entity-enriched models generalize\n",
      "to paraphrased questions ~5% better than the baseline BERT model.\n",
      "\n",
      "**Paper Id :1909.10572 \n",
      "Title :Hypernym Detection Using Strict Partial Order Networks\n",
      "  This paper introduces Strict Partial Order Networks (SPON), a novel neural\n",
      "network architecture designed to enforce asymmetry and transitive properties as\n",
      "soft constraints. We apply it to induce hypernymy relations by training with\n",
      "is-a pairs. We also present an augmented variant of SPON that can generalize\n",
      "type information learned for in-vocabulary terms to previously unseen ones. An\n",
      "extensive evaluation over eleven benchmarks across different tasks shows that\n",
      "SPON consistently either outperforms or attains the state of the art on all but\n",
      "one of these benchmarks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06599 \n",
      "Title :Phishing URL Detection Through Top-level Domain Analysis: A Descriptive\n",
      "  Approach\n",
      "  Phishing is considered to be one of the most prevalent cyber-attacks because\n",
      "of its immense flexibility and alarmingly high success rate. Even with adequate\n",
      "training and high situational awareness, it can still be hard for users to\n",
      "continually be aware of the URL of the website they are visiting. Traditional\n",
      "detection methods rely on blocklists and content analysis, both of which\n",
      "require time-consuming human verification. Thus, there have been attempts\n",
      "focusing on the predictive filtering of such URLs. This study aims to develop a\n",
      "machine-learning model to detect fraudulent URLs which can be used within the\n",
      "Splunk platform. Inspired from similar approaches in the literature, we trained\n",
      "the SVM and Random Forests algorithms using malicious and benign datasets found\n",
      "in the literature and one dataset that we created. We evaluated the algorithms'\n",
      "performance with precision and recall, reaching up to 85% precision and 87%\n",
      "recall in the case of Random Forests while SVM achieved up to 90% precision and\n",
      "88% recall using only descriptive features.\n",
      "\n",
      "**Paper Id :2006.03541 \n",
      "Title :Sentiment Analysis Based on Deep Learning: A Comparative Study\n",
      "  The study of public opinion can provide us with valuable information. The\n",
      "analysis of sentiment on social networks, such as Twitter or Facebook, has\n",
      "become a powerful means of learning about the users' opinions and has a wide\n",
      "range of applications. However, the efficiency and accuracy of sentiment\n",
      "analysis is being hindered by the challenges encountered in natural language\n",
      "processing (NLP). In recent years, it has been demonstrated that deep learning\n",
      "models are a promising solution to the challenges of NLP. This paper reviews\n",
      "the latest studies that have employed deep learning to solve sentiment analysis\n",
      "problems, such as sentiment polarity. Models using term frequency-inverse\n",
      "document frequency (TF-IDF) and word embedding have been applied to a series of\n",
      "datasets. Finally, a comparative study has been conducted on the experimental\n",
      "results obtained for the different models and input features\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06618 \n",
      "Title :Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective\n",
      "  Learning\n",
      "  Human society had a long history of suffering from cognitive biases leading\n",
      "to social prejudices and mass injustice. The prevalent existence of cognitive\n",
      "biases in large volumes of historical data can pose a threat of being\n",
      "manifested as unethical and seemingly inhuman predictions as outputs of AI\n",
      "systems trained on such data. To alleviate this problem, we propose a\n",
      "bias-aware multi-objective learning framework that given a set of identity\n",
      "attributes (e.g. gender, ethnicity etc.) and a subset of sensitive categories\n",
      "of the possible classes of prediction outputs, learns to reduce the frequency\n",
      "of predicting certain combinations of them, e.g. predicting stereotypes such as\n",
      "`most blacks use abusive language', or `fear is a virtue of women'. Our\n",
      "experiments conducted on an emotion prediction task with balanced class priors\n",
      "shows that a set of baseline bias-agnostic models exhibit cognitive biases with\n",
      "respect to gender, such as women are prone to be afraid whereas men are more\n",
      "prone to be angry. In contrast, our proposed bias-aware multi-objective\n",
      "learning methodology is shown to reduce such biases in the predictied emotions.\n",
      "\n",
      "**Paper Id :1905.08871 \n",
      "Title :Measuring the effects of confounders in medical supervised\n",
      "  classification problems: the Confounding Index (CI)\n",
      "  Over the years, there has been growing interest in using Machine Learning\n",
      "techniques for biomedical data processing. When tackling these tasks, one needs\n",
      "to bear in mind that biomedical data depends on a variety of characteristics,\n",
      "such as demographic aspects (age, gender, etc) or the acquisition technology,\n",
      "which might be unrelated with the target of the analysis. In supervised tasks,\n",
      "failing to match the ground truth targets with respect to such characteristics,\n",
      "called confounders, may lead to very misleading estimates of the predictive\n",
      "performance. Many strategies have been proposed to handle confounders, ranging\n",
      "from data selection, to normalization techniques, up to the use of training\n",
      "algorithm for learning with imbalanced data. However, all these solutions\n",
      "require the confounders to be known a priori. To this aim, we introduce a novel\n",
      "index that is able to measure the confounding effect of a data attribute in a\n",
      "bias-agnostic way. This index can be used to quantitatively compare the\n",
      "confounding effects of different variables and to inform correction methods\n",
      "such as normalization procedures or ad-hoc-prepared learning algorithms. The\n",
      "effectiveness of this index is validated on both simulated data and real-world\n",
      "neuroimaging data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06619 \n",
      "Title :India nudges to contain COVID-19 pandemic: a reactive public policy\n",
      "  analysis using machine-learning based topic modelling\n",
      "  India locked down 1.3 billion people on March 25, 2020 in the wake of\n",
      "COVID-19 pandemic. The economic cost of it was estimated at USD 98 billion,\n",
      "while the social costs are still unknown. This study investigated how\n",
      "government formed reactive policies to fight coronavirus across its policy\n",
      "sectors. Primary data was collected from the Press Information Bureau (PIB) in\n",
      "the form press releases of government plans, policies, programme initiatives\n",
      "and achievements. A text corpus of 260,852 words was created from 396 documents\n",
      "from the PIB. An unsupervised machine-based topic modelling using Latent\n",
      "Dirichlet Allocation (LDA) algorithm was performed on the text corpus. It was\n",
      "done to extract high probability topics in the policy sectors. The\n",
      "interpretation of the extracted topics was made through a nudge theoretic lens\n",
      "to derive the critical policy heuristics of the government. Results showed that\n",
      "most interventions were targeted to generate endogenous nudge by using external\n",
      "triggers. Notably, the nudges from the Prime Minister of India was critical in\n",
      "creating herd effect on lockdown and social distancing norms across the nation.\n",
      "A similar effect was also observed around the public health (e.g., masks in\n",
      "public spaces; Yoga and Ayurveda for immunity), transport (e.g., old trains\n",
      "converted to isolation wards), micro, small and medium enterprises (e.g., rapid\n",
      "production of PPE and masks), science and technology sector (e.g., diagnostic\n",
      "kits, robots and nano-technology), home affairs (e.g., surveillance and\n",
      "lockdown), urban (e.g. drones, GIS-tools) and education (e.g., online\n",
      "learning). A conclusion was drawn on leveraging these heuristics are crucial\n",
      "for lockdown easement planning.\n",
      "\n",
      "**Paper Id :1811.12166 \n",
      "Title :Prediction of ESG Compliance using a Heterogeneous Information Network\n",
      "  Negative screening is one method to avoid interactions with inappropriate\n",
      "entities. For example, financial institutions keep investment exclusion lists\n",
      "of inappropriate firms that have environmental, social, and government (ESG)\n",
      "problems. They create their investment exclusion lists by gathering information\n",
      "from various news sources to keep their portfolios profitable as well as green.\n",
      "International organizations also maintain smart sanctions lists that are used\n",
      "to prohibit trade with entities that are involved in illegal activities. In the\n",
      "present paper, we focus on the prediction of investment exclusion lists in the\n",
      "finance domain. We construct a vast heterogeneous information network that\n",
      "covers the necessary information surrounding each firm, which is assembled\n",
      "using seven professionally curated datasets and two open datasets, which\n",
      "results in approximately 50 million nodes and 400 million edges in total.\n",
      "Exploiting these vast datasets and motivated by how professional investigators\n",
      "and journalists undertake their daily investigations, we propose a model that\n",
      "can learn to predict firms that are more likely to be added to an investment\n",
      "exclusion list in the near future. Our approach is tested using the negative\n",
      "news investment exclusion list data of more than 35,000 firms worldwide from\n",
      "January 2012 to May 2018. Comparing with the state-of-the-art methods with and\n",
      "without using the network, we show that the predictive accuracy is\n",
      "substantially improved when using the vast information stored in the\n",
      "heterogeneous information network. This work suggests new ways to consolidate\n",
      "the diffuse information contained in big data to monitor dominant firms on a\n",
      "global scale for better risk management and more socially responsible\n",
      "investment.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.06835 \n",
      "Title :RegQCNET: Deep Quality Control for Image-to-template Brain MRI Affine\n",
      "  Registration\n",
      "  Affine registration of one or several brain image(s) onto a common reference\n",
      "space is a necessary prerequisite for many image processing tasks, such as\n",
      "brain segmentation or functional analysis. Manual assessment of registration\n",
      "quality is a tedious and time-consuming task, especially in studies comprising\n",
      "a large amount of data. An automated and reliable quality control (QC) becomes\n",
      "mandatory. Moreover, the computation time of the QC must be also compatible\n",
      "with the processing of massive datasets. Therefore, an automated deep neural\n",
      "network approaches appear as a method of choice to automatically assess\n",
      "registration quality.\n",
      "  In the current study, a compact 3D convolutional neural network (CNN),\n",
      "referred to as RegQCNET, is introduced to quantitatively predict the amplitude\n",
      "of an affine registration mismatch between a registered image and a reference\n",
      "template. This quantitative estimation of registration error is expressed using\n",
      "metric unit system. Therefore, a meaningful task-specific threshold can be\n",
      "manually or automatically defined in order to distinguish usable and non-usable\n",
      "images.\n",
      "  The robustness of the proposed RegQCNET is first analyzed on lifespan brain\n",
      "images undergoing various simulated spatial transformations and intensity\n",
      "variations between training and testing. Secondly, the potential of RegQCNET to\n",
      "classify images as usable or non-usable is evaluated using both manual and\n",
      "automatic thresholds. During our experiments, automatic thresholds are\n",
      "estimated using several computer-assisted classification models through\n",
      "cross-validation. To this end we used expert's visual quality control estimated\n",
      "on a lifespan cohort of 3953 brains. Finally, the RegQCNET accuracy is compared\n",
      "to usual image features.\n",
      "  Results show that the proposed deep learning QC is robust, fast and accurate\n",
      "to estimate affine registration error in processing pipeline.\n",
      "\n",
      "**Paper Id :2008.02952 \n",
      "Title :Few Shot Learning Framework to Reduce Inter-observer Variability in\n",
      "  Medical Images\n",
      "  Most computer aided pathology detection systems rely on large volumes of\n",
      "quality annotated data to aid diagnostics and follow up procedures. However,\n",
      "quality assuring large volumes of annotated medical image data can be\n",
      "subjective and expensive. In this work we present a novel standardization\n",
      "framework that implements three few-shot learning (FSL) models that can be\n",
      "iteratively trained by atmost 5 images per 3D stack to generate multiple\n",
      "regional proposals (RPs) per test image. These FSL models include a novel\n",
      "parallel echo state network (ParESN) framework and an augmented U-net model.\n",
      "Additionally, we propose a novel target label selection algorithm (TLSA) that\n",
      "measures relative agreeability between RPs and the manually annotated target\n",
      "labels to detect the \"best\" quality annotation per image. Using the FSL models,\n",
      "our system achieves 0.28-0.64 Dice coefficient across vendor image stacks for\n",
      "intra-retinal cyst segmentation. Additionally, the TLSA is capable of\n",
      "automatically classifying high quality target labels from their noisy\n",
      "counterparts for 60-97% of the images while ensuring manual supervision on\n",
      "remaining images. Also, the proposed framework with ParESN model minimizes\n",
      "manual annotation checking to 12-28% of the total number of images. The TLSA\n",
      "metrics further provide confidence scores for the automated annotation quality\n",
      "assurance. Thus, the proposed framework is flexible to extensions for quality\n",
      "image annotation curation of other image stacks as well.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.07688 \n",
      "Title :Keystroke Biometrics in Response to Fake News Propagation in a Global\n",
      "  Pandemic\n",
      "  This work proposes and analyzes the use of keystroke biometrics for content\n",
      "de-anonymization. Fake news have become a powerful tool to manipulate public\n",
      "opinion, especially during major events. In particular, the massive spread of\n",
      "fake news during the COVID-19 pandemic has forced governments and companies to\n",
      "fight against missinformation. In this context, the ability to link multiple\n",
      "accounts or profiles that spread such malicious content on the Internet while\n",
      "hiding in anonymity would enable proactive identification and blacklisting.\n",
      "Behavioral biometrics can be powerful tools in this fight. In this work, we\n",
      "have analyzed how the latest advances in keystroke biometric recognition can\n",
      "help to link behavioral typing patterns in experiments involving 100,000 users\n",
      "and more than 1 million typed sequences. Our proposed system is based on\n",
      "Recurrent Neural Networks adapted to the context of content de-anonymization.\n",
      "Assuming the challenge to link the typed content of a target user in a pool of\n",
      "candidate profiles, our results show that keystroke recognition can be used to\n",
      "reduce the list of candidate profiles by more than 90%. In addition, when\n",
      "keystroke is combined with auxiliary data (such as location), our system\n",
      "achieves a Rank-1 identification performance equal to 52.6% and 10.9% for a\n",
      "background candidate list composed of 1K and 100K profiles, respectively.\n",
      "\n",
      "**Paper Id :2005.06599 \n",
      "Title :Phishing URL Detection Through Top-level Domain Analysis: A Descriptive\n",
      "  Approach\n",
      "  Phishing is considered to be one of the most prevalent cyber-attacks because\n",
      "of its immense flexibility and alarmingly high success rate. Even with adequate\n",
      "training and high situational awareness, it can still be hard for users to\n",
      "continually be aware of the URL of the website they are visiting. Traditional\n",
      "detection methods rely on blocklists and content analysis, both of which\n",
      "require time-consuming human verification. Thus, there have been attempts\n",
      "focusing on the predictive filtering of such URLs. This study aims to develop a\n",
      "machine-learning model to detect fraudulent URLs which can be used within the\n",
      "Splunk platform. Inspired from similar approaches in the literature, we trained\n",
      "the SVM and Random Forests algorithms using malicious and benign datasets found\n",
      "in the literature and one dataset that we created. We evaluated the algorithms'\n",
      "performance with precision and recall, reaching up to 85% precision and 87%\n",
      "recall in the case of Random Forests while SVM achieved up to 90% precision and\n",
      "88% recall using only descriptive features.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.07694 \n",
      "Title :Constraining the Reionization History using Bayesian Normalizing Flows\n",
      "  The next generation 21 cm surveys open a new window onto the early stages of\n",
      "cosmic structure formation and provide new insights about the Epoch of\n",
      "Reionization (EoR). However, the non-Gaussian nature of the 21 cm signal along\n",
      "with the huge amount of data generated from these surveys will require more\n",
      "advanced techniques capable to efficiently extract the necessary information to\n",
      "constrain the Reionization History of the Universe. In this paper we present\n",
      "the use of Bayesian Neural Networks (BNNs) to predict the posterior\n",
      "distribution for four astrophysical and cosmological parameters. Besides\n",
      "achieving state-of-the-art prediction performances, the proposed methods\n",
      "provide accurate estimation of parameters uncertainties and infer correlations\n",
      "among them. Additionally, we demonstrate the advantages of Normalizing Flows\n",
      "(NF) combined with BNNs, being able to model more complex output distributions\n",
      "and thus capture key information as non-Gaussianities in the parameter\n",
      "conditional density distribution for astrophysical and cosmological dataset.\n",
      "Finally, we propose novel calibration methods employing Normalizing Flows after\n",
      "training, to produce reliable predictions, and we demonstrate the advantages of\n",
      "this approach both in terms of computational cost and prediction performances.\n",
      "\n",
      "**Paper Id :2002.10986 \n",
      "Title :A Deep Learning Framework for Simulation and Defect Prediction Applied\n",
      "  in Microelectronics\n",
      "  The prediction of upcoming events in industrial processes has been a\n",
      "long-standing research goal since it enables optimization of manufacturing\n",
      "parameters, planning of equipment maintenance and more importantly prediction\n",
      "and eventually prevention of defects. While existing approaches have\n",
      "accomplished substantial progress, they are mostly limited to processing of one\n",
      "dimensional signals or require parameter tuning to model environmental\n",
      "parameters. In this paper, we propose an alternative approach based on deep\n",
      "neural networks that simulates changes in the 3D structure of a monitored\n",
      "object in a batch based on previous 3D measurements. In particular, we propose\n",
      "an architecture based on 3D Convolutional Neural Networks (3DCNN) in order to\n",
      "model the geometric variations in manufacturing parameters and predict upcoming\n",
      "events related to sub-optimal performance. We validate our framework on a\n",
      "microelectronics use-case using the recently published PCB scans dataset where\n",
      "we simulate changes on the shape and volume of glue deposited on an Liquid\n",
      "Crystal Polymer (LCP) substrate before the attachment of integrated circuits\n",
      "(IC). Experimental evaluation examines the impact of different choices in the\n",
      "cost function during training and shows that the proposed method can be\n",
      "efficiently used for defect prediction.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.07882 \n",
      "Title :Curating a COVID-19 data repository and forecasting county-level death\n",
      "  counts in the United States\n",
      "  As the COVID-19 outbreak evolves, accurate forecasting continues to play an\n",
      "extremely important role in informing policy decisions. In this paper, we\n",
      "present our continuous curation of a large data repository containing COVID-19\n",
      "information from a range of sources. We use this data to develop predictions\n",
      "and corresponding prediction intervals for the short-term trajectory of\n",
      "COVID-19 cumulative death counts at the county-level in the United States up to\n",
      "two weeks ahead. Using data from January 22 to June 20, 2020, we develop and\n",
      "combine multiple forecasts using ensembling techniques, resulting in an\n",
      "ensemble we refer to as Combined Linear and Exponential Predictors (CLEP). Our\n",
      "individual predictors include county-specific exponential and linear\n",
      "predictors, a shared exponential predictor that pools data together across\n",
      "counties, an expanded shared exponential predictor that uses data from\n",
      "neighboring counties, and a demographics-based shared exponential predictor. We\n",
      "use prediction errors from the past five days to assess the uncertainty of our\n",
      "death predictions, resulting in generally-applicable prediction intervals,\n",
      "Maximum (absolute) Error Prediction Intervals (MEPI). MEPI achieves a coverage\n",
      "rate of more than 94% when averaged across counties for predicting cumulative\n",
      "recorded death counts two weeks in the future. Our forecasts are currently\n",
      "being used by the non-profit organization, Response4Life, to determine the\n",
      "medical supply need for individual hospitals and have directly contributed to\n",
      "the distribution of medical supplies across the country. We hope that our\n",
      "forecasts and data repository at https://covidseverity.com can help guide\n",
      "necessary county-specific decision-making and help counties prepare for their\n",
      "continued fight against COVID-19.\n",
      "\n",
      "**Paper Id :1811.04817 \n",
      "Title :A test case for application of convolutional neural networks to\n",
      "  spatio-temporal climate data: Re-identifying clustered weather patterns\n",
      "  Convolutional neural networks (CNNs) can potentially provide powerful tools\n",
      "for classifying and identifying patterns in climate and environmental data.\n",
      "However, because of the inherent complexities of such data, which are often\n",
      "spatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\n",
      "designed/evaluated for each specific dataset and application. Yet to start,\n",
      "CNN, a supervised technique, requires a large labeled dataset. Labeling demands\n",
      "(human) expert time, which combined with the limited number of relevant\n",
      "examples in this area, can discourage using CNNs for new problems. To address\n",
      "these challenges, here we (1) Propose an effective auto-labeling strategy based\n",
      "on using an unsupervised clustering algorithm and evaluating the performance of\n",
      "CNNs in re-identifying these clusters; (2) Use this approach to label thousands\n",
      "of daily large-scale weather patterns over North America in the outputs of a\n",
      "fully-coupled climate model and show the capabilities of CNNs in re-identifying\n",
      "the 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\n",
      "cluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\n",
      "nonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\n",
      "training samples per cluster. Effects of architecture and hyperparameters on\n",
      "the performance of CNNs are examined and discussed.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.07959 \n",
      "Title :Characteristic Functions on Graphs: Birds of a Feather, from Statistical\n",
      "  Descriptors to Parametric Models\n",
      "  In this paper, we propose a flexible notion of characteristic functions\n",
      "defined on graph vertices to describe the distribution of vertex features at\n",
      "multiple scales. We introduce FEATHER, a computationally efficient algorithm to\n",
      "calculate a specific variant of these characteristic functions where the\n",
      "probability weights of the characteristic function are defined as the\n",
      "transition probabilities of random walks. We argue that features extracted by\n",
      "this procedure are useful for node level machine learning tasks. We discuss the\n",
      "pooling of these node representations, resulting in compact descriptors of\n",
      "graphs that can serve as features for graph classification algorithms. We\n",
      "analytically prove that FEATHER describes isomorphic graphs with the same\n",
      "representation and exhibits robustness to data corruption. Using the node\n",
      "feature characteristic functions we define parametric models where evaluation\n",
      "points of the functions are learned parameters of supervised classifiers.\n",
      "Experiments on real world large datasets show that our proposed algorithm\n",
      "creates high quality representations, performs transfer learning efficiently,\n",
      "exhibits robustness to hyperparameter changes, and scales linearly with the\n",
      "input size.\n",
      "\n",
      "**Paper Id :2010.12455 \n",
      "Title :Primal-Dual Mesh Convolutional Neural Networks\n",
      "  Recent works in geometric deep learning have introduced neural networks that\n",
      "allow performing inference tasks on three-dimensional geometric data by\n",
      "defining convolution, and sometimes pooling, operations on triangle meshes.\n",
      "These methods, however, either consider the input mesh as a graph, and do not\n",
      "exploit specific geometric properties of meshes for feature aggregation and\n",
      "downsampling, or are specialized for meshes, but rely on a rigid definition of\n",
      "convolution that does not properly capture the local topology of the mesh. We\n",
      "propose a method that combines the advantages of both types of approaches,\n",
      "while addressing their limitations: we extend a primal-dual framework drawn\n",
      "from the graph-neural-network literature to triangle meshes, and define\n",
      "convolutions on two types of graphs constructed from an input mesh. Our method\n",
      "takes features for both edges and faces of a 3D mesh as input and dynamically\n",
      "aggregates them using an attention mechanism. At the same time, we introduce a\n",
      "pooling operation with a precise geometric interpretation, that allows handling\n",
      "variations in the mesh connectivity by clustering mesh faces in a task-driven\n",
      "fashion. We provide theoretical insights of our approach using tools from the\n",
      "mesh-simplification literature. In addition, we validate experimentally our\n",
      "method in the tasks of shape classification and shape segmentation, where we\n",
      "obtain comparable or superior performance to the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.08128 \n",
      "Title :Sparse Mixture of Local Experts for Efficient Speech Enhancement\n",
      "  In this paper, we investigate a deep learning approach for speech denoising\n",
      "through an efficient ensemble of specialist neural networks. By splitting up\n",
      "the speech denoising task into non-overlapping subproblems and introducing a\n",
      "classifier, we are able to improve denoising performance while also reducing\n",
      "computational complexity. More specifically, the proposed model incorporates a\n",
      "gating network which assigns noisy speech signals to an appropriate specialist\n",
      "network based on either speech degradation level or speaker gender. In our\n",
      "experiments, a baseline recurrent network is compared against an ensemble of\n",
      "similarly-designed smaller recurrent networks regulated by the auxiliary gating\n",
      "network. Using stochastically generated batches from a large noisy speech\n",
      "corpus, the proposed model learns to estimate a time-frequency masking matrix\n",
      "based on the magnitude spectrogram of an input mixture signal. Both baseline\n",
      "and specialist networks are trained to estimate the ideal ratio mask, while the\n",
      "gating network is trained to perform subproblem classification. Our findings\n",
      "demonstrate that a fine-tuned ensemble network is able to exceed the speech\n",
      "denoising capabilities of a generalist network, doing so with fewer model\n",
      "parameters.\n",
      "\n",
      "**Paper Id :2011.02195 \n",
      "Title :Correlation based Multi-phasal models for improved imagined speech EEG\n",
      "  recognition\n",
      "  Translation of imagined speech electroencephalogram(EEG) into human\n",
      "understandable commands greatly facilitates the design of naturalistic brain\n",
      "computer interfaces. To achieve improved imagined speech unit classification,\n",
      "this work aims to profit from the parallel information contained in\n",
      "multi-phasal EEG data recorded while speaking, imagining and performing\n",
      "articulatory movements corresponding to specific speech units. A bi-phase\n",
      "common representation learning module using neural networks is designed to\n",
      "model the correlation and reproducibility between an analysis phase and a\n",
      "support phase. The trained Correlation Network is then employed to extract\n",
      "discriminative features of the analysis phase. These features are further\n",
      "classified into five binary phonological categories using machine learning\n",
      "models such as Gaussian mixture based hidden Markov model and deep neural\n",
      "networks. The proposed approach further handles the non-availability of\n",
      "multi-phasal data during decoding. Topographic visualizations along with\n",
      "result-based inferences suggest that the multi-phasal correlation modelling\n",
      "approach proposed in the paper enhances imagined-speech EEG recognition\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.08598 \n",
      "Title :Sequential Recommender via Time-aware Attentive Memory Network\n",
      "  Recommendation systems aim to assist users to discover most preferred\n",
      "contents from an ever-growing corpus of items. Although recommenders have been\n",
      "greatly improved by deep learning, they still faces several challenges: (1)\n",
      "Behaviors are much more complex than words in sentences, so traditional\n",
      "attentive and recurrent models may fail in capturing the temporal dynamics of\n",
      "user preferences. (2) The preferences of users are multiple and evolving, so it\n",
      "is difficult to integrate long-term memory and short-term intent.\n",
      "  In this paper, we propose a temporal gating methodology to improve attention\n",
      "mechanism and recurrent units, so that temporal information can be considered\n",
      "in both information filtering and state transition. Additionally, we propose a\n",
      "Multi-hop Time-aware Attentive Memory network (MTAM) to integrate long-term and\n",
      "short-term preferences. We use the proposed time-aware GRU network to learn the\n",
      "short-term intent and maintain prior records in user memory. We treat the\n",
      "short-term intent as a query and design a multi-hop memory reading operation\n",
      "via the proposed time-aware attention to generate user representation based on\n",
      "the current intent and long-term memory. Our approach is scalable for candidate\n",
      "retrieval tasks and can be viewed as a non-linear generalization of latent\n",
      "factorization for dot-product based Top-K recommendation. Finally, we conduct\n",
      "extensive experiments on six benchmark datasets and the experimental results\n",
      "demonstrate the effectiveness of our MTAM and temporal gating methodology.\n",
      "\n",
      "**Paper Id :2005.01690 \n",
      "Title :Learning Geo-Contextual Embeddings for Commuting Flow Prediction\n",
      "  Predicting commuting flows based on infrastructure and land-use information\n",
      "is critical for urban planning and public policy development. However, it is a\n",
      "challenging task given the complex patterns of commuting flows. Conventional\n",
      "models, such as gravity model, are mainly derived from physics principles and\n",
      "limited by their predictive power in real-world scenarios where many factors\n",
      "need to be considered. Meanwhile, most existing machine learning-based methods\n",
      "ignore the spatial correlations and fail to model the influence of nearby\n",
      "regions. To address these issues, we propose Geo-contextual Multitask Embedding\n",
      "Learner (GMEL), a model that captures the spatial correlations from geographic\n",
      "contextual information for commuting flow prediction. Specifically, we first\n",
      "construct a geo-adjacency network containing the geographic contextual\n",
      "information. Then, an attention mechanism is proposed based on the framework of\n",
      "graph attention network (GAT) to capture the spatial correlations and encode\n",
      "geographic contextual information to embedding space. Two separate GATs are\n",
      "used to model supply and demand characteristics. A multitask learning framework\n",
      "is used to introduce stronger restrictions and enhance the effectiveness of the\n",
      "embedding representation. Finally, a gradient boosting machine is trained based\n",
      "on the learned embeddings to predict commuting flows. We evaluate our model\n",
      "using real-world datasets from New York City and the experimental results\n",
      "demonstrate the effectiveness of our proposal against the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.09059 \n",
      "Title :Basal Glucose Control in Type 1 Diabetes using Deep Reinforcement\n",
      "  Learning: An In Silico Validation\n",
      "  People with Type 1 diabetes (T1D) require regular exogenous infusion of\n",
      "insulin to maintain their blood glucose concentration in a therapeutically\n",
      "adequate target range. Although the artificial pancreas and continuous glucose\n",
      "monitoring have been proven to be effective in achieving closed-loop control,\n",
      "significant challenges still remain due to the high complexity of glucose\n",
      "dynamics and limitations in the technology. In this work, we propose a novel\n",
      "deep reinforcement learning model for single-hormone (insulin) and dual-hormone\n",
      "(insulin and glucagon) delivery. In particular, the delivery strategies are\n",
      "developed by double Q-learning with dilated recurrent neural networks. For\n",
      "designing and testing purposes, the FDA-accepted UVA/Padova Type 1 simulator\n",
      "was employed. First, we performed long-term generalized training to obtain a\n",
      "population model. Then, this model was personalized with a small data-set of\n",
      "subject-specific data. In silico results show that the single and dual-hormone\n",
      "delivery strategies achieve good glucose control when compared to a standard\n",
      "basal-bolus therapy with low-glucose insulin suspension. Specifically, in the\n",
      "adult cohort (n=10), percentage time in target range [70, 180] mg/dL improved\n",
      "from 77.6% to 80.9% with single-hormone control, and to $85.6\\%$ with\n",
      "dual-hormone control. In the adolescent cohort (n=10), percentage time in\n",
      "target range improved from 55.5% to 65.9% with single-hormone control, and to\n",
      "78.8% with dual-hormone control. In all scenarios, a significant decrease in\n",
      "hypoglycemia was observed. These results show that the use of deep\n",
      "reinforcement learning is a viable approach for closed-loop glucose control in\n",
      "T1D.\n",
      "\n",
      "**Paper Id :2002.05487 \n",
      "Title :End-to-end semantic segmentation of personalized deep brain structures\n",
      "  for non-invasive brain stimulation\n",
      "  Electro-stimulation or modulation of deep brain regions is commonly used in\n",
      "clinical procedures for the treatment of several nervous system disorders. In\n",
      "particular, transcranial direct current stimulation (tDCS) is widely used as an\n",
      "affordable clinical application that is applied through electrodes attached to\n",
      "the scalp. However, it is difficult to determine the amount and distribution of\n",
      "the electric field (EF) in the different brain regions due to anatomical\n",
      "complexity and high inter-subject variability. Personalized tDCS is an emerging\n",
      "clinical procedure that is used to tolerate electrode montage for accurate\n",
      "targeting. This procedure is guided by computational head models generated from\n",
      "anatomical images such as MRI. Distribution of the EF in segmented head models\n",
      "can be calculated through simulation studies. Therefore, fast, accurate, and\n",
      "feasible segmentation of different brain structures would lead to a better\n",
      "adjustment for customized tDCS studies. In this study, a single-encoder\n",
      "multi-decoders convolutional neural network is proposed for deep brain\n",
      "segmentation. The proposed architecture is trained to segment seven deep brain\n",
      "structures using T1-weighted MRI. Network generated models are compared with a\n",
      "reference model constructed using a semi-automatic method, and it presents a\n",
      "high matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate\n",
      "(DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution\n",
      "during tDCS in generated and reference models matched well each other,\n",
      "suggesting its potential usefulness in clinical practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.09787 \n",
      "Title :Self-Updating Models with Error Remediation\n",
      "  Many environments currently employ machine learning models for data\n",
      "processing and analytics that were built using a limited number of training\n",
      "data points. Once deployed, the models are exposed to significant amounts of\n",
      "previously-unseen data, not all of which is representative of the original,\n",
      "limited training data. However, updating these deployed models can be difficult\n",
      "due to logistical, bandwidth, time, hardware, and/or data sensitivity\n",
      "constraints. We propose a framework, Self-Updating Models with Error\n",
      "Remediation (SUMER), in which a deployed model updates itself as new data\n",
      "becomes available. SUMER uses techniques from semi-supervised learning and\n",
      "noise remediation to iteratively retrain a deployed model using\n",
      "intelligently-chosen predictions from the model as the labels for new training\n",
      "iterations. A key component of SUMER is the notion of error remediation as\n",
      "self-labeled data can be susceptible to the propagation of errors. We\n",
      "investigate the use of SUMER across various data sets and iterations. We find\n",
      "that self-updating models (SUMs) generally perform better than models that do\n",
      "not attempt to self-update when presented with additional previously-unseen\n",
      "data. This performance gap is accentuated in cases where there is only limited\n",
      "amounts of initial training data. We also find that the performance of SUMER is\n",
      "generally better than the performance of SUMs, demonstrating a benefit in\n",
      "applying error remediation. Consequently, SUMER can autonomously enhance the\n",
      "operational capabilities of existing data processing systems by intelligently\n",
      "updating models in dynamic environments.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.09992 \n",
      "Title :Beyond the storage capacity: data driven satisfiability transition\n",
      "  Data structure has a dramatic impact on the properties of neural networks,\n",
      "yet its significance in the established theoretical frameworks is poorly\n",
      "understood. Here we compute the Vapnik-Chervonenkis entropy of a kernel machine\n",
      "operating on data grouped into equally labelled subsets. At variance with the\n",
      "unstructured scenario, entropy is non-monotonic in the size of the training\n",
      "set, and displays an additional critical point besides the storage capacity.\n",
      "Remarkably, the same behavior occurs in margin classifiers even with randomly\n",
      "labelled data, as is elucidated by identifying the synaptic volume encoding the\n",
      "transition. These findings reveal aspects of expressivity lying beyond the\n",
      "condensed description provided by the storage capacity, and they indicate the\n",
      "path towards more realistic bounds for the generalization error of neural\n",
      "networks.\n",
      "\n",
      "**Paper Id :1905.04305 \n",
      "Title :Spectral Reconstruction with Deep Neural Networks\n",
      "  We explore artificial neural networks as a tool for the reconstruction of\n",
      "spectral functions from imaginary time Green's functions, a classic\n",
      "ill-conditioned inverse problem. Our ansatz is based on a supervised learning\n",
      "framework in which prior knowledge is encoded in the training data and the\n",
      "inverse transformation manifold is explicitly parametrised through a neural\n",
      "network. We systematically investigate this novel reconstruction approach,\n",
      "providing a detailed analysis of its performance on physically motivated mock\n",
      "data, and compare it to established methods of Bayesian inference. The\n",
      "reconstruction accuracy is found to be at least comparable, and potentially\n",
      "superior in particular at larger noise levels. We argue that the use of\n",
      "labelled training data in a supervised setting and the freedom in defining an\n",
      "optimisation objective are inherent advantages of the present approach and may\n",
      "lead to significant improvements over state-of-the-art methods in the future.\n",
      "Potential directions for further research are discussed in detail.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.10374 \n",
      "Title :Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric\n",
      "  Fields with a Generative Adversarial Network\n",
      "  Generative adversarial networks (GANs) have been recently adopted for\n",
      "super-resolution, an application closely related to what is referred to as\n",
      "\"downscaling\" in the atmospheric sciences: improving the spatial resolution of\n",
      "low-resolution images. The ability of conditional GANs to generate an ensemble\n",
      "of solutions for a given input lends itself naturally to stochastic\n",
      "downscaling, but the stochastic nature of GANs is not usually considered in\n",
      "super-resolution applications. Here, we introduce a recurrent, stochastic\n",
      "super-resolution GAN that can generate ensembles of time-evolving\n",
      "high-resolution atmospheric fields for an input consisting of a low-resolution\n",
      "sequence of images of the same field. We test the GAN using two datasets, one\n",
      "consisting of radar-measured precipitation from Switzerland, the other of cloud\n",
      "optical thickness derived from the Geostationary Earth Observing Satellite 16\n",
      "(GOES-16). We find that the GAN can generate realistic, temporally consistent\n",
      "super-resolution sequences for both datasets. The statistical properties of the\n",
      "generated ensemble are analyzed using rank statistics, a method adapted from\n",
      "ensemble weather forecasting; these analyses indicate that the GAN produces\n",
      "close to the correct amount of variability in its outputs. As the GAN generator\n",
      "is fully convolutional, it can be applied after training to input images larger\n",
      "than the images used to train it. It is also able to generate time series much\n",
      "longer than the training sequences, as demonstrated by applying the generator\n",
      "to a three-month dataset of the precipitation radar data. The source code to\n",
      "our GAN is available at https://github.com/jleinonen/downscaling-rnn-gan.\n",
      "\n",
      "**Paper Id :1811.04817 \n",
      "Title :A test case for application of convolutional neural networks to\n",
      "  spatio-temporal climate data: Re-identifying clustered weather patterns\n",
      "  Convolutional neural networks (CNNs) can potentially provide powerful tools\n",
      "for classifying and identifying patterns in climate and environmental data.\n",
      "However, because of the inherent complexities of such data, which are often\n",
      "spatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\n",
      "designed/evaluated for each specific dataset and application. Yet to start,\n",
      "CNN, a supervised technique, requires a large labeled dataset. Labeling demands\n",
      "(human) expert time, which combined with the limited number of relevant\n",
      "examples in this area, can discourage using CNNs for new problems. To address\n",
      "these challenges, here we (1) Propose an effective auto-labeling strategy based\n",
      "on using an unsupervised clustering algorithm and evaluating the performance of\n",
      "CNNs in re-identifying these clusters; (2) Use this approach to label thousands\n",
      "of daily large-scale weather patterns over North America in the outputs of a\n",
      "fully-coupled climate model and show the capabilities of CNNs in re-identifying\n",
      "the 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\n",
      "cluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\n",
      "nonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\n",
      "training samples per cluster. Effects of architecture and hyperparameters on\n",
      "the performance of CNNs are examined and discussed.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.10416 \n",
      "Title :Automated Question Answer medical model based on Deep Learning\n",
      "  Technology\n",
      "  Artificial intelligence can now provide more solutions for different\n",
      "problems, especially in the medical field. One of those problems the lack of\n",
      "answers to any given medical/health-related question. The Internet is full of\n",
      "forums that allow people to ask some specific questions and get great answers\n",
      "for them. Nevertheless, browsing these questions in order to locate one similar\n",
      "to your own, also finding a satisfactory answer is a difficult and\n",
      "time-consuming task. This research will introduce a solution to this problem by\n",
      "automating the process of generating qualified answers to these questions and\n",
      "creating a kind of digital doctor. Furthermore, this research will train an\n",
      "end-to-end model using the framework of RNN and the encoder-decoder to generate\n",
      "sensible and useful answers to a small set of medical/health issues. The\n",
      "proposed model was trained and evaluated using data from various online\n",
      "services, such as WebMD, HealthTap, eHealthForums, and iCliniq.\n",
      "\n",
      "**Paper Id :1904.06517 \n",
      "Title :Improving detection of protein-ligand binding sites with 3D segmentation\n",
      "  In recent years machine learning (ML) took bio- and cheminformatics fields by\n",
      "storm, providing new solutions for a vast repertoire of problems related to\n",
      "protein sequence, structure, and interactions analysis. ML techniques, deep\n",
      "neural networks especially, were proven more effective than classical models\n",
      "for tasks like predicting binding affinity for molecular complex. In this work\n",
      "we investigated the earlier stage of drug discovery process - finding druggable\n",
      "pockets on protein surface, that can be later used to design active molecules.\n",
      "For this purpose we developed a 3D fully convolutional neural network capable\n",
      "of binding site segmentation. Our solution has high prediction accuracy and\n",
      "provides intuitive representations of the results, which makes it easy to\n",
      "incorporate into drug discovery projects. The model's source code, together\n",
      "with scripts for most common use-cases is freely available at\n",
      "http://gitlab.com/cheminfIBB/kalasanty\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.10477 \n",
      "Title :Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and\n",
      "  Self-Control Gradient Estimator\n",
      "  Semantic hashing has become a crucial component of fast similarity search in\n",
      "many large-scale information retrieval systems, in particular, for text data.\n",
      "Variational auto-encoders (VAEs) with binary latent variables as hashing codes\n",
      "provide state-of-the-art performance in terms of precision for document\n",
      "retrieval. We propose a pairwise loss function with discrete latent VAE to\n",
      "reward within-class similarity and between-class dissimilarity for supervised\n",
      "hashing. Instead of solving the optimization relying on existing biased\n",
      "gradient estimators, an unbiased low-variance gradient estimator is adopted to\n",
      "optimize the hashing function by evaluating the non-differentiable loss\n",
      "function over two correlated sets of binary hashing codes to control the\n",
      "variance of gradient estimates. This new semantic hashing framework achieves\n",
      "superior performance compared to the state-of-the-arts, as demonstrated by our\n",
      "comprehensive experiments.\n",
      "\n",
      "**Paper Id :2002.06239 \n",
      "Title :Boosted Locality Sensitive Hashing: Discriminative Binary Codes for\n",
      "  Source Separation\n",
      "  Speech enhancement tasks have seen significant improvements with the advance\n",
      "of deep learning technology, but with the cost of increased computational\n",
      "complexity. In this study, we propose an adaptive boosting approach to learning\n",
      "locality sensitive hash codes, which represent audio spectra efficiently. We\n",
      "use the learned hash codes for single-channel speech denoising tasks as an\n",
      "alternative to a complex machine learning model, particularly to address the\n",
      "resource-constrained environments. Our adaptive boosting algorithm learns\n",
      "simple logistic regressors as the weak learners. Once trained, their binary\n",
      "classification results transform each spectrum of test noisy speech into a bit\n",
      "string. Simple bitwise operations calculate Hamming distance to find the\n",
      "K-nearest matching frames in the dictionary of training noisy speech spectra,\n",
      "whose associated ideal binary masks are averaged to estimate the denoising mask\n",
      "for that test mixture. Our proposed learning algorithm differs from AdaBoost in\n",
      "the sense that the projections are trained to minimize the distances between\n",
      "the self-similarity matrix of the hash codes and that of the original spectra,\n",
      "rather than the misclassification rate. We evaluate our discriminative hash\n",
      "codes on the TIMIT corpus with various noise types, and show comparative\n",
      "performance to deep learning methods in terms of denoising performance and\n",
      "complexity.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.10716 \n",
      "Title :Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for\n",
      "  Automatic Dialog Evaluation\n",
      "  Open Domain dialog system evaluation is one of the most important challenges\n",
      "in dialog research. Existing automatic evaluation metrics, such as BLEU are\n",
      "mostly reference-based. They calculate the difference between the generated\n",
      "response and a limited number of available references. Likert-score based\n",
      "self-reported user rating is widely adopted by social conversational systems,\n",
      "such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\n",
      "from bias and variance among different users. To alleviate this problem, we\n",
      "formulate dialog evaluation as a comparison task. We also propose an automatic\n",
      "evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\n",
      "automatically cleans self-reported user ratings as it trains on them.\n",
      "Specifically, we first use a self-supervised method to learn better dialog\n",
      "feature representation, and then use KNN and Shapley to remove confusing\n",
      "samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\n",
      "comparison task.\n",
      "\n",
      "**Paper Id :2009.12735 \n",
      "Title :Modeling Topical Relevance for Multi-Turn Dialogue Generation\n",
      "  Topic drift is a common phenomenon in multi-turn dialogue. Therefore, an\n",
      "ideal dialogue generation models should be able to capture the topic\n",
      "information of each context, detect the relevant context, and produce\n",
      "appropriate responses accordingly. However, existing models usually use word or\n",
      "sentence level similarities to detect the relevant contexts, which fail to well\n",
      "capture the topical level relevance. In this paper, we propose a new model,\n",
      "named STAR-BTM, to tackle this problem. Firstly, the Biterm Topic Model is\n",
      "pre-trained on the whole training dataset. Then, the topic level attention\n",
      "weights are computed based on the topic representation of each context.\n",
      "Finally, the attention weights and the topic distribution are utilized in the\n",
      "decoding process to generate the corresponding responses. Experimental results\n",
      "on both Chinese customer services data and English Ubuntu dialogue data show\n",
      "that STAR-BTM significantly outperforms several state-of-the-art methods, in\n",
      "terms of both metric-based and human evaluations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "**Paper Id :1905.13402 \n",
      "Title :Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep\n",
      "  Model-Based RL for Sparse Cost Robotic Tasks\n",
      "  Reinforcement learning (RL) for robotics is challenging due to the difficulty\n",
      "in hand-engineering a dense cost function, which can lead to unintended\n",
      "behavior, and dynamical uncertainty, which makes exploration and constraint\n",
      "satisfaction challenging. We address these issues with a new model-based\n",
      "reinforcement learning algorithm, Safety Augmented Value Estimation from\n",
      "Demonstrations (SAVED), which uses supervision that only identifies task\n",
      "completion and a modest set of suboptimal demonstrations to constrain\n",
      "exploration and learn efficiently while handling complex constraints. We then\n",
      "compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms\n",
      "on 6 standard simulation benchmarks involving navigation and manipulation and a\n",
      "physical knot-tying task on the da Vinci surgical robot. Results suggest that\n",
      "SAVED outperforms prior methods in terms of success rate, constraint\n",
      "satisfaction, and sample efficiency, making it feasible to safely learn a\n",
      "control policy directly on a real robot in less than an hour. For tasks on the\n",
      "robot, baselines succeed less than 5% of the time while SAVED has a success\n",
      "rate of over 75% in the first 50 training iterations. Code and supplementary\n",
      "material is available at https://tinyurl.com/saved-rl.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.11014 \n",
      "Title :Intent Mining from past conversations for Conversational Agent\n",
      "  Conversational systems are of primary interest in the AI community. Chatbots\n",
      "are increasingly being deployed to provide round-the-clock support and to\n",
      "increase customer engagement. Many of the commercial bot building frameworks\n",
      "follow a standard approach that requires one to build and train an intent model\n",
      "to recognize a user input. Intent models are trained in a supervised setting\n",
      "with a collection of textual utterance and intent label pairs. Gathering a\n",
      "substantial and wide coverage of training data for different intent is a\n",
      "bottleneck in the bot building process. Moreover, the cost of labeling a\n",
      "hundred to thousands of conversations with intent is a time consuming and\n",
      "laborious job. In this paper, we present an intent discovery framework that\n",
      "involves 4 primary steps: Extraction of textual utterances from a conversation\n",
      "using a pre-trained domain agnostic Dialog Act Classifier (Data Extraction),\n",
      "automatic clustering of similar user utterances (Clustering), manual annotation\n",
      "of clusters with an intent label (Labeling) and propagation of intent labels to\n",
      "the utterances from the previous step, which are not mapped to any cluster\n",
      "(Label Propagation); to generate intent training data from raw conversations.\n",
      "We have introduced a novel density-based clustering algorithm ITER-DBSCAN for\n",
      "unbalanced data clustering. Subject Matter Expert (Annotators with domain\n",
      "expertise) manually looks into the clustered user utterances and provides an\n",
      "intent label for discovery. We conducted user studies to validate the\n",
      "effectiveness of the trained intent model generated in terms of coverage of\n",
      "intents, accuracy and time saving concerning manual annotation. Although the\n",
      "system is developed for building an intent model for the conversational system,\n",
      "this framework can also be used for a short text clustering or as a labeling\n",
      "framework.\n",
      "\n",
      "**Paper Id :2009.10259 \n",
      "Title :ALICE: Active Learning with Contrastive Natural Language Explanations\n",
      "  Training a supervised neural network classifier typically requires many\n",
      "annotated training samples. Collecting and annotating a large number of data\n",
      "points are costly and sometimes even infeasible. Traditional annotation process\n",
      "uses a low-bandwidth human-machine communication interface: classification\n",
      "labels, each of which only provides several bits of information. We propose\n",
      "Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\n",
      "training framework that utilizes contrastive natural language explanations to\n",
      "improve data efficiency in learning. ALICE learns to first use active learning\n",
      "to select the most informative pairs of label classes to elicit contrastive\n",
      "natural language explanations from experts. Then it extracts knowledge from\n",
      "these explanations using a semantic parser. Finally, it incorporates the\n",
      "extracted knowledge through dynamically changing the learning model's\n",
      "structure. We applied ALICE in two visual recognition tasks, bird species\n",
      "classification and social relationship classification. We found by\n",
      "incorporating contrastive explanations, our models outperform baseline models\n",
      "that are trained with 40-100% more training data. We found that adding 1\n",
      "explanation leads to similar performance gain as adding 13-30 labeled training\n",
      "data points.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.11021 \n",
      "Title :Classification and Clustering of arXiv Documents, Sections, and\n",
      "  Abstracts, Comparing Encodings of Natural and Mathematical Language\n",
      "  In this paper, we show how selecting and combining encodings of natural and\n",
      "mathematical language affect classification and clustering of documents with\n",
      "mathematical content. We demonstrate this by using sets of documents, sections,\n",
      "and abstracts from the arXiv preprint server that are labeled by their subject\n",
      "class (mathematics, computer science, physics, etc.) to compare different\n",
      "encodings of text and formulae and evaluate the performance and runtimes of\n",
      "selected classification and clustering algorithms. Our encodings achieve\n",
      "classification accuracies up to $82.8\\%$ and cluster purities up to $69.4\\%$\n",
      "(number of clusters equals number of classes), and $99.9\\%$ (unspecified number\n",
      "of clusters) respectively. We observe a relatively low correlation between text\n",
      "and math similarity, which indicates the independence of text and formulae and\n",
      "motivates treating them as separate features of a document. The classification\n",
      "and clustering can be employed, e.g., for document search and recommendation.\n",
      "Furthermore, we show that the computer outperforms a human expert when\n",
      "classifying documents. Finally, we evaluate and discuss multi-label\n",
      "classification and formula semantification.\n",
      "\n",
      "**Paper Id :1811.04817 \n",
      "Title :A test case for application of convolutional neural networks to\n",
      "  spatio-temporal climate data: Re-identifying clustered weather patterns\n",
      "  Convolutional neural networks (CNNs) can potentially provide powerful tools\n",
      "for classifying and identifying patterns in climate and environmental data.\n",
      "However, because of the inherent complexities of such data, which are often\n",
      "spatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\n",
      "designed/evaluated for each specific dataset and application. Yet to start,\n",
      "CNN, a supervised technique, requires a large labeled dataset. Labeling demands\n",
      "(human) expert time, which combined with the limited number of relevant\n",
      "examples in this area, can discourage using CNNs for new problems. To address\n",
      "these challenges, here we (1) Propose an effective auto-labeling strategy based\n",
      "on using an unsupervised clustering algorithm and evaluating the performance of\n",
      "CNNs in re-identifying these clusters; (2) Use this approach to label thousands\n",
      "of daily large-scale weather patterns over North America in the outputs of a\n",
      "fully-coupled climate model and show the capabilities of CNNs in re-identifying\n",
      "the 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\n",
      "cluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\n",
      "nonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\n",
      "training samples per cluster. Effects of architecture and hyperparameters on\n",
      "the performance of CNNs are examined and discussed.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.11082 \n",
      "Title :Tractometry-based Anomaly Detection for Single-subject White Matter\n",
      "  Analysis\n",
      "  There is an urgent need for a paradigm shift from group-wise comparisons to\n",
      "individual diagnosis in diffusion MRI (dMRI) to enable the analysis of rare\n",
      "cases and clinically-heterogeneous groups. Deep autoencoders have shown great\n",
      "potential to detect anomalies in neuroimaging data. We present a framework that\n",
      "operates on the manifold of white matter (WM) pathways to learn normative\n",
      "microstructural features, and discriminate those at genetic risk from controls\n",
      "in a paediatric population.\n",
      "\n",
      "**Paper Id :2006.15969 \n",
      "Title :Interpretation of 3D CNNs for Brain MRI Data Classification\n",
      "  Deep learning shows high potential for many medical image analysis tasks.\n",
      "Neural networks can work with full-size data without extensive preprocessing\n",
      "and feature generation and, thus, information loss. Recent work has shown that\n",
      "the morphological difference in specific brain regions can be found on MRI with\n",
      "the means of Convolution Neural Networks (CNN). However, interpretation of the\n",
      "existing models is based on a region of interest and can not be extended to\n",
      "voxel-wise image interpretation on a whole image. In the current work, we\n",
      "consider the classification task on a large-scale open-source dataset of young\n",
      "healthy subjects -- an exploration of brain differences between men and women.\n",
      "In this paper, we extend the previous findings in gender differences from\n",
      "diffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D\n",
      "CNN interpretation comparing the results of three interpretation methods:\n",
      "Meaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute\n",
      "with the open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.11251 \n",
      "Title :A machine learning based software pipeline to pick the variable ordering\n",
      "  for algorithms with polynomial inputs\n",
      "  We are interested in the application of Machine Learning (ML) technology to\n",
      "improve mathematical software. It may seem that the probabilistic nature of ML\n",
      "tools would invalidate the exact results prized by such software, however, the\n",
      "algorithms which underpin the software often come with a range of choices which\n",
      "are good candidates for ML application. We refer to choices which have no\n",
      "effect on the mathematical correctness of the software, but do impact its\n",
      "performance.\n",
      "  In the past we experimented with one such choice: the variable ordering to\n",
      "use when building a Cylindrical Algebraic Decomposition (CAD). We used the\n",
      "Python library Scikit-Learn (sklearn) to experiment with different ML models,\n",
      "and developed new techniques for feature generation and hyper-parameter\n",
      "selection.\n",
      "  These techniques could easily be adapted for making decisions other than our\n",
      "immediate application of CAD variable ordering. Hence in this paper we present\n",
      "a software pipeline to use sklearn to pick the variable ordering for an\n",
      "algorithm that acts on a polynomial system. The code described is freely\n",
      "available online.\n",
      "\n",
      "**Paper Id :1911.12672 \n",
      "Title :Improved cross-validation for classifiers that make algorithmic choices\n",
      "  to minimise runtime without compromising output correctness\n",
      "  Our topic is the use of machine learning to improve software by making\n",
      "choices which do not compromise the correctness of the output, but do affect\n",
      "the time taken to produce such output. We are particularly concerned with\n",
      "computer algebra systems (CASs), and in particular, our experiments are for\n",
      "selecting the variable ordering to use when performing a cylindrical algebraic\n",
      "decomposition of $n$-dimensional real space with respect to the signs of a set\n",
      "of polynomials.\n",
      "  In our prior work we explored the different ML models that could be used, and\n",
      "how to identify suitable features of the input polynomials. In the present\n",
      "paper we both repeat our prior experiments on problems which have more\n",
      "variables (and thus exponentially more possible orderings), and examine the\n",
      "metric which our ML classifiers targets. The natural metric is computational\n",
      "runtime, with classifiers trained to pick the ordering which minimises this.\n",
      "However, this leads to the situation were models do not distinguish between any\n",
      "of the non-optimal orderings, whose runtimes may still vary dramatically. In\n",
      "this paper we investigate a modification to the cross-validation algorithms of\n",
      "the classifiers so that they do distinguish these cases, leading to improved\n",
      "results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.11619 \n",
      "Title :Bayesian Neural Networks at Scale: A Performance Analysis and Pruning\n",
      "  Study\n",
      "  Bayesian neural Networks (BNNs) are a promising method of obtaining\n",
      "statistical uncertainties for neural network predictions but with a higher\n",
      "computational overhead which can limit their practical usage. This work\n",
      "explores the use of high performance computing with distributed training to\n",
      "address the challenges of training BNNs at scale. We present a performance and\n",
      "scalability comparison of training the VGG-16 and Resnet-18 models on a\n",
      "Cray-XC40 cluster. We demonstrate that network pruning can speed up inference\n",
      "without accuracy loss and provide an open source software package,\n",
      "{\\it{BPrune}} to automate this pruning. For certain models we find that pruning\n",
      "up to 80\\% of the network results in only a 7.0\\% loss in accuracy. With the\n",
      "development of new hardware accelerators for Deep Learning, BNNs are of\n",
      "considerable interest for benchmarking performance. This analysis of training a\n",
      "BNN at scale outlines the limitations and benefits compared to a conventional\n",
      "neural network.\n",
      "\n",
      "**Paper Id :1906.01493 \n",
      "Title :Constructing Energy-efficient Mixed-precision Neural Networks through\n",
      "  Principal Component Analysis for Edge Intelligence\n",
      "  The `Internet of Things' has brought increased demand for AI-based edge\n",
      "computing in applications ranging from healthcare monitoring systems to\n",
      "autonomous vehicles. Quantization is a powerful tool to address the growing\n",
      "computational cost of such applications, and yields significant compression\n",
      "over full-precision networks. However, quantization can result in substantial\n",
      "loss of performance for complex image classification tasks. To address this, we\n",
      "propose a Principal Component Analysis (PCA) driven methodology to identify the\n",
      "important layers of a binary network, and design mixed-precision networks. The\n",
      "proposed Hybrid-Net achieves a more than 10% improvement in classification\n",
      "accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures\n",
      "on CIFAR-100 and ImageNet datasets while still achieving up to 94% of the\n",
      "energy-efficiency of XNOR-Nets. This work furthers the feasibility of using\n",
      "highly compressed neural networks for energy-efficient neural computing in edge\n",
      "devices.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.11797 \n",
      "Title :Functional Space Variational Inference for Uncertainty Estimation in\n",
      "  Computer Aided Diagnosis\n",
      "  Deep neural networks have revolutionized medical image analysis and disease\n",
      "diagnosis. Despite their impressive performance, it is difficult to generate\n",
      "well-calibrated probabilistic outputs for such networks, which makes them\n",
      "uninterpretable black boxes. Bayesian neural networks provide a principled\n",
      "approach for modelling uncertainty and increasing patient safety, but they have\n",
      "a large computational overhead and provide limited improvement in calibration.\n",
      "In this work, by taking skin lesion classification as an example task, we show\n",
      "that by shifting Bayesian inference to the functional space we can craft\n",
      "meaningful priors that give better calibrated uncertainty estimates at a much\n",
      "lower computational cost.\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.12379 \n",
      "Title :Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias?\n",
      "  An Empirical Study on Model Fairness\n",
      "  Machine learning models are increasingly being used in important\n",
      "decision-making software such as approving bank loans, recommending criminal\n",
      "sentencing, hiring employees, and so on. It is important to ensure the fairness\n",
      "of these models so that no discrimination is made based on protected attribute\n",
      "(e.g., race, sex, age) while decision making. Algorithms have been developed to\n",
      "measure unfairness and mitigate them to a certain extent. In this paper, we\n",
      "have focused on the empirical evaluation of fairness and mitigations on\n",
      "real-world machine learning models. We have created a benchmark of 40 top-rated\n",
      "models from Kaggle used for 5 different tasks, and then using a comprehensive\n",
      "set of fairness metrics, evaluated their fairness. Then, we have applied 7\n",
      "mitigation techniques on these models and analyzed the fairness, mitigation\n",
      "results, and impacts on performance. We have found that some model optimization\n",
      "techniques result in inducing unfairness in the models. On the other hand,\n",
      "although there are some fairness control mechanisms in machine learning\n",
      "libraries, they are not documented. The mitigation algorithm also exhibit\n",
      "common patterns such as mitigation in the post-processing is often costly (in\n",
      "terms of performance) and mitigation in the pre-processing stage is preferred\n",
      "in most cases. We have also presented different trade-off choices of fairness\n",
      "mitigation decisions. Our study suggests future research directions to reduce\n",
      "the gap between theoretical fairness aware algorithms and the software\n",
      "engineering methods to leverage them in practice.\n",
      "\n",
      "**Paper Id :1908.00176 \n",
      "Title :FairSight: Visual Analytics for Fairness in Decision Making\n",
      "  Data-driven decision making related to individuals has become increasingly\n",
      "pervasive, but the issue concerning the potential discrimination has been\n",
      "raised by recent studies. In response, researchers have made efforts to propose\n",
      "and implement fairness measures and algorithms, but those efforts have not been\n",
      "translated to the real-world practice of data-driven decision making. As such,\n",
      "there is still an urgent need to create a viable tool to facilitate fair\n",
      "decision making. We propose FairSight, a visual analytic system to address this\n",
      "need; it is designed to achieve different notions of fairness in ranking\n",
      "decisions through identifying the required actions -- understanding, measuring,\n",
      "diagnosing and mitigating biases -- that together lead to fairer decision\n",
      "making. Through a case study and user study, we demonstrate that the proposed\n",
      "visual analytic and diagnostic modules in the system are effective in\n",
      "understanding the fairness-aware decision pipeline and obtaining more fair\n",
      "outcomes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.12636 \n",
      "Title :Hard Shape-Constrained Kernel Machines\n",
      "  Shape constraints (such as non-negativity, monotonicity, convexity) play a\n",
      "central role in a large number of applications, as they usually improve\n",
      "performance for small sample size and help interpretability. However enforcing\n",
      "these shape requirements in a hard fashion is an extremely challenging problem.\n",
      "Classically, this task is tackled (i) in a soft way (without out-of-sample\n",
      "guarantees), (ii) by specialized transformation of the variables on a\n",
      "case-by-case basis, or (iii) by using highly restricted function classes, such\n",
      "as polynomials or polynomial splines. In this paper, we prove that hard affine\n",
      "shape constraints on function derivatives can be encoded in kernel machines\n",
      "which represent one of the most flexible and powerful tools in machine learning\n",
      "and statistics. Particularly, we present a tightened second-order cone\n",
      "constrained reformulation, that can be readily implemented in convex solvers.\n",
      "We prove performance guarantees on the solution, and demonstrate the efficiency\n",
      "of the approach in joint quantile regression with applications to economics and\n",
      "to the analysis of aircraft trajectories, among others.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.12719 \n",
      "Title :Exhaustive Neural Importance Sampling applied to Monte Carlo event\n",
      "  generation\n",
      "  The generation of accurate neutrino-nucleus cross-section models needed for\n",
      "neutrino oscillation experiments require simultaneously the description of many\n",
      "degrees of freedom and precise calculations to model nuclear responses. The\n",
      "detailed calculation of complete models makes the Monte Carlo generators slow\n",
      "and impractical. We present Exhaustive Neural Importance Sampling (ENIS), a\n",
      "method based on normalizing flows to find a suitable proposal density for\n",
      "rejection sampling automatically and efficiently, and discuss how this\n",
      "technique solves common issues of the rejection algorithm.\n",
      "\n",
      "**Paper Id :1905.11825 \n",
      "Title :Fast Data-Driven Simulation of Cherenkov Detectors Using Generative\n",
      "  Adversarial Networks\n",
      "  The increasing luminosities of future Large Hadron Collider runs and next\n",
      "generation of collider experiments will require an unprecedented amount of\n",
      "simulated events to be produced. Such large scale productions are extremely\n",
      "demanding in terms of computing resources. Thus new approaches to event\n",
      "generation and simulation of detector responses are needed. In LHCb, the\n",
      "accurate simulation of Cherenkov detectors takes a sizeable fraction of CPU\n",
      "time. An alternative approach is described here, when one generates high-level\n",
      "reconstructed observables using a generative neural network to bypass low level\n",
      "details. This network is trained to reproduce the particle species likelihood\n",
      "function values based on the track kinematic parameters and detector occupancy.\n",
      "The fast simulation is trained using real data samples collected by LHCb during\n",
      "run 2. We demonstrate that this approach provides high-fidelity results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.13140 \n",
      "Title :SSM-Net for Plants Disease Identification in Low Data Regime\n",
      "  Plant disease detection is an essential factor in increasing agricultural\n",
      "production. Due to the difficulty of disease detection, farmers spray various\n",
      "pesticides on their crops to protect them, causing great harm to crop growth\n",
      "and food standards. Deep learning can offer critical aid in detecting such\n",
      "diseases. However, it is highly inconvenient to collect a large volume of data\n",
      "on all forms of the diseases afflicting a specific plant species. In this\n",
      "paper, we propose a new metrics-based few-shot learning SSM net architecture,\n",
      "which consists of stacked siamese and matching network components to address\n",
      "the problem of disease detection in low data regimes. We demonstrated our\n",
      "experiments on two datasets: mini-leaves diseases and sugarcane diseases\n",
      "dataset. We have showcased that the SSM-Net approach can achieve better\n",
      "decision boundaries with an accuracy of 92.7% on the mini-leaves dataset and\n",
      "94.3% on the sugarcane dataset. The accuracy increased by ~10% and ~5%\n",
      "respectively, compared to the widely used VGG16 transfer learning approach.\n",
      "Furthermore, we attained F1 score of 0.90 using SSM Net on the sugarcane\n",
      "dataset and 0.91 on the mini-leaves dataset. Our code implementation is\n",
      "available on Github: https://github.com/shruti-jadon/PlantsDiseaseDetection.\n",
      "\n",
      "**Paper Id :2003.00682 \n",
      "Title :Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images\n",
      "  Lung disease is common throughout the world. These include chronic\n",
      "obstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc.\n",
      "Timely diagnosis of lung disease is essential. Many image processing and\n",
      "machine learning models have been developed for this purpose. Different forms\n",
      "of existing deep learning techniques including convolutional neural network\n",
      "(CNN), vanilla neural network, visual geometry group based neural network\n",
      "(VGG), and capsule network are applied for lung disease prediction.The basic\n",
      "CNN has poor performance for rotated, tilted, or other abnormal image\n",
      "orientation. Therefore, we propose a new hybrid deep learning framework by\n",
      "combining VGG, data augmentation and spatial transformer network (STN) with\n",
      "CNN. This new hybrid method is termed here as VGG Data STN with CNN (VDSNet).\n",
      "As implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The\n",
      "new model is applied to NIH chest X-ray image dataset collected from Kaggle\n",
      "repository. Full and sample versions of the dataset are considered. For both\n",
      "full and sample datasets, VDSNet outperforms existing methods in terms of a\n",
      "number of metrics including precision, recall, F0.5 score and validation\n",
      "accuracy. For the case of full dataset, VDSNet exhibits a validation accuracy\n",
      "of 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified\n",
      "capsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%,\n",
      "respectively. When sample dataset rather than full dataset is used, VDSNet\n",
      "requires much lower training time at the expense of a slightly lower validation\n",
      "accuracy. Hence, the proposed VDSNet framework will simplify the detection of\n",
      "lung disease for experts as well as for doctors.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.13787 \n",
      "Title :Assessing Centrality Without Knowing Connections\n",
      "  We consider the privacy-preserving computation of node influence in\n",
      "distributed social networks, as measured by egocentric betweenness centrality\n",
      "(EBC). Motivated by modern communication networks spanning multiple providers,\n",
      "we show for the first time how multiple mutually-distrusting parties can\n",
      "successfully compute node EBC while revealing only differentially-private\n",
      "information about their internal network connections. A theoretical utility\n",
      "analysis upper bounds a primary source of private EBC error---private release\n",
      "of ego networks---with high probability. Empirical results demonstrate\n",
      "practical applicability with a low 1.07 relative error achievable at strong\n",
      "privacy budget $\\epsilon=0.1$ on a Facebook graph, and insignificant\n",
      "performance degradation as the number of network provider parties grows.\n",
      "\n",
      "**Paper Id :1909.04421 \n",
      "Title :Privacy-Preserving Bandits\n",
      "  Contextual bandit algorithms~(CBAs) often rely on personal data to provide\n",
      "recommendations. Centralized CBA agents utilize potentially sensitive data from\n",
      "recent interactions to provide personalization to end-users. Keeping the\n",
      "sensitive data locally, by running a local agent on the user's device, protects\n",
      "the user's privacy, however, the agent requires longer to produce useful\n",
      "recommendations, as it does not leverage feedback from other users. This paper\n",
      "proposes a technique we call Privacy-Preserving Bandits (P2B); a system that\n",
      "updates local agents by collecting feedback from other local agents in a\n",
      "differentially-private manner. Comparisons of our proposed approach with a\n",
      "non-private, as well as a fully-private (local) system, show competitive\n",
      "performance on both synthetic benchmarks and real-world data. Specifically, we\n",
      "observed only a decrease of 2.6% and 3.6% in multi-label classification\n",
      "accuracy, and a CTR increase of 0.0025 in online advertising for a privacy\n",
      "budget $\\epsilon \\approx 0.693$. These results suggest P2B is an effective\n",
      "approach to challenges arising in on-device privacy-preserving personalization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2005.14124 \n",
      "Title :Active Fuzzing for Testing and Securing Cyber-Physical Systems\n",
      "  Cyber-physical systems (CPSs) in critical infrastructure face a pervasive\n",
      "threat from attackers, motivating research into a variety of countermeasures\n",
      "for securing them. Assessing the effectiveness of these countermeasures is\n",
      "challenging, however, as realistic benchmarks of attacks are difficult to\n",
      "manually construct, blindly testing is ineffective due to the enormous search\n",
      "spaces and resource requirements, and intelligent fuzzing approaches require\n",
      "impractical amounts of data and network access. In this work, we propose active\n",
      "fuzzing, an automatic approach for finding test suites of packet-level CPS\n",
      "network attacks, targeting scenarios in which attackers can observe sensors and\n",
      "manipulate packets, but have no existing knowledge about the payload encodings.\n",
      "Our approach learns regression models for predicting sensor values that will\n",
      "result from sampled network packets, and uses these predictions to guide a\n",
      "search for payload manipulations (i.e. bit flips) most likely to drive the CPS\n",
      "into an unsafe state. Key to our solution is the use of online active learning,\n",
      "which iteratively updates the models by sampling payloads that are estimated to\n",
      "maximally improve them. We evaluate the efficacy of active fuzzing by\n",
      "implementing it for a water purification plant testbed, finding it can\n",
      "automatically discover a test suite of flow, pressure, and over/underflow\n",
      "attacks, all with substantially less time, data, and network access than the\n",
      "most comparable approach. Finally, we demonstrate that our prediction models\n",
      "can also be utilised as countermeasures themselves, implementing them as\n",
      "anomaly detectors and early warning systems.\n",
      "\n",
      "**Paper Id :1906.10773 \n",
      "Title :Are Adversarial Perturbations a Showstopper for ML-Based CAD? A Case\n",
      "  Study on CNN-Based Lithographic Hotspot Detection\n",
      "  There is substantial interest in the use of machine learning (ML) based\n",
      "techniques throughout the electronic computer-aided design (CAD) flow,\n",
      "particularly those based on deep learning. However, while deep learning methods\n",
      "have surpassed state-of-the-art performance in several applications, they have\n",
      "exhibited intrinsic susceptibility to adversarial perturbations --- small but\n",
      "deliberate alterations to the input of a neural network, precipitating\n",
      "incorrect predictions. In this paper, we seek to investigate whether\n",
      "adversarial perturbations pose risks to ML-based CAD tools, and if so, how\n",
      "these risks can be mitigated. To this end, we use a motivating case study of\n",
      "lithographic hotspot detection, for which convolutional neural networks (CNN)\n",
      "have shown great promise. In this context, we show the first adversarial\n",
      "perturbation attacks on state-of-the-art CNN-based hotspot detectors;\n",
      "specifically, we show that small (on average 0.5% modified area), functionality\n",
      "preserving and design-constraint satisfying changes to a layout can nonetheless\n",
      "trick a CNN-based hotspot detector into predicting the modified layout as\n",
      "hotspot free (with up to 99.7% success). We propose an adversarial retraining\n",
      "strategy to improve the robustness of CNN-based hotspot detection and show that\n",
      "this strategy significantly improves robustness (by a factor of ~3) against\n",
      "adversarial attacks without compromising classification accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.00212 \n",
      "Title :Complex Sequential Understanding through the Awareness of Spatial and\n",
      "  Temporal Concepts\n",
      "  Understanding sequential information is a fundamental task for artificial\n",
      "intelligence. Current neural networks attempt to learn spatial and temporal\n",
      "information as a whole, limited their abilities to represent large scale\n",
      "spatial representations over long-range sequences. Here, we introduce a new\n",
      "modeling strategy called Semi-Coupled Structure (SCS), which consists of deep\n",
      "neural networks that decouple the complex spatial and temporal concepts\n",
      "learning. Semi-Coupled Structure can learn to implicitly separate input\n",
      "information into independent parts and process these parts respectively.\n",
      "Experiments demonstrate that a Semi-Coupled Structure can successfully annotate\n",
      "the outline of an object in images sequentially and perform video action\n",
      "recognition. For sequence-to-sequence problems, a Semi-Coupled Structure can\n",
      "predict future meteorological radar echo images based on observed images. Taken\n",
      "together, our results demonstrate that a Semi-Coupled Structure has the\n",
      "capacity to improve the performance of LSTM-like models on large scale\n",
      "sequential tasks.\n",
      "\n",
      "**Paper Id :2001.05313 \n",
      "Title :Tensor Graph Convolutional Networks for Text Classification\n",
      "  Compared to sequential learning models, graph-based neural networks exhibit\n",
      "some excellent properties, such as ability capturing global information. In\n",
      "this paper, we investigate graph-based neural networks for text classification\n",
      "problem. A new framework TensorGCN (tensor graph convolutional networks), is\n",
      "presented for this task. A text graph tensor is firstly constructed to describe\n",
      "semantic, syntactic, and sequential contextual information. Then, two kinds of\n",
      "propagation learning perform on the text graph tensor. The first is intra-graph\n",
      "propagation used for aggregating information from neighborhood nodes in a\n",
      "single graph. The second is inter-graph propagation used for harmonizing\n",
      "heterogeneous information between graphs. Extensive experiments are conducted\n",
      "on benchmark datasets, and the results illustrate the effectiveness of our\n",
      "proposed framework. Our proposed TensorGCN presents an effective way to\n",
      "harmonize and integrate heterogeneous information from different kinds of\n",
      "graphs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.00731 \n",
      "Title :Second-Order Provable Defenses against Adversarial Attacks\n",
      "  A robustness certificate is the minimum distance of a given input to the\n",
      "decision boundary of the classifier (or its lower bound). For {\\it any} input\n",
      "perturbations with a magnitude smaller than the certificate value, the\n",
      "classification output will provably remain unchanged. Exactly computing the\n",
      "robustness certificates for neural networks is difficult since it requires\n",
      "solving a non-convex optimization. In this paper, we provide\n",
      "computationally-efficient robustness certificates for neural networks with\n",
      "differentiable activation functions in two steps. First, we show that if the\n",
      "eigenvalues of the Hessian of the network are bounded, we can compute a\n",
      "robustness certificate in the $l_2$ norm efficiently using convex optimization.\n",
      "Second, we derive a computationally-efficient differentiable upper bound on the\n",
      "curvature of a deep network. We also use the curvature bound as a\n",
      "regularization term during the training of the network to boost its certified\n",
      "robustness. Putting these results together leads to our proposed {\\bf\n",
      "C}urvature-based {\\bf R}obustness {\\bf C}ertificate (CRC) and {\\bf\n",
      "C}urvature-based {\\bf R}obust {\\bf T}raining (CRT). Our numerical results show\n",
      "that CRT leads to significantly higher certified robust accuracy compared to\n",
      "interval-bound propagation (IBP) based training. We achieve certified robust\n",
      "accuracy 69.79\\%, 57.78\\% and 53.19\\% while IBP-based methods achieve 44.96\\%,\n",
      "44.74\\% and 44.66\\% on 2,3 and 4 layer networks respectively on the\n",
      "MNIST-dataset.\n",
      "\n",
      "**Paper Id :1901.07114 \n",
      "Title :Training Neural Networks as Learning Data-adaptive Kernels: Provable\n",
      "  Representation and Approximation Benefits\n",
      "  Consider the problem: given the data pair $(\\mathbf{x}, \\mathbf{y})$ drawn\n",
      "from a population with $f_*(x) = \\mathbf{E}[\\mathbf{y} | \\mathbf{x} = x]$,\n",
      "specify a neural network model and run gradient flow on the weights over time\n",
      "until reaching any stationarity. How does $f_t$, the function computed by the\n",
      "neural network at time $t$, relate to $f_*$, in terms of approximation and\n",
      "representation? What are the provable benefits of the adaptive representation\n",
      "by neural networks compared to the pre-specified fixed basis representation in\n",
      "the classical nonparametric literature? We answer the above questions via a\n",
      "dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the\n",
      "training process of neural networks. Firstly, we show that when reaching any\n",
      "local stationarity, gradient flow learns an adaptive RKHS representation and\n",
      "performs the global least-squares projection onto the adaptive RKHS,\n",
      "simultaneously. Secondly, we prove that as the RKHS is data-adaptive and\n",
      "task-specific, the residual for $f_*$ lies in a subspace that is potentially\n",
      "much smaller than the orthogonal complement of the RKHS. The result formalizes\n",
      "the representation and approximation benefits of neural networks. Lastly, we\n",
      "show that the neural network function computed by gradient flow converges to\n",
      "the kernel ridgeless regression with an adaptive kernel, in the limit of\n",
      "vanishing regularization. The adaptive kernel viewpoint provides new angles of\n",
      "studying the approximation, representation, generalization, and optimization\n",
      "advantages of neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.00940 \n",
      "Title :Sparse Identification of Slow Timescale Dynamics\n",
      "  Multiscale phenomena that evolve on multiple distinct timescales are\n",
      "prevalent throughout the sciences. It is often the case that the governing\n",
      "equations of the persistent and approximately periodic fast scales are\n",
      "prescribed, while the emergent slow scale evolution is unknown. Yet the\n",
      "course-grained, slow scale dynamics is often of greatest interest in practice.\n",
      "In this work we present an accurate and efficient method for extracting the\n",
      "slow timescale dynamics from signals exhibiting multiple timescales that are\n",
      "amenable to averaging. The method relies on tracking the signal at\n",
      "evenly-spaced intervals with length given by the period of the fast timescale,\n",
      "which is discovered using clustering techniques in conjunction with the dynamic\n",
      "mode decomposition. Sparse regression techniques are then used to discover a\n",
      "mapping which describes iterations from one data point to the next. We show\n",
      "that for sufficiently disparate timescales this discovered mapping can be used\n",
      "to discover the continuous-time slow dynamics, thus providing a novel tool for\n",
      "extracting dynamics on multiple timescales.\n",
      "\n",
      "**Paper Id :2002.05909 \n",
      "Title :Deep reconstruction of strange attractors from time series\n",
      "  Experimental measurements of physical systems often have a limited number of\n",
      "independent channels, causing essential dynamical variables to remain\n",
      "unobserved. However, many popular methods for unsupervised inference of latent\n",
      "dynamics from experimental data implicitly assume that the measurements have\n",
      "higher intrinsic dimensionality than the underlying system---making coordinate\n",
      "identification a dimensionality reduction problem. Here, we study the opposite\n",
      "limit, in which hidden governing coordinates must be inferred from only a\n",
      "low-dimensional time series of measurements. Inspired by classical analysis\n",
      "techniques for partial observations of chaotic attractors, we introduce a\n",
      "general embedding technique for univariate and multivariate time series,\n",
      "consisting of an autoencoder trained with a novel latent-space loss function.\n",
      "We show that our technique reconstructs the strange attractors of synthetic and\n",
      "real-world systems better than existing techniques, and that it creates\n",
      "consistent, predictive representations of even stochastic systems. We conclude\n",
      "by using our technique to discover dynamical attractors in diverse systems such\n",
      "as patient electrocardiograms, household electricity usage, neural spiking, and\n",
      "eruptions of the Old Faithful geyser---demonstrating diverse applications of\n",
      "our technique for exploratory data analysis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01022 \n",
      "Title :A novel approach for multi-agent cooperative pursuit to capture grouped\n",
      "  evaders\n",
      "  An approach of mobile multi-agent pursuit based on application of\n",
      "self-organizing feature map (SOFM) and along with that reinforcement learning\n",
      "based on agent group role membership function (AGRMF) model is proposed. This\n",
      "method promotes dynamic organization of the pursuers' groups and also makes\n",
      "pursuers' group evader according to their desire based on SOFM and AGRMF\n",
      "techniques. This helps to overcome the shortcomings of the pursuers that they\n",
      "cannot fully reorganize when the goal is too independent in process of AGRMF\n",
      "models operation. Besides, we also discuss a new reward function. After the\n",
      "formation of the group, reinforcement learning is applied to get the optimal\n",
      "solution for each agent. The results of each step in capturing process will\n",
      "finally affect the AGR membership function to speed up the convergence of the\n",
      "competitive neural network. The experiments result shows that this approach is\n",
      "more effective for the mobile agents to capture evaders.\n",
      "\n",
      "**Paper Id :2006.08122 \n",
      "Title :Classification and Recognition of Encrypted EEG Data Neural Network\n",
      "  With the rapid development of Machine Learning technology applied in\n",
      "electroencephalography (EEG) signals, Brain-Computer Interface (BCI) has\n",
      "emerged as a novel and convenient human-computer interaction for smart home,\n",
      "intelligent medical and other Internet of Things (IoT) scenarios. However,\n",
      "security issues such as sensitive information disclosure and unauthorized\n",
      "operations have not received sufficient concerns. There are still some defects\n",
      "with the existing solutions to encrypted EEG data such as low accuracy, high\n",
      "time complexity or slow processing speed. For this reason, a classification and\n",
      "recognition method of encrypted EEG data based on neural network is proposed,\n",
      "which adopts Paillier encryption algorithm to encrypt EEG data and meanwhile\n",
      "resolves the problem of floating point operations. In addition, it improves\n",
      "traditional feed-forward neural network (FNN) by using the approximate function\n",
      "instead of activation function and realizes multi-classification of encrypted\n",
      "EEG data. Extensive experiments are conducted to explore the effect of several\n",
      "metrics (such as the hidden neuron size and the learning rate updated by\n",
      "improved simulated annealing algorithm) on the recognition results. Followed by\n",
      "security and time cost analysis, the proposed model and approach are validated\n",
      "and evaluated on public EEG datasets provided by PhysioNet, BCI Competition IV\n",
      "and EPILEPSIAE. The experimental results show that our proposal has the\n",
      "satisfactory accuracy, efficiency and feasibility compared with other\n",
      "solutions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01247 \n",
      "Title :Wavelet Scattering Networks for Atomistic Systems with Extrapolation of\n",
      "  Material Properties\n",
      "  The dream of machine learning in materials science is for a model to learn\n",
      "the underlying physics of an atomic system, allowing it to move beyond\n",
      "interpolation of the training set to the prediction of properties that were not\n",
      "present in the original training data. In addition to advances in machine\n",
      "learning architectures and training techniques, achieving this ambitious goal\n",
      "requires a method to convert a 3D atomic system into a feature representation\n",
      "that preserves rotational and translational symmetry, smoothness under small\n",
      "perturbations, and invariance under re-ordering. The atomic orbital wavelet\n",
      "scattering transform preserves these symmetries by construction, and has\n",
      "achieved great success as a featurization method for machine learning energy\n",
      "prediction. Both in small molecules and in the bulk amorphous\n",
      "$\\text{Li}_{\\alpha}\\text{Si}$ system, machine learning models using wavelet\n",
      "scattering coefficients as features have demonstrated a comparable accuracy to\n",
      "Density Functional Theory at a small fraction of the computational cost. In\n",
      "this work, we test the generalizability of our $\\text{Li}_{\\alpha}\\text{Si}$\n",
      "energy predictor to properties that were not included in the training set, such\n",
      "as elastic constants and migration barriers. We demonstrate that statistical\n",
      "feature selection methods can reduce over-fitting and lead to remarkable\n",
      "accuracy in these extrapolation tasks.\n",
      "\n",
      "**Paper Id :2006.01541 \n",
      "Title :Committee neural network potentials control generalization errors and\n",
      "  enable active learning\n",
      "  It is well known in the field of machine learning that committee models\n",
      "improve accuracy, provide generalization error estimates, and enable active\n",
      "learning strategies. In this work, we adapt these concepts to interatomic\n",
      "potentials based on artificial neural networks. Instead of a single model,\n",
      "multiple models that share the same atomic environment descriptors yield an\n",
      "average that outperforms its individual members as well as a measure of the\n",
      "generalization error in the form of the committee disagreement. We not only use\n",
      "this disagreement to identify the most relevant configurations to build up the\n",
      "model's training set in an active learning procedure, but also monitor and bias\n",
      "it during simulations to control the generalization error. This facilitates the\n",
      "adaptive development of committee neural network potentials and their training\n",
      "sets, while keeping the number of ab initio calculations to a minimum. To\n",
      "illustrate the benefits of this methodology, we apply it to the development of\n",
      "a committee model for water in the condensed phase. Starting from a single\n",
      "reference ab initio simulation, we use active learning to expand into new state\n",
      "points and to describe the quantum nature of the nuclei. The final model,\n",
      "trained on 814 reference calculations, yields excellent results under a range\n",
      "of conditions, from liquid water at ambient and elevated temperatures and\n",
      "pressures to different phases of ice, and the air-water interface - all\n",
      "including nuclear quantum effects. This approach to committee models will\n",
      "enable the systematic development of robust machine learning models for a broad\n",
      "range of systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01263 \n",
      "Title :A comparative study of 2D image segmentation algorithms for traumatic\n",
      "  brain lesions using CT data from the ProTECTIII multicenter clinical trial\n",
      "  Automated segmentation of medical imaging is of broad interest to clinicians\n",
      "and machine learning researchers alike. The goal of segmentation is to increase\n",
      "efficiency and simplicity of visualization and quantification of regions of\n",
      "interest within a medical image. Image segmentation is a difficult task because\n",
      "of multiparametric heterogeneity within the images, an obstacle that has proven\n",
      "especially challenging in efforts to automate the segmentation of brain lesions\n",
      "from non-contrast head computed tomography (CT). In this research, we have\n",
      "experimented with multiple available deep learning architectures to segment\n",
      "different phenotypes of hemorrhagic lesions found after moderate to severe\n",
      "traumatic brain injury (TBI). These include: intraparenchymal hemorrhage (IPH),\n",
      "subdural hematoma (SDH), epidural hematoma (EDH), and traumatic contusions. We\n",
      "were able to achieve an optimal Dice Coefficient1 score of 0.94 using UNet++ 2D\n",
      "Architecture with Focal Tversky Loss Function, an increase from 0.85 using UNet\n",
      "2D with Binary Cross-Entropy Loss Function in intraparenchymal hemorrhage (IPH)\n",
      "cases. Furthermore, using the same setting, we were able to achieve the Dice\n",
      "Coefficient score of 0.90 and 0.86 in cases of Extra-Axial bleeds and Traumatic\n",
      "contusions, respectively.\n",
      "\n",
      "**Paper Id :2003.11617 \n",
      "Title :Covid-19: Automatic detection from X-Ray images utilizing Transfer\n",
      "  Learning with Convolutional Neural Networks\n",
      "  In this study, a dataset of X-Ray images from patients with common pneumonia,\n",
      "Covid-19, and normal incidents was utilized for the automatic detection of the\n",
      "Coronavirus. The aim of the study is to evaluate the performance of\n",
      "state-of-the-art Convolutional Neural Network architectures proposed over\n",
      "recent years for medical image classification. Specifically, the procedure\n",
      "called transfer learning was adopted. With transfer learning, the detection of\n",
      "various abnormalities in small medical image datasets is an achievable target,\n",
      "often yielding remarkable results. The dataset utilized in this experiment is a\n",
      "collection of 1427 X-Ray images. 224 images with confirmed Covid-19, 700 images\n",
      "with confirmed common pneumonia, and 504 images of normal conditions are\n",
      "included. The data was collected from the available X-Ray images on public\n",
      "medical repositories. With transfer learning, an overall accuracy of 97.82% in\n",
      "the detection of Covid-19 is achieved.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01310 \n",
      "Title :Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019\n",
      "  Person identification based on eye movements is getting more and more\n",
      "attention, as it is anti-spoofing resistant and can be useful for continuous\n",
      "authentication. Therefore, it is noteworthy for researchers to know who and\n",
      "what is relevant in the field, including authors, journals, conferences, and\n",
      "institutions. This paper presents a comprehensive quantitative overview of the\n",
      "field of eye movement biometrics using a bibliometric approach. All data and\n",
      "analyses are based on documents written in English published between 2004 and\n",
      "2019. Scopus was used to perform information retrieval. This research focused\n",
      "on temporal evolution, leading authors, most cited papers, leading journals,\n",
      "competitions and collaboration networks.\n",
      "\n",
      "**Paper Id :1912.11084 \n",
      "Title :Where Are We? Using Scopus to Map the Literature at the Intersection\n",
      "  Between Artificial Intelligence and Research on Crime\n",
      "  Research on Artificial Intelligence (AI) applications has spread over many\n",
      "scientific disciplines. Scientists have tested the power of intelligent\n",
      "algorithms developed to predict (or learn from) natural, physical and social\n",
      "phenomena. This also applies to crime-related research problems. Nonetheless,\n",
      "studies that map the current state of the art at the intersection between AI\n",
      "and crime are lacking. What are the current research trends in terms of topics\n",
      "in this area? What is the structure of scientific collaboration when\n",
      "considering works investigating criminal issues using machine learning, deep\n",
      "learning, and AI in general? What are the most active countries in this\n",
      "specific scientific sphere? Using data retrieved from the Scopus database, this\n",
      "work quantitatively analyzes 692 published works at the intersection between AI\n",
      "and crime employing network science to respond to these questions. Results show\n",
      "that researchers are mainly focusing on cyber-related criminal topics and that\n",
      "relevant themes such as algorithmic discrimination, fairness, and ethics are\n",
      "considerably overlooked. Furthermore, data highlight the extremely disconnected\n",
      "structure of co-authorship networks. Such disconnectedness may represent a\n",
      "substantial obstacle to a more solid community of scientists interested in\n",
      "these topics. Additionally, the graph of scientific collaboration indicates\n",
      "that countries that are more prone to engage in international partnerships are\n",
      "generally less central in the network. This means that scholars working in\n",
      "highly productive countries (e.g. the United States, China) tend to mostly\n",
      "collaborate domestically. Finally, current issues and future developments\n",
      "within this scientific area are also discussed.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01448 \n",
      "Title :Sparse Cholesky covariance parametrization for recovering latent\n",
      "  structure in ordered data\n",
      "  The sparse Cholesky parametrization of the inverse covariance matrix can be\n",
      "interpreted as a Gaussian Bayesian network; however its counterpart, the\n",
      "covariance Cholesky factor, has received, with few notable exceptions, little\n",
      "attention so far, despite having a natural interpretation as a hidden variable\n",
      "model for ordered signal data. To fill this gap, in this paper we focus on\n",
      "arbitrary zero patterns in the Cholesky factor of a covariance matrix. We\n",
      "discuss how these models can also be extended, in analogy with Gaussian\n",
      "Bayesian networks, to data where no apparent order is available. For the\n",
      "ordered scenario, we propose a novel estimation method that is based on matrix\n",
      "loss penalization, as opposed to the existing regression-based approaches. The\n",
      "performance of this sparse model for the Cholesky factor, together with our\n",
      "novel estimator, is assessed in a simulation setting, as well as over spatial\n",
      "and temporal real data where a natural ordering arises among the variables. We\n",
      "give guidelines, based on the empirical results, about which of the methods\n",
      "analysed is more appropriate for each setting.\n",
      "\n",
      "**Paper Id :2004.07210 \n",
      "Title :On Box-Cox Transformation for Image Normality and Pattern Classification\n",
      "  A unique member of the power transformation family is known as the Box-Cox\n",
      "transformation. The latter can be seen as a mathematical operation that leads\n",
      "to finding the optimum lambda ({\\lambda}) value that maximizes the\n",
      "log-likelihood function to transform a data to a normal distribution and to\n",
      "reduce heteroscedasticity. In data analytics, a normality assumption underlies\n",
      "a variety of statistical test models. This technique, however, is best known in\n",
      "statistical analysis to handle one-dimensional data. Herein, this paper\n",
      "revolves around the utility of such a tool as a pre-processing step to\n",
      "transform two-dimensional data, namely, digital images and to study its effect.\n",
      "Moreover, to reduce time complexity, it suffices to estimate the parameter\n",
      "lambda in real-time for large two-dimensional matrices by merely considering\n",
      "their probability density function as a statistical inference of the underlying\n",
      "data distribution. We compare the effect of this light-weight Box-Cox\n",
      "transformation with well-established state-of-the-art low light image\n",
      "enhancement techniques. We also demonstrate the effectiveness of our approach\n",
      "through several test-bed data sets for generic improvement of visual appearance\n",
      "of images and for ameliorating the performance of a colour pattern\n",
      "classification algorithm as an example application. Results with and without\n",
      "the proposed approach, are compared using the AlexNet (transfer deep learning)\n",
      "pretrained model. To the best of our knowledge, this is the first time that the\n",
      "Box-Cox transformation is extended to digital images by exploiting histogram\n",
      "transformation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01509 \n",
      "Title :Detection of gravitational-wave signals from binary neutron star mergers\n",
      "  using machine learning\n",
      "  As two neutron stars merge, they emit gravitational waves that can\n",
      "potentially be detected by earth bound detectors. Matched-filtering based\n",
      "algorithms have traditionally been used to extract quiet signals embedded in\n",
      "noise. We introduce a novel neural-network based machine learning algorithm\n",
      "that uses time series strain data from gravitational-wave detectors to detect\n",
      "signals from non-spinning binary neutron star mergers. For the Advanced LIGO\n",
      "design sensitivity, our network has an average sensitive distance of 130 Mpc at\n",
      "a false-alarm rate of 10 per month. Compared to other state-of-the-art machine\n",
      "learning algorithms, we find an improvement by a factor of 6 in sensitivity to\n",
      "signals with signal-to-noise ratio below 25. However, this approach is not yet\n",
      "competitive with traditional matched-filtering based methods. A conservative\n",
      "estimate indicates that our algorithm introduces on average 10.2 s of latency\n",
      "between signal arrival and generating an alert. We give an exact description of\n",
      "our testing procedure, which can not only be applied to machine learning based\n",
      "algorithms but all other search algorithms as well. We thereby improve the\n",
      "ability to compare machine learning and classical searches.\n",
      "\n",
      "**Paper Id :2007.06585 \n",
      "Title :Gravitational-wave selection effects using neural-network classifiers\n",
      "  We present a novel machine-learning approach to estimate selection effects in\n",
      "gravitational-wave observations. Using techniques similar to those commonly\n",
      "employed in image classification and pattern recognition, we train a series of\n",
      "neural-network classifiers to predict the LIGO/Virgo detectability of\n",
      "gravitational-wave signals from compact-binary mergers. We include the effect\n",
      "of spin precession, higher-order modes, and multiple detectors and show that\n",
      "their omission, as it is common in large population studies, tends to\n",
      "overestimate the inferred merger rate in selected regions of the parameter\n",
      "space. Although here we train our classifiers using a simple signal-to-noise\n",
      "ratio threshold, our approach is ready to be used in conjunction with full\n",
      "pipeline injections, thus paving the way toward including actual distributions\n",
      "of astrophysical and noise triggers into gravitational-wave population\n",
      "analyses.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01510 \n",
      "Title :Recht-R\\'e Noncommutative Arithmetic-Geometric Mean Conjecture is False\n",
      "  Stochastic optimization algorithms have become indispensable in modern\n",
      "machine learning. An unresolved foundational question in this area is the\n",
      "difference between with-replacement sampling and without-replacement sampling\n",
      "-- does the latter have superior convergence rate compared to the former? A\n",
      "groundbreaking result of Recht and R\\'e reduces the problem to a noncommutative\n",
      "analogue of the arithmetic-geometric mean inequality where $n$ positive numbers\n",
      "are replaced by $n$ positive definite matrices. If this inequality holds for\n",
      "all $n$, then without-replacement sampling indeed outperforms with-replacement\n",
      "sampling. The conjectured Recht-R\\'e inequality has so far only been\n",
      "established for $n = 2$ and a special case of $n = 3$. We will show that the\n",
      "Recht-R\\'e conjecture is false for general $n$. Our approach relies on the\n",
      "noncommutative Positivstellensatz, which allows us to reduce the conjectured\n",
      "inequality to a semidefinite program and the validity of the conjecture to\n",
      "certain bounds for the optimum values, which we show are false as soon as $n =\n",
      "5$.\n",
      "\n",
      "**Paper Id :1902.08753 \n",
      "Title :Quantum Learning Boolean Linear Functions w.r.t. Product Distributions\n",
      "  The problem of learning Boolean linear functions from quantum examples w.r.t.\n",
      "the uniform distribution can be solved on a quantum computer using the\n",
      "Bernstein-Vazirani algorithm. A similar strategy can be applied in the case of\n",
      "noisy quantum training data, as was observed in arXiv:1702.08255v2 [quant-ph].\n",
      "However, extensions of these learning algorithms beyond the uniform\n",
      "distribution have not yet been studied. We employ the biased quantum Fourier\n",
      "transform introduced in arXiv:1802.05690v2 [quant-ph] to develop efficient\n",
      "quantum algorithms for learning Boolean linear functions on $n$ bits from\n",
      "quantum examples w.r.t. a biased product distribution. Our first procedure is\n",
      "applicable to any (except full) bias and requires $\\mathcal{O}(\\ln (n))$\n",
      "quantum examples. The number of quantum examples used by our second algorithm\n",
      "is independent of $n$, but the strategy is applicable only for small bias.\n",
      "Moreover, we show that the second procedure is stable w.r.t. noisy training\n",
      "data and w.r.t. faulty quantum gates. This also enables us to solve a version\n",
      "of the learning problem in which the underlying distribution is not known in\n",
      "advance. Finally, we prove lower bounds on the classical and quantum sample\n",
      "complexities of the learning problem. Whereas classically, $\\Omega (n)$\n",
      "examples are necessary independently of the bias, we are able to establish a\n",
      "quantum sample complexity lower bound of $\\Omega (\\ln (n))$ only under an\n",
      "assumption of large bias. Nevertheless, this allows for a discussion of the\n",
      "performance of our suggested learning algorithms w.r.t. sample complexity. With\n",
      "our analysis we contribute to a more quantitative understanding of the power\n",
      "and limitations of quantum training data for learning classical functions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01541 \n",
      "Title :Committee neural network potentials control generalization errors and\n",
      "  enable active learning\n",
      "  It is well known in the field of machine learning that committee models\n",
      "improve accuracy, provide generalization error estimates, and enable active\n",
      "learning strategies. In this work, we adapt these concepts to interatomic\n",
      "potentials based on artificial neural networks. Instead of a single model,\n",
      "multiple models that share the same atomic environment descriptors yield an\n",
      "average that outperforms its individual members as well as a measure of the\n",
      "generalization error in the form of the committee disagreement. We not only use\n",
      "this disagreement to identify the most relevant configurations to build up the\n",
      "model's training set in an active learning procedure, but also monitor and bias\n",
      "it during simulations to control the generalization error. This facilitates the\n",
      "adaptive development of committee neural network potentials and their training\n",
      "sets, while keeping the number of ab initio calculations to a minimum. To\n",
      "illustrate the benefits of this methodology, we apply it to the development of\n",
      "a committee model for water in the condensed phase. Starting from a single\n",
      "reference ab initio simulation, we use active learning to expand into new state\n",
      "points and to describe the quantum nature of the nuclei. The final model,\n",
      "trained on 814 reference calculations, yields excellent results under a range\n",
      "of conditions, from liquid water at ambient and elevated temperatures and\n",
      "pressures to different phases of ice, and the air-water interface - all\n",
      "including nuclear quantum effects. This approach to committee models will\n",
      "enable the systematic development of robust machine learning models for a broad\n",
      "range of systems.\n",
      "\n",
      "**Paper Id :2003.13418 \n",
      "Title :Machine Learning Enabled Discovery of Application Dependent Design\n",
      "  Principles for Two-dimensional Materials\n",
      "  The large-scale search for high-performing candidate 2D materials is limited\n",
      "to calculating a few simple descriptors, usually with first-principles density\n",
      "functional theory calculations. In this work, we alleviate this issue by\n",
      "extending and generalizing crystal graph convolutional neural networks to\n",
      "systems with planar periodicity, and train an ensemble of models to predict\n",
      "thermodynamic, mechanical, and electronic properties. To demonstrate the\n",
      "utility of this approach, we carry out a screening of nearly 45,000 structures\n",
      "for two largely disjoint applications: namely, mechanically robust composites\n",
      "and photovoltaics. An analysis of the uncertainty associated with our methods\n",
      "indicates the ensemble of neural networks is well-calibrated and has errors\n",
      "comparable with those from accurate first-principles density functional theory\n",
      "calculations. The ensemble of models allows us to gauge the confidence of our\n",
      "predictions, and to find the candidates most likely to exhibit effective\n",
      "performance in their applications. Since the datasets used in our screening\n",
      "were combinatorically generated, we are also able to investigate, using an\n",
      "innovative method, structural and compositional design principles that impact\n",
      "the properties of the structures surveyed and which can act as a generative\n",
      "model basis for future material discovery through reverse engineering. Our\n",
      "approach allowed us to recover some well-accepted design principles: for\n",
      "instance, we find that hybrid organic-inorganic perovskites with lead and tin\n",
      "tend to be good candidates for solar cell applications.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01644 \n",
      "Title :Learning Efficient Representations of Mouse Movements to Predict User\n",
      "  Attention\n",
      "  Tracking mouse cursor movements can be used to predict user attention on\n",
      "heterogeneous page layouts like SERPs. So far, previous work has relied heavily\n",
      "on handcrafted features, which is a time-consuming approach that often requires\n",
      "domain expertise. We investigate different representations of mouse cursor\n",
      "movements, including time series, heatmaps, and trajectory-based images, to\n",
      "build and contrast both recurrent and convolutional neural networks that can\n",
      "predict user attention to direct displays, such as SERP advertisements. Our\n",
      "models are trained over raw mouse cursor data and achieve competitive\n",
      "performance. We conclude that neural network models should be adopted for\n",
      "downstream tasks involving mouse cursor movements, since they can provide an\n",
      "invaluable implicit feedback signal for re-ranking and evaluation.\n",
      "\n",
      "**Paper Id :1905.13209 \n",
      "Title :AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n",
      "  Architectures\n",
      "  Learning to represent videos is a very challenging task both algorithmically\n",
      "and computationally. Standard video CNN architectures have been designed by\n",
      "directly extending architectures devised for image understanding to include the\n",
      "time dimension, using modules such as 3D convolutions, or by using two-stream\n",
      "design to capture both appearance and motion in videos. We interpret a video\n",
      "CNN as a collection of multi-stream convolutional blocks connected to each\n",
      "other, and propose the approach of automatically finding neural architectures\n",
      "with better connectivity and spatio-temporal interactions for video\n",
      "understanding. This is done by evolving a population of overly-connected\n",
      "architectures guided by connection weight learning. Architectures combining\n",
      "representations that abstract different input types (i.e., RGB and optical\n",
      "flow) at multiple temporal resolutions are searched for, allowing different\n",
      "types or sources of information to interact with each other. Our method,\n",
      "referred to as AssembleNet, outperforms prior approaches on public video\n",
      "datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n",
      "34.27% accuracy on Moments-in-Time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01673 \n",
      "Title :Learning Opinion Dynamics From Social Traces\n",
      "  Opinion dynamics - the research field dealing with how people's opinions form\n",
      "and evolve in a social context - traditionally uses agent-based models to\n",
      "validate the implications of sociological theories. These models encode the\n",
      "causal mechanism that drives the opinion formation process, and have the\n",
      "advantage of being easy to interpret. However, as they do not exploit the\n",
      "availability of data, their predictive power is limited. Moreover, parameter\n",
      "calibration and model selection are manual and difficult tasks.\n",
      "  In this work we propose an inference mechanism for fitting a generative,\n",
      "agent-like model of opinion dynamics to real-world social traces. Given a set\n",
      "of observables (e.g., actions and interactions between agents), our model can\n",
      "recover the most-likely latent opinion trajectories that are compatible with\n",
      "the assumptions about the process dynamics. This type of model retains the\n",
      "benefits of agent-based ones (i.e., causal interpretation), while adding the\n",
      "ability to perform model selection and hypothesis testing on real data.\n",
      "  We showcase our proposal by translating a classical agent-based model of\n",
      "opinion dynamics into its generative counterpart. We then design an inference\n",
      "algorithm based on online expectation maximization to learn the latent\n",
      "parameters of the model. Such algorithm can recover the latent opinion\n",
      "trajectories from traces generated by the classical agent-based model. In\n",
      "addition, it can identify the most likely set of macro parameters used to\n",
      "generate a data trace, thus allowing testing of sociological hypotheses.\n",
      "Finally, we apply our model to real-world data from Reddit to explore the\n",
      "long-standing question about the impact of backfire effect. Our results suggest\n",
      "a low prominence of the effect in Reddit's political conversation.\n",
      "\n",
      "**Paper Id :1910.09358 \n",
      "Title :A Decision-Theoretic Approach for Model Interpretability in Bayesian\n",
      "  Framework\n",
      "  A salient approach to interpretable machine learning is to restrict modeling\n",
      "to simple models. In the Bayesian framework, this can be pursued by restricting\n",
      "the model structure and prior to favor interpretable models. Fundamentally,\n",
      "however, interpretability is about users' preferences, not the data generation\n",
      "mechanism; it is more natural to formulate interpretability as a utility\n",
      "function. In this work, we propose an interpretability utility, which\n",
      "explicates the trade-off between explanation fidelity and interpretability in\n",
      "the Bayesian framework. The method consists of two steps. First, a reference\n",
      "model, possibly a black-box Bayesian predictive model which does not compromise\n",
      "accuracy, is fitted to the training data. Second, a proxy model from an\n",
      "interpretable model family that best mimics the predictive behaviour of the\n",
      "reference model is found by optimizing the interpretability utility function.\n",
      "The approach is model agnostic -- neither the interpretable model nor the\n",
      "reference model are restricted to a certain class of models -- and the\n",
      "optimization problem can be solved using standard tools. Through experiments on\n",
      "real-word data sets, using decision trees as interpretable models and Bayesian\n",
      "additive regression models as reference models, we show that for the same level\n",
      "of interpretability, our approach generates more accurate models than the\n",
      "alternative of restricting the prior. We also propose a systematic way to\n",
      "measure stability of interpretabile models constructed by different\n",
      "interpretability approaches and show that our proposed approach generates more\n",
      "stable models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.01759 \n",
      "Title :Sparse Perturbations for Improved Convergence in Stochastic Zeroth-Order\n",
      "  Optimization\n",
      "  Interest in stochastic zeroth-order (SZO) methods has recently been revived\n",
      "in black-box optimization scenarios such as adversarial black-box attacks to\n",
      "deep neural networks. SZO methods only require the ability to evaluate the\n",
      "objective function at random input points, however, their weakness is the\n",
      "dependency of their convergence speed on the dimensionality of the function to\n",
      "be evaluated. We present a sparse SZO optimization method that reduces this\n",
      "factor to the expected dimensionality of the random perturbation during\n",
      "learning. We give a proof that justifies this reduction for sparse SZO\n",
      "optimization for non-convex functions without making any assumptions on\n",
      "sparsity of objective function or gradient. Furthermore, we present\n",
      "experimental results for neural networks on MNIST and CIFAR that show faster\n",
      "convergence in training loss and test accuracy, and a smaller distance of the\n",
      "gradient approximation to the true gradient in sparse SZO compared to dense\n",
      "SZO.\n",
      "\n",
      "**Paper Id :1902.08234 \n",
      "Title :An Empirical Study of Large-Batch Stochastic Gradient Descent with\n",
      "  Structured Covariance Noise\n",
      "  The choice of batch-size in a stochastic optimization algorithm plays a\n",
      "substantial role for both optimization and generalization. Increasing the\n",
      "batch-size used typically improves optimization but degrades generalization. To\n",
      "address the problem of improving generalization while maintaining optimal\n",
      "convergence in large-batch training, we propose to add covariance noise to the\n",
      "gradients. We demonstrate that the learning performance of our method is more\n",
      "accurately captured by the structure of the covariance matrix of the noise\n",
      "rather than by the variance of gradients. Moreover, over the convex-quadratic,\n",
      "we prove in theory that it can be characterized by the Frobenius norm of the\n",
      "noise matrix. Our empirical studies with standard deep learning\n",
      "model-architectures and datasets shows that our method not only improves\n",
      "generalization performance in large-batch training, but furthermore, does so in\n",
      "a way where the optimization performance remains desirable and the training\n",
      "duration is not elongated.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.02341 \n",
      "Title :Non-Euclidean Universal Approximation\n",
      "  Modifications to a neural network's input and output layers are often\n",
      "required to accommodate the specificities of most practical learning tasks.\n",
      "However, the impact of such changes on architecture's approximation\n",
      "capabilities is largely not understood. We present general conditions\n",
      "describing feature and readout maps that preserve an architecture's ability to\n",
      "approximate any continuous functions uniformly on compacts. As an application,\n",
      "we show that if an architecture is capable of universal approximation, then\n",
      "modifying its final layer to produce binary values creates a new architecture\n",
      "capable of deterministically approximating any classifier. In particular, we\n",
      "obtain guarantees for deep CNNs and deep feed-forward networks. Our results\n",
      "also have consequences within the scope of geometric deep learning.\n",
      "Specifically, when the input and output spaces are Cartan-Hadamard manifolds,\n",
      "we obtain geometrically meaningful feature and readout maps satisfying our\n",
      "criteria. Consequently, commonly used non-Euclidean regression models between\n",
      "spaces of symmetric positive definite matrices are extended to universal DNNs.\n",
      "The same result allows us to show that the hyperbolic feed-forward networks,\n",
      "used for hierarchical learning, are universal. Our result is also used to show\n",
      "that the common practice of randomizing all but the last two layers of a DNN\n",
      "produces a universal family of functions with probability one. We also provide\n",
      "conditions on a DNN's first (resp. last) few layer's connections and activation\n",
      "function which guarantee that these layers can have a width equal to the input\n",
      "(resp. output) space's dimension while not negatively affecting the\n",
      "architecture's approximation capabilities.\n",
      "\n",
      "**Paper Id :1802.03774 \n",
      "Title :On Kernel Method-Based Connectionist Models and Supervised Deep Learning\n",
      "  Without Backpropagation\n",
      "  We propose a novel family of connectionist models based on kernel machines\n",
      "and consider the problem of learning layer-by-layer a compositional hypothesis\n",
      "class, i.e., a feedforward, multilayer architecture, in a supervised setting.\n",
      "In terms of the models, we present a principled method to \"kernelize\" (partly\n",
      "or completely) any neural network (NN). With this method, we obtain a\n",
      "counterpart of any given NN that is powered by kernel machines instead of\n",
      "neurons. In terms of learning, when learning a feedforward deep architecture in\n",
      "a supervised setting, one needs to train all the components simultaneously\n",
      "using backpropagation (BP) since there are no explicit targets for the hidden\n",
      "layers (Rumelhart86). We consider without loss of generality the two-layer case\n",
      "and present a general framework that explicitly characterizes a target for the\n",
      "hidden layer that is optimal for minimizing the objective function of the\n",
      "network. This characterization then makes possible a purely greedy training\n",
      "scheme that learns one layer at a time, starting from the input layer. We\n",
      "provide realizations of the abstract framework under certain architectures and\n",
      "objective functions. Based on these realizations, we present a layer-wise\n",
      "training algorithm for an l-layer feedforward network for classification, where\n",
      "l>=2 can be arbitrary. This algorithm can be given an intuitive geometric\n",
      "interpretation that makes the learning dynamics transparent. Empirical results\n",
      "are provided to complement our theory. We show that the kernelized networks,\n",
      "trained layer-wise, compare favorably with classical kernel machines as well as\n",
      "other connectionist models trained by BP. We also visualize the inner workings\n",
      "of the greedy kernelized models to validate our claim on the transparency of\n",
      "the layer-wise algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.02456 \n",
      "Title :A Distributed Trust Framework for Privacy-Preserving Machine Learning\n",
      "  When training a machine learning model, it is standard procedure for the\n",
      "researcher to have full knowledge of both the data and model. However, this\n",
      "engenders a lack of trust between data owners and data scientists. Data owners\n",
      "are justifiably reluctant to relinquish control of private information to third\n",
      "parties. Privacy-preserving techniques distribute computation in order to\n",
      "ensure that data remains in the control of the owner while learning takes\n",
      "place. However, architectures distributed amongst multiple agents introduce an\n",
      "entirely new set of security and trust complications. These include data\n",
      "poisoning and model theft. This paper outlines a distributed infrastructure\n",
      "which is used to facilitate peer-to-peer trust between distributed agents;\n",
      "collaboratively performing a privacy-preserving workflow. Our outlined\n",
      "prototype sets industry gatekeepers and governance bodies as credential\n",
      "issuers. Before participating in the distributed learning workflow, malicious\n",
      "actors must first negotiate valid credentials. We detail a proof of concept\n",
      "using Hyperledger Aries, Decentralised Identifiers (DIDs) and Verifiable\n",
      "Credentials (VCs) to establish a distributed trust architecture during a\n",
      "privacy-preserving machine learning experiment. Specifically, we utilise secure\n",
      "and authenticated DID communication channels in order to facilitate a federated\n",
      "learning workflow related to mental health care data.\n",
      "\n",
      "**Paper Id :2009.02267 \n",
      "Title :Zero-Bias Deep Learning for Accurate Identification of Internet of\n",
      "  Things (IoT) Devices\n",
      "  The Internet of Things (IoT) provides applications and services that would\n",
      "otherwise not be possible. However, the open nature of IoT make it vulnerable\n",
      "to cybersecurity threats. Especially, identity spoofing attacks, where an\n",
      "adversary passively listens to existing radio communications and then mimic the\n",
      "identity of legitimate devices to conduct malicious activities. Existing\n",
      "solutions employ cryptographic signatures to verify the trustworthiness of\n",
      "received information. In prevalent IoT, secret keys for cryptography can\n",
      "potentially be disclosed and disable the verification mechanism.\n",
      "Non-cryptographic device verification is needed to ensure trustworthy IoT. In\n",
      "this paper, we propose an enhanced deep learning framework for IoT device\n",
      "identification using physical layer signals. Specifically, we enable our\n",
      "framework to report unseen IoT devices and introduce the zero-bias layer to\n",
      "deep neural networks to increase robustness and interpretability. We have\n",
      "evaluated the effectiveness of the proposed framework using real data from\n",
      "ADS-B (Automatic Dependent Surveillance-Broadcast), an application of IoT in\n",
      "aviation. The proposed framework has the potential to be applied to accurate\n",
      "identification of IoT devices in a variety of IoT applications and services.\n",
      "Codes and data are available in IEEE Dataport.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.02493 \n",
      "Title :Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE\n",
      "  Neural ordinary differential equations (NODEs) have recently attracted\n",
      "increasing attention; however, their empirical performance on benchmark tasks\n",
      "(e.g. image classification) are significantly inferior to discrete-layer\n",
      "models. We demonstrate an explanation for their poorer performance is the\n",
      "inaccuracy of existing gradient estimation methods: the adjoint method has\n",
      "numerical errors in reverse-mode integration; the naive method directly\n",
      "back-propagates through ODE solvers, but suffers from a redundantly deep\n",
      "computation graph when searching for the optimal stepsize. We propose the\n",
      "Adaptive Checkpoint Adjoint (ACA) method: in automatic differentiation, ACA\n",
      "applies a trajectory checkpoint strategy which records the forward-mode\n",
      "trajectory as the reverse-mode trajectory to guarantee accuracy; ACA deletes\n",
      "redundant components for shallow computation graphs; and ACA supports adaptive\n",
      "solvers. On image classification tasks, compared with the adjoint and naive\n",
      "method, ACA achieves half the error rate in half the training time; NODE\n",
      "trained with ACA outperforms ResNet in both accuracy and test-retest\n",
      "reliability. On time-series modeling, ACA outperforms competing methods.\n",
      "Finally, in an example of the three-body problem, we show NODE with ACA can\n",
      "incorporate physical knowledge to achieve better accuracy. We provide the\n",
      "PyTorch implementation of ACA:\n",
      "\\url{https://github.com/juntang-zhuang/torch-ACA}.\n",
      "\n",
      "**Paper Id :2002.10306 \n",
      "Title :Adaptive Propagation Graph Convolutional Network\n",
      "  Graph convolutional networks (GCNs) are a family of neural network models\n",
      "that perform inference on graph data by interleaving vertex-wise operations and\n",
      "message-passing exchanges across nodes. Concerning the latter, two key\n",
      "questions arise: (i) how to design a differentiable exchange protocol (e.g., a\n",
      "1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize\n",
      "the trade-off in complexity with respect to the local updates. In this paper,\n",
      "we show that state-of-the-art results can be achieved by adapting the number of\n",
      "communication steps independently at every node. In particular, we endow each\n",
      "node with a halting unit (inspired by Graves' adaptive computation time) that\n",
      "after every exchange decides whether to continue communicating or not. We show\n",
      "that the proposed adaptive propagation GCN (AP-GCN) achieves superior or\n",
      "similar results to the best proposed models so far on a number of benchmarks,\n",
      "while requiring a small overhead in terms of additional parameters. We also\n",
      "investigate a regularization term to enforce an explicit trade-off between\n",
      "communication and accuracy. The code for the AP-GCN experiments is released as\n",
      "an open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.03227 \n",
      "Title :Population-Based Black-Box Optimization for Biological Sequence Design\n",
      "  The use of black-box optimization for the design of new biological sequences\n",
      "is an emerging research area with potentially revolutionary impact. The cost\n",
      "and latency of wet-lab experiments requires methods that find good sequences in\n",
      "few experimental rounds of large batches of sequences--a setting that\n",
      "off-the-shelf black-box optimization methods are ill-equipped to handle. We\n",
      "find that the performance of existing methods varies drastically across\n",
      "optimization tasks, posing a significant obstacle to real-world applications.\n",
      "To improve robustness, we propose Population-Based Black-Box Optimization\n",
      "(P3BO), which generates batches of sequences by sampling from an ensemble of\n",
      "methods. The number of sequences sampled from any method is proportional to the\n",
      "quality of sequences it previously proposed, allowing P3BO to combine the\n",
      "strengths of individual methods while hedging against their innate brittleness.\n",
      "Adapting the hyper-parameters of each of the methods online using evolutionary\n",
      "optimization further improves performance. Through extensive experiments on\n",
      "in-silico optimization tasks, we show that P3BO outperforms any single method\n",
      "in its population, proposing higher quality sequences as well as more diverse\n",
      "batches. As such, P3BO and Adaptive-P3BO are a crucial step towards deploying\n",
      "ML to real-world sequence design.\n",
      "\n",
      "**Paper Id :1909.02119 \n",
      "Title :Inductive-bias-driven Reinforcement Learning For Efficient Schedules in\n",
      "  Heterogeneous Clusters\n",
      "  The problem of scheduling of workloads onto heterogeneous processors (e.g.,\n",
      "CPUs, GPUs, FPGAs) is of fundamental importance in modern data centers. Current\n",
      "system schedulers rely on application/system-specific heuristics that have to\n",
      "be built on a case-by-case basis. Recent work has demonstrated ML techniques\n",
      "for automating the heuristic search by using black-box approaches which require\n",
      "significant training data and time, which make them challenging to use in\n",
      "practice. This paper presents Symphony, a scheduling framework that addresses\n",
      "the challenge in two ways: (i) a domain-driven Bayesian reinforcement learning\n",
      "(RL) model for scheduling, which inherently models the resource dependencies\n",
      "identified from the system architecture; and (ii) a sampling-based technique to\n",
      "compute the gradients of a Bayesian model without performing full probabilistic\n",
      "inference. Together, these techniques reduce both the amount of training data\n",
      "and the time required to produce scheduling policies that significantly\n",
      "outperform black-box approaches by up to 2.2x.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.03267 \n",
      "Title :Convolutional Neural Networks for Global Human Settlements Mapping from\n",
      "  Sentinel-2 Satellite Imagery\n",
      "  Spatially consistent and up-to-date maps of human settlements are crucial for\n",
      "addressing policies related to urbanization and sustainability, especially in\n",
      "the era of an increasingly urbanized world.The availability of open and free\n",
      "Sentinel-2 data of the Copernicus Earth Observation program offers a new\n",
      "opportunity for wall-to-wall mapping of human settlements at a global\n",
      "scale.This paper presents a deep-learning-based framework for a fully automated\n",
      "extraction of built-up areas at a spatial resolution of 10 m from a global\n",
      "composite of Sentinel-2 imagery.A multi-neuro modeling methodology building on\n",
      "a simple Convolution Neural Networks architecture for pixel-wise image\n",
      "classification of built-up areas is developed.The core features of the proposed\n",
      "model are the image patch of size 5 x 5 pixels adequate for describing built-up\n",
      "areas from Sentinel-2 imagery and the lightweight topology with a total number\n",
      "of 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened\n",
      "layers.The deployment of the model on the global Sentinel-2 image composite\n",
      "provides the most detailed and complete map reporting about built-up areas for\n",
      "reference year 2018. The validation of the results with an independent\n",
      "reference data-set of building footprints covering 277 sites across the world\n",
      "establishes the reliability of the built-up layer produced by the proposed\n",
      "framework and the model robustness.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.03541 \n",
      "Title :Sentiment Analysis Based on Deep Learning: A Comparative Study\n",
      "  The study of public opinion can provide us with valuable information. The\n",
      "analysis of sentiment on social networks, such as Twitter or Facebook, has\n",
      "become a powerful means of learning about the users' opinions and has a wide\n",
      "range of applications. However, the efficiency and accuracy of sentiment\n",
      "analysis is being hindered by the challenges encountered in natural language\n",
      "processing (NLP). In recent years, it has been demonstrated that deep learning\n",
      "models are a promising solution to the challenges of NLP. This paper reviews\n",
      "the latest studies that have employed deep learning to solve sentiment analysis\n",
      "problems, such as sentiment polarity. Models using term frequency-inverse\n",
      "document frequency (TF-IDF) and word embedding have been applied to a series of\n",
      "datasets. Finally, a comparative study has been conducted on the experimental\n",
      "results obtained for the different models and input features\n",
      "\n",
      "**Paper Id :2005.06599 \n",
      "Title :Phishing URL Detection Through Top-level Domain Analysis: A Descriptive\n",
      "  Approach\n",
      "  Phishing is considered to be one of the most prevalent cyber-attacks because\n",
      "of its immense flexibility and alarmingly high success rate. Even with adequate\n",
      "training and high situational awareness, it can still be hard for users to\n",
      "continually be aware of the URL of the website they are visiting. Traditional\n",
      "detection methods rely on blocklists and content analysis, both of which\n",
      "require time-consuming human verification. Thus, there have been attempts\n",
      "focusing on the predictive filtering of such URLs. This study aims to develop a\n",
      "machine-learning model to detect fraudulent URLs which can be used within the\n",
      "Splunk platform. Inspired from similar approaches in the literature, we trained\n",
      "the SVM and Random Forests algorithms using malicious and benign datasets found\n",
      "in the literature and one dataset that we created. We evaluated the algorithms'\n",
      "performance with precision and recall, reaching up to 85% precision and 87%\n",
      "recall in the case of Random Forests while SVM achieved up to 90% precision and\n",
      "88% recall using only descriptive features.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.03859 \n",
      "Title :Online learning of both state and dynamics using ensemble Kalman filters\n",
      "  The reconstruction of the dynamics of an observed physical system as a\n",
      "surrogate model has been brought to the fore by recent advances in machine\n",
      "learning. To deal with partial and noisy observations in that endeavor, machine\n",
      "learning representations of the surrogate model can be used within a Bayesian\n",
      "data assimilation framework. However, these approaches require to consider long\n",
      "time series of observational data, meant to be assimilated all together. This\n",
      "paper investigates the possibility to learn both the dynamics and the state\n",
      "online, i.e. to update their estimates at any time, in particular when new\n",
      "observations are acquired. The estimation is based on the ensemble Kalman\n",
      "filter (EnKF) family of algorithms using a rather simple representation for the\n",
      "surrogate model and state augmentation. We consider the implication of learning\n",
      "dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an\n",
      "iterative EnKF and we discuss in each case issues and algorithmic solutions. We\n",
      "then demonstrate numerically the efficiency and assess the accuracy of these\n",
      "methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.\n",
      "\n",
      "**Paper Id :2001.06270 \n",
      "Title :Bayesian inference of chaotic dynamics by merging data assimilation,\n",
      "  machine learning and expectation-maximization\n",
      "  The reconstruction from observations of high-dimensional chaotic dynamics\n",
      "such as geophysical flows is hampered by (i) the partial and noisy observations\n",
      "that can realistically be obtained, (ii) the need to learn from long time\n",
      "series of data, and (iii) the unstable nature of the dynamics. To achieve such\n",
      "inference from the observations over long time series, it has been suggested to\n",
      "combine data assimilation and machine learning in several ways. We show how to\n",
      "unify these approaches from a Bayesian perspective using\n",
      "expectation-maximization and coordinate descents. In doing so, the model, the\n",
      "state trajectory and model error statistics are estimated all together.\n",
      "Implementations and approximations of these methods are discussed. Finally, we\n",
      "numerically and successfully test the approach on two relevant low-order\n",
      "chaotic models with distinct identifiability.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.03963 \n",
      "Title :Combinatorial Black-Box Optimization with Expert Advice\n",
      "  We consider the problem of black-box function optimization over the boolean\n",
      "hypercube. Despite the vast literature on black-box function optimization over\n",
      "continuous domains, not much attention has been paid to learning models for\n",
      "optimization over combinatorial domains until recently. However, the\n",
      "computational complexity of the recently devised algorithms are prohibitive\n",
      "even for moderate numbers of variables; drawing one sample using the existing\n",
      "algorithms is more expensive than a function evaluation for many black-box\n",
      "functions of interest. To address this problem, we propose a computationally\n",
      "efficient model learning algorithm based on multilinear polynomials and\n",
      "exponential weight updates. In the proposed algorithm, we alternate between\n",
      "simulated annealing with respect to the current polynomial representation and\n",
      "updating the weights using monomial experts' advice. Numerical experiments on\n",
      "various datasets in both unconstrained and sum-constrained boolean optimization\n",
      "indicate the competitive performance of the proposed algorithm, while improving\n",
      "the computational time up to several orders of magnitude compared to\n",
      "state-of-the-art algorithms in the literature.\n",
      "\n",
      "**Paper Id :2007.13243 \n",
      "Title :Scalable Derivative-Free Optimization for Nonlinear Least-Squares\n",
      "  Problems\n",
      "  Derivative-free - or zeroth-order - optimization (DFO) has gained recent\n",
      "attention for its ability to solve problems in a variety of application areas,\n",
      "including machine learning, particularly involving objectives which are\n",
      "stochastic and/or expensive to compute. In this work, we develop a novel\n",
      "model-based DFO method for solving nonlinear least-squares problems. We improve\n",
      "on state-of-the-art DFO by performing dimensionality reduction in the\n",
      "observational space using sketching methods, avoiding the construction of a\n",
      "full local model. Our approach has a per-iteration computational cost which is\n",
      "linear in problem dimension in a big data regime, and numerical evidence\n",
      "demonstrates that, compared to existing software, it has dramatically improved\n",
      "runtime performance on overdetermined least-squares problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.03965 \n",
      "Title :Generative Adversarial Phonology: Modeling unsupervised phonetic and\n",
      "  phonological learning with neural networks\n",
      "  Training deep neural networks on well-understood dependencies in speech data\n",
      "can provide new insights into how they learn internal representations. This\n",
      "paper argues that acquisition of speech can be modeled as a dependency between\n",
      "random space and generated speech data in the Generative Adversarial Network\n",
      "architecture and proposes a methodology to uncover the network's internal\n",
      "representations that correspond to phonetic and phonological properties. The\n",
      "Generative Adversarial architecture is uniquely appropriate for modeling\n",
      "phonetic and phonological learning because the network is trained on\n",
      "unannotated raw acoustic data and learning is unsupervised without any\n",
      "language-specific assumptions or pre-assumed levels of abstraction. A\n",
      "Generative Adversarial Network was trained on an allophonic distribution in\n",
      "English. The network successfully learns the allophonic alternation: the\n",
      "network's generated speech signal contains the conditional distribution of\n",
      "aspiration duration. The paper proposes a technique for establishing the\n",
      "network's internal representations that identifies latent variables that\n",
      "correspond to, for example, presence of [s] and its spectral properties. By\n",
      "manipulating these variables, we actively control the presence of [s] and its\n",
      "frication amplitude in the generated outputs. This suggests that the network\n",
      "learns to use latent variables as an approximation of phonetic and phonological\n",
      "representations. Crucially, we observe that the dependencies learned in\n",
      "training extend beyond the training interval, which allows for additional\n",
      "exploration of learning representations. The paper also discusses how the\n",
      "network's architecture and innovative outputs resemble and differ from\n",
      "linguistic behavior in language acquisition, speech disorders, and speech\n",
      "errors, and how well-understood dependencies in speech data can help us\n",
      "interpret how neural networks learn their representations.\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.04016 \n",
      "Title :A Multitask Learning Approach for Diacritic Restoration\n",
      "  In many languages like Arabic, diacritics are used to specify pronunciations\n",
      "as well as meanings. Such diacritics are often omitted in written text,\n",
      "increasing the number of possible pronunciations and meanings for a word. This\n",
      "results in a more ambiguous text making computational processing on such text\n",
      "more difficult. Diacritic restoration is the task of restoring missing\n",
      "diacritics in the written text. Most state-of-the-art diacritic restoration\n",
      "models are built on character level information which helps generalize the\n",
      "model to unseen data, but presumably lose useful information at the word level.\n",
      "Thus, to compensate for this loss, we investigate the use of multi-task\n",
      "learning to jointly optimize diacritic restoration with related NLP problems\n",
      "namely word segmentation, part-of-speech tagging, and syntactic diacritization.\n",
      "We use Arabic as a case study since it has sufficient data resources for tasks\n",
      "that we consider in our joint modeling. Our joint models significantly\n",
      "outperform the baselines and are comparable to the state-of-the-art models that\n",
      "are more complex relying on morphological analyzers and/or a lot more data\n",
      "(e.g. dialectal data).\n",
      "\n",
      "**Paper Id :2004.11714 \n",
      "Title :Residual Energy-Based Models for Text Generation\n",
      "  Text generation is ubiquitous in many NLP tasks, from summarization, to\n",
      "dialogue and machine translation. The dominant parametric approach is based on\n",
      "locally normalized models which predict one word at a time. While these work\n",
      "remarkably well, they are plagued by exposure bias due to the greedy nature of\n",
      "the generation process. In this work, we investigate un-normalized energy-based\n",
      "models (EBMs) which operate not at the token but at the sequence level. In\n",
      "order to make training tractable, we first work in the residual of a pretrained\n",
      "locally normalized language model and second we train using noise contrastive\n",
      "estimation. Furthermore, since the EBM works at the sequence level, we can\n",
      "leverage pretrained bi-directional contextual representations, such as BERT and\n",
      "RoBERTa. Our experiments on two large language modeling datasets show that\n",
      "residual EBMs yield lower perplexity compared to locally normalized baselines.\n",
      "Moreover, generation via importance sampling is very efficient and of higher\n",
      "quality than the baseline models according to human evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.04311 \n",
      "Title :Little Ball of Fur: A Python Library for Graph Sampling\n",
      "  Sampling graphs is an important task in data mining. In this paper, we\n",
      "describe Little Ball of Fur a Python library that includes more than twenty\n",
      "graph sampling algorithms. Our goal is to make node, edge, and\n",
      "exploration-based network sampling techniques accessible to a large number of\n",
      "professionals, researchers, and students in a single streamlined framework. We\n",
      "created this framework with a focus on a coherent application public interface\n",
      "which has a convenient design, generic input data requirements, and reasonable\n",
      "baseline settings of algorithms. Here we overview these design foundations of\n",
      "the framework in detail with illustrative code snippets. We show the practical\n",
      "usability of the library by estimating various global statistics of social\n",
      "networks and web graphs. Experiments demonstrate that Little Ball of Fur can\n",
      "speed up node and whole graph embedding techniques considerably with mildly\n",
      "deteriorating the predictive value of distilled features.\n",
      "\n",
      "**Paper Id :2007.03634 \n",
      "Title :PinnerSage: Multi-Modal User Embedding Framework for Recommendations at\n",
      "  Pinterest\n",
      "  Latent user representations are widely adopted in the tech industry for\n",
      "powering personalized recommender systems. Most prior work infers a single high\n",
      "dimensional embedding to represent a user, which is a good starting point but\n",
      "falls short in delivering a full understanding of the user's interests. In this\n",
      "work, we introduce PinnerSage, an end-to-end recommender system that represents\n",
      "each user via multi-modal embeddings and leverages this rich representation of\n",
      "users to provides high quality personalized recommendations. PinnerSage\n",
      "achieves this by clustering users' actions into conceptually coherent clusters\n",
      "with the help of a hierarchical clustering method (Ward) and summarizes the\n",
      "clusters via representative pins (Medoids) for efficiency and interpretability.\n",
      "PinnerSage is deployed in production at Pinterest and we outline the several\n",
      "design decisions that makes it run seamlessly at a very large scale. We conduct\n",
      "several offline and online A/B experiments to show that our method\n",
      "significantly outperforms single embedding methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.04432 \n",
      "Title :AdaDeep: A Usage-Driven, Automated Deep Model Compression Framework for\n",
      "  Enabling Ubiquitous Intelligent Mobiles\n",
      "  Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a\n",
      "tremendously growing demand for bringing DNN-powered intelligence into mobile\n",
      "platforms. While the potential of deploying DNNs on resource-constrained\n",
      "platforms has been demonstrated by DNN compression techniques, the current\n",
      "practice suffers from two limitations: 1) merely stand-alone compression\n",
      "schemes are investigated even though each compression technique only suit for\n",
      "certain types of DNN layers; and 2) mostly compression techniques are optimized\n",
      "for DNNs' inference accuracy, without explicitly considering other\n",
      "application-driven system performance (e.g., latency and energy cost) and the\n",
      "varying resource availability across platforms (e.g., storage and processing\n",
      "capability). To this end, we propose AdaDeep, a usage-driven, automated DNN\n",
      "compression framework for systematically exploring the desired trade-off\n",
      "between performance and resource constraints, from a holistic system level.\n",
      "Specifically, in a layer-wise manner, AdaDeep automatically selects the most\n",
      "suitable combination of compression techniques and the corresponding\n",
      "compression hyperparameters for a given DNN. Thorough evaluations on six\n",
      "datasets and across twelve devices demonstrate that AdaDeep can achieve up to\n",
      "$18.6\\times$ latency reduction, $9.8\\times$ energy-efficiency improvement, and\n",
      "$37.3\\times$ storage reduction in DNNs while incurring negligible accuracy\n",
      "loss. Furthermore, AdaDeep also uncovers multiple novel combinations of\n",
      "compression techniques.\n",
      "\n",
      "**Paper Id :2008.04878 \n",
      "Title :Hardware-Centric AutoML for Mixed-Precision Quantization\n",
      "  Model quantization is a widely used technique to compress and accelerate deep\n",
      "neural network (DNN) inference. Emergent DNN hardware accelerators begin to\n",
      "support mixed precision (1-8 bits) to further improve the computation\n",
      "efficiency, which raises a great challenge to find the optimal bitwidth for\n",
      "each layer: it requires domain experts to explore the vast design space trading\n",
      "off among accuracy, latency, energy, and model size, which is both\n",
      "time-consuming and sub-optimal. Conventional quantization algorithm ignores the\n",
      "different hardware architectures and quantizes all the layers in a uniform way.\n",
      "In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\n",
      "framework which leverages the reinforcement learning to automatically determine\n",
      "the quantization policy, and we take the hardware accelerator's feedback in the\n",
      "design loop. Rather than relying on proxy signals such as FLOPs and model size,\n",
      "we employ a hardware simulator to generate direct feedback signals (latency and\n",
      "energy) to the RL agent. Compared with conventional methods, our framework is\n",
      "fully automated and can specialize the quantization policy for different neural\n",
      "network architectures and hardware architectures. Our framework effectively\n",
      "reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with\n",
      "negligible loss of accuracy compared with the fixed bitwidth (8 bits)\n",
      "quantization. Our framework reveals that the optimal policies on different\n",
      "hardware architectures (i.e., edge and cloud architectures) under different\n",
      "resource constraints (i.e., latency, energy, and model size) are drastically\n",
      "different. We interpreted the implication of different quantization policies,\n",
      "which offer insights for both neural network architecture design and hardware\n",
      "architecture design.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.05065 \n",
      "Title :Self-Distillation as Instance-Specific Label Smoothing\n",
      "  It has been recently demonstrated that multi-generational self-distillation\n",
      "can improve generalization. Despite this intriguing observation, reasons for\n",
      "the enhancement remain poorly understood. In this paper, we first demonstrate\n",
      "experimentally that the improved performance of multi-generational\n",
      "self-distillation is in part associated with the increasing diversity in\n",
      "teacher predictions. With this in mind, we offer a new interpretation for\n",
      "teacher-student training as amortized MAP estimation, such that teacher\n",
      "predictions enable instance-specific regularization. Our framework allows us to\n",
      "theoretically relate self-distillation to label smoothing, a commonly used\n",
      "technique that regularizes predictive uncertainty, and suggests the importance\n",
      "of predictive diversity in addition to predictive uncertainty. We present\n",
      "experimental results using multiple datasets and neural network architectures\n",
      "that, overall, demonstrate the utility of predictive diversity. Finally, we\n",
      "propose a novel instance-specific label smoothing technique that promotes\n",
      "predictive diversity without the need for a separately trained teacher model.\n",
      "We provide an empirical evaluation of the proposed method, which, we find,\n",
      "often outperforms classical label smoothing.\n",
      "\n",
      "**Paper Id :2002.08274 \n",
      "Title :Residual Correlation in Graph Neural Network Regression\n",
      "  A graph neural network transforms features in each vertex's neighborhood into\n",
      "a vector representation of the vertex. Afterward, each vertex's representation\n",
      "is used independently for predicting its label. This standard pipeline\n",
      "implicitly assumes that vertex labels are conditionally independent given their\n",
      "neighborhood features. However, this is a strong assumption, and we show that\n",
      "it is far from true on many real-world graph datasets. Focusing on regression\n",
      "tasks, we find that this conditional independence assumption severely limits\n",
      "predictive power. This should not be that surprising, given that traditional\n",
      "graph-based semi-supervised learning methods such as label propagation work in\n",
      "the opposite fashion by explicitly modeling the correlation in predicted\n",
      "outcomes.\n",
      "  Here, we address this problem with an interpretable and efficient framework\n",
      "that can improve any graph neural network architecture simply by exploiting\n",
      "correlation structure in the regression residuals. In particular, we model the\n",
      "joint distribution of residuals on vertices with a parameterized multivariate\n",
      "Gaussian, and estimate the parameters by maximizing the marginal likelihood of\n",
      "the observed labels. Our framework achieves substantially higher accuracy than\n",
      "competing baselines, and the learned parameters can be interpreted as the\n",
      "strength of correlation among connected vertices. Furthermore, we develop\n",
      "linear time algorithms for low-variance, unbiased model parameter estimates,\n",
      "allowing us to scale to large networks. We also provide a basic version of our\n",
      "method that makes stronger assumptions on correlation structure but is painless\n",
      "to implement, often leading to great practical performance with minimal\n",
      "overhead.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.05078 \n",
      "Title :Differentiable Expected Hypervolume Improvement for Parallel\n",
      "  Multi-Objective Bayesian Optimization\n",
      "  In many real-world scenarios, decision makers seek to efficiently optimize\n",
      "multiple competing objectives in a sample-efficient fashion. Multi-objective\n",
      "Bayesian optimization (BO) is a common approach, but many of the\n",
      "best-performing acquisition functions do not have known analytic gradients and\n",
      "suffer from high computational overhead. We leverage recent advances in\n",
      "programming models and hardware acceleration for multi-objective BO using\n",
      "Expected Hypervolume Improvement (EHVI)---an algorithm notorious for its high\n",
      "computational complexity. We derive a novel formulation of q-Expected\n",
      "Hypervolume Improvement (qEHVI), an acquisition function that extends EHVI to\n",
      "the parallel, constrained evaluation setting. qEHVI is an exact computation of\n",
      "the joint EHVI of q new candidate points (up to Monte-Carlo (MC) integration\n",
      "error). Whereas previous EHVI formulations rely on gradient-free acquisition\n",
      "optimization or approximated gradients, we compute exact gradients of the MC\n",
      "estimator via auto-differentiation, thereby enabling efficient and effective\n",
      "optimization using first-order and quasi-second-order methods. Our empirical\n",
      "evaluation demonstrates that qEHVI is computationally tractable in many\n",
      "practical scenarios and outperforms state-of-the-art multi-objective BO\n",
      "algorithms at a fraction of their wall time.\n",
      "\n",
      "**Paper Id :2007.13243 \n",
      "Title :Scalable Derivative-Free Optimization for Nonlinear Least-Squares\n",
      "  Problems\n",
      "  Derivative-free - or zeroth-order - optimization (DFO) has gained recent\n",
      "attention for its ability to solve problems in a variety of application areas,\n",
      "including machine learning, particularly involving objectives which are\n",
      "stochastic and/or expensive to compute. In this work, we develop a novel\n",
      "model-based DFO method for solving nonlinear least-squares problems. We improve\n",
      "on state-of-the-art DFO by performing dimensionality reduction in the\n",
      "observational space using sketching methods, avoiding the construction of a\n",
      "full local model. Our approach has a per-iteration computational cost which is\n",
      "linear in problem dimension in a big data regime, and numerical evidence\n",
      "demonstrates that, compared to existing software, it has dramatically improved\n",
      "runtime performance on overdetermined least-squares problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.05163 \n",
      "Title :ConfNet2Seq: Full Length Answer Generation from Spoken Questions\n",
      "  Conversational and task-oriented dialogue systems aim to interact with the\n",
      "user using natural responses through multi-modal interfaces, such as text or\n",
      "speech. These desired responses are in the form of full-length natural answers\n",
      "generated over facts retrieved from a knowledge source. While the task of\n",
      "generating natural answers to questions from an answer span has been widely\n",
      "studied, there has been little research on natural sentence generation over\n",
      "spoken content. We propose a novel system to generate full length natural\n",
      "language answers from spoken questions and factoid answers. The spoken sequence\n",
      "is compactly represented as a confusion network extracted from a pre-trained\n",
      "Automatic Speech Recognizer. This is the first attempt towards generating\n",
      "full-length natural answers from a graph input(confusion network) to the best\n",
      "of our knowledge. We release a large-scale dataset of 259,788 samples of spoken\n",
      "questions, their factoid answers and corresponding full-length textual answers.\n",
      "Following our proposed approach, we achieve comparable performance with best\n",
      "ASR hypothesis.\n",
      "\n",
      "**Paper Id :1907.04553 \n",
      "Title :Neural Reasoning, Fast and Slow, for Video Question Answering\n",
      "  What does it take to design a machine that learns to answer natural questions\n",
      "about a video? A Video QA system must simultaneously understand language,\n",
      "represent visual content over space-time, and iteratively transform these\n",
      "representations in response to lingual content in the query, and finally\n",
      "arriving at a sensible answer. While recent advances in lingual and visual\n",
      "question answering have enabled sophisticated representations and neural\n",
      "reasoning mechanisms, major challenges in Video QA remain on dynamic grounding\n",
      "of concepts, relations and actions to support the reasoning process. Inspired\n",
      "by the dual-process account of human reasoning, we design a dual process neural\n",
      "architecture, which is composed of a question-guided video processing module\n",
      "(System 1, fast and reactive) followed by a generic reasoning module (System 2,\n",
      "slow and deliberative). System 1 is a hierarchical model that encodes visual\n",
      "patterns about objects, actions and relations in space-time given the textual\n",
      "cues from the question. The encoded representation is a set of high-level\n",
      "visual features, which are then passed to System 2. Here multi-step inference\n",
      "follows to iteratively chain visual elements as instructed by the textual\n",
      "elements. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets\n",
      "(real), demonstrating competitive results, with a large margin in the case of\n",
      "multi-step reasoning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.05467 \n",
      "Title :Pruning neural networks without any data by iteratively conserving\n",
      "  synaptic flow\n",
      "  Pruning the parameters of deep neural networks has generated intense interest\n",
      "due to potential savings in time, memory and energy both during training and at\n",
      "test time. Recent works have identified, through an expensive sequence of\n",
      "training and pruning cycles, the existence of winning lottery tickets or sparse\n",
      "trainable subnetworks at initialization. This raises a foundational question:\n",
      "can we identify highly sparse trainable subnetworks at initialization, without\n",
      "ever training, or indeed without ever looking at the data? We provide an\n",
      "affirmative answer to this question through theory driven algorithm design. We\n",
      "first mathematically formulate and experimentally verify a conservation law\n",
      "that explains why existing gradient-based pruning algorithms at initialization\n",
      "suffer from layer-collapse, the premature pruning of an entire layer rendering\n",
      "a network untrainable. This theory also elucidates how layer-collapse can be\n",
      "entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow\n",
      "Pruning (SynFlow). This algorithm can be interpreted as preserving the total\n",
      "flow of synaptic strengths through the network at initialization subject to a\n",
      "sparsity constraint. Notably, this algorithm makes no reference to the training\n",
      "data and consistently competes with or outperforms existing state-of-the-art\n",
      "pruning algorithms at initialization over a range of models (VGG and ResNet),\n",
      "datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to\n",
      "99.99 percent). Thus our data-agnostic pruning algorithm challenges the\n",
      "existing paradigm that, at initialization, data must be used to quantify which\n",
      "synapses are important.\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.05543 \n",
      "Title :Machine Learning for Imaging Cherenkov Detectors\n",
      "  Imaging Cherenkov detectors are largely used in modern nuclear and particle\n",
      "physics experiments where cutting-edge solutions are needed to face always more\n",
      "growing computing demands. This is a fertile ground for AI-based approaches and\n",
      "at present we are witnessing the onset of new highly efficient and fast\n",
      "applications. This paper focuses on novel directions with applications to\n",
      "Cherenkov detectors. In particular, recent advances on detector design and\n",
      "calibration, as well as particle identification are presented.\n",
      "\n",
      "**Paper Id :1905.11825 \n",
      "Title :Fast Data-Driven Simulation of Cherenkov Detectors Using Generative\n",
      "  Adversarial Networks\n",
      "  The increasing luminosities of future Large Hadron Collider runs and next\n",
      "generation of collider experiments will require an unprecedented amount of\n",
      "simulated events to be produced. Such large scale productions are extremely\n",
      "demanding in terms of computing resources. Thus new approaches to event\n",
      "generation and simulation of detector responses are needed. In LHCb, the\n",
      "accurate simulation of Cherenkov detectors takes a sizeable fraction of CPU\n",
      "time. An alternative approach is described here, when one generates high-level\n",
      "reconstructed observables using a generative neural network to bypass low level\n",
      "details. This network is trained to reproduce the particle species likelihood\n",
      "function values based on the track kinematic parameters and detector occupancy.\n",
      "The fast simulation is trained using real data samples collected by LHCb during\n",
      "run 2. We demonstrate that this approach provides high-fidelity results.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.05832 \n",
      "Title :Adaptive Reinforcement Learning through Evolving Self-Modifying Neural\n",
      "  Networks\n",
      "  The adaptive learning capabilities seen in biological neural networks are\n",
      "largely a product of the self-modifying behavior emerging from online plastic\n",
      "changes in synaptic connectivity. Current methods in Reinforcement Learning\n",
      "(RL) only adjust to new interactions after reflection over a specified time\n",
      "interval, preventing the emergence of online adaptivity. Recent work addressing\n",
      "this by endowing artificial neural networks with neuromodulated plasticity have\n",
      "been shown to improve performance on simple RL tasks trained using\n",
      "backpropagation, but have yet to scale up to larger problems. Here we study the\n",
      "problem of meta-learning in a challenging quadruped domain, where each leg of\n",
      "the quadruped has a chance of becoming unusable, requiring the agent to adapt\n",
      "by continuing locomotion with the remaining limbs. Results demonstrate that\n",
      "agents evolved using self-modifying plastic networks are more capable of\n",
      "adapting to complex meta-learning learning tasks, even outperforming the same\n",
      "network updated using gradient-based algorithms while taking less time to\n",
      "train.\n",
      "\n",
      "**Paper Id :2001.01982 \n",
      "Title :Intrinsic Motivation and Episodic Memories for Robot Exploration of\n",
      "  High-Dimensional Sensory Spaces\n",
      "  This work presents an architecture that generates curiosity-driven\n",
      "goal-directed exploration behaviours for an image sensor of a microfarming\n",
      "robot. A combination of deep neural networks for offline unsupervised learning\n",
      "of low-dimensional features from images, and of online learning of shallow\n",
      "neural networks representing the inverse and forward kinematics of the system\n",
      "have been used. The artificial curiosity system assigns interest values to a\n",
      "set of pre-defined goals, and drives the exploration towards those that are\n",
      "expected to maximise the learning progress. We propose the integration of an\n",
      "episodic memory in intrinsic motivation systems to face catastrophic forgetting\n",
      "issues, typically experienced when performing online updates of artificial\n",
      "neural networks. Our results show that adopting an episodic memory system not\n",
      "only prevents the computational models from quickly forgetting knowledge that\n",
      "has been previously acquired, but also provides new avenues for modulating the\n",
      "balance between plasticity and stability of the models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.06922 \n",
      "Title :Incorporating User Micro-behaviors and Item Knowledge into Multi-task\n",
      "  Learning for Session-based Recommendation\n",
      "  Session-based recommendation (SR) has become an important and popular\n",
      "component of various e-commerce platforms, which aims to predict the next\n",
      "interacted item based on a given session. Most of existing SR models only focus\n",
      "on exploiting the consecutive items in a session interacted by a certain user,\n",
      "to capture the transition pattern among the items. Although some of them have\n",
      "been proven effective, the following two insights are often neglected. First, a\n",
      "user's micro-behaviors, such as the manner in which the user locates an item,\n",
      "the activities that the user commits on an item (e.g., reading comments, adding\n",
      "to cart), offer fine-grained and deep understanding of the user's preference.\n",
      "Second, the item attributes, also known as item knowledge, provide side\n",
      "information to model the transition pattern among interacted items and\n",
      "alleviate the data sparsity problem. These insights motivate us to propose a\n",
      "novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors\n",
      "and item Knowledge into Multi-task learning for Session-based Recommendation.\n",
      "Specifically, a given session is modeled on micro-behavior level in MKM-SR,\n",
      "i.e., with a sequence of item-operation pairs rather than a sequence of items,\n",
      "to capture the transition pattern in the session sufficiently. Furthermore, we\n",
      "propose a multi-task learning paradigm to involve learning knowledge embeddings\n",
      "which plays a role as an auxiliary task to promote the major task of SR. It\n",
      "enables our model to obtain better session representations, resulting in more\n",
      "precise SR recommendation results. The extensive evaluations on two benchmark\n",
      "datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models,\n",
      "justifying the strategy of incorporating knowledge learning.\n",
      "\n",
      "**Paper Id :2006.10233 \n",
      "Title :A Knowledge-Enhanced Recommendation Model with Attribute-Level\n",
      "  Co-Attention\n",
      "  Deep neural networks (DNNs) have been widely employed in recommender systems\n",
      "including incorporating attention mechanism for performance improvement.\n",
      "However, most of existing attention-based models only apply item-level\n",
      "attention on user side, restricting the further enhancement of recommendation\n",
      "performance. In this paper, we propose a knowledge-enhanced recommendation\n",
      "model ACAM, which incorporates item attributes distilled from knowledge graphs\n",
      "(KGs) as side information, and is built with a co-attention mechanism on\n",
      "attribute-level to achieve performance gains. Specifically, each user and item\n",
      "in ACAM are represented by a set of attribute embeddings at first. Then, user\n",
      "representations and item representations are augmented simultaneously through\n",
      "capturing the correlations between different attributes by a co-attention\n",
      "module. Our extensive experiments over two realistic datasets show that the\n",
      "user representations and item representations augmented by attribute-level\n",
      "co-attention gain ACAM's superiority over the state-of-the-art deep models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.07686 \n",
      "Title :Predictive modeling approaches in laser-based material processing\n",
      "  Predictive modelling represents an emerging field that combines existing and\n",
      "novel methodologies aimed to rapidly understand physical mechanisms and\n",
      "concurrently develop new materials, processes and structures. In the current\n",
      "study, previously-unexplored predictive modelling in a key-enabled technology,\n",
      "the laser-based manufacturing, aims to automate and forecast the effect of\n",
      "laser processing on material structures. The focus is centred on the\n",
      "performance of representative statistical and machine learning algorithms in\n",
      "predicting the outcome of laser processing on a range of materials. Results on\n",
      "experimental data showed that predictive models were able to satisfactorily\n",
      "learn the mapping between the laser input variables and the observed material\n",
      "structure. These results are further integrated with simulation data aiming to\n",
      "elucidate the multiscale physical processes upon laser-material interaction. As\n",
      "a consequence, we augmented the adjusted simulated data to the experimental and\n",
      "substantially improved the predictive performance, due to the availability of\n",
      "increased number of sampling points. In parallel, a metric to identify and\n",
      "quantify the regions with high predictive uncertainty, is presented, revealing\n",
      "that high uncertainty occurs around the transition boundaries. Our results can\n",
      "set the basis for a systematic methodology towards reducing material design,\n",
      "testing and production cost via the replacement of expensive trial-and-error\n",
      "based manufacturing procedure with a precise pre-fabrication predictive tool.\n",
      "\n",
      "**Paper Id :2002.10986 \n",
      "Title :A Deep Learning Framework for Simulation and Defect Prediction Applied\n",
      "  in Microelectronics\n",
      "  The prediction of upcoming events in industrial processes has been a\n",
      "long-standing research goal since it enables optimization of manufacturing\n",
      "parameters, planning of equipment maintenance and more importantly prediction\n",
      "and eventually prevention of defects. While existing approaches have\n",
      "accomplished substantial progress, they are mostly limited to processing of one\n",
      "dimensional signals or require parameter tuning to model environmental\n",
      "parameters. In this paper, we propose an alternative approach based on deep\n",
      "neural networks that simulates changes in the 3D structure of a monitored\n",
      "object in a batch based on previous 3D measurements. In particular, we propose\n",
      "an architecture based on 3D Convolutional Neural Networks (3DCNN) in order to\n",
      "model the geometric variations in manufacturing parameters and predict upcoming\n",
      "events related to sub-optimal performance. We validate our framework on a\n",
      "microelectronics use-case using the recently published PCB scans dataset where\n",
      "we simulate changes on the shape and volume of glue deposited on an Liquid\n",
      "Crystal Polymer (LCP) substrate before the attachment of integrated circuits\n",
      "(IC). Experimental evaluation examines the impact of different choices in the\n",
      "cost function during training and shows that the proposed method can be\n",
      "efficiently used for defect prediction.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.08122 \n",
      "Title :Classification and Recognition of Encrypted EEG Data Neural Network\n",
      "  With the rapid development of Machine Learning technology applied in\n",
      "electroencephalography (EEG) signals, Brain-Computer Interface (BCI) has\n",
      "emerged as a novel and convenient human-computer interaction for smart home,\n",
      "intelligent medical and other Internet of Things (IoT) scenarios. However,\n",
      "security issues such as sensitive information disclosure and unauthorized\n",
      "operations have not received sufficient concerns. There are still some defects\n",
      "with the existing solutions to encrypted EEG data such as low accuracy, high\n",
      "time complexity or slow processing speed. For this reason, a classification and\n",
      "recognition method of encrypted EEG data based on neural network is proposed,\n",
      "which adopts Paillier encryption algorithm to encrypt EEG data and meanwhile\n",
      "resolves the problem of floating point operations. In addition, it improves\n",
      "traditional feed-forward neural network (FNN) by using the approximate function\n",
      "instead of activation function and realizes multi-classification of encrypted\n",
      "EEG data. Extensive experiments are conducted to explore the effect of several\n",
      "metrics (such as the hidden neuron size and the learning rate updated by\n",
      "improved simulated annealing algorithm) on the recognition results. Followed by\n",
      "security and time cost analysis, the proposed model and approach are validated\n",
      "and evaluated on public EEG datasets provided by PhysioNet, BCI Competition IV\n",
      "and EPILEPSIAE. The experimental results show that our proposal has the\n",
      "satisfactory accuracy, efficiency and feasibility compared with other\n",
      "solutions.\n",
      "\n",
      "**Paper Id :2002.09821 \n",
      "Title :A Multi-view CNN-based Acoustic Classification System for Automatic\n",
      "  Animal Species Identification\n",
      "  Automatic identification of animal species by their vocalization is an\n",
      "important and challenging task. Although many kinds of audio monitoring system\n",
      "have been proposed in the literature, they suffer from several disadvantages\n",
      "such as non-trivial feature selection, accuracy degradation because of\n",
      "environmental noise or intensive local computation. In this paper, we propose a\n",
      "deep learning based acoustic classification framework for Wireless Acoustic\n",
      "Sensor Network (WASN). The proposed framework is based on cloud architecture\n",
      "which relaxes the computational burden on the wireless sensor node. To improve\n",
      "the recognition accuracy, we design a multi-view Convolution Neural Network\n",
      "(CNN) to extract the short-, middle-, and long-term dependencies in parallel.\n",
      "The evaluation on two real datasets shows that the proposed architecture can\n",
      "achieve high accuracy and outperforms traditional classification systems\n",
      "significantly when the environmental noise dominate the audio signal (low SNR).\n",
      "Moreover, we implement and deploy the proposed system on a testbed and analyse\n",
      "the system performance in real-world environments. Both simulation and\n",
      "real-world evaluation demonstrate the accuracy and robustness of the proposed\n",
      "acoustic classification system in distinguishing species of animals.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.08347 \n",
      "Title :Application of Fuzzy Rule based System for Highway Research Board\n",
      "  Classification of Soils\n",
      "  Fuzzy rule-based model is a powerful tool for imitating the human way of\n",
      "thinking and solving uncertainty-related problems as it allows for\n",
      "understandable and interpretable rule bases. The objective of this paper is to\n",
      "study the applicability of fuzzy rule-based modelling to quantify soil\n",
      "classification for engineering purposes by qualitatively considering soil index\n",
      "properties. The classification system of the Highway Research Board is\n",
      "considered to illustrate a fuzzy rule-based model. The soil's index properties\n",
      "are fuzzified using triangular functions, and the fuzzy membership values are\n",
      "calculated. Fuzzy arithmetical operators are then applied to the membership\n",
      "values obtained for classification. Fuzzy decision tree classification\n",
      "algorithm is used to derive fuzzy if-then rules to quantify qualitative soil\n",
      "classification. The proposed system is implemented in MATLAB. The results\n",
      "obtained are checked and the implementation of the proposed model is measured\n",
      "against the outcomes of the laboratory tests.\n",
      "\n",
      "**Paper Id :1804.10168 \n",
      "Title :BEST : A decision tree algorithm that handles missing values\n",
      "  The main contribution of this paper is the development of a new decision tree\n",
      "algorithm. The proposed approach allows users to guide the algorithm through\n",
      "the data partitioning process. We believe this feature has many applications\n",
      "but in this paper we demonstrate how to utilize this algorithm to analyse data\n",
      "sets containing missing values. We tested our algorithm against simulated data\n",
      "sets with various missing data structures and a real data set. The results\n",
      "demonstrate that this new classification procedure efficiently handles missing\n",
      "values and produces results that are slightly more accurate and more\n",
      "interpretable than most common procedures without any imputations or\n",
      "pre-processing.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.09016 \n",
      "Title :Acoustic prediction of flowrate: varying liquid jet stream onto a free\n",
      "  surface\n",
      "  Information on liquid jet stream flow is crucial in many real world\n",
      "applications. In a large number of cases, these flows fall directly onto free\n",
      "surfaces (e.g. pools), creating a splash with accompanying splashing sounds.\n",
      "The sound produced is supplied by energy interactions between the liquid jet\n",
      "stream and the passive free surface. In this investigation, we collect the\n",
      "sound of a water jet of varying flowrate falling into a pool of water, and use\n",
      "this sound to predict the flowrate and flowrate trajectory involved. Two\n",
      "approaches are employed: one uses machine-learning models trained using audio\n",
      "features extracted from the collected sound to predict the flowrate (and\n",
      "subsequently the flowrate trajectory). In contrast, the second method directly\n",
      "uses acoustic parameters related to the spectral energy of the liquid-liquid\n",
      "interaction to estimate the flowrate trajectory. The actual flowrate, however,\n",
      "is determined directly using a gravimetric method: tracking the change in mass\n",
      "of the pooling liquid over time. We show here that the two methods agree well\n",
      "with the actual flowrate and offer comparable performance in accurately\n",
      "predicting the flowrate trajectory, and accordingly offer insights for\n",
      "potential real-life applications using sound.\n",
      "\n",
      "**Paper Id :2006.10111 \n",
      "Title :Pendant Drop Tensiometry: A Machine Learning Approach\n",
      "  Modern pendant drop tensiometry relies on numerical solution of the\n",
      "Young-Laplace equation and allow to determine the surface tension from a single\n",
      "picture of a pendant drop with high precision. Most of these techniques solve\n",
      "the Young-Laplace equation many times over to find the material parameters that\n",
      "provide a fit to a supplied image of a real droplet. Here we introduce a\n",
      "machine learning approach to solve this problem in a computationally more\n",
      "efficient way. We train a deep neural network to determine the surface tension\n",
      "of a given droplet shape using a large training set of numerically generated\n",
      "droplet shapes. We show that the deep learning approach is superior to the\n",
      "current state of the art shape fitting approach in speed and precision, in\n",
      "particular if shapes in the training set reflect the sensitivity of the droplet\n",
      "shape with respect to surface tension. In order to derive such an optimized\n",
      "training set we clarify the role of the Worthington number as quality indicator\n",
      "in conventional shape fitting and in the machine learning approach. Our\n",
      "approach demonstrates the capabilities of deep neural networks in the material\n",
      "parameter determination from rheological deformation experiments in general.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.09113 \n",
      "Title :Topological defects and confinement with machine learning: the case of\n",
      "  monopoles in compact electrodynamics\n",
      "  We investigate the advantages of machine learning techniques to recognize the\n",
      "dynamics of topological objects in quantum field theories. We consider the\n",
      "compact U(1) gauge theory in three spacetime dimensions as the simplest example\n",
      "of a theory that exhibits confinement and mass gap phenomena generated by\n",
      "monopoles. We train a neural network with a generated set of monopole\n",
      "configurations to distinguish between confinement and deconfinement phases,\n",
      "from which it is possible to determine the deconfinement transition point and\n",
      "to predict several observables. The model uses a supervised learning approach\n",
      "and treats the monopole configurations as three-dimensional images (holograms).\n",
      "We show that the model can determine the transition temperature with accuracy,\n",
      "which depends on the criteria implemented in the algorithm. More importantly,\n",
      "we train the neural network with configurations from a single lattice size\n",
      "before making predictions for configurations from other lattice sizes, from\n",
      "which a reliable estimation of the critical temperatures are obtained.\n",
      "\n",
      "**Paper Id :1810.08179 \n",
      "Title :Thermodynamics and Feature Extraction by Machine Learning\n",
      "  Machine learning methods are powerful in distinguishing different phases of\n",
      "matter in an automated way and provide a new perspective on the study of\n",
      "physical phenomena. We train a Restricted Boltzmann Machine (RBM) on data\n",
      "constructed with spin configurations sampled from the Ising Hamiltonian at\n",
      "different values of temperature and external magnetic field using Monte Carlo\n",
      "methods. From the trained machine we obtain the flow of iterative\n",
      "reconstruction of spin state configurations to faithfully reproduce the\n",
      "observables of the physical system. We find that the flow of the trained RBM\n",
      "approaches the spin configurations of the maximal possible specific heat which\n",
      "resemble the near criticality region of the Ising model. In the special case of\n",
      "the vanishing magnetic field the trained RBM converges to the critical point of\n",
      "the Renormalization Group (RG) flow of the lattice model. Our results suggest\n",
      "an alternative explanation of how the machine identifies the physical phase\n",
      "transitions, by recognizing certain properties of the configuration like the\n",
      "maximization of the specific heat, instead of associating directly the\n",
      "recognition procedure with the RG flow and its fixed points. Then from the\n",
      "reconstructed data we deduce the critical exponent associated to the\n",
      "magnetization to find satisfactory agreement with the actual physical value. We\n",
      "assume no prior knowledge about the criticality of the system and its\n",
      "Hamiltonian.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.09162 \n",
      "Title :Estimation of Video Streaming KQIs for Radio Access Negotiation in\n",
      "  Network Slicing Scenarios\n",
      "  The use of multimedia content has hugely increased in recent times, becoming\n",
      "one of the most important services for the users of mobile networks.\n",
      "Consequently, network operators struggle to optimize their infrastructure to\n",
      "support the best video service-provision. As an additional challenge, 5G\n",
      "introduces the concept of network slicing as a new paradigm that presents a\n",
      "completely different view of the network configuration and optimization. A main\n",
      "challenge of this scheme is to establish which specific resources would provide\n",
      "the necessary quality of service for the users using the slice. To address\n",
      "this, the present work presents a complete framework for this support of the\n",
      "slice negotiation process through the estimation of the provided Video\n",
      "Streaming Key Quality Indicators (KQIs), which are calculated from network\n",
      "low-layer configuration parameters and metrics. The proposed estimator is then\n",
      "evaluated in a real cellular scenario.\n",
      "\n",
      "**Paper Id :2008.11432 \n",
      "Title :Time-Aware Music Recommender Systems: Modeling the Evolution of Implicit\n",
      "  User Preferences and User Listening Habits in A Collaborative Filtering\n",
      "  Approach\n",
      "  Online streaming services have become the most popular way of listening to\n",
      "music. The majority of these services are endowed with recommendation\n",
      "mechanisms that help users to discover songs and artists that may interest them\n",
      "from the vast amount of music available. However, many are not reliable as they\n",
      "may not take into account contextual aspects or the ever-evolving user\n",
      "behavior. Therefore, it is necessary to develop systems that consider these\n",
      "aspects. In the field of music, time is one of the most important factors\n",
      "influencing user preferences and managing its effects, and is the motivation\n",
      "behind the work presented in this paper. Here, the temporal information\n",
      "regarding when songs are played is examined. The purpose is to model both the\n",
      "evolution of user preferences in the form of evolving implicit ratings and user\n",
      "listening behavior. In the collaborative filtering method proposed in this\n",
      "work, daily listening habits are captured in order to characterize users and\n",
      "provide them with more reliable recommendations. The results of the validation\n",
      "prove that this approach outperforms other methods in generating both\n",
      "context-aware and context-free recommendations\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.09526 \n",
      "Title :Cross-lingual Retrieval for Iterative Self-Supervised Training\n",
      "  Recent studies have demonstrated the cross-lingual alignment ability of\n",
      "multilingual pretrained language models. In this work, we found that the\n",
      "cross-lingual alignment can be further improved by training seq2seq models on\n",
      "sentence pairs mined using their own encoder outputs. We utilized these\n",
      "findings to develop a new approach -- cross-lingual retrieval for iterative\n",
      "self-supervised training (CRISS), where mining and training processes are\n",
      "applied iteratively, improving cross-lingual alignment and translation ability\n",
      "at the same time. Using this method, we achieved state-of-the-art unsupervised\n",
      "machine translation results on 9 language directions with an average\n",
      "improvement of 2.4 BLEU, and on the Tatoeba sentence retrieval task in the\n",
      "XTREME benchmark on 16 languages with an average improvement of 21.5% in\n",
      "absolute accuracy. Furthermore, CRISS also brings an additional 1.8 BLEU\n",
      "improvement on average compared to mBART, when finetuned on supervised machine\n",
      "translation downstream tasks.\n",
      "\n",
      "**Paper Id :1911.09812 \n",
      "Title :Zero-Resource Cross-Lingual Named Entity Recognition\n",
      "  Recently, neural methods have achieved state-of-the-art (SOTA) results in\n",
      "Named Entity Recognition (NER) tasks for many languages without the need for\n",
      "manually crafted features. However, these models still require manually\n",
      "annotated training data, which is not available for many languages. In this\n",
      "paper, we propose an unsupervised cross-lingual NER model that can transfer NER\n",
      "knowledge from one language to another in a completely unsupervised way without\n",
      "relying on any bilingual dictionary or parallel data. Our model achieves this\n",
      "through word-level adversarial learning and augmented fine-tuning with\n",
      "parameter sharing and feature augmentation. Experiments on five different\n",
      "languages demonstrate the effectiveness of our approach, outperforming existing\n",
      "models by a good margin and setting a new SOTA for each language pair.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.09833 \n",
      "Title :Generative Modelling for Controllable Audio Synthesis of Expressive\n",
      "  Piano Performance\n",
      "  We present a controllable neural audio synthesizer based on Gaussian Mixture\n",
      "Variational Autoencoders (GM-VAE), which can generate realistic piano\n",
      "performances in the audio domain that closely follows temporal conditions of\n",
      "two essential style features for piano performances: articulation and dynamics.\n",
      "We demonstrate how the model is able to apply fine-grained style morphing over\n",
      "the course of synthesizing the audio. This is based on conditions which are\n",
      "latent variables that can be sampled from the prior or inferred from other\n",
      "pieces. One of the envisioned use cases is to inspire creative and brand new\n",
      "interpretations for existing pieces of piano music.\n",
      "\n",
      "**Paper Id :2002.05511 \n",
      "Title :Deep Autotuner: a Pitch Correcting Network for Singing Performances\n",
      "  We introduce a data-driven approach to automatic pitch correction of solo\n",
      "singing performances. The proposed approach predicts note-wise pitch shifts\n",
      "from the relationship between the respective spectrograms of the singing and\n",
      "accompaniment. This approach differs from commercial systems, where vocal track\n",
      "notes are usually shifted to be centered around pitches in a user-defined\n",
      "score, or mapped to the closest pitch among the twelve equal-tempered scale\n",
      "degrees. The proposed system treats pitch as a continuous value rather than\n",
      "relying on a set of discretized notes found in musical scores, thus allowing\n",
      "for improvisation and harmonization in the singing performance. We train our\n",
      "neural network model using a dataset of 4,702 amateur karaoke performances\n",
      "selected for good intonation. Our model is trained on both incorrect\n",
      "intonation, for which it learns a correction, and intentional pitch variation,\n",
      "which it learns to preserve. The proposed deep neural network with gated\n",
      "recurrent units on top of convolutional layers shows promising performance on\n",
      "the real-world score-free singing pitch correction task of autotuning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.10111 \n",
      "Title :Pendant Drop Tensiometry: A Machine Learning Approach\n",
      "  Modern pendant drop tensiometry relies on numerical solution of the\n",
      "Young-Laplace equation and allow to determine the surface tension from a single\n",
      "picture of a pendant drop with high precision. Most of these techniques solve\n",
      "the Young-Laplace equation many times over to find the material parameters that\n",
      "provide a fit to a supplied image of a real droplet. Here we introduce a\n",
      "machine learning approach to solve this problem in a computationally more\n",
      "efficient way. We train a deep neural network to determine the surface tension\n",
      "of a given droplet shape using a large training set of numerically generated\n",
      "droplet shapes. We show that the deep learning approach is superior to the\n",
      "current state of the art shape fitting approach in speed and precision, in\n",
      "particular if shapes in the training set reflect the sensitivity of the droplet\n",
      "shape with respect to surface tension. In order to derive such an optimized\n",
      "training set we clarify the role of the Worthington number as quality indicator\n",
      "in conventional shape fitting and in the machine learning approach. Our\n",
      "approach demonstrates the capabilities of deep neural networks in the material\n",
      "parameter determination from rheological deformation experiments in general.\n",
      "\n",
      "**Paper Id :2002.10986 \n",
      "Title :A Deep Learning Framework for Simulation and Defect Prediction Applied\n",
      "  in Microelectronics\n",
      "  The prediction of upcoming events in industrial processes has been a\n",
      "long-standing research goal since it enables optimization of manufacturing\n",
      "parameters, planning of equipment maintenance and more importantly prediction\n",
      "and eventually prevention of defects. While existing approaches have\n",
      "accomplished substantial progress, they are mostly limited to processing of one\n",
      "dimensional signals or require parameter tuning to model environmental\n",
      "parameters. In this paper, we propose an alternative approach based on deep\n",
      "neural networks that simulates changes in the 3D structure of a monitored\n",
      "object in a batch based on previous 3D measurements. In particular, we propose\n",
      "an architecture based on 3D Convolutional Neural Networks (3DCNN) in order to\n",
      "model the geometric variations in manufacturing parameters and predict upcoming\n",
      "events related to sub-optimal performance. We validate our framework on a\n",
      "microelectronics use-case using the recently published PCB scans dataset where\n",
      "we simulate changes on the shape and volume of glue deposited on an Liquid\n",
      "Crystal Polymer (LCP) substrate before the attachment of integrated circuits\n",
      "(IC). Experimental evaluation examines the impact of different choices in the\n",
      "cost function during training and shows that the proposed method can be\n",
      "efficiently used for defect prediction.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.10112 \n",
      "Title :Interface learning of multiphysics and multiscale systems\n",
      "  Complex natural or engineered systems comprise multiple characteristic\n",
      "scales, multiple spatiotemporal domains, and even multiple physical closure\n",
      "laws. To address such challenges, we introduce an interface learning paradigm\n",
      "and put forth a data-driven closure approach based on memory embedding to\n",
      "provide physically correct boundary conditions at the interface. To enable the\n",
      "interface learning for hyperbolic systems by considering the domain of\n",
      "influence and wave structures into account, we put forth the concept of upwind\n",
      "learning towards a physics-informed domain decomposition. The promise of the\n",
      "proposed approach is shown for a set of canonical illustrative problems. We\n",
      "highlight that high-performance computing environments can benefit from this\n",
      "methodology to reduce communication costs among processing units in emerging\n",
      "machine learning ready heterogeneous platforms toward exascale era.\n",
      "\n",
      "**Paper Id :2007.02168 \n",
      "Title :Scalable Differentiable Physics for Learning and Control\n",
      "  Differentiable physics is a powerful approach to learning and control\n",
      "problems that involve physical objects and environments. While notable progress\n",
      "has been made, the capabilities of differentiable physics solvers remain\n",
      "limited. We develop a scalable framework for differentiable physics that can\n",
      "support a large number of objects and their interactions. To accommodate\n",
      "objects with arbitrary geometry and topology, we adopt meshes as our\n",
      "representation and leverage the sparsity of contacts for scalable\n",
      "differentiable collision handling. Collisions are resolved in localized regions\n",
      "to minimize the number of optimization variables even when the number of\n",
      "simulated objects is high. We further accelerate implicit differentiation of\n",
      "optimization with nonlinear constraints. Experiments demonstrate that the\n",
      "presented framework requires up to two orders of magnitude less memory and\n",
      "computation in comparison to recent particle-based methods. We further validate\n",
      "the approach on inverse problems and control scenarios, where it outperforms\n",
      "derivative-free and model-free baselines by at least an order of magnitude.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.10160 \n",
      "Title :Mat\\'ern Gaussian processes on Riemannian manifolds\n",
      "  Gaussian processes are an effective model class for learning unknown\n",
      "functions, particularly in settings where accurately representing predictive\n",
      "uncertainty is of key importance. Motivated by applications in the physical\n",
      "sciences, the widely-used Mat\\'ern class of Gaussian processes has recently\n",
      "been generalized to model functions whose domains are Riemannian manifolds, by\n",
      "re-expressing said processes as solutions of stochastic partial differential\n",
      "equations. In this work, we propose techniques for computing the kernels of\n",
      "these processes on compact Riemannian manifolds via spectral theory of the\n",
      "Laplace-Beltrami operator in a fully constructive manner, thereby allowing them\n",
      "to be trained via standard scalable techniques such as inducing point methods.\n",
      "We also extend the generalization from the Mat\\'ern to the widely-used squared\n",
      "exponential Gaussian process. By allowing Riemannian Mat\\'ern Gaussian\n",
      "processes to be trained using well-understood techniques, our work enables\n",
      "their use in mini-batch, online, and non-conjugate settings, and makes them\n",
      "more accessible to machine learning practitioners.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.10233 \n",
      "Title :A Knowledge-Enhanced Recommendation Model with Attribute-Level\n",
      "  Co-Attention\n",
      "  Deep neural networks (DNNs) have been widely employed in recommender systems\n",
      "including incorporating attention mechanism for performance improvement.\n",
      "However, most of existing attention-based models only apply item-level\n",
      "attention on user side, restricting the further enhancement of recommendation\n",
      "performance. In this paper, we propose a knowledge-enhanced recommendation\n",
      "model ACAM, which incorporates item attributes distilled from knowledge graphs\n",
      "(KGs) as side information, and is built with a co-attention mechanism on\n",
      "attribute-level to achieve performance gains. Specifically, each user and item\n",
      "in ACAM are represented by a set of attribute embeddings at first. Then, user\n",
      "representations and item representations are augmented simultaneously through\n",
      "capturing the correlations between different attributes by a co-attention\n",
      "module. Our extensive experiments over two realistic datasets show that the\n",
      "user representations and item representations augmented by attribute-level\n",
      "co-attention gain ACAM's superiority over the state-of-the-art deep models.\n",
      "\n",
      "**Paper Id :2006.06922 \n",
      "Title :Incorporating User Micro-behaviors and Item Knowledge into Multi-task\n",
      "  Learning for Session-based Recommendation\n",
      "  Session-based recommendation (SR) has become an important and popular\n",
      "component of various e-commerce platforms, which aims to predict the next\n",
      "interacted item based on a given session. Most of existing SR models only focus\n",
      "on exploiting the consecutive items in a session interacted by a certain user,\n",
      "to capture the transition pattern among the items. Although some of them have\n",
      "been proven effective, the following two insights are often neglected. First, a\n",
      "user's micro-behaviors, such as the manner in which the user locates an item,\n",
      "the activities that the user commits on an item (e.g., reading comments, adding\n",
      "to cart), offer fine-grained and deep understanding of the user's preference.\n",
      "Second, the item attributes, also known as item knowledge, provide side\n",
      "information to model the transition pattern among interacted items and\n",
      "alleviate the data sparsity problem. These insights motivate us to propose a\n",
      "novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors\n",
      "and item Knowledge into Multi-task learning for Session-based Recommendation.\n",
      "Specifically, a given session is modeled on micro-behavior level in MKM-SR,\n",
      "i.e., with a sequence of item-operation pairs rather than a sequence of items,\n",
      "to capture the transition pattern in the session sufficiently. Furthermore, we\n",
      "propose a multi-task learning paradigm to involve learning knowledge embeddings\n",
      "which plays a role as an auxiliary task to promote the major task of SR. It\n",
      "enables our model to obtain better session representations, resulting in more\n",
      "precise SR recommendation results. The extensive evaluations on two benchmark\n",
      "datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models,\n",
      "justifying the strategy of incorporating knowledge learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.10299 \n",
      "Title :A quantum extension of SVM-perf for training nonlinear SVMs in almost\n",
      "  linear time\n",
      "  We propose a quantum algorithm for training nonlinear support vector machines\n",
      "(SVM) for feature space learning where classical input data is encoded in the\n",
      "amplitudes of quantum states. Based on the classical SVM-perf algorithm of\n",
      "Joachims, our algorithm has a running time which scales linearly in the number\n",
      "of training examples $m$ (up to polylogarithmic factors) and applies to the\n",
      "standard soft-margin $\\ell_1$-SVM model. In contrast, while classical SVM-perf\n",
      "has demonstrated impressive performance on both linear and nonlinear SVMs, its\n",
      "efficiency is guaranteed only in certain cases: it achieves linear $m$ scaling\n",
      "only for linear SVMs, where classification is performed in the original input\n",
      "data space, or for the special cases of low-rank or shift-invariant kernels.\n",
      "Similarly, previously proposed quantum algorithms either have super-linear\n",
      "scaling in $m$, or else apply to different SVM models such as the hard-margin\n",
      "or least squares $\\ell_2$-SVM which lack certain desirable properties of the\n",
      "soft-margin $\\ell_1$-SVM model. We classically simulate our algorithm and give\n",
      "evidence that it can perform well in practice, and not only for asymptotically\n",
      "large data sets.\n",
      "\n",
      "**Paper Id :1805.08837 \n",
      "Title :Quantum classification of the MNIST dataset with Slow Feature Analysis\n",
      "  Quantum machine learning carries the promise to revolutionize information and\n",
      "communication technologies. While a number of quantum algorithms with potential\n",
      "exponential speedups have been proposed already, it is quite difficult to\n",
      "provide convincing evidence that quantum computers with quantum memories will\n",
      "be in fact useful to solve real-world problems. Our work makes considerable\n",
      "progress towards this goal.\n",
      "  We design quantum techniques for Dimensionality Reduction and for\n",
      "Classification, and combine them to provide an efficient and high accuracy\n",
      "quantum classifier that we test on the MNIST dataset. More precisely, we\n",
      "propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality\n",
      "reduction technique that maps the dataset in a lower dimensional space where we\n",
      "can apply a novel quantum classification procedure, the Quantum Frobenius\n",
      "Distance (QFD). We simulate the quantum classifier (including errors) and show\n",
      "that it can provide classification of the MNIST handwritten digit dataset, a\n",
      "widely used dataset for benchmarking classification algorithms, with $98.5\\%$\n",
      "accuracy, similar to the classical case. The running time of the quantum\n",
      "classifier is polylogarithmic in the dimension and number of data points. We\n",
      "also provide evidence that the other parameters on which the running time\n",
      "depends (condition number, Frobenius norm, error threshold, etc.) scale\n",
      "favorably in practice, thus ascertaining the efficiency of our algorithm.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.10560 \n",
      "Title :Gradient Amplification: An efficient way to train deep neural networks\n",
      "  Improving performance of deep learning models and reducing their training\n",
      "times are ongoing challenges in deep neural networks. There are several\n",
      "approaches proposed to address these challenges one of which is to increase the\n",
      "depth of the neural networks. Such deeper networks not only increase training\n",
      "times, but also suffer from vanishing gradients problem while training. In this\n",
      "work, we propose gradient amplification approach for training deep learning\n",
      "models to prevent vanishing gradients and also develop a training strategy to\n",
      "enable or disable gradient amplification method across several epochs with\n",
      "different learning rates. We perform experiments on VGG-19 and resnet\n",
      "(Resnet-18 and Resnet-34) models, and study the impact of amplification\n",
      "parameters on these models in detail. Our proposed approach improves\n",
      "performance of these deep learning models even at higher learning rates,\n",
      "thereby allowing these models to achieve higher performance with reduced\n",
      "training time.\n",
      "\n",
      "**Paper Id :1910.09798 \n",
      "Title :Improving Siamese Networks for One Shot Learning using Kernel Based\n",
      "  Activation functions\n",
      "  The lack of a large amount of training data has always been the constraining\n",
      "factor in solving a lot of problems in machine learning, making One Shot\n",
      "Learning one of the most intriguing ideas in machine learning. It aims to learn\n",
      "information about object categories from one, or only a few training examples.\n",
      "This process of learning in deep learning is usually accomplished by proper\n",
      "objective function, i.e; loss function and embeddings extraction i.e;\n",
      "architecture. In this paper, we discussed about metrics based deep learning\n",
      "architectures for one shot learning such as Siamese neural networks and present\n",
      "a method to improve on their accuracy using Kafnets (kernel-based\n",
      "non-parametric activation functions for neural networks) by learning proper\n",
      "embeddings with relatively less number of epochs. Using kernel activation\n",
      "functions, we are able to achieve strong results which exceed those of ReLU\n",
      "based deep learning models in terms of embeddings structure, loss convergence,\n",
      "and accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.11161 \n",
      "Title :iSeeBetter: Spatio-temporal video super-resolution using recurrent\n",
      "  generative back-projection networks\n",
      "  Recently, learning-based models have enhanced the performance of single-image\n",
      "super-resolution (SISR). However, applying SISR successively to each video\n",
      "frame leads to a lack of temporal coherency. Convolutional neural networks\n",
      "(CNNs) outperform traditional approaches in terms of image quality metrics such\n",
      "as peak signal to noise ratio (PSNR) and structural similarity (SSIM). However,\n",
      "generative adversarial networks (GANs) offer a competitive advantage by being\n",
      "able to mitigate the issue of a lack of finer texture details, usually seen\n",
      "with CNNs when super-resolving at large upscaling factors. We present\n",
      "iSeeBetter, a novel GAN-based spatio-temporal approach to video\n",
      "super-resolution (VSR) that renders temporally consistent super-resolution\n",
      "videos. iSeeBetter extracts spatial and temporal information from the current\n",
      "and neighboring frames using the concept of recurrent back-projection networks\n",
      "as its generator. Furthermore, to improve the \"naturality\" of the\n",
      "super-resolved image while eliminating artifacts seen with traditional\n",
      "algorithms, we utilize the discriminator from super-resolution generative\n",
      "adversarial network (SRGAN). Although mean squared error (MSE) as a primary\n",
      "loss-minimization objective improves PSNR/SSIM, these metrics may not capture\n",
      "fine details in the image resulting in misrepresentation of perceptual quality.\n",
      "To address this, we use a four-fold (MSE, perceptual, adversarial, and\n",
      "total-variation (TV)) loss function. Our results demonstrate that iSeeBetter\n",
      "offers superior VSR fidelity and surpasses state-of-the-art performance.\n",
      "\n",
      "**Paper Id :2003.09148 \n",
      "Title :Event-based Asynchronous Sparse Convolutional Networks\n",
      "  Event cameras are bio-inspired sensors that respond to per-pixel brightness\n",
      "changes in the form of asynchronous and sparse \"events\". Recently, pattern\n",
      "recognition algorithms, such as learning-based methods, have made significant\n",
      "progress with event cameras by converting events into synchronous dense,\n",
      "image-like representations and applying traditional machine learning methods\n",
      "developed for standard cameras. However, these approaches discard the spatial\n",
      "and temporal sparsity inherent in event data at the cost of higher\n",
      "computational complexity and latency. In this work, we present a general\n",
      "framework for converting models trained on synchronous image-like event\n",
      "representations into asynchronous models with identical output, thus directly\n",
      "leveraging the intrinsic asynchronous and sparse nature of the event data. We\n",
      "show both theoretically and experimentally that this drastically reduces the\n",
      "computational complexity and latency of high-capacity, synchronous neural\n",
      "networks without sacrificing accuracy. In addition, our framework has several\n",
      "desirable characteristics: (i) it exploits spatio-temporal sparsity of events\n",
      "explicitly, (ii) it is agnostic to the event representation, network\n",
      "architecture, and task, and (iii) it does not require any train-time change,\n",
      "since it is compatible with the standard neural networks' training process. We\n",
      "thoroughly validate the proposed framework on two computer vision tasks: object\n",
      "detection and object recognition. In these tasks, we reduce the computational\n",
      "complexity up to 20 times with respect to high-latency neural networks. At the\n",
      "same time, we outperform state-of-the-art asynchronous approaches up to 24% in\n",
      "prediction accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.11223 \n",
      "Title :Unified Representation Learning for Efficient Medical Image Analysis\n",
      "  Medical image analysis typically includes several tasks such as image\n",
      "enhancement, detection, segmentation, and classification. These tasks are often\n",
      "implemented through separate machine learning methods, or recently through deep\n",
      "learning methods. We propose a novel multitask deep learning-based approach,\n",
      "called unified representation (U-Rep), that can be used to simultaneously\n",
      "perform several medical image analysis tasks. U-Rep is modality-specific and\n",
      "takes into consideration inter-task relationships. The proposed U-Rep can be\n",
      "trained using unlabeled data or limited amounts of labeled data. The trained\n",
      "U-Rep is then shared to simultaneously learn key tasks in medical image\n",
      "analysis, such as segmentation, classification and visual assessment. We also\n",
      "show that pre-processing operations, such as noise reduction and image\n",
      "enhancement, can be learned while constructing U-Rep. Our experimental results,\n",
      "on two medical image datasets, show that U-Rep improves generalization, and\n",
      "decreases resource utilization and training time while preventing unnecessary\n",
      "repetitions of building task-specific models in isolation. We believe that the\n",
      "proposed method (U-Rep) would tread a path toward promising future research in\n",
      "medical image analysis, especially for tasks with unlabeled data or limited\n",
      "amounts of labeled data.\n",
      "\n",
      "**Paper Id :1907.03063 \n",
      "Title :MRI Super-Resolution with Ensemble Learning and Complementary Priors\n",
      "  Magnetic resonance imaging (MRI) is a widely used medical imaging modality.\n",
      "However, due to the limitations in hardware, scan time, and throughput, it is\n",
      "often clinically challenging to obtain high-quality MR images. The\n",
      "super-resolution approach is potentially promising to improve MR image quality\n",
      "without any hardware upgrade. In this paper, we propose an ensemble learning\n",
      "and deep learning framework for MR image super-resolution. In our study, we\n",
      "first enlarged low resolution images using 5 commonly used super-resolution\n",
      "algorithms and obtained differentially enlarged image datasets with\n",
      "complementary priors. Then, a generative adversarial network (GAN) is trained\n",
      "with each dataset to generate super-resolution MR images. Finally, a\n",
      "convolutional neural network is used for ensemble learning that synergizes the\n",
      "outputs of GANs into the final MR super-resolution images. According to our\n",
      "results, the ensemble learning results outcome any one of GAN outputs. Compared\n",
      "with some state-of-the-art deep learning-based super-resolution methods, our\n",
      "approach is advantageous in suppressing artifacts and keeping more image\n",
      "details.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.11372 \n",
      "Title :Learning Tversky Similarity\n",
      "  In this paper, we advocate Tversky's ratio model as an appropriate basis for\n",
      "computational approaches to semantic similarity, that is, the comparison of\n",
      "objects such as images in a semantically meaningful way. We consider the\n",
      "problem of learning Tversky similarity measures from suitable training data\n",
      "indicating whether two objects tend to be similar or dissimilar.\n",
      "Experimentally, we evaluate our approach to similarity learning on two image\n",
      "datasets, showing that is performs very well compared to existing methods.\n",
      "\n",
      "**Paper Id :2010.02458 \n",
      "Title :Identifying Spurious Correlations for Robust Text Classification\n",
      "  The predictions of text classifiers are often driven by spurious correlations\n",
      "-- e.g., the term `Spielberg' correlates with positively reviewed movies, even\n",
      "though the term itself does not semantically convey a positive sentiment. In\n",
      "this paper, we propose a method to distinguish spurious and genuine\n",
      "correlations in text classification. We treat this as a supervised\n",
      "classification problem, using features derived from treatment effect estimators\n",
      "to distinguish spurious correlations from \"genuine\" ones. Due to the generic\n",
      "nature of these features and their small dimensionality, we find that the\n",
      "approach works well even with limited training examples, and that it is\n",
      "possible to transport the word classifier to new domains. Experiments on four\n",
      "datasets (sentiment classification and toxicity detection) suggest that using\n",
      "this approach to inform feature selection also leads to more robust\n",
      "classification, as measured by improved worst-case accuracy on the samples\n",
      "affected by spurious correlations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.11555 \n",
      "Title :A deep convolutional neural network model for rapid prediction of\n",
      "  fluvial flood inundation\n",
      "  Most of the two-dimensional (2D) hydraulic/hydrodynamic models are still\n",
      "computationally too demanding for real-time applications. In this paper, an\n",
      "innovative modelling approach based on a deep convolutional neural network\n",
      "(CNN) method is presented for rapid prediction of fluvial flood inundation. The\n",
      "CNN model is trained using outputs from a 2D hydraulic model (i.e. LISFLOOD-FP)\n",
      "to predict water depths. The pre-trained model is then applied to simulate the\n",
      "January 2005 and December 2015 floods in Carlisle, UK. The CNN predictions are\n",
      "compared favourably with the outputs produced by LISFLOOD-FP. The performance\n",
      "of the CNN model is further confirmed by benchmarking against a support vector\n",
      "regression (SVR) method. The results show that the CNN model outperforms SVR by\n",
      "a large margin. The CNN model is highly accurate in capturing flooded cells as\n",
      "indicated by several quantitative assessment matrices. The estimated error for\n",
      "reproducing maximum flood depth is 0 ~ 0.2 meters for the 2005 event and 0 ~\n",
      "0.5 meters for the 2015 event at over 99% of the cells covering the\n",
      "computational domain. The proposed CNN method offers great potential for\n",
      "real-time flood modelling/forecasting considering its simplicity, superior\n",
      "performance and computational efficiency.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.11873 \n",
      "Title :An Opportunistic Bandit Approach for User Interface Experimentation\n",
      "  Facing growing competition from online rivals, the retail industry is\n",
      "increasingly investing in their online shopping platforms to win the high-stake\n",
      "battle of customer' loyalty. User experience is playing an essential role in\n",
      "this competition, and retailers are continuously experimenting and optimizing\n",
      "their user interface for better user experience. The cost of experimentation is\n",
      "dominated by the opportunity cost of providing a suboptimal service to the\n",
      "customers. Through this paper, we demonstrate the effectiveness of\n",
      "opportunistic bandits to make the experiments as inexpensive as possible using\n",
      "real online retail data. In fact, we model user interface experimentation as an\n",
      "opportunistic bandit problem, in which the cost of exploration varies under a\n",
      "factor extracted from customer features. We achieve significant regret\n",
      "reduction by mitigating costly exploration and providing extra contextual\n",
      "information that helps to guide the testing process. Moreover, we analyze the\n",
      "advantages and challenges of using opportunistic bandits for online retail\n",
      "experimentation.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.11968 \n",
      "Title :A sparse code increases the speed and efficiency of neuro-dynamic\n",
      "  programming for optimal control tasks with correlated inputs\n",
      "  Sparse codes in neuroscience have been suggested to offer certain\n",
      "computational advantages over other neural representations of sensory data. To\n",
      "explore this viewpoint, a sparse code is used to represent natural images in an\n",
      "optimal control task solved with neuro-dynamic programming, and its\n",
      "computational properties are investigated. The central finding is that when\n",
      "feature inputs to a linear network are correlated, an over-complete sparse code\n",
      "increases the memory capacity of the network in an efficient manner beyond that\n",
      "possible for any complete code with the same-sized input, and also increases\n",
      "the speed of learning the network weights. A complete sparse code is found to\n",
      "maximise the memory capacity of a linear network by decorrelating its feature\n",
      "inputs to transform the design matrix of the least-squares problem to one of\n",
      "full rank. It also conditions the Hessian matrix of the least-squares problem,\n",
      "thereby increasing the rate of convergence to the optimal network weights.\n",
      "Other types of decorrelating codes would also achieve this. However, an\n",
      "over-complete sparse code is found to be approximately decorrelated, extracting\n",
      "a larger number of approximately decorrelated features from the same-sized\n",
      "input, allowing it to efficiently increase memory capacity beyond that possible\n",
      "for any complete code: a 2.25 times over-complete sparse code is shown to at\n",
      "least double memory capacity compared with a complete sparse code using the\n",
      "same input. This is used in sequential learning to store a potentially large\n",
      "number of optimal control tasks in the network, while catastrophic forgetting\n",
      "is avoided using a partitioned representation, yielding a cost-to-go function\n",
      "approximator that generalizes over the states in each partition. Sparse code\n",
      "advantages over dense codes and local codes are also discussed.\n",
      "\n",
      "**Paper Id :2007.00936 \n",
      "Title :Deep Neural Networks for Nonlinear Model Order Reduction of Unsteady\n",
      "  Flows\n",
      "  Unsteady fluid systems are nonlinear high-dimensional dynamical systems that\n",
      "may exhibit multiple complex phenomena both in time and space. Reduced Order\n",
      "Modeling (ROM) of fluid flows has been an active research topic in the recent\n",
      "decade with the primary goal to decompose complex flows to a set of features\n",
      "most important for future state prediction and control, typically using a\n",
      "dimensionality reduction technique. In this work, a novel data-driven technique\n",
      "based on the power of deep neural networks for reduced order modeling of the\n",
      "unsteady fluid flows is introduced. An autoencoder network is used for\n",
      "nonlinear dimension reduction and feature extraction as an alternative for\n",
      "singular value decomposition (SVD). Then, the extracted features are used as an\n",
      "input for long short-term memory network (LSTM) to predict the velocity field\n",
      "at future time instances. The proposed autoencoder-LSTM method is compared with\n",
      "non-intrusive reduced order models based on dynamic mode decomposition (DMD)\n",
      "and proper orthogonal decomposition (POD). Moreover, an autoencoder-DMD\n",
      "algorithm is introduced for reduced order modeling, which uses the autoencoder\n",
      "network for dimensionality reduction rather than SVD rank truncation. Results\n",
      "show that the autoencoder-LSTM method is considerably capable of predicting\n",
      "fluid flow evolution, where higher values for coefficient of determination\n",
      "$R^{2}$ are obtained using autoencoder-LSTM compared to other models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.12087 \n",
      "Title :Progressive Graph Learning for Open-Set Domain Adaptation\n",
      "  Domain shift is a fundamental problem in visual recognition which typically\n",
      "arises when the source and target data follow different distributions. The\n",
      "existing domain adaptation approaches which tackle this problem work in the\n",
      "closed-set setting with the assumption that the source and the target data\n",
      "share exactly the same classes of objects. In this paper, we tackle a more\n",
      "realistic problem of open-set domain shift where the target data contains\n",
      "additional classes that are not present in the source data. More specifically,\n",
      "we introduce an end-to-end Progressive Graph Learning (PGL) framework where a\n",
      "graph neural network with episodic training is integrated to suppress\n",
      "underlying conditional shift and adversarial learning is adopted to close the\n",
      "gap between the source and target distributions. Compared to the existing\n",
      "open-set adaptation approaches, our approach guarantees to achieve a tighter\n",
      "upper bound of the target error. Extensive experiments on three standard\n",
      "open-set benchmarks evidence that our approach significantly outperforms the\n",
      "state-of-the-arts in open-set domain adaptation.\n",
      "\n",
      "**Paper Id :2003.04108 \n",
      "Title :Stable Policy Optimization via Off-Policy Divergence Regularization\n",
      "  Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization\n",
      "(PPO) are among the most successful policy gradient approaches in deep\n",
      "reinforcement learning (RL). While these methods achieve state-of-the-art\n",
      "performance across a wide range of challenging tasks, there is room for\n",
      "improvement in the stabilization of the policy learning and how the off-policy\n",
      "data are used. In this paper we revisit the theoretical foundations of these\n",
      "algorithms and propose a new algorithm which stabilizes the policy improvement\n",
      "through a proximity term that constrains the discounted state-action visitation\n",
      "distribution induced by consecutive policies to be close to one another. This\n",
      "proximity term, expressed in terms of the divergence between the visitation\n",
      "distributions, is learned in an off-policy and adversarial manner. We\n",
      "empirically show that our proposed method can have a beneficial effect on\n",
      "stability and improve final performance in benchmark high-dimensional control\n",
      "tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.12229 \n",
      "Title :Improving performance of CNN to predict likelihood of COVID-19 using\n",
      "  chest X-ray images with preprocessing algorithms\n",
      "  As the rapid spread of coronavirus disease (COVID-19) worldwide, chest X-ray\n",
      "radiography has also been used to detect COVID-19 infected pneumonia and assess\n",
      "its severity or monitor its prognosis in the hospitals due to its low cost, low\n",
      "radiation dose, and wide accessibility. However, how to more accurately and\n",
      "efficiently detect COVID-19 infected pneumonia and distinguish it from other\n",
      "community-acquired pneumonia remains a challenge. In order to address this\n",
      "challenge, we in this study develop and test a new computer-aided diagnosis\n",
      "(CAD) scheme. It includes several image pre-processing algorithms to remove\n",
      "diaphragms, normalize image contrast-to-noise ratio, and generate three input\n",
      "images, then links to a transfer learning based convolutional neural network (a\n",
      "VGG16 based CNN model) to classify chest X-ray images into three classes of\n",
      "COVID-19 infected pneumonia, other community-acquired pneumonia and normal\n",
      "(non-pneumonia) cases. To this purpose, a publicly available dataset of 8,474\n",
      "chest X-ray images is used, which includes 415 confirmed COVID-19 infected\n",
      "pneumonia, 5,179 community-acquired pneumonia, and 2,880 non-pneumonia cases.\n",
      "The dataset is divided into two subsets with 90% and 10% of images in each\n",
      "subset to train and test the CNN-based CAD scheme. The testing results achieve\n",
      "94.0% of overall accuracy in classifying three classes and 98.6% accuracy in\n",
      "detecting Covid-19 infected cases. Thus, the study demonstrates the feasibility\n",
      "of developing a CAD scheme of chest X-ray images and providing radiologists\n",
      "useful decision-making supporting tools in detecting and diagnosis of COVID-19\n",
      "infected pneumonia.\n",
      "\n",
      "**Paper Id :2007.14777 \n",
      "Title :PDCOVIDNet: A Parallel-Dilated Convolutional Neural Network Architecture\n",
      "  for Detecting COVID-19 from Chest X-Ray Images\n",
      "  The COVID-19 pandemic continues to severely undermine the prosperity of the\n",
      "global health system. To combat this pandemic, effective screening techniques\n",
      "for infected patients are indispensable. There is no doubt that the use of\n",
      "chest X-ray images for radiological assessment is one of the essential\n",
      "screening techniques. Some of the early studies revealed that the patient's\n",
      "chest X-ray images showed abnormalities, which is natural for patients infected\n",
      "with COVID-19. In this paper, we proposed a parallel-dilated convolutional\n",
      "neural network (CNN) based COVID-19 detection system from chest x-ray images,\n",
      "named as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available\n",
      "chest X-ray collection fully preloaded and enhanced, and then classified by the\n",
      "proposed method. Differing convolution dilation rate in a parallel form\n",
      "demonstrates the proof-of-principle for using PDCOVIDNet to extract\n",
      "radiological features for COVID-19 detection. Accordingly, we have assisted our\n",
      "method with two visualization methods, which are specifically designed to\n",
      "increase understanding of the key components associated with COVID-19\n",
      "infection. Both visualization methods compute gradients for a given image\n",
      "category related to feature maps of the last convolutional layer to create a\n",
      "class-discriminative region. In our experiment, we used a total of 2,905 chest\n",
      "X-ray images, comprising three cases (such as COVID-19, normal, and viral\n",
      "pneumonia), and empirical evaluations revealed that the proposed method\n",
      "extracted more significant features expeditiously related to the suspected\n",
      "disease. The experimental results demonstrate that our proposed method\n",
      "significantly improves performance metrics: accuracy, precision, recall, and F1\n",
      "scores reach 96.58%, 96.58%, 96.59%, and 96.58%, respectively, which is\n",
      "comparable or enhanced compared with the state-of-the-art methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.12278 \n",
      "Title :HNHN: Hypergraph Networks with Hyperedge Neurons\n",
      "  Hypergraphs provide a natural representation for many real world datasets. We\n",
      "propose a novel framework, HNHN, for hypergraph representation learning. HNHN\n",
      "is a hypergraph convolution network with nonlinear activation functions applied\n",
      "to both hypernodes and hyperedges, combined with a normalization scheme that\n",
      "can flexibly adjust the importance of high-cardinality hyperedges and\n",
      "high-degree vertices depending on the dataset. We demonstrate improved\n",
      "performance of HNHN in both classification accuracy and speed on real world\n",
      "datasets when compared to state of the art methods.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.12813 \n",
      "Title :NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep\n",
      "  Neural Networks\n",
      "  Deciding the amount of neurons during the design of a deep neural network to\n",
      "maximize performance is not intuitive. In this work, we attempt to search for\n",
      "the neuron (filter) configuration of a fixed network architecture that\n",
      "maximizes accuracy. Using iterative pruning methods as a proxy, we parameterize\n",
      "the change of the neuron (filter) number of each layer with respect to the\n",
      "change in parameters, allowing us to efficiently scale an architecture across\n",
      "arbitrary sizes. We also introduce architecture descent which iteratively\n",
      "refines the parameterized function used for model scaling. The combination of\n",
      "both proposed methods is coined as NeuralScale. To prove the efficiency of\n",
      "NeuralScale in terms of parameters, we show empirical simulations on VGG11,\n",
      "MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark\n",
      "datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41%\n",
      "for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet\n",
      "respectively under a parameter-constrained setting (output neurons (filters) of\n",
      "default configuration with scaling factor of 0.25).\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.13132 \n",
      "Title :On Counterfactual Explanations under Predictive Multiplicity\n",
      "  Counterfactual explanations are usually obtained by identifying the smallest\n",
      "change made to an input to change a prediction made by a fixed model (hereafter\n",
      "called sparse methods). Recent work, however, has revitalized an old insight:\n",
      "there often does not exist one superior solution to a prediction problem with\n",
      "respect to commonly used measures of interest (e.g. error rate). In fact, often\n",
      "multiple different classifiers give almost equal solutions. This phenomenon is\n",
      "known as predictive multiplicity (Breiman, 2001; Marx et al., 2019). In this\n",
      "work, we derive a general upper bound for the costs of counterfactual\n",
      "explanations under predictive multiplicity. Most notably, it depends on a\n",
      "discrepancy notion between two classifiers, which describes how differently\n",
      "they treat negatively predicted individuals. We then compare sparse and data\n",
      "support approaches empirically on real-world data. The results show that data\n",
      "support methods are more robust to multiplicity of different models. At the\n",
      "same time, we show that those methods have provably higher cost of generating\n",
      "counterfactual explanations under one fixed model. In summary, our theoretical\n",
      "and empiricaln results challenge the commonly held view that counterfactual\n",
      "recommendations should be sparse in general.\n",
      "\n",
      "**Paper Id :2003.06005 \n",
      "Title :Model Agnostic Multilevel Explanations\n",
      "  In recent years, post-hoc local instance-level and global dataset-level\n",
      "explainability of black-box models has received a lot of attention. Much less\n",
      "attention has been given to obtaining insights at intermediate or group levels,\n",
      "which is a need outlined in recent works that study the challenges in realizing\n",
      "the guidelines in the General Data Protection Regulation (GDPR). In this paper,\n",
      "we propose a meta-method that, given a typical local explainability method, can\n",
      "build a multilevel explanation tree. The leaves of this tree correspond to the\n",
      "local explanations, the root corresponds to the global explanation, and\n",
      "intermediate levels correspond to explanations for groups of data points that\n",
      "it automatically clusters. The method can also leverage side information, where\n",
      "users can specify points for which they may want the explanations to be\n",
      "similar. We argue that such a multilevel structure can also be an effective\n",
      "form of communication, where one could obtain few explanations that\n",
      "characterize the entire dataset by considering an appropriate level in our\n",
      "explanation tree. Explanations for novel test points can be cost-efficiently\n",
      "obtained by associating them with the closest training points. When the local\n",
      "explainability technique is generalized additive (viz. LIME, GAMs), we develop\n",
      "a fast approximate algorithm for building the multilevel tree and study its\n",
      "convergence behavior. We validate the effectiveness of the proposed technique\n",
      "based on two human studies -- one with experts and the other with non-expert\n",
      "users -- on real world datasets, and show that we produce high fidelity sparse\n",
      "explanations on several other public datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.13222 \n",
      "Title :Certified variational quantum algorithms for eigenstate preparation\n",
      "  Solutions to many-body problem instances often involve an intractable number\n",
      "of degrees of freedom and admit no known approximations in general form. In\n",
      "practice, representing quantum-mechanical states of a given Hamiltonian using\n",
      "available numerical methods, in particular those based on variational Monte\n",
      "Carlo simulations, become exponentially more challenging with increasing system\n",
      "size. Recently quantum algorithms implemented as variational models have been\n",
      "proposed to accelerate such simulations. The variational ansatz states are\n",
      "characterized by a polynomial number of parameters devised in a way to minimize\n",
      "the expectation value of a given Hamiltonian, which is emulated by local\n",
      "measurements. In this study, we develop a means to certify the termination of\n",
      "variational algorithms. We demonstrate our approach by applying it to three\n",
      "models: the transverse field Ising model, the model of one-dimensional spinless\n",
      "fermions with competing interactions, and the Schwinger model of quantum\n",
      "electrodynamics. By means of comparison, we observe that our approach shows\n",
      "better performance near critical points in these models. We hence take a\n",
      "further step to improve the applicability and to certify the results of\n",
      "variational quantum simulators.\n",
      "\n",
      "**Paper Id :2005.00544 \n",
      "Title :Variational Quantum Eigensolver for Frustrated Quantum Systems\n",
      "  Hybrid quantum-classical algorithms have been proposed as a potentially\n",
      "viable application of quantum computers. A particular example - the variational\n",
      "quantum eigensolver, or VQE - is designed to determine a global minimum in an\n",
      "energy landscape specified by a quantum Hamiltonian, which makes it appealing\n",
      "for the needs of quantum chemistry. Experimental realizations have been\n",
      "reported in recent years and theoretical estimates of its efficiency are a\n",
      "subject of intense effort. Here we consider the performance of the VQE\n",
      "technique for a Hubbard-like model describing a one-dimensional chain of\n",
      "fermions with competing nearest- and next-nearest-neighbor interactions. We\n",
      "find that recovering the VQE solution allows one to obtain the correlation\n",
      "function of the ground state consistent with the exact result. We also study\n",
      "the barren plateau phenomenon for the Hamiltonian in question and find that the\n",
      "severity of this effect depends on the encoding of fermions to qubits. Our\n",
      "results are consistent with the current knowledge about the barren plateaus in\n",
      "quantum optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.13546 \n",
      "Title :Crossmodal Language Grounding in an Embodied Neurocognitive Model\n",
      "  Human infants are able to acquire natural language seemingly easily at an\n",
      "early age. Their language learning seems to occur simultaneously with learning\n",
      "other cognitive functions as well as with playful interactions with the\n",
      "environment and caregivers. From a neuroscientific perspective, natural\n",
      "language is embodied, grounded in most, if not all, sensory and sensorimotor\n",
      "modalities, and acquired by means of crossmodal integration. However,\n",
      "characterising the underlying mechanisms in the brain is difficult and\n",
      "explaining the grounding of language in crossmodal perception and action\n",
      "remains challenging. In this paper, we present a neurocognitive model for\n",
      "language grounding which reflects bio-inspired mechanisms such as an implicit\n",
      "adaptation of timescales as well as end-to-end multimodal abstraction. It\n",
      "addresses developmental robotic interaction and extends its learning\n",
      "capabilities using larger-scale knowledge-based data. In our scenario, we\n",
      "utilise the humanoid robot NICO in obtaining the EMIL data collection, in which\n",
      "the cognitive robot interacts with objects in a children's playground\n",
      "environment while receiving linguistic labels from a caregiver. The model\n",
      "analysis shows that crossmodally integrated representations are sufficient for\n",
      "acquiring language merely from sensory input through interaction with objects\n",
      "in an environment. The representations self-organise hierarchically and embed\n",
      "temporal and spatial information through composition and decomposition. This\n",
      "model can also provide the basis for further crossmodal integration of\n",
      "perceptually grounded cognitive representations.\n",
      "\n",
      "**Paper Id :1907.04553 \n",
      "Title :Neural Reasoning, Fast and Slow, for Video Question Answering\n",
      "  What does it take to design a machine that learns to answer natural questions\n",
      "about a video? A Video QA system must simultaneously understand language,\n",
      "represent visual content over space-time, and iteratively transform these\n",
      "representations in response to lingual content in the query, and finally\n",
      "arriving at a sensible answer. While recent advances in lingual and visual\n",
      "question answering have enabled sophisticated representations and neural\n",
      "reasoning mechanisms, major challenges in Video QA remain on dynamic grounding\n",
      "of concepts, relations and actions to support the reasoning process. Inspired\n",
      "by the dual-process account of human reasoning, we design a dual process neural\n",
      "architecture, which is composed of a question-guided video processing module\n",
      "(System 1, fast and reactive) followed by a generic reasoning module (System 2,\n",
      "slow and deliberative). System 1 is a hierarchical model that encodes visual\n",
      "patterns about objects, actions and relations in space-time given the textual\n",
      "cues from the question. The encoded representation is a set of high-level\n",
      "visual features, which are then passed to System 2. Here multi-step inference\n",
      "follows to iteratively chain visual elements as instructed by the textual\n",
      "elements. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets\n",
      "(real), demonstrating competitive results, with a large margin in the case of\n",
      "multi-step reasoning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.13791 \n",
      "Title :Post-DAE: Anatomically Plausible Segmentation via Post-Processing with\n",
      "  Denoising Autoencoders\n",
      "  We introduce Post-DAE, a post-processing method based on denoising\n",
      "autoencoders (DAE) to improve the anatomical plausibility of arbitrary\n",
      "biomedical image segmentation algorithms. Some of the most popular segmentation\n",
      "methods (e.g. based on convolutional neural networks or random forest\n",
      "classifiers) incorporate additional post-processing steps to ensure that the\n",
      "resulting masks fulfill expected connectivity constraints. These methods\n",
      "operate under the hypothesis that contiguous pixels with similar aspect should\n",
      "belong to the same class. Even if valid in general, this assumption does not\n",
      "consider more complex priors like topological restrictions or convexity, which\n",
      "cannot be easily incorporated into these methods. Post-DAE leverages the latest\n",
      "developments in manifold learning via denoising autoencoders. First, we learn a\n",
      "compact and non-linear embedding that represents the space of anatomically\n",
      "plausible segmentations. Then, given a segmentation mask obtained with an\n",
      "arbitrary method, we reconstruct its anatomically plausible version by\n",
      "projecting it onto the learnt manifold. The proposed method is trained using\n",
      "unpaired segmentation mask, what makes it independent of intensity information\n",
      "and image modality. We performed experiments in binary and multi-label\n",
      "segmentation of chest X-ray and cardiac magnetic resonance images. We show how\n",
      "erroneous and noisy segmentation masks can be improved using Post-DAE. With\n",
      "almost no additional computation cost, our method brings erroneous\n",
      "segmentations back to a feasible space.\n",
      "\n",
      "**Paper Id :2010.12455 \n",
      "Title :Primal-Dual Mesh Convolutional Neural Networks\n",
      "  Recent works in geometric deep learning have introduced neural networks that\n",
      "allow performing inference tasks on three-dimensional geometric data by\n",
      "defining convolution, and sometimes pooling, operations on triangle meshes.\n",
      "These methods, however, either consider the input mesh as a graph, and do not\n",
      "exploit specific geometric properties of meshes for feature aggregation and\n",
      "downsampling, or are specialized for meshes, but rely on a rigid definition of\n",
      "convolution that does not properly capture the local topology of the mesh. We\n",
      "propose a method that combines the advantages of both types of approaches,\n",
      "while addressing their limitations: we extend a primal-dual framework drawn\n",
      "from the graph-neural-network literature to triangle meshes, and define\n",
      "convolutions on two types of graphs constructed from an input mesh. Our method\n",
      "takes features for both edges and faces of a 3D mesh as input and dynamically\n",
      "aggregates them using an attention mechanism. At the same time, we introduce a\n",
      "pooling operation with a precise geometric interpretation, that allows handling\n",
      "variations in the mesh connectivity by clustering mesh faces in a task-driven\n",
      "fashion. We provide theoretical insights of our approach using tools from the\n",
      "mesh-simplification literature. In addition, we validate experimentally our\n",
      "method in the tasks of shape classification and shape segmentation, where we\n",
      "obtain comparable or superior performance to the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.13924 \n",
      "Title :K-Prototype Segmentation Analysis on Large-scale Ridesourcing Trip Data\n",
      "  Shared mobility-on-demand services are expanding rapidly in cities around the\n",
      "world. As a prominent example, app-based ridesourcing is becoming an integral\n",
      "part of many urban transportation ecosystems. Despite the centrality, limited\n",
      "public availability of detailed temporal and spatial data on ridesourcing trips\n",
      "has limited research on how new services interact with traditional mobility\n",
      "options and how they impact travel in cities. Improving data-sharing agreements\n",
      "are opening unprecedented opportunities for research in this area. This study\n",
      "examines emerging patterns of mobility using recently released City of Chicago\n",
      "public ridesourcing data. The detailed spatio-temporal ridesourcing data are\n",
      "matched with weather, transit, and taxi data to gain a deeper understanding of\n",
      "ridesourcings role in Chicagos mobility system. The goal is to investigate the\n",
      "systematic variations in patronage of ride-hailing. K-prototypes is utilized to\n",
      "detect user segments owing to its ability to accept mixed variable data types.\n",
      "An extension of the K-means algorithm, its output is a classification of the\n",
      "data into several clusters called prototypes. Six ridesourcing prototypes are\n",
      "identified and discussed based on significant differences in relation to\n",
      "adverse weather conditions, competition with alternative modes, location and\n",
      "timing of use, and tendency for ridesplitting. The paper discusses implications\n",
      "of the identified clusters related to affordability, equity and competition\n",
      "with transit.\n",
      "\n",
      "**Paper Id :1910.02498 \n",
      "Title :Predicting popularity of EV charging infrastructure from GIS data\n",
      "  The availability of charging infrastructure is essential for large-scale\n",
      "adoption of electric vehicles (EV). Charging patterns and the utilization of\n",
      "infrastructure have consequences not only for the energy demand, loading local\n",
      "power grids but influence the economic returns, parking policies and further\n",
      "adoption of EVs. We develop a data-driven approach that is exploiting\n",
      "predictors compiled from GIS data describing the urban context and urban\n",
      "activities near charging infrastructure to explore correlations with a\n",
      "comprehensive set of indicators measuring the performance of charging\n",
      "infrastructure. The best fit was identified for the size of the unique group of\n",
      "visitors (popularity) attracted by the charging infrastructure. Consecutively,\n",
      "charging infrastructure is ranked by popularity. The question of whether or not\n",
      "a given charging spot belongs to the top tier is posed as a binary\n",
      "classification problem and predictive performance of logistic regression\n",
      "regularized with an l-1 penalty, random forests and gradient boosted regression\n",
      "trees is evaluated. Obtained results indicate that the collected predictors\n",
      "contain information that can be used to predict the popularity of charging\n",
      "infrastructure. The significance of predictors and how they are linked with the\n",
      "popularity are explored as well. The proposed methodology can be used to inform\n",
      "charging infrastructure deployment strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14084 \n",
      "Title :Multilabel Classification by Hierarchical Partitioning and\n",
      "  Data-dependent Grouping\n",
      "  In modern multilabel classification problems, each data instance belongs to a\n",
      "small number of classes from a large set of classes. In other words, these\n",
      "problems involve learning very sparse binary label vectors. Moreover, in\n",
      "large-scale problems, the labels typically have certain (unknown) hierarchy. In\n",
      "this paper we exploit the sparsity of label vectors and the hierarchical\n",
      "structure to embed them in low-dimensional space using label groupings.\n",
      "Consequently, we solve the classification problem in a much lower dimensional\n",
      "space and then obtain labels in the original space using an appropriately\n",
      "defined lifting. Our method builds on the work of (Ubaru & Mazumdar, 2017),\n",
      "where the idea of group testing was also explored for multilabel\n",
      "classification. We first present a novel data-dependent grouping approach,\n",
      "where we use a group construction based on a low-rank Nonnegative Matrix\n",
      "Factorization (NMF) of the label matrix of training instances. The construction\n",
      "also allows us, using recent results, to develop a fast prediction algorithm\n",
      "that has a logarithmic runtime in the number of labels. We then present a\n",
      "hierarchical partitioning approach that exploits the label hierarchy in large\n",
      "scale problems to divide up the large label space and create smaller\n",
      "sub-problems, which can then be solved independently via the grouping approach.\n",
      "Numerical results on many benchmark datasets illustrate that, compared to other\n",
      "popular methods, our proposed methods achieve competitive accuracy with\n",
      "significantly lower computational costs.\n",
      "\n",
      "**Paper Id :1811.00821 \n",
      "Title :OrthoNet: Multilayer Network Data Clustering\n",
      "  Network data appears in very diverse applications, like biological, social,\n",
      "or sensor networks. Clustering of network nodes into categories or communities\n",
      "has thus become a very common task in machine learning and data mining. Network\n",
      "data comes with some information about the network edges. In some cases, this\n",
      "network information can even be given with multiple views or multiple layers,\n",
      "each one representing a different type of relationship between the network\n",
      "nodes. Increasingly often, network nodes also carry a feature vector. We\n",
      "propose in this paper to extend the node clustering problem, that commonly\n",
      "considers only the network information, to a problem where both the network\n",
      "information and the node features are considered together for learning a\n",
      "clustering-friendly representation of the feature space. Specifically, we\n",
      "design a generic two-step algorithm for multilayer network data clustering. The\n",
      "first step aggregates the different layers of network information into a graph\n",
      "representation given by the geometric mean of the network Laplacian matrices.\n",
      "The second step uses a neural net to learn a feature embedding that is\n",
      "consistent with the structure given by the network layers. We propose a novel\n",
      "algorithm for efficiently training the neural net via stochastic gradient\n",
      "descent, which encourages the neural net outputs to span the leading\n",
      "eigenvectors of the aggregated Laplacian matrix, in order to capture the\n",
      "pairwise interactions on the network, and provide a clustering-friendly\n",
      "representation of the feature space. We demonstrate with an extensive set of\n",
      "experiments on synthetic and real datasets that our method leads to a\n",
      "significant improvement w.r.t. state-of-the-art multilayer graph clustering\n",
      "algorithms, as it judiciously combines nodes features and network information\n",
      "in the node embedding algorithms.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14117 \n",
      "Title :Fast Learning of Graph Neural Networks with Guaranteed Generalizability:\n",
      "  One-hidden-layer Case\n",
      "  Although graph neural networks (GNNs) have made great progress recently on\n",
      "learning from graph-structured data in practice, their theoretical guarantee on\n",
      "generalizability remains elusive in the literature. In this paper, we provide a\n",
      "theoretically-grounded generalizability analysis of GNNs with one hidden layer\n",
      "for both regression and binary classification problems. Under the assumption\n",
      "that there exists a ground-truth GNN model (with zero generalization error),\n",
      "the objective of GNN learning is to estimate the ground-truth GNN parameters\n",
      "from the training data. To achieve this objective, we propose a learning\n",
      "algorithm that is built on tensor initialization and accelerated gradient\n",
      "descent. We then show that the proposed learning algorithm converges to the\n",
      "ground-truth GNN model for the regression problem, and to a model sufficiently\n",
      "close to the ground-truth for the binary classification problem. Moreover, for\n",
      "both cases, the convergence rate of the proposed learning algorithm is proven\n",
      "to be linear and faster than the vanilla gradient descent algorithm. We further\n",
      "explore the relationship between the sample complexity of GNNs and their\n",
      "underlying graph properties. Lastly, we provide numerical experiments to\n",
      "demonstrate the validity of our analysis and the effectiveness of the proposed\n",
      "learning algorithm for GNNs.\n",
      "\n",
      "**Paper Id :2002.08274 \n",
      "Title :Residual Correlation in Graph Neural Network Regression\n",
      "  A graph neural network transforms features in each vertex's neighborhood into\n",
      "a vector representation of the vertex. Afterward, each vertex's representation\n",
      "is used independently for predicting its label. This standard pipeline\n",
      "implicitly assumes that vertex labels are conditionally independent given their\n",
      "neighborhood features. However, this is a strong assumption, and we show that\n",
      "it is far from true on many real-world graph datasets. Focusing on regression\n",
      "tasks, we find that this conditional independence assumption severely limits\n",
      "predictive power. This should not be that surprising, given that traditional\n",
      "graph-based semi-supervised learning methods such as label propagation work in\n",
      "the opposite fashion by explicitly modeling the correlation in predicted\n",
      "outcomes.\n",
      "  Here, we address this problem with an interpretable and efficient framework\n",
      "that can improve any graph neural network architecture simply by exploiting\n",
      "correlation structure in the regression residuals. In particular, we model the\n",
      "joint distribution of residuals on vertices with a parameterized multivariate\n",
      "Gaussian, and estimate the parameters by maximizing the marginal likelihood of\n",
      "the observed labels. Our framework achieves substantially higher accuracy than\n",
      "competing baselines, and the learned parameters can be interpreted as the\n",
      "strength of correlation among connected vertices. Furthermore, we develop\n",
      "linear time algorithms for low-variance, unbiased model parameter estimates,\n",
      "allowing us to scale to large networks. We also provide a basic version of our\n",
      "method that makes stronger assumptions on correlation structure but is painless\n",
      "to implement, often leading to great practical performance with minimal\n",
      "overhead.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14141 \n",
      "Title :Inverse Active Sensing: Modeling and Understanding Timely\n",
      "  Decision-Making\n",
      "  Evidence-based decision-making entails collecting (costly) observations about\n",
      "an underlying phenomenon of interest, and subsequently committing to an\n",
      "(informed) decision on the basis of accumulated evidence. In this setting,\n",
      "active sensing is the goal-oriented problem of efficiently selecting which\n",
      "acquisitions to make, and when and what decision to settle on. As its\n",
      "complement, inverse active sensing seeks to uncover an agent's preferences and\n",
      "strategy given their observable decision-making behavior. In this paper, we\n",
      "develop an expressive, unified framework for the general setting of\n",
      "evidence-based decision-making under endogenous, context-dependent time\n",
      "pressure---which requires negotiating (subjective) tradeoffs between accuracy,\n",
      "speediness, and cost of information. Using this language, we demonstrate how it\n",
      "enables modeling intuitive notions of surprise, suspense, and optimality in\n",
      "decision strategies (the forward problem). Finally, we illustrate how this\n",
      "formulation enables understanding decision-making behavior by quantifying\n",
      "preferences implicit in observed decision strategies (the inverse problem).\n",
      "\n",
      "**Paper Id :2002.04335 \n",
      "Title :Static and Dynamic Values of Computation in MCTS\n",
      "  Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for\n",
      "planning, and has powered many recent advances in artificial intelligence. In\n",
      "MCTS, one typically performs computations (i.e., simulations) to collect\n",
      "statistics about the possible future consequences of actions, and then chooses\n",
      "accordingly. Many popular MCTS methods such as UCT and its variants decide\n",
      "which computations to perform by trading-off exploration and exploitation. In\n",
      "this work, we take a more direct approach, and explicitly quantify the value of\n",
      "a computation based on its expected impact on the quality of the action\n",
      "eventually chosen. Our approach goes beyond the \"myopic\" limitations of\n",
      "existing computation-value-based methods in two senses: (I) we are able to\n",
      "account for the impact of non-immediate (ie, future) computations (II) on\n",
      "non-immediate actions. We show that policies that greedily optimize computation\n",
      "values are optimal under certain assumptions and obtain results that are\n",
      "competitive with the state-of-the-art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14293 \n",
      "Title :Neural Decomposition: Functional ANOVA with Variational Autoencoders\n",
      "  Variational Autoencoders (VAEs) have become a popular approach for\n",
      "dimensionality reduction. However, despite their ability to identify latent\n",
      "low-dimensional structures embedded within high-dimensional data, these latent\n",
      "representations are typically hard to interpret on their own. Due to the\n",
      "black-box nature of VAEs, their utility for healthcare and genomics\n",
      "applications has been limited. In this paper, we focus on characterising the\n",
      "sources of variation in Conditional VAEs. Our goal is to provide a\n",
      "feature-level variance decomposition, i.e. to decompose variation in the data\n",
      "by separating out the marginal additive effects of latent variables z and fixed\n",
      "inputs c from their non-linear interactions. We propose to achieve this through\n",
      "what we call Neural Decomposition - an adaptation of the well-known concept of\n",
      "functional ANOVA variance decomposition from classical statistics to deep\n",
      "learning models. We show how identifiability can be achieved by training models\n",
      "subject to constraints on the marginal properties of the decoder networks. We\n",
      "demonstrate the utility of our Neural Decomposition on a series of synthetic\n",
      "examples as well as high-dimensional genomics data.\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14348 \n",
      "Title :Audeo: Audio Generation for a Silent Performance Video\n",
      "  We present a novel system that gets as an input video frames of a musician\n",
      "playing the piano and generates the music for that video. Generation of music\n",
      "from visual cues is a challenging problem and it is not clear whether it is an\n",
      "attainable goal at all. Our main aim in this work is to explore the\n",
      "plausibility of such a transformation and to identify cues and components able\n",
      "to carry the association of sounds with visual events. To achieve the\n",
      "transformation we built a full pipeline named `\\textit{Audeo}' containing three\n",
      "components. We first translate the video frames of the keyboard and the\n",
      "musician hand movements into raw mechanical musical symbolic representation\n",
      "Piano-Roll (Roll) for each video frame which represents the keys pressed at\n",
      "each time step. We then adapt the Roll to be amenable for audio synthesis by\n",
      "including temporal correlations. This step turns out to be critical for\n",
      "meaningful audio generation. As a last step, we implement Midi synthesizers to\n",
      "generate realistic music. \\textit{Audeo} converts video to audio smoothly and\n",
      "clearly with only a few setup constraints. We evaluate \\textit{Audeo} on `in\n",
      "the wild' piano performance videos and obtain that their generated music is of\n",
      "reasonable audio quality and can be successfully recognized with high precision\n",
      "by popular music identification software.\n",
      "\n",
      "**Paper Id :2002.10981 \n",
      "Title :AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent\n",
      "  Videos with Deep Learning\n",
      "  In movie productions, the Foley Artist is responsible for creating an overlay\n",
      "soundtrack that helps the movie come alive for the audience. This requires the\n",
      "artist to first identify the sounds that will enhance the experience for the\n",
      "listener thereby reinforcing the Directors's intention for a given scene. In\n",
      "this paper, we present AutoFoley, a fully-automated deep learning tool that can\n",
      "be used to synthesize a representative audio track for videos. AutoFoley can be\n",
      "used in the applications where there is either no corresponding audio file\n",
      "associated with the video or in cases where there is a need to identify\n",
      "critical scenarios and provide a synthesized, reinforced soundtrack. An\n",
      "important performance criterion of the synthesized soundtrack is to be\n",
      "time-synchronized with the input video, which provides for a realistic and\n",
      "believable portrayal of the synthesized sound. Unlike existing sound prediction\n",
      "and generation architectures, our algorithm is capable of precise recognition\n",
      "of actions as well as inter-frame relations in fast moving video clips by\n",
      "incorporating an interpolation technique and Temporal Relationship Networks\n",
      "(TRN). We employ a robust multi-scale Recurrent Neural Network (RNN) associated\n",
      "with a Convolutional Neural Network (CNN) for a better understanding of the\n",
      "intricate input-to-output associations over time. To evaluate AutoFoley, we\n",
      "create and introduce a large scale audio-video dataset containing a variety of\n",
      "sounds frequently used as Foley effects in movies. Our experiments show that\n",
      "the synthesized sounds are realistically portrayed with accurate temporal\n",
      "synchronization of the associated visual inputs. Human qualitative testing of\n",
      "AutoFoley show over 73% of the test subjects considered the generated\n",
      "soundtrack as original, which is a noteworthy improvement in cross-modal\n",
      "research in sound synthesis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14619 \n",
      "Title :Recurrent Quantum Neural Networks\n",
      "  Recurrent neural networks are the foundation of many sequence-to-sequence\n",
      "models in machine learning, such as machine translation and speech synthesis.\n",
      "In contrast, applied quantum computing is in its infancy. Nevertheless there\n",
      "already exist quantum machine learning models such as variational quantum\n",
      "eigensolvers which have been used successfully e.g. in the context of energy\n",
      "minimization tasks. In this work we construct a quantum recurrent neural\n",
      "network (QRNN) with demonstrable performance on non-trivial tasks such as\n",
      "sequence learning and integer digit classification. The QRNN cell is built from\n",
      "parametrized quantum neurons, which, in conjunction with amplitude\n",
      "amplification, create a nonlinear activation of polynomials of its inputs and\n",
      "cell state, and allow the extraction of a probability distribution over\n",
      "predicted classes at each step. To study the model's performance, we provide an\n",
      "implementation in pytorch, which allows the relatively efficient optimization\n",
      "of parametrized quantum circuits with thousands of parameters. We establish a\n",
      "QRNN training setup by benchmarking optimization hyperparameters, and analyse\n",
      "suitable network topologies for simple memorisation and sequence prediction\n",
      "tasks from Elman's seminal paper (1990) on temporal structure learning. We then\n",
      "proceed to evaluate the QRNN on MNIST classification, both by feeding the QRNN\n",
      "each image pixel-by-pixel; and by utilising modern data augmentation as\n",
      "preprocessing step. Finally, we analyse to what extent the unitary nature of\n",
      "the network counteracts the vanishing gradient problem that plagues many\n",
      "existing quantum classifiers and classical RNNs.\n",
      "\n",
      "**Paper Id :1902.10445 \n",
      "Title :Efficient Learning for Deep Quantum Neural Networks\n",
      "  Neural networks enjoy widespread success in both research and industry and,\n",
      "with the imminent advent of quantum technology, it is now a crucial challenge\n",
      "to design quantum neural networks for fully quantum learning tasks. Here we\n",
      "propose the use of quantum neurons as a building block for quantum feed-forward\n",
      "neural networks capable of universal quantum computation. We describe the\n",
      "efficient training of these networks using the fidelity as a cost function and\n",
      "provide both classical and efficient quantum implementations. Our method allows\n",
      "for fast optimisation with reduced memory requirements: the number of qudits\n",
      "required scales with only the width, allowing the optimisation of deep\n",
      "networks. We benchmark our proposal for the quantum task of learning an unknown\n",
      "unitary and find remarkable generalisation behaviour and a striking robustness\n",
      "to noisy training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14744 \n",
      "Title :Graph Optimal Transport for Cross-Domain Alignment\n",
      "  Cross-domain alignment between two sets of entities (e.g., objects in an\n",
      "image, words in a sentence) is fundamental to both computer vision and natural\n",
      "language processing. Existing methods mainly focus on designing advanced\n",
      "attention mechanisms to simulate soft alignment, with no training signals to\n",
      "explicitly encourage alignment. The learned attention matrices are also dense\n",
      "and lacks interpretability. We propose Graph Optimal Transport (GOT), a\n",
      "principled framework that germinates from recent advances in Optimal Transport\n",
      "(OT). In GOT, cross-domain alignment is formulated as a graph matching problem,\n",
      "by representing entities into a dynamically-constructed graph. Two types of OT\n",
      "distances are considered: (i) Wasserstein distance (WD) for node (entity)\n",
      "matching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure)\n",
      "matching. Both WD and GWD can be incorporated into existing neural network\n",
      "models, effectively acting as a drop-in regularizer. The inferred transport\n",
      "plan also yields sparse and self-normalized alignment, enhancing the\n",
      "interpretability of the learned model. Experiments show consistent\n",
      "outperformance of GOT over baselines across a wide range of tasks, including\n",
      "image-text retrieval, visual question answering, image captioning, machine\n",
      "translation, and text summarization.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14755 \n",
      "Title :DeltaGrad: Rapid retraining of machine learning models\n",
      "  Machine learning models are not static and may need to be retrained on\n",
      "slightly changed datasets, for instance, with the addition or deletion of a set\n",
      "of data points. This has many applications, including privacy, robustness, bias\n",
      "reduction, and uncertainty quantifcation. However, it is expensive to retrain\n",
      "models from scratch. To address this problem, we propose the DeltaGrad\n",
      "algorithm for rapid retraining machine learning models based on information\n",
      "cached during the training phase. We provide both theoretical and empirical\n",
      "support for the effectiveness of DeltaGrad, and show that it compares favorably\n",
      "to the state of the art.\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14763 \n",
      "Title :PAC-Bayesian Bound for the Conditional Value at Risk\n",
      "  Conditional Value at Risk (CVaR) is a family of \"coherent risk measures\"\n",
      "which generalize the traditional mathematical expectation. Widely used in\n",
      "mathematical finance, it is garnering increasing interest in machine learning,\n",
      "e.g., as an alternate approach to regularization, and as a means for ensuring\n",
      "fairness. This paper presents a generalization bound for learning algorithms\n",
      "that minimize the CVaR of the empirical loss. The bound is of PAC-Bayesian type\n",
      "and is guaranteed to be small when the empirical CVaR is small. We achieve this\n",
      "by reducing the problem of estimating CVaR to that of merely estimating an\n",
      "expectation. This then enables us, as a by-product, to obtain concentration\n",
      "inequalities for CVaR even when the random variable in question is unbounded.\n",
      "\n",
      "**Paper Id :2002.03967 \n",
      "Title :Regularized Optimal Transport is Ground Cost Adversarial\n",
      "  Regularizing the optimal transport (OT) problem has proven crucial for OT\n",
      "theory to impact the field of machine learning. For instance, it is known that\n",
      "regularizing OT problems with entropy leads to faster computations and better\n",
      "differentiation using the Sinkhorn algorithm, as well as better sample\n",
      "complexity bounds than classic OT. In this work we depart from this practical\n",
      "perspective and propose a new interpretation of regularization as a robust\n",
      "mechanism, and show using Fenchel duality that any convex regularization of OT\n",
      "can be interpreted as ground cost adversarial. This incidentally gives access\n",
      "to a robust dissimilarity measure on the ground space, which can in turn be\n",
      "used in other applications. We propose algorithms to compute this robust cost,\n",
      "and illustrate the interest of this approach empirically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.14937 \n",
      "Title :Joints in Random Forests\n",
      "  Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative\n",
      "learners and tools of central importance to the everyday machine learning\n",
      "practitioner and data scientist. Due to their discriminative nature, however,\n",
      "they lack principled methods to process inputs with missing features or to\n",
      "detect outliers, which requires pairing them with imputation techniques or a\n",
      "separate generative model. In this paper, we demonstrate that DTs and RFs can\n",
      "naturally be interpreted as generative models, by drawing a connection to\n",
      "Probabilistic Circuits, a prominent class of tractable probabilistic models.\n",
      "This reinterpretation equips them with a full joint distribution over the\n",
      "feature space and leads to Generative Decision Trees (GeDTs) and Generative\n",
      "Forests (GeFs), a family of novel hybrid generative-discriminative models. This\n",
      "family of models retains the overall characteristics of DTs and RFs while\n",
      "additionally being able to handle missing features by means of marginalisation.\n",
      "Under certain assumptions, frequently made for Bayes consistency results, we\n",
      "show that consistency in GeDTs and GeFs extend to any pattern of missing input\n",
      "features, if missing at random. Empirically, we show that our models often\n",
      "outperform common routines to treat missing data, such as K-nearest neighbour\n",
      "imputation, and moreover, that our models can naturally detect outliers by\n",
      "monitoring the marginal probability of input features.\n",
      "\n",
      "**Paper Id :1907.00865 \n",
      "Title :Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale\n",
      "  Bayesian Deep Learning\n",
      "  We propose Radial Bayesian Neural Networks (BNNs): a variational approximate\n",
      "posterior for BNNs which scales well to large models while maintaining a\n",
      "distribution over weight-space with full support. Other scalable Bayesian deep\n",
      "learning methods, like MC dropout or deep ensembles, have discrete support-they\n",
      "assign zero probability to almost all of the weight-space. Unlike these\n",
      "discrete support methods, Radial BNNs' full support makes them suitable for use\n",
      "as a prior for sequential inference. In addition, they solve the conceptual\n",
      "challenges with the a priori implausibility of weight distributions with\n",
      "discrete support. The Radial BNN is motivated by avoiding a sampling problem in\n",
      "'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble'\n",
      "pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are\n",
      "robust to hyperparameters and can be efficiently applied to a challenging\n",
      "real-world medical application without needing ad-hoc tweaks and intensive\n",
      "tuning. In fact, in this setting Radial BNNs out-perform discrete-support\n",
      "methods like MC dropout. Lastly, by using Radial BNNs as a theoretically\n",
      "principled, robust alternative to MFVI we make significant strides in a\n",
      "Bayesian continual learning evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.15257 \n",
      "Title :Generative Damage Learning for Concrete Aging Detection using\n",
      "  Auto-flight Images\n",
      "  In order to monitor the state of large-scale infrastructures, image\n",
      "acquisition by autonomous flight drones is efficient for stable angle and\n",
      "high-quality images. Supervised learning requires a large data set consisting\n",
      "of images and annotation labels. It takes a long time to accumulate images,\n",
      "including identifying the damaged regions of interest (ROIs). In recent years,\n",
      "unsupervised deep learning approaches such as generative adversarial networks\n",
      "(GANs) for anomaly detection algorithms have progressed. When a damaged image\n",
      "is a generator input, it tends to reverse from the damaged state to the healthy\n",
      "state generated image. Using the distance of distribution between the real\n",
      "damaged image and the generated reverse aging healthy state fake image, it is\n",
      "possible to detect the concrete damage automatically from unsupervised\n",
      "learning. This paper proposes an anomaly detection method using unpaired\n",
      "image-to-image translation mapping from damaged images to reverse aging fakes\n",
      "that approximates healthy conditions. We apply our method to field studies, and\n",
      "we examine the usefulness of our method for health monitoring of concrete\n",
      "damage.\n",
      "\n",
      "**Paper Id :2002.05878 \n",
      "Title :An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset\n",
      "  The Waymo Open Dataset has been released recently, providing a platform to\n",
      "crowdsource some fundamental challenges for automated vehicles (AVs), such as\n",
      "3D detection and tracking. While~the dataset provides a large amount of\n",
      "high-quality and multi-source driving information, people in academia are more\n",
      "interested in the underlying driving policy programmed in Waymo self-driving\n",
      "cars, which is inaccessible due to AV manufacturers' proprietary protection.\n",
      "Accordingly, academic researchers have to make various assumptions to implement\n",
      "AV components in their models or simulations, which may not represent the\n",
      "realistic interactions in real-world traffic. Thus, this paper introduces an\n",
      "approach to learn a long short-term memory (LSTM)-based model for imitating the\n",
      "behavior of Waymo's self-driving model. The proposed model has been evaluated\n",
      "based on Mean Absolute Error (MAE). The experimental results show that our\n",
      "model outperforms several baseline models in driving action prediction. In\n",
      "addition, a visualization tool is presented for verifying the performance of\n",
      "the model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.15311 \n",
      "Title :Seasonal Averaged One-Dependence Estimators: A Novel Algorithm to\n",
      "  Address Seasonal Concept Drift in High-Dimensional Stream Classification\n",
      "  Stream classification methods classify a continuous stream of data as new\n",
      "labelled samples arrive. They often also have to deal with concept drift. This\n",
      "paper focuses on seasonal drift in stream classification, which can be found in\n",
      "many real-world application data sources. Traditional approaches of stream\n",
      "classification consider seasonal drift by including seasonal dummy/indicator\n",
      "variables or building separate models for each season. But these approaches\n",
      "have strong limitations in high-dimensional classification problems, or with\n",
      "complex seasonal patterns. This paper explores how to best handle seasonal\n",
      "drift in the specific context of news article categorization (or\n",
      "classification/tagging), where seasonal drift is overwhelmingly the main type\n",
      "of drift present in the data, and for which the data are high-dimensional. We\n",
      "introduce a novel classifier named Seasonal Averaged One-Dependence Estimators\n",
      "(SAODE), which extends the AODE classifier to handle seasonal drift by\n",
      "including time as a super parent. We assess our SAODE model using two large\n",
      "real-world text mining related datasets each comprising approximately a million\n",
      "records, against nine state-of-the-art stream and concept drift classification\n",
      "models, with and without seasonal indicators and with separate models built for\n",
      "each season. Across five different evaluation techniques, we show that our\n",
      "model consistently outperforms other methods by a large margin where the\n",
      "results are statistically significant.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.15404 \n",
      "Title :Multi-element microscope optimization by a learned sensing network with\n",
      "  composite physical layers\n",
      "  Standard microscopes offer a variety of settings to help improve the\n",
      "visibility of different specimens to the end microscope user. Increasingly,\n",
      "however, digital microscopes are used to capture images for automated\n",
      "interpretation by computer algorithms (e.g., for feature classification,\n",
      "detection or segmentation), often without any human involvement. In this work,\n",
      "we investigate an approach to jointly optimize multiple microscope settings,\n",
      "together with a classification network, for improved performance with such\n",
      "automated tasks. We explore the interplay between optimization of programmable\n",
      "illumination and pupil transmission, using experimentally imaged blood smears\n",
      "for automated malaria parasite detection, to show that multi-element \"learned\n",
      "sensing\" outperforms its single-element counterpart. While not necessarily\n",
      "ideal for human interpretation, the network's resulting low-resolution\n",
      "microscope images (20X-comparable) offer a machine learning network sufficient\n",
      "contrast to match the classification performance of corresponding\n",
      "high-resolution imagery (100X-comparable), pointing a path towards accurate\n",
      "automation over large fields-of-view.\n",
      "\n",
      "**Paper Id :1911.04357 \n",
      "Title :Limited View and Sparse Photoacoustic Tomography for Neuroimaging with\n",
      "  Deep Learning\n",
      "  Photoacoustic tomography (PAT) is a nonionizing imaging modality capable of\n",
      "acquiring high contrast and resolution images of optical absorption at depths\n",
      "greater than traditional optical imaging techniques. Practical considerations\n",
      "with instrumentation and geometry limit the number of available acoustic\n",
      "sensors and their view of the imaging target, which result in significant image\n",
      "reconstruction artifacts degrading image quality. Iterative reconstruction\n",
      "methods can be used to reduce artifacts but are computationally expensive. In\n",
      "this work, we propose a novel deep learning approach termed pixelwise deep\n",
      "learning (PixelDL) that first employs pixelwise interpolation governed by the\n",
      "physics of photoacoustic wave propagation and then uses a convolution neural\n",
      "network to directly reconstruct an image. Simulated photoacoustic data from\n",
      "synthetic vasculature phantom and mouse-brain vasculature were used for\n",
      "training and testing, respectively. Results demonstrated that PixelDL achieved\n",
      "comparable performance to iterative methods and outperformed other CNN-based\n",
      "approaches for correcting artifacts. PixelDL is a computationally efficient\n",
      "approach that enables for realtime PAT rendering and for improved image\n",
      "quality, quantification, and interpretation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.15632 \n",
      "Title :FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based\n",
      "  IIoT Applications\n",
      "  Along with the proliferation of Artificial Intelligence (AI) and Internet of\n",
      "Things (IoT) techniques, various kinds of adversarial attacks are increasingly\n",
      "emerging to fool Deep Neural Networks (DNNs) used by Industrial IoT (IIoT)\n",
      "applications. Due to biased training data or vulnerable underlying models,\n",
      "imperceptible modifications on inputs made by adversarial attacks may result in\n",
      "devastating consequences. Although existing methods are promising in defending\n",
      "such malicious attacks, most of them can only deal with limited existing attack\n",
      "types, which makes the deployment of large-scale IIoT devices a great\n",
      "challenge. To address this problem, we present an effective federated defense\n",
      "approach named FDA3 that can aggregate defense knowledge against adversarial\n",
      "examples from different sources. Inspired by federated learning, our proposed\n",
      "cloud-based architecture enables the sharing of defense capabilities against\n",
      "different attacks among IIoT devices. Comprehensive experimental results show\n",
      "that the generated DNNs by our approach can not only resist more malicious\n",
      "attacks than existing attack-specific adversarial training methods, but also\n",
      "can prevent IIoT applications from new attacks.\n",
      "\n",
      "**Paper Id :1911.06502 \n",
      "Title :Simple iterative method for generating targeted universal adversarial\n",
      "  perturbations\n",
      "  Deep neural networks (DNNs) are vulnerable to adversarial attacks. In\n",
      "particular, a single perturbation known as the universal adversarial\n",
      "perturbation (UAP) can foil most classification tasks conducted by DNNs. Thus,\n",
      "different methods for generating UAPs are required to fully evaluate the\n",
      "vulnerability of DNNs. A realistic evaluation would be with cases that consider\n",
      "targeted attacks; wherein the generated UAP causes DNN to classify an input\n",
      "into a specific class. However, the development of UAPs for targeted attacks\n",
      "has largely fallen behind that of UAPs for non-targeted attacks. Therefore, we\n",
      "propose a simple iterative method to generate UAPs for targeted attacks. Our\n",
      "method combines the simple iterative method for generating non-targeted UAPs\n",
      "and the fast gradient sign method for generating a targeted adversarial\n",
      "perturbation for an input. We applied the proposed method to state-of-the-art\n",
      "DNN models for image classification and proved the existence of almost\n",
      "imperceptible UAPs for targeted attacks; further, we demonstrated that such\n",
      "UAPs are easily generatable.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.15969 \n",
      "Title :Interpretation of 3D CNNs for Brain MRI Data Classification\n",
      "  Deep learning shows high potential for many medical image analysis tasks.\n",
      "Neural networks can work with full-size data without extensive preprocessing\n",
      "and feature generation and, thus, information loss. Recent work has shown that\n",
      "the morphological difference in specific brain regions can be found on MRI with\n",
      "the means of Convolution Neural Networks (CNN). However, interpretation of the\n",
      "existing models is based on a region of interest and can not be extended to\n",
      "voxel-wise image interpretation on a whole image. In the current work, we\n",
      "consider the classification task on a large-scale open-source dataset of young\n",
      "healthy subjects -- an exploration of brain differences between men and women.\n",
      "In this paper, we extend the previous findings in gender differences from\n",
      "diffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D\n",
      "CNN interpretation comparing the results of three interpretation methods:\n",
      "Meaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute\n",
      "with the open-source library.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.16365 \n",
      "Title :Multi-Partition Embedding Interaction with Block Term Format for\n",
      "  Knowledge Graph Completion\n",
      "  Knowledge graph completion is an important task that aims to predict the\n",
      "missing relational link between entities. Knowledge graph embedding methods\n",
      "perform this task by representing entities and relations as embedding vectors\n",
      "and modeling their interactions to compute the matching score of each triple.\n",
      "Previous work has usually treated each embedding as a whole and has modeled the\n",
      "interactions between these whole embeddings, potentially making the model\n",
      "excessively expensive or requiring specially designed interaction mechanisms.\n",
      "In this work, we propose the multi-partition embedding interaction (MEI) model\n",
      "with block term format to systematically address this problem. MEI divides each\n",
      "embedding into a multi-partition vector to efficiently restrict the\n",
      "interactions. Each local interaction is modeled with the Tucker tensor format\n",
      "and the full interaction is modeled with the block term tensor format, enabling\n",
      "MEI to control the trade-off between expressiveness and computational cost,\n",
      "learn the interaction mechanisms from data automatically, and achieve\n",
      "state-of-the-art performance on the link prediction task. In addition, we\n",
      "theoretically study the parameter efficiency problem and derive a simple\n",
      "empirically verified criterion for optimal parameter trade-off. We also apply\n",
      "the framework of MEI to provide a new generalized explanation for several\n",
      "specially designed interaction mechanisms in previous models.\n",
      "\n",
      "**Paper Id :1905.10702 \n",
      "Title :MDE: Multiple Distance Embeddings for Link Prediction in Knowledge\n",
      "  Graphs\n",
      "  Over the past decade, knowledge graphs became popular for capturing\n",
      "structured domain knowledge. Relational learning models enable the prediction\n",
      "of missing links inside knowledge graphs. More specifically, latent distance\n",
      "approaches model the relationships among entities via a distance between latent\n",
      "representations. Translating embedding models (e.g., TransE) are among the most\n",
      "popular latent distance approaches which use one distance function to learn\n",
      "multiple relation patterns. However, they are mostly inefficient in capturing\n",
      "symmetric relations since the representation vector norm for all the symmetric\n",
      "relations becomes equal to zero. They also lose information when learning\n",
      "relations with reflexive patterns since they become symmetric and transitive.\n",
      "We propose the Multiple Distance Embedding model (MDE) that addresses these\n",
      "limitations and a framework to collaboratively combine variant latent\n",
      "distance-based terms. Our solution is based on two principles: 1) we use a\n",
      "limit-based loss instead of a margin ranking loss and, 2) by learning\n",
      "independent embedding vectors for each of the terms we can collectively train\n",
      "and predict using contradicting distance terms. We further demonstrate that MDE\n",
      "allows modeling relations with (anti)symmetry, inversion, and composition\n",
      "patterns. We propose MDE as a neural network model that allows us to map\n",
      "non-linear relations between the embedding vectors and the expected output of\n",
      "the score function. Our empirical results show that MDE performs competitively\n",
      "to state-of-the-art embedding models on several benchmark datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.16540 \n",
      "Title :Associative Memory in Iterated Overparameterized Sigmoid Autoencoders\n",
      "  Recent work showed that overparameterized autoencoders can be trained to\n",
      "implement associative memory via iterative maps, when the trained input-output\n",
      "Jacobian of the network has all of its eigenvalue norms strictly below one.\n",
      "Here, we theoretically analyze this phenomenon for sigmoid networks by\n",
      "leveraging recent developments in deep learning theory, especially the\n",
      "correspondence between training neural networks in the infinite-width limit and\n",
      "performing kernel regression with the Neural Tangent Kernel (NTK). We find that\n",
      "overparameterized sigmoid autoencoders can have attractors in the NTK limit for\n",
      "both training with a single example and multiple examples under certain\n",
      "conditions. In particular, for multiple training examples, we find that the\n",
      "norm of the largest Jacobian eigenvalue drops below one with increasing input\n",
      "norm, leading to associative memory.\n",
      "\n",
      "**Paper Id :1808.00408 \n",
      "Title :Geometry of energy landscapes and the optimizability of deep neural\n",
      "  networks\n",
      "  Deep neural networks are workhorse models in machine learning with multiple\n",
      "layers of non-linear functions composed in series. Their loss function is\n",
      "highly non-convex, yet empirically even gradient descent minimisation is\n",
      "sufficient to arrive at accurate and predictive models. It is hitherto unknown\n",
      "why are deep neural networks easily optimizable. We analyze the energy\n",
      "landscape of a spin glass model of deep neural networks using random matrix\n",
      "theory and algebraic geometry. We analytically show that the multilayered\n",
      "structure holds the key to optimizability: Fixing the number of parameters and\n",
      "increasing network depth, the number of stationary points in the loss function\n",
      "decreases, minima become more clustered in parameter space, and the tradeoff\n",
      "between the depth and width of minima becomes less severe. Our analytical\n",
      "results are numerically verified through comparison with neural networks\n",
      "trained on a set of classical benchmark datasets. Our model uncovers generic\n",
      "design principles of machine learning models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2006.16926 \n",
      "Title :A Deep Learning Pipeline for Patient Diagnosis Prediction Using\n",
      "  Electronic Health Records\n",
      "  Augmentation of disease diagnosis and decision-making in healthcare with\n",
      "machine learning algorithms is gaining much impetus in recent years. In\n",
      "particular, in the current epidemiological situation caused by COVID-19\n",
      "pandemic, swift and accurate prediction of disease diagnosis with machine\n",
      "learning algorithms could facilitate identification and care of vulnerable\n",
      "clusters of population, such as those having multi-morbidity conditions. In\n",
      "order to build a useful disease diagnosis prediction system, advancement in\n",
      "both data representation and development of machine learning architectures are\n",
      "imperative. First, with respect to data collection and representation, we face\n",
      "severe problems due to multitude of formats and lack of coherency prevalent in\n",
      "Electronic Health Records (EHRs). This causes hindrance in extraction of\n",
      "valuable information contained in EHRs. Currently, no universal global data\n",
      "standard has been established. As a useful solution, we develop and publish a\n",
      "Python package to transform public health dataset into an easy to access\n",
      "universal format. This data transformation to an international health data\n",
      "format facilitates researchers to easily combine EHR datasets with clinical\n",
      "datasets of diverse formats. Second, machine learning algorithms that predict\n",
      "multiple disease diagnosis categories simultaneously remain underdeveloped. We\n",
      "propose two novel model architectures in this regard. First, DeepObserver,\n",
      "which uses structured numerical data to predict the diagnosis categories and\n",
      "second, ClinicalBERT_Multi, that incorporates rich information available in\n",
      "clinical notes via natural language processing methods and also provides\n",
      "interpretable visualizations to medical practitioners. We show that both models\n",
      "can predict multiple diagnoses simultaneously with high accuracy.\n",
      "\n",
      "**Paper Id :2005.03227 \n",
      "Title :Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent\n",
      "  Multi-View Representation Learning\n",
      "  Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread\n",
      "rapidly across the world. Due to the large number of affected patients and\n",
      "heavy labor for doctors, computer-aided diagnosis with machine learning\n",
      "algorithm is urgently needed, and could largely reduce the efforts of\n",
      "clinicians and accelerate the diagnosis process. Chest computed tomography (CT)\n",
      "has been recognized as an informative tool for diagnosis of the disease. In\n",
      "this study, we propose to conduct the diagnosis of COVID-19 with a series of\n",
      "features extracted from CT images. To fully explore multiple features\n",
      "describing CT images from different views, a unified latent representation is\n",
      "learned which can completely encode information from different aspects of\n",
      "features and is endowed with promising class structure for separability.\n",
      "Specifically, the completeness is guaranteed with a group of backward neural\n",
      "networks (each for one type of features), while by using class labels the\n",
      "representation is enforced to be compact within COVID-19/community-acquired\n",
      "pneumonia (CAP) and also a large margin is guaranteed between different types\n",
      "of pneumonia. In this way, our model can well avoid overfitting compared to the\n",
      "case of directly projecting highdimensional features into classes. Extensive\n",
      "experimental results show that the proposed method outperforms all comparison\n",
      "methods, and rather stable performances are observed when varying the numbers\n",
      "of training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.00224 \n",
      "Title :Debiased Contrastive Learning\n",
      "  A prominent technique for self-supervised representation learning has been to\n",
      "contrast semantically similar and dissimilar pairs of samples. Without access\n",
      "to labels, dissimilar (negative) points are typically taken to be randomly\n",
      "sampled datapoints, implicitly accepting that these points may, in reality,\n",
      "actually have the same label. Perhaps unsurprisingly, we observe that sampling\n",
      "negative examples from truly different labels improves performance, in a\n",
      "synthetic setting where labels are available. Motivated by this observation, we\n",
      "develop a debiased contrastive objective that corrects for the sampling of\n",
      "same-label datapoints, even without knowledge of the true labels. Empirically,\n",
      "the proposed objective consistently outperforms the state-of-the-art for\n",
      "representation learning in vision, language, and reinforcement learning\n",
      "benchmarks. Theoretically, we establish generalization bounds for the\n",
      "downstream classification task.\n",
      "\n",
      "**Paper Id :2003.07406 \n",
      "Title :Neighborhood-based Pooling for Population-level Label Distribution\n",
      "  Learning\n",
      "  Supervised machine learning often requires human-annotated data. While\n",
      "annotator disagreement is typically interpreted as evidence of noise,\n",
      "population-level label distribution learning (PLDL) treats the collection of\n",
      "annotations for each data item as a sample of the opinions of a population of\n",
      "human annotators, among whom disagreement may be proper and expected, even with\n",
      "no noise present. From this perspective, a typical training set may contain a\n",
      "large number of very small-sized samples, one for each data item, none of\n",
      "which, by itself, is large enough to be considered representative of the\n",
      "underlying population's beliefs about that item. We propose an algorithmic\n",
      "framework and new statistical tests for PLDL that account for sampling size. We\n",
      "apply them to previously proposed methods for sharing labels across similar\n",
      "data items. We also propose new approaches for label sharing, which we call\n",
      "neighborhood-based pooling.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.00936 \n",
      "Title :Deep Neural Networks for Nonlinear Model Order Reduction of Unsteady\n",
      "  Flows\n",
      "  Unsteady fluid systems are nonlinear high-dimensional dynamical systems that\n",
      "may exhibit multiple complex phenomena both in time and space. Reduced Order\n",
      "Modeling (ROM) of fluid flows has been an active research topic in the recent\n",
      "decade with the primary goal to decompose complex flows to a set of features\n",
      "most important for future state prediction and control, typically using a\n",
      "dimensionality reduction technique. In this work, a novel data-driven technique\n",
      "based on the power of deep neural networks for reduced order modeling of the\n",
      "unsteady fluid flows is introduced. An autoencoder network is used for\n",
      "nonlinear dimension reduction and feature extraction as an alternative for\n",
      "singular value decomposition (SVD). Then, the extracted features are used as an\n",
      "input for long short-term memory network (LSTM) to predict the velocity field\n",
      "at future time instances. The proposed autoencoder-LSTM method is compared with\n",
      "non-intrusive reduced order models based on dynamic mode decomposition (DMD)\n",
      "and proper orthogonal decomposition (POD). Moreover, an autoencoder-DMD\n",
      "algorithm is introduced for reduced order modeling, which uses the autoencoder\n",
      "network for dimensionality reduction rather than SVD rank truncation. Results\n",
      "show that the autoencoder-LSTM method is considerably capable of predicting\n",
      "fluid flow evolution, where higher values for coefficient of determination\n",
      "$R^{2}$ are obtained using autoencoder-LSTM compared to other models.\n",
      "\n",
      "**Paper Id :1810.05547 \n",
      "Title :Physics-Driven Regularization of Deep Neural Networks for Enhanced\n",
      "  Engineering Design and Analysis\n",
      "  In this paper, we introduce a physics-driven regularization method for\n",
      "training of deep neural networks (DNNs) for use in engineering design and\n",
      "analysis problems. In particular, we focus on prediction of a physical system,\n",
      "for which in addition to training data, partial or complete information on a\n",
      "set of governing laws is also available. These laws often appear in the form of\n",
      "differential equations, derived from first principles, empirically-validated\n",
      "laws, or domain expertise, and are usually neglected in data-driven prediction\n",
      "of engineering systems. We propose a training approach that utilizes the known\n",
      "governing laws and regularizes data-driven DNN models by penalizing divergence\n",
      "from those laws. The first two numerical examples are synthetic examples, where\n",
      "we show that in constructing a DNN model that best fits the measurements from a\n",
      "physical system, the use of our proposed regularization results in DNNs that\n",
      "are more interpretable with smaller generalization errors, compared to other\n",
      "common regularization methods. The last two examples concern metamodeling for a\n",
      "random Burgers' system and for aerodynamic analysis of passenger vehicles,\n",
      "where we demonstrate that the proposed regularization provides superior\n",
      "generalization accuracy compared to other common alternatives.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.01062 \n",
      "Title :Are there any 'object detectors' in the hidden layers of CNNs trained to\n",
      "  identify objects or scenes?\n",
      "  Various methods of measuring unit selectivity have been developed with the\n",
      "aim of better understanding how neural networks work. But the different\n",
      "measures provide divergent estimates of selectivity, and this has led to\n",
      "different conclusions regarding the conditions in which selective object\n",
      "representations are learned and the functional relevance of these\n",
      "representations. In an attempt to better characterize object selectivity, we\n",
      "undertake a comparison of various selectivity measures on a large set of units\n",
      "in AlexNet, including localist selectivity, precision, class-conditional mean\n",
      "activity selectivity (CCMAS), network dissection,the human interpretation of\n",
      "activation maximization (AM) images, and standard signal-detection measures. We\n",
      "find that the different measures provide different estimates of object\n",
      "selectivity, with precision and CCMAS measures providing misleadingly high\n",
      "estimates. Indeed, the most selective units had a poor hit-rate or a high\n",
      "false-alarm rate (or both) in object classification, making them poor object\n",
      "detectors. We fail to find any units that are even remotely as selective as the\n",
      "'grandmother cell' units reported in recurrent neural networks. In order to\n",
      "generalize these results, we compared selectivity measures on units in VGG-16\n",
      "and GoogLeNet trained on the ImageNet or Places-365 datasets that have been\n",
      "described as 'object detectors'. Again, we find poor hit-rates and high\n",
      "false-alarm rates for object classification. We conclude that signal-detection\n",
      "measures provide a better assessment of single-unit selectivity compared to\n",
      "common alternative approaches, and that deep convolutional networks of image\n",
      "classification do not learn object detectors in their hidden layers.\n",
      "\n",
      "**Paper Id :2002.08247 \n",
      "Title :Learning Global Transparent Models Consistent with Local Contrastive\n",
      "  Explanations\n",
      "  There is a rich and growing literature on producing local\n",
      "contrastive/counterfactual explanations for black-box models (e.g. neural\n",
      "networks).\n",
      "  In these methods, for an input, an explanation is in the form of a contrast\n",
      "point differing in very few features from the original input and lying in a\n",
      "different class. Other works try to build globally interpretable models like\n",
      "decision trees and rule lists based on the data using actual labels or based on\n",
      "the black-box models predictions. Although these interpretable global models\n",
      "can be useful, they may not be consistent with local explanations from a\n",
      "specific black-box of choice. In this work, we explore the question: Can we\n",
      "produce a transparent global model that is simultaneously accurate and\n",
      "consistent with the local (contrastive) explanations of the black-box model? We\n",
      "introduce a natural local consistency metric that quantifies if the local\n",
      "explanations and predictions of the black-box model are also consistent with\n",
      "the proxy global transparent model. Based on a key insight we propose a novel\n",
      "method where we create custom boolean features from sparse local contrastive\n",
      "explanations of the black-box model and then train a globally transparent model\n",
      "on just these, and showcase empirically that such models have higher local\n",
      "consistency compared with other known strategies, while still being close in\n",
      "performance to models that are trained with access to the original data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.01135 \n",
      "Title :Student-Teacher Curriculum Learning via Reinforcement Learning:\n",
      "  Predicting Hospital Inpatient Admission Location\n",
      "  Accurate and reliable prediction of hospital admission location is important\n",
      "due to resource-constraints and space availability in a clinical setting,\n",
      "particularly when dealing with patients who come from the emergency department.\n",
      "In this work we propose a student-teacher network via reinforcement learning to\n",
      "deal with this specific problem. A representation of the weights of the student\n",
      "network is treated as the state and is fed as an input to the teacher network.\n",
      "The teacher network's action is to select the most appropriate batch of data to\n",
      "train the student network on from a training set sorted according to entropy.\n",
      "By validating on three datasets, not only do we show that our approach\n",
      "outperforms state-of-the-art methods on tabular data and performs competitively\n",
      "on image recognition, but also that novel curricula are learned by the teacher\n",
      "network. We demonstrate experimentally that the teacher network can actively\n",
      "learn about the student network and guide it to achieve better performance than\n",
      "if trained alone.\n",
      "\n",
      "**Paper Id :1909.03742 \n",
      "Title :Efficient Continual Learning in Neural Networks with Embedding\n",
      "  Regularization\n",
      "  Continual learning of deep neural networks is a key requirement for scaling\n",
      "them up to more complex applicative scenarios and for achieving real lifelong\n",
      "learning of these architectures. Previous approaches to the problem have\n",
      "considered either the progressive increase in the size of the networks, or have\n",
      "tried to regularize the network behavior to equalize it with respect to\n",
      "previously observed tasks. In the latter case, it is essential to understand\n",
      "what type of information best represents this past behavior. Common techniques\n",
      "include regularizing the past outputs, gradients, or individual weights. In\n",
      "this work, we propose a new, relatively simple and efficient method to perform\n",
      "continual learning by regularizing instead the network internal embeddings. To\n",
      "make the approach scalable, we also propose a dynamic sampling strategy to\n",
      "reduce the memory footprint of the required external storage. We show that our\n",
      "method performs favorably with respect to state-of-the-art approaches in the\n",
      "literature, while requiring significantly less space in memory and\n",
      "computational time. In addition, inspired inspired by to recent works, we\n",
      "evaluate the impact of selecting a more flexible model for the activation\n",
      "functions inside the network, evaluating the impact of catastrophic forgetting\n",
      "on the activation functions themselves.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.01160 \n",
      "Title :Tight Bounds on Minimax Regret under Logarithmic Loss via\n",
      "  Self-Concordance\n",
      "  We consider the classical problem of sequential probability assignment under\n",
      "logarithmic loss while competing against an arbitrary, potentially\n",
      "nonparametric class of experts. We obtain tight bounds on the minimax regret\n",
      "via a new approach that exploits the self-concordance property of the\n",
      "logarithmic loss. We show that for any expert class with (sequential) metric\n",
      "entropy $\\mathcal{O}(\\gamma^{-p})$ at scale $\\gamma$, the minimax regret is\n",
      "$\\mathcal{O}(n^{p/(p+1)})$, and that this rate cannot be improved without\n",
      "additional assumptions on the expert class under consideration. As an\n",
      "application of our techniques, we resolve the minimax regret for nonparametric\n",
      "Lipschitz classes of experts.\n",
      "\n",
      "**Paper Id :1902.04376 \n",
      "Title :An adaptive stochastic optimization algorithm for resource allocation\n",
      "  We consider the classical problem of sequential resource allocation where a\n",
      "decision maker must repeatedly divide a budget between several resources, each\n",
      "with diminishing returns. This can be recast as a specific stochastic\n",
      "optimization problem where the objective is to maximize the cumulative reward,\n",
      "or equivalently to minimize the regret. We construct an algorithm that is {\\em\n",
      "adaptive} to the complexity of the problem, expressed in term of the regularity\n",
      "of the returns of the resources, measured by the exponent in the {\\L}ojasiewicz\n",
      "inequality (or by their universal concavity parameter). Our\n",
      "parameter-independent algorithm recovers the optimal rates for strongly-concave\n",
      "functions and the classical fast rates of multi-armed bandit (for linear reward\n",
      "functions). Moreover, the algorithm improves existing results on stochastic\n",
      "optimization in this regret minimization setting for intermediate cases.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.02010 \n",
      "Title :DessiLBI: Exploring Structural Sparsity of Deep Networks via\n",
      "  Differential Inclusion Paths\n",
      "  Over-parameterization is ubiquitous nowadays in training neural networks to\n",
      "benefit both optimization in seeking global optima and generalization in\n",
      "reducing prediction error. However, compressive networks are desired in many\n",
      "real world applications and direct training of small networks may be trapped in\n",
      "local optima. In this paper, instead of pruning or distilling\n",
      "over-parameterized models to compressive ones, we propose a new approach based\n",
      "on differential inclusions of inverse scale spaces. Specifically, it generates\n",
      "a family of models from simple to complex ones that couples a pair of\n",
      "parameters to simultaneously train over-parameterized deep models and\n",
      "structural sparsity on weights of fully connected and convolutional layers.\n",
      "Such a differential inclusion scheme has a simple discretization, proposed as\n",
      "Deep structurally splitting Linearized Bregman Iteration (DessiLBI), whose\n",
      "global convergence analysis in deep learning is established that from any\n",
      "initializations, algorithmic iterations converge to a critical point of\n",
      "empirical risks. Experimental evidence shows that DessiLBI achieve comparable\n",
      "and even better performance than the competitive optimizers in exploring the\n",
      "structural sparsity of several widely used backbones on the benchmark datasets.\n",
      "Remarkably, with early stopping, DessiLBI unveils \"winning tickets\" in early\n",
      "epochs: the effective sparse structure with comparable test accuracy to fully\n",
      "trained over-parameterized models.\n",
      "\n",
      "**Paper Id :1904.06194 \n",
      "Title :Compressing deep neural networks by matrix product operators\n",
      "  A deep neural network is a parametrization of a multilayer mapping of signals\n",
      "in terms of many alternatively arranged linear and nonlinear transformations.\n",
      "The linear transformations, which are generally used in the fully connected as\n",
      "well as convolutional layers, contain most of the variational parameters that\n",
      "are trained and stored. Compressing a deep neural network to reduce its number\n",
      "of variational parameters but not its prediction power is an important but\n",
      "challenging problem toward the establishment of an optimized scheme in training\n",
      "efficiently these parameters and in lowering the risk of overfitting. Here we\n",
      "show that this problem can be effectively solved by representing linear\n",
      "transformations with matrix product operators (MPOs), which is a tensor network\n",
      "originally proposed in physics to characterize the short-range entanglement in\n",
      "one-dimensional quantum states. We have tested this approach in five typical\n",
      "neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\n",
      "widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\n",
      "representation indeed sets up a faithful and efficient mapping between input\n",
      "and output signals, which can keep or even improve the prediction accuracy with\n",
      "a dramatically reduced number of parameters. Our method greatly simplifies the\n",
      "representations in deep learning, and opens a possible route toward\n",
      "establishing a framework of modern neural networks which might be simpler and\n",
      "cheaper, but more efficient.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.02040 \n",
      "Title :Discount Factor as a Regularizer in Reinforcement Learning\n",
      "  Specifying a Reinforcement Learning (RL) task involves choosing a suitable\n",
      "planning horizon, which is typically modeled by a discount factor. It is known\n",
      "that applying RL algorithms with a lower discount factor can act as a\n",
      "regularizer, improving performance in the limited data regime. Yet the exact\n",
      "nature of this regularizer has not been investigated. In this work, we fill in\n",
      "this gap. For several Temporal-Difference (TD) learning methods, we show an\n",
      "explicit equivalence between using a reduced discount factor and adding an\n",
      "explicit regularization term to the algorithm's loss. Motivated by the\n",
      "equivalence, we empirically study this technique compared to standard $L_2$\n",
      "regularization by extensive experiments in discrete and continuous domains,\n",
      "using tabular and functional representations. Our experiments suggest the\n",
      "regularization effectiveness is strongly related to properties of the available\n",
      "data, such as size, distribution, and mixing rate.\n",
      "\n",
      "**Paper Id :1902.08234 \n",
      "Title :An Empirical Study of Large-Batch Stochastic Gradient Descent with\n",
      "  Structured Covariance Noise\n",
      "  The choice of batch-size in a stochastic optimization algorithm plays a\n",
      "substantial role for both optimization and generalization. Increasing the\n",
      "batch-size used typically improves optimization but degrades generalization. To\n",
      "address the problem of improving generalization while maintaining optimal\n",
      "convergence in large-batch training, we propose to add covariance noise to the\n",
      "gradients. We demonstrate that the learning performance of our method is more\n",
      "accurately captured by the structure of the covariance matrix of the noise\n",
      "rather than by the variance of gradients. Moreover, over the convex-quadratic,\n",
      "we prove in theory that it can be characterized by the Frobenius norm of the\n",
      "noise matrix. Our empirical studies with standard deep learning\n",
      "model-architectures and datasets shows that our method not only improves\n",
      "generalization performance in large-batch training, but furthermore, does so in\n",
      "a way where the optimization performance remains desirable and the training\n",
      "duration is not elongated.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.02168 \n",
      "Title :Scalable Differentiable Physics for Learning and Control\n",
      "  Differentiable physics is a powerful approach to learning and control\n",
      "problems that involve physical objects and environments. While notable progress\n",
      "has been made, the capabilities of differentiable physics solvers remain\n",
      "limited. We develop a scalable framework for differentiable physics that can\n",
      "support a large number of objects and their interactions. To accommodate\n",
      "objects with arbitrary geometry and topology, we adopt meshes as our\n",
      "representation and leverage the sparsity of contacts for scalable\n",
      "differentiable collision handling. Collisions are resolved in localized regions\n",
      "to minimize the number of optimization variables even when the number of\n",
      "simulated objects is high. We further accelerate implicit differentiation of\n",
      "optimization with nonlinear constraints. Experiments demonstrate that the\n",
      "presented framework requires up to two orders of magnitude less memory and\n",
      "computation in comparison to recent particle-based methods. We further validate\n",
      "the approach on inverse problems and control scenarios, where it outperforms\n",
      "derivative-free and model-free baselines by at least an order of magnitude.\n",
      "\n",
      "**Paper Id :2001.08861 \n",
      "Title :Encoding Physical Constraints in Differentiable Newton-Euler Algorithm\n",
      "  The recursive Newton-Euler Algorithm (RNEA) is a popular technique for\n",
      "computing the dynamics of robots. RNEA can be framed as a differentiable\n",
      "computational graph, enabling the dynamics parameters of the robot to be\n",
      "learned from data via modern auto-differentiation toolboxes. However, the\n",
      "dynamics parameters learned in this manner can be physically implausible. In\n",
      "this work, we incorporate physical constraints in the learning by adding\n",
      "structure to the learned parameters. This results in a framework that can learn\n",
      "physically plausible dynamics via gradient descent, improving the training\n",
      "speed as well as generalization of the learned dynamics models. We evaluate our\n",
      "method on real-time inverse dynamics control tasks on a 7 degree of freedom\n",
      "robot arm, both in simulation and on the real robot. Our experiments study a\n",
      "spectrum of structure added to the parameters of the differentiable RNEA\n",
      "algorithm, and compare their performance and generalization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.02334 \n",
      "Title :Multi-Manifold Learning for Large-scale Targeted Advertising System\n",
      "  Messenger advertisements (ads) give direct and personal user experience\n",
      "yielding high conversion rates and sales. However, people are skeptical about\n",
      "ads and sometimes perceive them as spam, which eventually leads to a decrease\n",
      "in user satisfaction. Targeted advertising, which serves ads to individuals who\n",
      "may exhibit interest in a particular advertising message, is strongly required.\n",
      "The key to the success of precise user targeting lies in learning the accurate\n",
      "user and ad representation in the embedding space. Most of the previous studies\n",
      "have limited the representation learning in the Euclidean space, but recent\n",
      "studies have suggested hyperbolic manifold learning for the distinct projection\n",
      "of complex network properties emerging from real-world datasets such as social\n",
      "networks, recommender systems, and advertising. We propose a framework that can\n",
      "effectively learn the hierarchical structure in users and ads on the hyperbolic\n",
      "space, and extend to the Multi-Manifold Learning. Our method constructs\n",
      "multiple hyperbolic manifolds with learnable curvatures and maps the\n",
      "representation of user and ad to each manifold. The origin of each manifold is\n",
      "set as the centroid of each user cluster. The user preference for each ad is\n",
      "estimated using the distance between two entities in the hyperbolic space, and\n",
      "the final prediction is determined by aggregating the values calculated from\n",
      "the learned multiple manifolds. We evaluate our method on public benchmark\n",
      "datasets and a large-scale commercial messenger system LINE, and demonstrate\n",
      "its effectiveness through improved performance.\n",
      "\n",
      "**Paper Id :2005.01690 \n",
      "Title :Learning Geo-Contextual Embeddings for Commuting Flow Prediction\n",
      "  Predicting commuting flows based on infrastructure and land-use information\n",
      "is critical for urban planning and public policy development. However, it is a\n",
      "challenging task given the complex patterns of commuting flows. Conventional\n",
      "models, such as gravity model, are mainly derived from physics principles and\n",
      "limited by their predictive power in real-world scenarios where many factors\n",
      "need to be considered. Meanwhile, most existing machine learning-based methods\n",
      "ignore the spatial correlations and fail to model the influence of nearby\n",
      "regions. To address these issues, we propose Geo-contextual Multitask Embedding\n",
      "Learner (GMEL), a model that captures the spatial correlations from geographic\n",
      "contextual information for commuting flow prediction. Specifically, we first\n",
      "construct a geo-adjacency network containing the geographic contextual\n",
      "information. Then, an attention mechanism is proposed based on the framework of\n",
      "graph attention network (GAT) to capture the spatial correlations and encode\n",
      "geographic contextual information to embedding space. Two separate GATs are\n",
      "used to model supply and demand characteristics. A multitask learning framework\n",
      "is used to introduce stronger restrictions and enhance the effectiveness of the\n",
      "embedding representation. Finally, a gradient boosting machine is trained based\n",
      "on the learned embeddings to predict commuting flows. We evaluate our model\n",
      "using real-world datasets from New York City and the experimental results\n",
      "demonstrate the effectiveness of our proposal against the state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.02376 \n",
      "Title :Block Model Guided Unsupervised Feature Selection\n",
      "  Feature selection is a core area of data mining with a recent innovation of\n",
      "graph-driven unsupervised feature selection for linked data. In this setting we\n",
      "have a dataset $\\mathbf{Y}$ consisting of $n$ instances each with $m$ features\n",
      "and a corresponding $n$ node graph (whose adjacency matrix is $\\mathbf{A}$)\n",
      "with an edge indicating that the two instances are similar. Existing efforts\n",
      "for unsupervised feature selection on attributed networks have explored either\n",
      "directly regenerating the links by solving for $f$ such that\n",
      "$f(\\mathbf{y}_i,\\mathbf{y}_j) \\approx \\mathbf{A}_{i,j}$ or finding community\n",
      "structure in $\\mathbf{A}$ and using the features in $\\mathbf{Y}$ to predict\n",
      "these communities. However, graph-driven unsupervised feature selection remains\n",
      "an understudied area with respect to exploring more complex guidance. Here we\n",
      "take the novel approach of first building a block model on the graph and then\n",
      "using the block model for feature selection. That is, we discover\n",
      "$\\mathbf{F}\\mathbf{M}\\mathbf{F}^T \\approx \\mathbf{A}$ and then find a subset of\n",
      "features $\\mathcal{S}$ that induces another graph to preserve both $\\mathbf{F}$\n",
      "and $\\mathbf{M}$. We call our approach Block Model Guided Unsupervised Feature\n",
      "Selection (BMGUFS). Experimental results show that our method outperforms the\n",
      "state of the art on several real-world public datasets in finding high-quality\n",
      "features for clustering.\n",
      "\n",
      "**Paper Id :1901.07114 \n",
      "Title :Training Neural Networks as Learning Data-adaptive Kernels: Provable\n",
      "  Representation and Approximation Benefits\n",
      "  Consider the problem: given the data pair $(\\mathbf{x}, \\mathbf{y})$ drawn\n",
      "from a population with $f_*(x) = \\mathbf{E}[\\mathbf{y} | \\mathbf{x} = x]$,\n",
      "specify a neural network model and run gradient flow on the weights over time\n",
      "until reaching any stationarity. How does $f_t$, the function computed by the\n",
      "neural network at time $t$, relate to $f_*$, in terms of approximation and\n",
      "representation? What are the provable benefits of the adaptive representation\n",
      "by neural networks compared to the pre-specified fixed basis representation in\n",
      "the classical nonparametric literature? We answer the above questions via a\n",
      "dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the\n",
      "training process of neural networks. Firstly, we show that when reaching any\n",
      "local stationarity, gradient flow learns an adaptive RKHS representation and\n",
      "performs the global least-squares projection onto the adaptive RKHS,\n",
      "simultaneously. Secondly, we prove that as the RKHS is data-adaptive and\n",
      "task-specific, the residual for $f_*$ lies in a subspace that is potentially\n",
      "much smaller than the orthogonal complement of the RKHS. The result formalizes\n",
      "the representation and approximation benefits of neural networks. Lastly, we\n",
      "show that the neural network function computed by gradient flow converges to\n",
      "the kernel ridgeless regression with an adaptive kernel, in the limit of\n",
      "vanishing regularization. The adaptive kernel viewpoint provides new angles of\n",
      "studying the approximation, representation, generalization, and optimization\n",
      "advantages of neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.03326 \n",
      "Title :An Integer Programming Approach to Deep Neural Networks with Binary\n",
      "  Activation Functions\n",
      "  We study deep neural networks with binary activation functions (BDNN), i.e.\n",
      "the activation function only has two states. We show that the BDNN can be\n",
      "reformulated as a mixed-integer linear program which can be solved to global\n",
      "optimality by classical integer programming solvers. Additionally, a heuristic\n",
      "solution algorithm is presented and we study the model under data uncertainty,\n",
      "applying a two-stage robust optimization approach. We implemented our methods\n",
      "on random and real datasets and show that the heuristic version of the BDNN\n",
      "outperforms classical deep neural networks on the Breast Cancer Wisconsin\n",
      "dataset while performing worse on random data.\n",
      "\n",
      "**Paper Id :1808.00408 \n",
      "Title :Geometry of energy landscapes and the optimizability of deep neural\n",
      "  networks\n",
      "  Deep neural networks are workhorse models in machine learning with multiple\n",
      "layers of non-linear functions composed in series. Their loss function is\n",
      "highly non-convex, yet empirically even gradient descent minimisation is\n",
      "sufficient to arrive at accurate and predictive models. It is hitherto unknown\n",
      "why are deep neural networks easily optimizable. We analyze the energy\n",
      "landscape of a spin glass model of deep neural networks using random matrix\n",
      "theory and algebraic geometry. We analytically show that the multilayered\n",
      "structure holds the key to optimizability: Fixing the number of parameters and\n",
      "increasing network depth, the number of stationary points in the loss function\n",
      "decreases, minima become more clustered in parameter space, and the tradeoff\n",
      "between the depth and width of minima becomes less severe. Our analytical\n",
      "results are numerically verified through comparison with neural networks\n",
      "trained on a set of classical benchmark datasets. Our model uncovers generic\n",
      "design principles of machine learning models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.03511 \n",
      "Title :Estimating Generalization under Distribution Shifts via Domain-Invariant\n",
      "  Representations\n",
      "  When machine learning models are deployed on a test distribution different\n",
      "from the training distribution, they can perform poorly, but overestimate their\n",
      "performance. In this work, we aim to better estimate a model's performance\n",
      "under distribution shift, without supervision. To do so, we use a set of\n",
      "domain-invariant predictors as a proxy for the unknown, true target labels.\n",
      "Since the error of the resulting risk estimate depends on the target risk of\n",
      "the proxy model, we study generalization of domain-invariant representations\n",
      "and show that the complexity of the latent representation has a significant\n",
      "influence on the target risk. Empirically, our approach (1) enables self-tuning\n",
      "of domain adaptation models, and (2) accurately estimates the target error of\n",
      "given models under distribution shift. Other applications include model\n",
      "selection, deciding early stopping and error detection.\n",
      "\n",
      "**Paper Id :1902.08234 \n",
      "Title :An Empirical Study of Large-Batch Stochastic Gradient Descent with\n",
      "  Structured Covariance Noise\n",
      "  The choice of batch-size in a stochastic optimization algorithm plays a\n",
      "substantial role for both optimization and generalization. Increasing the\n",
      "batch-size used typically improves optimization but degrades generalization. To\n",
      "address the problem of improving generalization while maintaining optimal\n",
      "convergence in large-batch training, we propose to add covariance noise to the\n",
      "gradients. We demonstrate that the learning performance of our method is more\n",
      "accurately captured by the structure of the covariance matrix of the noise\n",
      "rather than by the variance of gradients. Moreover, over the convex-quadratic,\n",
      "we prove in theory that it can be characterized by the Frobenius norm of the\n",
      "noise matrix. Our empirical studies with standard deep learning\n",
      "model-architectures and datasets shows that our method not only improves\n",
      "generalization performance in large-batch training, but furthermore, does so in\n",
      "a way where the optimization performance remains desirable and the training\n",
      "duration is not elongated.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.03634 \n",
      "Title :PinnerSage: Multi-Modal User Embedding Framework for Recommendations at\n",
      "  Pinterest\n",
      "  Latent user representations are widely adopted in the tech industry for\n",
      "powering personalized recommender systems. Most prior work infers a single high\n",
      "dimensional embedding to represent a user, which is a good starting point but\n",
      "falls short in delivering a full understanding of the user's interests. In this\n",
      "work, we introduce PinnerSage, an end-to-end recommender system that represents\n",
      "each user via multi-modal embeddings and leverages this rich representation of\n",
      "users to provides high quality personalized recommendations. PinnerSage\n",
      "achieves this by clustering users' actions into conceptually coherent clusters\n",
      "with the help of a hierarchical clustering method (Ward) and summarizes the\n",
      "clusters via representative pins (Medoids) for efficiency and interpretability.\n",
      "PinnerSage is deployed in production at Pinterest and we outline the several\n",
      "design decisions that makes it run seamlessly at a very large scale. We conduct\n",
      "several offline and online A/B experiments to show that our method\n",
      "significantly outperforms single embedding methods.\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.05314 \n",
      "Title :ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection\n",
      "  In this paper, we introduce ID-Conditioned Auto-Encoder for unsupervised\n",
      "anomaly detection. Our method is an adaptation of the Class-Conditioned\n",
      "Auto-Encoder (C2AE) designed for the open-set recognition. Assuming that\n",
      "non-anomalous samples constitute of distinct IDs, we apply Conditioned\n",
      "Auto-Encoder with labels provided by these IDs. Opposed to C2AE, our approach\n",
      "omits the classification subtask and reduces the learning process to the single\n",
      "run. We simplify the learning process further by fixing a constant vector as\n",
      "the target for non-matching labels. We apply our method in the context of\n",
      "sounds for machine condition monitoring. We evaluate our method on the ToyADMOS\n",
      "and MIMII datasets from the DCASE 2020 Challenge Task 2. We conduct an ablation\n",
      "study to indicate which steps of our method influences results the most.\n",
      "\n",
      "**Paper Id :2002.06239 \n",
      "Title :Boosted Locality Sensitive Hashing: Discriminative Binary Codes for\n",
      "  Source Separation\n",
      "  Speech enhancement tasks have seen significant improvements with the advance\n",
      "of deep learning technology, but with the cost of increased computational\n",
      "complexity. In this study, we propose an adaptive boosting approach to learning\n",
      "locality sensitive hash codes, which represent audio spectra efficiently. We\n",
      "use the learned hash codes for single-channel speech denoising tasks as an\n",
      "alternative to a complex machine learning model, particularly to address the\n",
      "resource-constrained environments. Our adaptive boosting algorithm learns\n",
      "simple logistic regressors as the weak learners. Once trained, their binary\n",
      "classification results transform each spectrum of test noisy speech into a bit\n",
      "string. Simple bitwise operations calculate Hamming distance to find the\n",
      "K-nearest matching frames in the dictionary of training noisy speech spectra,\n",
      "whose associated ideal binary masks are averaged to estimate the denoising mask\n",
      "for that test mixture. Our proposed learning algorithm differs from AdaBoost in\n",
      "the sense that the projections are trained to minimize the distances between\n",
      "the self-similarity matrix of the hash codes and that of the original spectra,\n",
      "rather than the misclassification rate. We evaluate our discriminative hash\n",
      "codes on the TIMIT corpus with various noise types, and show comparative\n",
      "performance to deep learning methods in terms of denoising performance and\n",
      "complexity.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.05500 \n",
      "Title :Scientific Discovery by Generating Counterfactuals using Image\n",
      "  Translation\n",
      "  Model explanation techniques play a critical role in understanding the source\n",
      "of a model's performance and making its decisions transparent. Here we\n",
      "investigate if explanation techniques can also be used as a mechanism for\n",
      "scientific discovery. We make three contributions: first, we propose a\n",
      "framework to convert predictions from explanation techniques to a mechanism of\n",
      "discovery. Second, we show how generative models in combination with black-box\n",
      "predictors can be used to generate hypotheses (without human priors) that can\n",
      "be critically examined. Third, with these techniques we study classification\n",
      "models for retinal images predicting Diabetic Macular Edema (DME), where recent\n",
      "work showed that a CNN trained on these images is likely learning novel\n",
      "features in the image. We demonstrate that the proposed framework is able to\n",
      "explain the underlying scientific mechanism, thus bridging the gap between the\n",
      "model's performance and human understanding.\n",
      "\n",
      "**Paper Id :2002.05049 \n",
      "Title :Detect and Correct Bias in Multi-Site Neuroimaging Datasets\n",
      "  The desire to train complex machine learning algorithms and to increase the\n",
      "statistical power in association studies drives neuroimaging research to use\n",
      "ever-larger datasets. The most obvious way to increase sample size is by\n",
      "pooling scans from independent studies. However, simple pooling is often\n",
      "ill-advised as selection, measurement, and confounding biases may creep in and\n",
      "yield spurious correlations. In this work, we combine 35,320 magnetic resonance\n",
      "images of the brain from 17 studies to examine bias in neuroimaging. In the\n",
      "first experiment, Name That Dataset, we provide empirical evidence for the\n",
      "presence of bias by showing that scans can be correctly assigned to their\n",
      "respective dataset with 71.5% accuracy. Given such evidence, we take a closer\n",
      "look at confounding bias, which is often viewed as the main shortcoming in\n",
      "observational studies. In practice, we neither know all potential confounders\n",
      "nor do we have data on them. Hence, we model confounders as unknown, latent\n",
      "variables. Kolmogorov complexity is then used to decide whether the confounded\n",
      "or the causal model provides the simplest factorization of the graphical model.\n",
      "Finally, we present methods for dataset harmonization and study their ability\n",
      "to remove bias in imaging features. In particular, we propose an extension of\n",
      "the recently introduced ComBat algorithm to control for global variation across\n",
      "image features, inspired by adjusting for population stratification in\n",
      "genetics. Our results demonstrate that harmonization can reduce\n",
      "dataset-specific information in image features. Further, confounding bias can\n",
      "be reduced and even turned into a causal relationship. However, harmonziation\n",
      "also requires caution as it can easily remove relevant subject-specific\n",
      "information. Code is available at https://github.com/ai-med/Dataset-Bias.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.05732 \n",
      "Title :Online Parameter-Free Learning of Multiple Low Variance Tasks\n",
      "  We propose a method to learn a common bias vector for a growing sequence of\n",
      "low-variance tasks. Unlike state-of-the-art approaches, our method does not\n",
      "require tuning any hyper-parameter. Our approach is presented in the\n",
      "non-statistical setting and can be of two variants. The \"aggressive\" one\n",
      "updates the bias after each datapoint, the \"lazy\" one updates the bias only at\n",
      "the end of each task. We derive an across-tasks regret bound for the method.\n",
      "When compared to state-of-the-art approaches, the aggressive variant returns\n",
      "faster rates, the lazy one recovers standard rates, but with no need of tuning\n",
      "hyper-parameters. We then adapt the methods to the statistical setting: the\n",
      "aggressive variant becomes a multi-task learning method, the lazy one a\n",
      "meta-learning method. Experiments confirm the effectiveness of our methods in\n",
      "practice.\n",
      "\n",
      "**Paper Id :2002.10306 \n",
      "Title :Adaptive Propagation Graph Convolutional Network\n",
      "  Graph convolutional networks (GCNs) are a family of neural network models\n",
      "that perform inference on graph data by interleaving vertex-wise operations and\n",
      "message-passing exchanges across nodes. Concerning the latter, two key\n",
      "questions arise: (i) how to design a differentiable exchange protocol (e.g., a\n",
      "1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize\n",
      "the trade-off in complexity with respect to the local updates. In this paper,\n",
      "we show that state-of-the-art results can be achieved by adapting the number of\n",
      "communication steps independently at every node. In particular, we endow each\n",
      "node with a halting unit (inspired by Graves' adaptive computation time) that\n",
      "after every exchange decides whether to continue communicating or not. We show\n",
      "that the proposed adaptive propagation GCN (AP-GCN) achieves superior or\n",
      "similar results to the best proposed models so far on a number of benchmarks,\n",
      "while requiring a small overhead in terms of additional parameters. We also\n",
      "investigate a regularization term to enforce an explicit trade-off between\n",
      "communication and accuracy. The code for the AP-GCN experiments is released as\n",
      "an open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.05830 \n",
      "Title :AutoEmbedder: A semi-supervised DNN embedding system for clustering\n",
      "  Clustering is widely used in unsupervised learning method that deals with\n",
      "unlabeled data. Deep clustering has become a popular study area that relates\n",
      "clustering with Deep Neural Network (DNN) architecture. Deep clustering method\n",
      "downsamples high dimensional data, which may also relate clustering loss. Deep\n",
      "clustering is also introduced in semi-supervised learning (SSL). Most SSL\n",
      "methods depend on pairwise constraint information, which is a matrix containing\n",
      "knowledge if data pairs can be in the same cluster or not. This paper\n",
      "introduces a novel embedding system named AutoEmbedder, that downsamples higher\n",
      "dimensional data to clusterable embedding points. To the best of our knowledge,\n",
      "this is the first research endeavor that relates to traditional classifier DNN\n",
      "architecture with a pairwise loss reduction technique. The training process is\n",
      "semi-supervised and uses Siamese network architecture to compute pairwise\n",
      "constraint loss in the feature learning phase. The AutoEmbedder outperforms\n",
      "most of the existing DNN based semi-supervised methods tested on famous\n",
      "datasets.\n",
      "\n",
      "**Paper Id :1910.09798 \n",
      "Title :Improving Siamese Networks for One Shot Learning using Kernel Based\n",
      "  Activation functions\n",
      "  The lack of a large amount of training data has always been the constraining\n",
      "factor in solving a lot of problems in machine learning, making One Shot\n",
      "Learning one of the most intriguing ideas in machine learning. It aims to learn\n",
      "information about object categories from one, or only a few training examples.\n",
      "This process of learning in deep learning is usually accomplished by proper\n",
      "objective function, i.e; loss function and embeddings extraction i.e;\n",
      "architecture. In this paper, we discussed about metrics based deep learning\n",
      "architectures for one shot learning such as Siamese neural networks and present\n",
      "a method to improve on their accuracy using Kafnets (kernel-based\n",
      "non-parametric activation functions for neural networks) by learning proper\n",
      "embeddings with relatively less number of epochs. Using kernel activation\n",
      "functions, we are able to achieve strong results which exceed those of ReLU\n",
      "based deep learning models in terms of embeddings structure, loss convergence,\n",
      "and accuracy.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.06390 \n",
      "Title :A Feature Analysis for Multimodal News Retrieval\n",
      "  Content-based information retrieval is based on the information contained in\n",
      "documents rather than using metadata such as keywords. Most information\n",
      "retrieval methods are either based on text or image. In this paper, we\n",
      "investigate the usefulness of multimodal features for cross-lingual news search\n",
      "in various domains: politics, health, environment, sport, and finance. To this\n",
      "end, we consider five feature types for image and text and compare the\n",
      "performance of the retrieval system using different combinations. Experimental\n",
      "results show that retrieval results can be improved when considering both\n",
      "visual and textual information. In addition, it is observed that among textual\n",
      "features entity overlap outperforms word embeddings, while geolocation\n",
      "embeddings achieve better performance among visual features in the retrieval\n",
      "task.\n",
      "\n",
      "**Paper Id :2007.08617 \n",
      "Title :Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval\n",
      "  The abundance of multimodal data (e.g. social media posts) has inspired\n",
      "interest in cross-modal retrieval methods. Popular approaches rely on a variety\n",
      "of metric learning losses, which prescribe what the proximity of image and text\n",
      "should be, in the learned space. However, most prior methods have focused on\n",
      "the case where image and text convey redundant information; in contrast,\n",
      "real-world image-text pairs convey complementary information with little\n",
      "overlap. Further, images in news articles and media portray topics in a\n",
      "visually diverse fashion; thus, we need to take special care to ensure a\n",
      "meaningful image representation. We propose novel within-modality losses which\n",
      "encourage semantic coherency in both the text and image subspaces, which does\n",
      "not necessarily align with visual coherency. Our method ensures that not only\n",
      "are paired images and texts close, but the expected image-image and text-text\n",
      "relationships are also observed. Our approach improves the results of\n",
      "cross-modal retrieval on four datasets compared to five baselines.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.06585 \n",
      "Title :Gravitational-wave selection effects using neural-network classifiers\n",
      "  We present a novel machine-learning approach to estimate selection effects in\n",
      "gravitational-wave observations. Using techniques similar to those commonly\n",
      "employed in image classification and pattern recognition, we train a series of\n",
      "neural-network classifiers to predict the LIGO/Virgo detectability of\n",
      "gravitational-wave signals from compact-binary mergers. We include the effect\n",
      "of spin precession, higher-order modes, and multiple detectors and show that\n",
      "their omission, as it is common in large population studies, tends to\n",
      "overestimate the inferred merger rate in selected regions of the parameter\n",
      "space. Although here we train our classifiers using a simple signal-to-noise\n",
      "ratio threshold, our approach is ready to be used in conjunction with full\n",
      "pipeline injections, thus paving the way toward including actual distributions\n",
      "of astrophysical and noise triggers into gravitational-wave population\n",
      "analyses.\n",
      "\n",
      "**Paper Id :2006.01509 \n",
      "Title :Detection of gravitational-wave signals from binary neutron star mergers\n",
      "  using machine learning\n",
      "  As two neutron stars merge, they emit gravitational waves that can\n",
      "potentially be detected by earth bound detectors. Matched-filtering based\n",
      "algorithms have traditionally been used to extract quiet signals embedded in\n",
      "noise. We introduce a novel neural-network based machine learning algorithm\n",
      "that uses time series strain data from gravitational-wave detectors to detect\n",
      "signals from non-spinning binary neutron star mergers. For the Advanced LIGO\n",
      "design sensitivity, our network has an average sensitive distance of 130 Mpc at\n",
      "a false-alarm rate of 10 per month. Compared to other state-of-the-art machine\n",
      "learning algorithms, we find an improvement by a factor of 6 in sensitivity to\n",
      "signals with signal-to-noise ratio below 25. However, this approach is not yet\n",
      "competitive with traditional matched-filtering based methods. A conservative\n",
      "estimate indicates that our algorithm introduces on average 10.2 s of latency\n",
      "between signal arrival and generating an alert. We give an exact description of\n",
      "our testing procedure, which can not only be applied to machine learning based\n",
      "algorithms but all other search algorithms as well. We thereby improve the\n",
      "ability to compare machine learning and classical searches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.06695 \n",
      "Title :A Motion Taxonomy for Manipulation Embedding\n",
      "  To represent motions from a mechanical point of view, this paper explores\n",
      "motion embedding using the motion taxonomy. With this taxonomy, manipulations\n",
      "can be described and represented as binary strings called motion codes. Motion\n",
      "codes capture mechanical properties, such as contact type and trajectory, that\n",
      "should be used to define suitable distance metrics between motions or loss\n",
      "functions for deep learning and reinforcement learning. Motion codes can also\n",
      "be used to consolidate aliases or cluster motion types that share similar\n",
      "properties. Using existing data sets as a reference, we discuss how motion\n",
      "codes can be created and assigned to actions that are commonly seen in\n",
      "activities of daily living based on intuition as well as real data. Motion\n",
      "codes are compared to vectors from pre-trained Word2Vec models, and we show\n",
      "that motion codes maintain distances that closely match the reality of\n",
      "manipulation.\n",
      "\n",
      "**Paper Id :1911.04975 \n",
      "Title :word2ket: Space-efficient Word Embeddings inspired by Quantum\n",
      "  Entanglement\n",
      "  Deep learning natural language processing models often use vector word\n",
      "embeddings, such as word2vec or GloVe, to represent words. A discrete sequence\n",
      "of words can be much more easily integrated with downstream neural layers if it\n",
      "is represented as a sequence of continuous vectors. Also, semantic\n",
      "relationships between words, learned from a text corpus, can be encoded in the\n",
      "relative configurations of the embedding vectors. However, storing and\n",
      "accessing embedding vectors for all words in a dictionary requires large amount\n",
      "of space, and may stain systems with limited GPU memory. Here, we used\n",
      "approaches inspired by quantum computing to propose two related methods, {\\em\n",
      "word2ket} and {\\em word2ketXS}, for storing word embedding matrix during\n",
      "training and inference in a highly efficient way. Our approach achieves a\n",
      "hundred-fold or more reduction in the space required to store the embeddings\n",
      "with almost no relative drop in accuracy in practical natural language\n",
      "processing tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.07443 \n",
      "Title :Deep PQR: Solving Inverse Reinforcement Learning using Anchor Actions\n",
      "  We propose a reward function estimation framework for inverse reinforcement\n",
      "learning with deep energy-based policies. We name our method PQR, as it\n",
      "sequentially estimates the Policy, the $Q$-function, and the Reward function by\n",
      "deep learning. PQR does not assume that the reward solely depends on the state,\n",
      "instead it allows for a dependency on the choice of action. Moreover, PQR\n",
      "allows for stochastic state transitions. To accomplish this, we assume the\n",
      "existence of one anchor action whose reward is known, typically the action of\n",
      "doing nothing, yielding no reward. We present both estimators and algorithms\n",
      "for the PQR method. When the environment transition is known, we prove that the\n",
      "PQR reward estimator uniquely recovers the true reward. With unknown\n",
      "transitions, we bound the estimation error of PQR. Finally, the performance of\n",
      "PQR is demonstrated by synthetic and real-world datasets.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.07523 \n",
      "Title :Atomistic Structure Learning Algorithm with surrogate energy model\n",
      "  relaxation\n",
      "  The recently proposed Atomistic Structure Learning Algorithm (ASLA) builds on\n",
      "neural network enabled image recognition and reinforcement learning. It enables\n",
      "fully autonomous structure determination when used in combination with a\n",
      "first-principles total energy calculator, e.g. a density functional theory\n",
      "(DFT) program. To save on the computational requirements, ASLA utilizes the DFT\n",
      "program in a single-point mode, i.e. without allowing for relaxation of the\n",
      "structural candidates according to the force information at the DFT level. In\n",
      "this work, we augment ASLA to establish a surrogate energy model concurrently\n",
      "with its structure search. This enables approximative but computationally cheap\n",
      "relaxation of the structural candidates before the single-point energy\n",
      "evaluation with the computationally expensive DFT program. We demonstrate a\n",
      "significantly increased performance of ASLA for building benzene while\n",
      "utilizing a surrogate energy landscape. Further we apply this model-enhanced\n",
      "ASLA in a thorough investigation of the c(4x8) phase of the Ag(111) surface\n",
      "oxide. ASLA successfully identifies a surface reconstruction which has\n",
      "previously only been guessed on the basis of scanning tunnelling microscopy\n",
      "images.\n",
      "\n",
      "**Paper Id :2003.13418 \n",
      "Title :Machine Learning Enabled Discovery of Application Dependent Design\n",
      "  Principles for Two-dimensional Materials\n",
      "  The large-scale search for high-performing candidate 2D materials is limited\n",
      "to calculating a few simple descriptors, usually with first-principles density\n",
      "functional theory calculations. In this work, we alleviate this issue by\n",
      "extending and generalizing crystal graph convolutional neural networks to\n",
      "systems with planar periodicity, and train an ensemble of models to predict\n",
      "thermodynamic, mechanical, and electronic properties. To demonstrate the\n",
      "utility of this approach, we carry out a screening of nearly 45,000 structures\n",
      "for two largely disjoint applications: namely, mechanically robust composites\n",
      "and photovoltaics. An analysis of the uncertainty associated with our methods\n",
      "indicates the ensemble of neural networks is well-calibrated and has errors\n",
      "comparable with those from accurate first-principles density functional theory\n",
      "calculations. The ensemble of models allows us to gauge the confidence of our\n",
      "predictions, and to find the candidates most likely to exhibit effective\n",
      "performance in their applications. Since the datasets used in our screening\n",
      "were combinatorically generated, we are also able to investigate, using an\n",
      "innovative method, structural and compositional design principles that impact\n",
      "the properties of the structures surveyed and which can act as a generative\n",
      "model basis for future material discovery through reverse engineering. Our\n",
      "approach allowed us to recover some well-accepted design principles: for\n",
      "instance, we find that hybrid organic-inorganic perovskites with lead and tin\n",
      "tend to be good candidates for solar cell applications.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.07695 \n",
      "Title :Label Propagation with Augmented Anchors: A Simple Semi-Supervised\n",
      "  Learning baseline for Unsupervised Domain Adaptation\n",
      "  Motivated by the problem relatedness between unsupervised domain adaptation\n",
      "(UDA) and semi-supervised learning (SSL), many state-of-the-art UDA methods\n",
      "adopt SSL principles (e.g., the cluster assumption) as their learning\n",
      "ingredients. However, they tend to overlook the very domain-shift nature of\n",
      "UDA. In this work, we take a step further to study the proper extensions of SSL\n",
      "techniques for UDA. Taking the algorithm of label propagation (LP) as an\n",
      "example, we analyze the challenges of adopting LP to UDA and theoretically\n",
      "analyze the conditions of affinity graph/matrix construction in order to\n",
      "achieve better propagation of true labels to unlabeled instances. Our analysis\n",
      "suggests a new algorithm of Label Propagation with Augmented Anchors (A$^2$LP),\n",
      "which could potentially improve LP via generation of unlabeled virtual\n",
      "instances (i.e., the augmented anchors) with high-confidence label predictions.\n",
      "To make the proposed A$^2$LP useful for UDA, we propose empirical schemes to\n",
      "generate such virtual instances. The proposed schemes also tackle the\n",
      "domain-shift challenge of UDA by alternating between pseudo labeling via\n",
      "A$^2$LP and domain-invariant feature learning. Experiments show that such a\n",
      "simple SSL extension improves over representative UDA methods of\n",
      "domain-invariant feature learning, and could empower two state-of-the-art\n",
      "methods on benchmark UDA datasets. Our results show the value of further\n",
      "investigation on SSL techniques for UDA problems.\n",
      "\n",
      "**Paper Id :1909.11723 \n",
      "Title :Revisit Knowledge Distillation: a Teacher-free Framework\n",
      "  Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome\n",
      "teacher model into a lightweight student model. Its success is generally\n",
      "attributed to the privileged information on similarities among categories\n",
      "provided by the teacher model, and in this sense, only strong teacher models\n",
      "are deployed to teach weaker students in practice. In this work, we challenge\n",
      "this common belief by following experimental observations: 1) beyond the\n",
      "acknowledgment that the teacher can improve the student, the student can also\n",
      "enhance the teacher significantly by reversing the KD procedure; 2) a\n",
      "poorly-trained teacher with much lower accuracy than the student can still\n",
      "improve the latter significantly. To explain these observations, we provide a\n",
      "theoretical analysis of the relationships between KD and label smoothing\n",
      "regularization. We prove that 1) KD is a type of learned label smoothing\n",
      "regularization and 2) label smoothing regularization provides a virtual teacher\n",
      "model for KD. From these results, we argue that the success of KD is not fully\n",
      "due to the similarity information between categories, but also to the\n",
      "regularization of soft targets, which is equally or even more important.\n",
      "  Based on these analyses, we further propose a novel Teacher-free Knowledge\n",
      "Distillation (Tf-KD) framework, where a student model learns from itself or\n",
      "manually-designed regularization distribution. The Tf-KD achieves comparable\n",
      "performance with normal KD from a superior teacher, which is well applied when\n",
      "teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly\n",
      "deployed for training deep neural networks. Without any extra computation cost,\n",
      "Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established\n",
      "baseline models, which is superior to label smoothing regularization. The codes\n",
      "are in: \\url{https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation}\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.07997 \n",
      "Title :Overview of CheckThat! 2020: Automatic Identification and Verification\n",
      "  of Claims in Social Media\n",
      "  We present an overview of the third edition of the CheckThat! Lab at CLEF\n",
      "2020. The lab featured five tasks in two different languages: English and\n",
      "Arabic. The first four tasks compose the full pipeline of claim verification in\n",
      "social media: Task 1 on check-worthiness estimation, Task 2 on retrieving\n",
      "previously fact-checked claims, Task 3 on evidence retrieval, and Task 4 on\n",
      "claim verification. The lab is completed with Task 5 on check-worthiness\n",
      "estimation in political debates and speeches. A total of 67 teams registered to\n",
      "participate in the lab (up from 47 at CLEF 2019), and 23 of them actually\n",
      "submitted runs (compared to 14 at CLEF 2019). Most teams used deep neural\n",
      "networks based on BERT, LSTMs, or CNNs, and achieved sizable improvements over\n",
      "the baselines on all tasks. Here we describe the tasks setup, the evaluation\n",
      "results, and a summary of the approaches used by the participants, and we\n",
      "discuss some lessons learned. Last but not least, we release to the research\n",
      "community all datasets from the lab as well as the evaluation scripts, which\n",
      "should enable further research in the important tasks of check-worthiness\n",
      "estimation and automatic claim verification.\n",
      "\n",
      "**Paper Id :2001.08546 \n",
      "Title :CheckThat! at CLEF 2020: Enabling the Automatic Identification and\n",
      "  Verification of Claims in Social Media\n",
      "  We describe the third edition of the CheckThat! Lab, which is part of the\n",
      "2020 Cross-Language Evaluation Forum (CLEF). CheckThat! proposes four\n",
      "complementary tasks and a related task from previous lab editions, offered in\n",
      "English, Arabic, and Spanish. Task 1 asks to predict which tweets in a Twitter\n",
      "stream are worth fact-checking. Task 2 asks to determine whether a claim posted\n",
      "in a tweet can be verified using a set of previously fact-checked claims. Task\n",
      "3 asks to retrieve text snippets from a given set of Web pages that would be\n",
      "useful for verifying a target tweet's claim. Task 4 asks to predict the\n",
      "veracity of a target tweet's claim using a set of Web pages and potentially\n",
      "useful snippets in them. Finally, the lab offers a fifth task that asks to\n",
      "predict the check-worthiness of the claims made in English political debates\n",
      "and speeches. CheckThat! features a full evaluation framework. The evaluation\n",
      "is carried out using mean average precision or precision at rank k for ranking\n",
      "tasks, and F1 for classification tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.08024 \n",
      "Title :A Survey on Computational Propaganda Detection\n",
      "  Propaganda campaigns aim at influencing people's mindset with the purpose of\n",
      "advancing a specific agenda. They exploit the anonymity of the Internet, the\n",
      "micro-profiling ability of social networks, and the ease of automatically\n",
      "creating and managing coordinated networks of accounts, to reach millions of\n",
      "social network users with persuasive messages, specifically targeted to topics\n",
      "each individual user is sensitive to, and ultimately influencing the outcome on\n",
      "a targeted issue. In this survey, we review the state of the art on\n",
      "computational propaganda detection from the perspective of Natural Language\n",
      "Processing and Network Analysis, arguing about the need for combined efforts\n",
      "between these communities. We further discuss current challenges and future\n",
      "research directions.\n",
      "\n",
      "**Paper Id :2005.05854 \n",
      "Title :Prta: A System to Support the Analysis of Propaganda Techniques in the\n",
      "  News\n",
      "  Recent events, such as the 2016 US Presidential Campaign, Brexit and the\n",
      "COVID-19 \"infodemic\", have brought into the spotlight the dangers of online\n",
      "disinformation. There has been a lot of research focusing on fact-checking and\n",
      "disinformation detection. However, little attention has been paid to the\n",
      "specific rhetorical and psychological techniques used to convey propaganda\n",
      "messages. Revealing the use of such techniques can help promote media literacy\n",
      "and critical thinking, and eventually contribute to limiting the impact of\n",
      "\"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion\n",
      "Techniques Analyzer) allows users to explore the articles crawled on a regular\n",
      "basis by highlighting the spans in which propaganda techniques occur and to\n",
      "compare them on the basis of their use of propaganda techniques. The system\n",
      "further reports statistics about the use of such techniques, overall and over\n",
      "time, or according to filtering criteria specified by the user based on time\n",
      "interval, keywords, and/or political orientation of the media. Moreover, it\n",
      "allows users to analyze any text or URL through a dedicated interface or via an\n",
      "API. The system is available online: https://www.tanbih.org/prta\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.08617 \n",
      "Title :Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval\n",
      "  The abundance of multimodal data (e.g. social media posts) has inspired\n",
      "interest in cross-modal retrieval methods. Popular approaches rely on a variety\n",
      "of metric learning losses, which prescribe what the proximity of image and text\n",
      "should be, in the learned space. However, most prior methods have focused on\n",
      "the case where image and text convey redundant information; in contrast,\n",
      "real-world image-text pairs convey complementary information with little\n",
      "overlap. Further, images in news articles and media portray topics in a\n",
      "visually diverse fashion; thus, we need to take special care to ensure a\n",
      "meaningful image representation. We propose novel within-modality losses which\n",
      "encourage semantic coherency in both the text and image subspaces, which does\n",
      "not necessarily align with visual coherency. Our method ensures that not only\n",
      "are paired images and texts close, but the expected image-image and text-text\n",
      "relationships are also observed. Our approach improves the results of\n",
      "cross-modal retrieval on four datasets compared to five baselines.\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.08634 \n",
      "Title :Effective models and predictability of chaotic multiscale systems via\n",
      "  machine learning\n",
      "  We scrutinize the use of machine learning, based on reservoir computing, to\n",
      "build data-driven effective models of multiscale chaotic systems. We show that,\n",
      "for a wide scale separation, machine learning generates effective models akin\n",
      "to those obtained using multiscale asymptotic techniques and, remarkably,\n",
      "remains effective in predictability also when the scale separation is reduced.\n",
      "We also show that predictability can be improved by hybridizing the reservoir\n",
      "with an imperfect model.\n",
      "\n",
      "**Paper Id :2003.04166 \n",
      "Title :Learning entropy production via neural networks\n",
      "  This Letter presents a neural estimator for entropy production, or NEEP, that\n",
      "estimates entropy production (EP) from trajectories of relevant variables\n",
      "without detailed information on the system dynamics. For steady state, we\n",
      "rigorously prove that the estimator, which can be built up from different\n",
      "choices of deep neural networks, provides stochastic EP by optimizing the\n",
      "objective function proposed here. We verify the NEEP with the stochastic\n",
      "processes of the bead-spring and discrete flashing ratchet models, and also\n",
      "demonstrate that our method is applicable to high-dimensional data and can\n",
      "provide coarse-grained EP for Markov systems with unobservable states.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.09485 \n",
      "Title :Sequencing seismograms: A panoptic view of scattering in the core-mantle\n",
      "  boundary region\n",
      "  Scattering of seismic waves can reveal subsurface structures but usually in a\n",
      "piecemeal way focused on specific target areas. We used a manifold learning\n",
      "algorithm called \"the Sequencer\" to simultaneously analyze thousands of\n",
      "seismograms of waves diffracting along the core-mantle boundary and obtain a\n",
      "panoptic view of scattering across the Pacific region. In nearly half of the\n",
      "diffracting waveforms, we detected seismic waves scattered by three-dimensional\n",
      "structures near the core-mantle boundary. The prevalence of these scattered\n",
      "arrivals shows that the region hosts pervasive lateral heterogeneity. Our\n",
      "analysis revealed loud signals due to a plume root beneath Hawaii and a\n",
      "previously unrecognized ultralow-velocity zone beneath the Marquesas Islands.\n",
      "These observations illustrate how approaches flexible enough to detect robust\n",
      "patterns with little to no user supervision can reveal distinctive insights\n",
      "into the deep Earth.\n",
      "\n",
      "**Paper Id :2009.02708 \n",
      "Title :Deep Learning for the Analysis of Disruption Precursors based on Plasma\n",
      "  Tomography\n",
      "  The JET baseline scenario is being developed to achieve high fusion\n",
      "performance and sustained fusion power. However, with higher plasma current and\n",
      "higher input power, an increase in pulse disruptivity is being observed.\n",
      "Although there is a wide range of possible disruption causes, the present\n",
      "disruptions seem to be closely related to radiative phenomena such as impurity\n",
      "accumulation, core radiation, and radiative collapse. In this work, we focus on\n",
      "bolometer tomography to reconstruct the plasma radiation profile and, on top of\n",
      "it, we apply anomaly detection to identify the radiation patterns that precede\n",
      "major disruptions. The approach makes extensive use of machine learning. First,\n",
      "we train a surrogate model for plasma tomography based on matrix\n",
      "multiplication, which provides a fast method to compute the plasma radiation\n",
      "profiles across the full extent of any given pulse. Then, we train a\n",
      "variational autoencoder to reproduce the radiation profiles by encoding them\n",
      "into a latent distribution and subsequently decoding them. As an anomaly\n",
      "detector, the variational autoencoder struggles to reproduce unusual behaviors,\n",
      "which includes not only the actual disruptions but their precursors as well.\n",
      "These precursors are identified based on an analysis of the anomaly score\n",
      "across all baseline pulses in two recent campaigns at JET.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.09551 \n",
      "Title :Understanding Spatial Relations through Multiple Modalities\n",
      "  Recognizing spatial relations and reasoning about them is essential in\n",
      "multiple applications including navigation, direction giving and human-computer\n",
      "interaction in general. Spatial relations between objects can either be\n",
      "explicit -- expressed as spatial prepositions, or implicit -- expressed by\n",
      "spatial verbs such as moving, walking, shifting, etc. Both these, but implicit\n",
      "relations in particular, require significant common sense understanding. In\n",
      "this paper, we introduce the task of inferring implicit and explicit spatial\n",
      "relations between two entities in an image. We design a model that uses both\n",
      "textual and visual information to predict the spatial relations, making use of\n",
      "both positional and size information of objects and image embeddings. We\n",
      "contrast our spatial model with powerful language models and show how our\n",
      "modeling complements the power of these, improving prediction accuracy and\n",
      "coverage and facilitates dealing with unseen subjects, objects and relations.\n",
      "\n",
      "**Paper Id :2007.06390 \n",
      "Title :A Feature Analysis for Multimodal News Retrieval\n",
      "  Content-based information retrieval is based on the information contained in\n",
      "documents rather than using metadata such as keywords. Most information\n",
      "retrieval methods are either based on text or image. In this paper, we\n",
      "investigate the usefulness of multimodal features for cross-lingual news search\n",
      "in various domains: politics, health, environment, sport, and finance. To this\n",
      "end, we consider five feature types for image and text and compare the\n",
      "performance of the retrieval system using different combinations. Experimental\n",
      "results show that retrieval results can be improved when considering both\n",
      "visual and textual information. In addition, it is observed that among textual\n",
      "features entity overlap outperforms word embeddings, while geolocation\n",
      "embeddings achieve better performance among visual features in the retrieval\n",
      "task.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.09590 \n",
      "Title :AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation\n",
      "  In this paper, we propose an adaptive weighting regression (AWR) method to\n",
      "leverage the advantages of both detection-based and regression-based methods.\n",
      "Hand joint coordinates are estimated as discrete integration of all pixels in\n",
      "dense representation, guided by adaptive weight maps. This learnable\n",
      "aggregation process introduces both dense and joint supervision that allows\n",
      "end-to-end training and brings adaptability to weight maps, making the network\n",
      "more accurate and robust. Comprehensive exploration experiments are conducted\n",
      "to validate the effectiveness and generality of AWR under various experimental\n",
      "settings, especially its usefulness for different types of dense representation\n",
      "and input modality. Our method outperforms other state-of-the-art methods on\n",
      "four publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017\n",
      "dataset.\n",
      "\n",
      "**Paper Id :2002.12177 \n",
      "Title :Evolving Losses for Unsupervised Video Representation Learning\n",
      "  We present a new method to learn video representations from large-scale\n",
      "unlabeled video data. Ideally, this representation will be generic and\n",
      "transferable, directly usable for new tasks such as action recognition and zero\n",
      "or few-shot learning. We formulate unsupervised representation learning as a\n",
      "multi-modal, multi-task learning problem, where the representations are shared\n",
      "across different modalities via distillation. Further, we introduce the concept\n",
      "of loss function evolution by using an evolutionary search algorithm to\n",
      "automatically find optimal combination of loss functions capturing many\n",
      "(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\n",
      "representation evaluation metric using distribution matching to a large\n",
      "unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\n",
      "constraint, which is not guided by any labeling, produces similar results to\n",
      "weakly-supervised, task-specific ones. The proposed unsupervised representation\n",
      "learning results in a single RGB network and outperforms previous methods.\n",
      "Notably, it is also more effective than several label-based methods (e.g.,\n",
      "ImageNet), with the exception of large, fully labeled video datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.09654 \n",
      "Title :Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed\n",
      "  Datasets\n",
      "  We present a new loss function called Distribution-Balanced Loss for the\n",
      "multi-label recognition problems that exhibit long-tailed class distributions.\n",
      "Compared to conventional single-label classification problem, multi-label\n",
      "recognition problems are often more challenging due to two significant issues,\n",
      "namely the co-occurrence of labels and the dominance of negative labels (when\n",
      "treated as multiple binary classification problems). The Distribution-Balanced\n",
      "Loss tackles these issues through two key modifications to the standard binary\n",
      "cross-entropy loss: 1) a new way to re-balance the weights that takes into\n",
      "account the impact caused by label co-occurrence, and 2) a negative tolerant\n",
      "regularization to mitigate the over-suppression of negative labels. Experiments\n",
      "on both Pascal VOC and COCO show that the models trained with this new loss\n",
      "function achieve significant performance gains over existing methods. Code and\n",
      "models are available at: https://github.com/wutong16/DistributionBalancedLoss .\n",
      "\n",
      "**Paper Id :2010.02322 \n",
      "Title :SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup\n",
      "  Active learning is an important technique for low-resource sequence labeling\n",
      "tasks. However, current active sequence labeling methods use the queried\n",
      "samples alone in each iteration, which is an inefficient way of leveraging\n",
      "human annotations. We propose a simple but effective data augmentation method\n",
      "to improve the label efficiency of active sequence labeling. Our method,\n",
      "SeqMix, simply augments the queried samples by generating extra labeled\n",
      "sequences in each iteration. The key difficulty is to generate plausible\n",
      "sequences along with token-level labels. In SeqMix, we address this challenge\n",
      "by performing mixup for both sequences and token-level labels of the queried\n",
      "samples. Furthermore, we design a discriminator during sequence mixup, which\n",
      "judges whether the generated sequences are plausible or not. Our experiments on\n",
      "Named Entity Recognition and Event Detection tasks show that SeqMix can improve\n",
      "the standard active sequence labeling method by $2.27\\%$--$3.75\\%$ in terms of\n",
      "$F_1$ scores. The code and data for SeqMix can be found at\n",
      "https://github.com/rz-zhang/SeqMix\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.09670 \n",
      "Title :Prediction Intervals: Split Normal Mixture from Quality-Driven Deep\n",
      "  Ensembles\n",
      "  Prediction intervals are a machine- and human-interpretable way to represent\n",
      "predictive uncertainty in a regression analysis. In this paper, we present a\n",
      "method for generating prediction intervals along with point estimates from an\n",
      "ensemble of neural networks. We propose a multi-objective loss function fusing\n",
      "quality measures related to prediction intervals and point estimates, and a\n",
      "penalty function, which enforces semantic integrity of the results and\n",
      "stabilizes the training process of the neural networks. The ensembled\n",
      "prediction intervals are aggregated as a split normal mixture accounting for\n",
      "possible multimodality and asymmetricity of the posterior predictive\n",
      "distribution, and resulting in prediction intervals that capture aleatoric and\n",
      "epistemic uncertainty. Our results show that both our quality-driven loss\n",
      "function and our aggregation method contribute to well-calibrated prediction\n",
      "intervals and point estimates.\n",
      "\n",
      "**Paper Id :1901.08361 \n",
      "Title :Learning Global Pairwise Interactions with Bayesian Neural Networks\n",
      "  Estimating global pairwise interaction effects, i.e., the difference between\n",
      "the joint effect and the sum of marginal effects of two input features, with\n",
      "uncertainty properly quantified, is centrally important in science\n",
      "applications. We propose a non-parametric probabilistic method for detecting\n",
      "interaction effects of unknown form. First, the relationship between the\n",
      "features and the output is modelled using a Bayesian neural network, capable of\n",
      "representing complex interactions and principled uncertainty. Second,\n",
      "interaction effects and their uncertainty are estimated from the trained model.\n",
      "For the second step, we propose an intuitive global interaction measure:\n",
      "Bayesian Group Expected Hessian (GEH), which aggregates information of local\n",
      "interactions as captured by the Hessian. GEH provides a natural trade-off\n",
      "between type I and type II error and, moreover, comes with theoretical\n",
      "guarantees ensuring that the estimated interaction effects and their\n",
      "uncertainty can be improved by training a more accurate BNN. The method\n",
      "empirically outperforms available non-probabilistic alternatives on simulated\n",
      "and real-world data. Finally, we demonstrate its ability to detect\n",
      "interpretable interactions between higher-level features (at deeper layers of\n",
      "the neural network).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.09820 \n",
      "Title :Reinforcement Communication Learning in Different Social Network\n",
      "  Structures\n",
      "  Social network structure is one of the key determinants of human language\n",
      "evolution. Previous work has shown that the network of social interactions\n",
      "shapes decentralized learning in human groups, leading to the emergence of\n",
      "different kinds of communicative conventions. We examined the effects of social\n",
      "network organization on the properties of communication systems emerging in\n",
      "decentralized, multi-agent reinforcement learning communities. We found that\n",
      "the global connectivity of a social network drives the convergence of\n",
      "populations on shared and symmetric communication systems, preventing the\n",
      "agents from forming many local \"dialects\". Moreover, the agent's degree is\n",
      "inversely related to the consistency of its use of communicative conventions.\n",
      "These results show the importance of the basic properties of social network\n",
      "structure on reinforcement communication learning and suggest a new\n",
      "interpretation of findings on human convergence on word conventions.\n",
      "\n",
      "**Paper Id :2002.01365 \n",
      "Title :Compositional Languages Emerge in a Neural Iterated Learning Model\n",
      "  The principle of compositionality, which enables natural language to\n",
      "represent complex concepts via a structured combination of simpler ones, allows\n",
      "us to convey an open-ended set of messages using a limited vocabulary. If\n",
      "compositionality is indeed a natural property of language, we may expect it to\n",
      "appear in communication protocols that are created by neural agents in language\n",
      "games. In this paper, we propose an effective neural iterated learning (NIL)\n",
      "algorithm that, when applied to interacting neural agents, facilitates the\n",
      "emergence of a more structured type of language. Indeed, these languages\n",
      "provide learning speed advantages to neural agents during training, which can\n",
      "be incrementally amplified via NIL. We provide a probabilistic model of NIL and\n",
      "an explanation of why the advantage of compositional language exist. Our\n",
      "experiments confirm our analysis, and also demonstrate that the emerged\n",
      "languages largely improve the generalizing power of the neural agent\n",
      "communication.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.10115 \n",
      "Title :Towards robust sensing for Autonomous Vehicles: An adversarial\n",
      "  perspective\n",
      "  Autonomous Vehicles rely on accurate and robust sensor observations for\n",
      "safety critical decision-making in a variety of conditions. Fundamental\n",
      "building blocks of such systems are sensors and classifiers that process\n",
      "ultrasound, RADAR, GPS, LiDAR and camera signals~\\cite{Khan2018}. It is of\n",
      "primary importance that the resulting decisions are robust to perturbations,\n",
      "which can take the form of different types of nuisances and data\n",
      "transformations, and can even be adversarial perturbations (APs). Adversarial\n",
      "perturbations are purposefully crafted alterations of the environment or of the\n",
      "sensory measurements, with the objective of attacking and defeating the\n",
      "autonomous systems. A careful evaluation of the vulnerabilities of their\n",
      "sensing system(s) is necessary in order to build and deploy safer systems in\n",
      "the fast-evolving domain of AVs. To this end, we survey the emerging field of\n",
      "sensing in adversarial settings: after reviewing adversarial attacks on sensing\n",
      "modalities for autonomous systems, we discuss countermeasures and present\n",
      "future research directions.\n",
      "\n",
      "**Paper Id :1908.08649 \n",
      "Title :Adversary-resilient Distributed and Decentralized Statistical Inference\n",
      "  and Machine Learning: An Overview of Recent Advances Under the Byzantine\n",
      "  Threat Model\n",
      "  While the last few decades have witnessed a huge body of work devoted to\n",
      "inference and learning in distributed and decentralized setups, much of this\n",
      "work assumes a non-adversarial setting in which individual nodes---apart from\n",
      "occasional statistical failures---operate as intended within the algorithmic\n",
      "framework. In recent years, however, cybersecurity threats from malicious\n",
      "non-state actors and rogue entities have forced practitioners and researchers\n",
      "to rethink the robustness of distributed and decentralized algorithms against\n",
      "adversarial attacks. As a result, we now have a plethora of algorithmic\n",
      "approaches that guarantee robustness of distributed and/or decentralized\n",
      "inference and learning under different adversarial threat models. Driven in\n",
      "part by the world's growing appetite for data-driven decision making, however,\n",
      "securing of distributed/decentralized frameworks for inference and learning\n",
      "against adversarial threats remains a rapidly evolving research area. In this\n",
      "article, we provide an overview of some of the most recent developments in this\n",
      "area under the threat model of Byzantine attacks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.10284 \n",
      "Title :Learning High-Level Policies for Model Predictive Control\n",
      "  The combination of policy search and deep neural networks holds the promise\n",
      "of automating a variety of decision-making tasks. Model Predictive\n",
      "Control~(MPC) provides robust solutions to robot control tasks by making use of\n",
      "a dynamical model of the system and solving an optimization problem online over\n",
      "a short planning horizon. In this work, we leverage probabilistic\n",
      "decision-making approaches and the generalization capability of artificial\n",
      "neural networks to the powerful online optimization by learning a deep\n",
      "high-level policy for the MPC~(High-MPC). Conditioning on robot's local\n",
      "observations, the trained neural network policy is capable of adaptively\n",
      "selecting high-level decision variables for the low-level MPC controller, which\n",
      "then generates optimal control commands for the robot. First, we formulate the\n",
      "search of high-level decision variables for MPC as a policy search problem,\n",
      "specifically, a probabilistic inference problem. The problem can be solved in a\n",
      "closed-form solution. Second, we propose a self-supervised learning algorithm\n",
      "for learning a neural network high-level policy, which is useful for online\n",
      "hyperparameter adaptations in highly dynamic environments. We demonstrate the\n",
      "importance of incorporating the online adaption into autonomous robots by using\n",
      "the proposed method to solve a challenging control problem, where the task is\n",
      "to control a simulated quadrotor to fly through a swinging gate. We show that\n",
      "our approach can handle situations that are difficult for standard MPC.\n",
      "\n",
      "**Paper Id :2001.08092 \n",
      "Title :Local Policy Optimization for Trajectory-Centric Reinforcement Learning\n",
      "  The goal of this paper is to present a method for simultaneous trajectory and\n",
      "local stabilizing policy optimization to generate local policies for\n",
      "trajectory-centric model-based reinforcement learning (MBRL). This is motivated\n",
      "by the fact that global policy optimization for non-linear systems could be a\n",
      "very challenging problem both algorithmically and numerically. However, a lot\n",
      "of robotic manipulation tasks are trajectory-centric, and thus do not require a\n",
      "global model or policy. Due to inaccuracies in the learned model estimates, an\n",
      "open-loop trajectory optimization process mostly results in very poor\n",
      "performance when used on the real system. Motivated by these problems, we try\n",
      "to formulate the problem of trajectory optimization and local policy synthesis\n",
      "as a single optimization problem. It is then solved simultaneously as an\n",
      "instance of nonlinear programming. We provide some results for analysis as well\n",
      "as achieved performance of the proposed technique under some simplifying\n",
      "assumptions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.10467 \n",
      "Title :Second-Order Pooling for Graph Neural Networks\n",
      "  Graph neural networks have achieved great success in learning node\n",
      "representations for graph tasks such as node classification and link\n",
      "prediction. Graph representation learning requires graph pooling to obtain\n",
      "graph representations from node representations. It is challenging to develop\n",
      "graph pooling methods due to the variable sizes and isomorphic structures of\n",
      "graphs. In this work, we propose to use second-order pooling as graph pooling,\n",
      "which naturally solves the above challenges. In addition, compared to existing\n",
      "graph pooling methods, second-order pooling is able to use information from all\n",
      "nodes and collect second-order statistics, making it more powerful. We show\n",
      "that direct use of second-order pooling with graph neural networks leads to\n",
      "practical problems. To overcome these problems, we propose two novel global\n",
      "graph pooling methods based on second-order pooling; namely, bilinear mapping\n",
      "and attentional second-order pooling. In addition, we extend attentional\n",
      "second-order pooling to hierarchical graph pooling for more flexible use in\n",
      "GNNs. We perform thorough experiments on graph classification tasks to\n",
      "demonstrate the effectiveness and superiority of our proposed methods.\n",
      "Experimental results show that our methods improve the performance\n",
      "significantly and consistently.\n",
      "\n",
      "**Paper Id :2003.08420 \n",
      "Title :Unsupervised Hierarchical Graph Representation Learning by Mutual\n",
      "  Information Maximization\n",
      "  Graph representation learning based on graph neural networks (GNNs) can\n",
      "greatly improve the performance of downstream tasks, such as node and graph\n",
      "classification. However, the general GNN models do not aggregate node\n",
      "information in a hierarchical manner, and can miss key higher-order structural\n",
      "features of many graphs. The hierarchical aggregation also enables the graph\n",
      "representations to be explainable. In addition, supervised graph representation\n",
      "learning requires labeled data, which is expensive and error-prone. To address\n",
      "these issues, we present an unsupervised graph representation learning method,\n",
      "Unsupervised Hierarchical Graph Representation (UHGR), which can generate\n",
      "hierarchical representations of graphs. Our method focuses on maximizing mutual\n",
      "information between \"local\" and high-level \"global\" representations, which\n",
      "enables us to learn the node embeddings and graph embeddings without any\n",
      "labeled data. To demonstrate the effectiveness of the proposed method, we\n",
      "perform the node and graph classification using the learned node and graph\n",
      "embeddings. The results show that the proposed method achieves comparable\n",
      "results to state-of-the-art supervised methods on several benchmarks. In\n",
      "addition, our visualization of hierarchical representations indicates that our\n",
      "method can capture meaningful and interpretable clusters.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.10720 \n",
      "Title :Unsupervised Heterogeneous Coupling Learning for Categorical\n",
      "  Representation\n",
      "  Complex categorical data is often hierarchically coupled with heterogeneous\n",
      "relationships between attributes and attribute values and the couplings between\n",
      "objects. Such value-to-object couplings are heterogeneous with complementary\n",
      "and inconsistent interactions and distributions. Limited research exists on\n",
      "unlabeled categorical data representations, ignores the heterogeneous and\n",
      "hierarchical couplings, underestimates data characteristics and complexities,\n",
      "and overuses redundant information, etc. The deep representation learning of\n",
      "unlabeled categorical data is challenging, overseeing such value-to-object\n",
      "couplings, complementarity and inconsistency, and requiring large data,\n",
      "disentanglement, and high computational power. This work introduces a shallow\n",
      "but powerful UNsupervised heTerogeneous couplIng lEarning (UNTIE) approach for\n",
      "representing coupled categorical data by untying the interactions between\n",
      "couplings and revealing heterogeneous distributions embedded in each type of\n",
      "couplings. UNTIE is efficiently optimized w.r.t. a kernel k-means objective\n",
      "function for unsupervised representation learning of heterogeneous and\n",
      "hierarchical value-to-object couplings. Theoretical analysis shows that UNTIE\n",
      "can represent categorical data with maximal separability while effectively\n",
      "represent heterogeneous couplings and disclose their roles in categorical data.\n",
      "The UNTIE-learned representations make significant performance improvement\n",
      "against the state-of-the-art categorical representations and deep\n",
      "representation models on 25 categorical data sets with diversified\n",
      "characteristics.\n",
      "\n",
      "**Paper Id :2003.08420 \n",
      "Title :Unsupervised Hierarchical Graph Representation Learning by Mutual\n",
      "  Information Maximization\n",
      "  Graph representation learning based on graph neural networks (GNNs) can\n",
      "greatly improve the performance of downstream tasks, such as node and graph\n",
      "classification. However, the general GNN models do not aggregate node\n",
      "information in a hierarchical manner, and can miss key higher-order structural\n",
      "features of many graphs. The hierarchical aggregation also enables the graph\n",
      "representations to be explainable. In addition, supervised graph representation\n",
      "learning requires labeled data, which is expensive and error-prone. To address\n",
      "these issues, we present an unsupervised graph representation learning method,\n",
      "Unsupervised Hierarchical Graph Representation (UHGR), which can generate\n",
      "hierarchical representations of graphs. Our method focuses on maximizing mutual\n",
      "information between \"local\" and high-level \"global\" representations, which\n",
      "enables us to learn the node embeddings and graph embeddings without any\n",
      "labeled data. To demonstrate the effectiveness of the proposed method, we\n",
      "perform the node and graph classification using the learned node and graph\n",
      "embeddings. The results show that the proposed method achieves comparable\n",
      "results to state-of-the-art supervised methods on several benchmarks. In\n",
      "addition, our visualization of hierarchical representations indicates that our\n",
      "method can capture meaningful and interpretable clusters.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.13243 \n",
      "Title :Scalable Derivative-Free Optimization for Nonlinear Least-Squares\n",
      "  Problems\n",
      "  Derivative-free - or zeroth-order - optimization (DFO) has gained recent\n",
      "attention for its ability to solve problems in a variety of application areas,\n",
      "including machine learning, particularly involving objectives which are\n",
      "stochastic and/or expensive to compute. In this work, we develop a novel\n",
      "model-based DFO method for solving nonlinear least-squares problems. We improve\n",
      "on state-of-the-art DFO by performing dimensionality reduction in the\n",
      "observational space using sketching methods, avoiding the construction of a\n",
      "full local model. Our approach has a per-iteration computational cost which is\n",
      "linear in problem dimension in a big data regime, and numerical evidence\n",
      "demonstrates that, compared to existing software, it has dramatically improved\n",
      "runtime performance on overdetermined least-squares problems.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.13290 \n",
      "Title :Deep Learning Methods for Solving Linear Inverse Problems: Research\n",
      "  Directions and Paradigms\n",
      "  The linear inverse problem is fundamental to the development of various\n",
      "scientific areas. Innumerable attempts have been carried out to solve different\n",
      "variants of the linear inverse problem in different applications. Nowadays, the\n",
      "rapid development of deep learning provides a fresh perspective for solving the\n",
      "linear inverse problem, which has various well-designed network architectures\n",
      "results in state-of-the-art performance in many applications. In this paper, we\n",
      "present a comprehensive survey of the recent progress in the development of\n",
      "deep learning for solving various linear inverse problems. We review how deep\n",
      "learning methods are used in solving different linear inverse problems, and\n",
      "explore the structured neural network architectures that incorporate knowledge\n",
      "used in traditional methods. Furthermore, we identify open challenges and\n",
      "potential future directions along this research line.\n",
      "\n",
      "**Paper Id :2004.11848 \n",
      "Title :Deep learning for smart fish farming: applications, opportunities and\n",
      "  challenges\n",
      "  With the rapid emergence of deep learning (DL) technology, it has been\n",
      "successfully used in various fields including aquaculture. This change can\n",
      "create new opportunities and a series of challenges for information and data\n",
      "processing in smart fish farming. This paper focuses on the applications of DL\n",
      "in aquaculture, including live fish identification, species classification,\n",
      "behavioral analysis, feeding decision-making, size or biomass estimation, water\n",
      "quality prediction. In addition, the technical details of DL methods applied to\n",
      "smart fish farming are also analyzed, including data, algorithms, computing\n",
      "power, and performance. The results of this review show that the most\n",
      "significant contribution of DL is the ability to automatically extract\n",
      "features. However, challenges still exist; DL is still in an era of weak\n",
      "artificial intelligence. A large number of labeled data are needed for\n",
      "training, which has become a bottleneck restricting further DL applications in\n",
      "aquaculture. Nevertheless, DL still offers breakthroughs in the handling of\n",
      "complex data in aquaculture. In brief, our purpose is to provide researchers\n",
      "and practitioners with a better understanding of the current state of the art\n",
      "of DL in aquaculture, which can provide strong support for the implementation\n",
      "of smart fish farming.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.13435 \n",
      "Title :Label-Consistency based Graph Neural Networks for Semi-supervised Node\n",
      "  Classification\n",
      "  Graph neural networks (GNNs) achieve remarkable success in graph-based\n",
      "semi-supervised node classification, leveraging the information from\n",
      "neighboring nodes to improve the representation learning of target node. The\n",
      "success of GNNs at node classification depends on the assumption that connected\n",
      "nodes tend to have the same label. However, such an assumption does not always\n",
      "work, limiting the performance of GNNs at node classification. In this paper,\n",
      "we propose label-consistency based graph neural network(LC-GNN), leveraging\n",
      "node pairs unconnected but with the same labels to enlarge the receptive field\n",
      "of nodes in GNNs. Experiments on benchmark datasets demonstrate the proposed\n",
      "LC-GNN outperforms traditional GNNs in graph-based semi-supervised node\n",
      "classification.We further show the superiority of LC-GNN in sparse scenarios\n",
      "with only a handful of labeled nodes.\n",
      "\n",
      "**Paper Id :2001.06137 \n",
      "Title :Graph Inference Learning for Semi-supervised Classification\n",
      "  In this work, we address semi-supervised classification of graph data, where\n",
      "the categories of those unlabeled nodes are inferred from labeled nodes as well\n",
      "as graph structures. Recent works often solve this problem via advanced graph\n",
      "convolution in a conventionally supervised manner, but the performance could\n",
      "degrade significantly when labeled data is scarce. To this end, we propose a\n",
      "Graph Inference Learning (GIL) framework to boost the performance of\n",
      "semi-supervised node classification by learning the inference of node labels on\n",
      "graph topology. To bridge the connection between two nodes, we formally define\n",
      "a structure relation by encapsulating node attributes, between-node paths, and\n",
      "local topological structures together, which can make the inference\n",
      "conveniently deduced from one node to another node. For learning the inference\n",
      "process, we further introduce meta-optimization on structure relations from\n",
      "training nodes to validation nodes, such that the learnt graph inference\n",
      "capability can be better self-adapted to testing nodes. Comprehensive\n",
      "evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and\n",
      "NELL) demonstrate the superiority of our proposed GIL when compared against\n",
      "state-of-the-art methods on the semi-supervised node classification task.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.13866 \n",
      "Title :se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n",
      "  Residuals in Synthetic Domains\n",
      "  Tracking the 6D pose of objects in video sequences is important for robot\n",
      "manipulation. This task, however, introduces multiple challenges: (i) robot\n",
      "manipulation involves significant occlusions; (ii) data and annotations are\n",
      "troublesome and difficult to collect for 6D poses, which complicates machine\n",
      "learning solutions, and (iii) incremental error drift often accumulates in long\n",
      "term tracking to necessitate re-initialization of the object's pose. This work\n",
      "proposes a data-driven optimization approach for long-term, 6D pose tracking.\n",
      "It aims to identify the optimal relative pose given the current RGB-D\n",
      "observation and a synthetic image conditioned on the previous best estimate and\n",
      "the object's model. The key contribution in this context is a novel neural\n",
      "network architecture, which appropriately disentangles the feature encoding to\n",
      "help reduce domain shift, and an effective 3D orientation representation via\n",
      "Lie Algebra. Consequently, even when the network is trained only with synthetic\n",
      "data can work effectively over real images. Comprehensive experiments over\n",
      "benchmarks - existing ones as well as a new dataset with significant occlusions\n",
      "related to object manipulation - show that the proposed approach achieves\n",
      "consistently robust estimates and outperforms alternatives, even though they\n",
      "have been trained with real images. The approach is also the most\n",
      "computationally efficient among the alternatives and achieves a tracking\n",
      "frequency of 90.9Hz.\n",
      "\n",
      "**Paper Id :2008.03787 \n",
      "Title :Neural Manipulation Planning on Constraint Manifolds\n",
      "  The presence of task constraints imposes a significant challenge to motion\n",
      "planning. Despite all recent advancements, existing algorithms are still\n",
      "computationally expensive for most planning problems. In this paper, we present\n",
      "Constrained Motion Planning Networks (CoMPNet), the first neural planner for\n",
      "multimodal kinematic constraints. Our approach comprises the following\n",
      "components: i) constraint and environment perception encoders; ii) neural robot\n",
      "configuration generator that outputs configurations on/near the constraint\n",
      "manifold(s), and iii) a bidirectional planning algorithm that takes the\n",
      "generated configurations to create a feasible robot motion trajectory. We show\n",
      "that CoMPNet solves practical motion planning tasks involving both\n",
      "unconstrained and constrained problems. Furthermore, it generalizes to new\n",
      "unseen locations of the objects, i.e., not seen during training, in the given\n",
      "environments with high success rates. When compared to the state-of-the-art\n",
      "constrained motion planning algorithms, CoMPNet outperforms by order of\n",
      "magnitude improvement in computational speed with a significantly lower\n",
      "variance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.14147 \n",
      "Title :Team Deep Mixture of Experts for Distributed Power Control\n",
      "  In the context of wireless networking, it was recently shown that multiple\n",
      "DNNs can be jointly trained to offer a desired collaborative behaviour capable\n",
      "of coping with a broad range of sensing uncertainties. In particular, it was\n",
      "established that DNNs can be used to derive policies that are robust with\n",
      "respect to the information noise statistic affecting the local information\n",
      "(e.g. CSI in a wireless network) used by each agent (e.g. transmitter) to make\n",
      "its decision. While promising, a major challenge in the implementation of such\n",
      "method is that information noise statistics may differ from agent to agent and,\n",
      "more importantly, that such statistics may not be available at the time of\n",
      "training or may evolve over time, making burdensome retraining necessary. This\n",
      "situation makes it desirable to devise a \"universal\" machine learning model,\n",
      "which can be trained once for all so as to allow for decentralized cooperation\n",
      "in any future feedback noise environment. With this goal in mind, we propose an\n",
      "architecture inspired from the well-known Mixture of Experts (MoE) model, which\n",
      "was previously used for non-linear regression and classification tasks in\n",
      "various contexts, such as computer vision and speech recognition. We consider\n",
      "the decentralized power control problem as an example to showcase the validity\n",
      "of the proposed model and to compare it against other power control algorithms.\n",
      "We show the ability of the so called Team-DMoE model to efficiently track\n",
      "time-varying statistical scenarios.\n",
      "\n",
      "**Paper Id :2006.03859 \n",
      "Title :Online learning of both state and dynamics using ensemble Kalman filters\n",
      "  The reconstruction of the dynamics of an observed physical system as a\n",
      "surrogate model has been brought to the fore by recent advances in machine\n",
      "learning. To deal with partial and noisy observations in that endeavor, machine\n",
      "learning representations of the surrogate model can be used within a Bayesian\n",
      "data assimilation framework. However, these approaches require to consider long\n",
      "time series of observational data, meant to be assimilated all together. This\n",
      "paper investigates the possibility to learn both the dynamics and the state\n",
      "online, i.e. to update their estimates at any time, in particular when new\n",
      "observations are acquired. The estimation is based on the ensemble Kalman\n",
      "filter (EnKF) family of algorithms using a rather simple representation for the\n",
      "surrogate model and state augmentation. We consider the implication of learning\n",
      "dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an\n",
      "iterative EnKF and we discuss in each case issues and algorithmic solutions. We\n",
      "then demonstrate numerically the efficiency and assess the accuracy of these\n",
      "methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.14152 \n",
      "Title :At-Scale Sparse Deep Neural Network Inference with Efficient GPU\n",
      "  Implementation\n",
      "  This paper presents GPU performance optimization and scaling results for\n",
      "inference models of the Sparse Deep Neural Network Challenge 2020. Demands for\n",
      "network quality have increased rapidly, pushing the size and thus the memory\n",
      "requirements of many neural networks beyond the capacity of available\n",
      "accelerators. Sparse deep neural networks (SpDNN) have shown promise for\n",
      "reining in the memory footprint of large neural networks. However, there is\n",
      "room for improvement in implementing SpDNN operations on GPUs. This work\n",
      "presents optimized sparse matrix multiplication kernels fused with the ReLU\n",
      "function. The optimized kernels reuse input feature maps from the shared memory\n",
      "and sparse weights from registers. For multi-GPU parallelism, our SpDNN\n",
      "implementation duplicates weights and statically partition the feature maps\n",
      "across GPUs. Results for the challenge benchmarks show that the proposed kernel\n",
      "design and multi-GPU parallelization achieve up to 180 tera-edges per second\n",
      "inference throughput. These results are up to 4.3x faster for a single GPU and\n",
      "an order of magnitude faster at full scale than those of the champion of the\n",
      "2019 Sparse Deep Neural Network Graph Challenge for the same generation of\n",
      "NVIDIA V100 GPUs. Using the same implementation, we also show single-GPU\n",
      "throughput on NVIDIA A100 is 2.37$\\times$ faster than V100.\n",
      "\n",
      "**Paper Id :2011.03479 \n",
      "Title :Massively Parallel Graph Drawing and Representation Learning\n",
      "  To fully exploit the performance potential of modern multi-core processors,\n",
      "machine learning and data mining algorithms for big data must be parallelized\n",
      "in multiple ways. Today's CPUs consist of multiple cores, each following an\n",
      "independent thread of control, and each equipped with multiple arithmetic units\n",
      "which can perform the same operation on a vector of multiple data objects.\n",
      "Graph embedding, i.e. converting the vertices of a graph into numerical vectors\n",
      "is a data mining task of high importance and is useful for graph drawing\n",
      "(low-dimensional vectors) and graph representation learning (high-dimensional\n",
      "vectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by\n",
      "Minimizing the Predictive Entropy), an information-theoretic method which can\n",
      "generate low and high-dimensional vectors. MulticoreGEMPE applies MIMD\n",
      "(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single\n",
      "Instructions Multiple Data, using AVX-512) parallelism. We propose general\n",
      "ideas applicable in other graph-based algorithms like \\emph{vectorized hashing}\n",
      "and \\emph{vectorized reduction}. Our experimental evaluation demonstrates the\n",
      "superiority of our approach.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.14589 \n",
      "Title :Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis\n",
      "  Understanding how certain brain regions relate to a specific neurological\n",
      "disorder has been an important area of neuroimaging research. A promising\n",
      "approach to identify the salient regions is using Graph Neural Networks (GNNs),\n",
      "which can be used to analyze graph structured data, e.g. brain networks\n",
      "constructed by functional magnetic resonance imaging (fMRI). We propose an\n",
      "interpretable GNN framework with a novel salient region selection mechanism to\n",
      "determine neurological brain biomarkers associated with disorders.\n",
      "Specifically, we design novel regularized pooling layers that highlight salient\n",
      "regions of interests (ROIs) so that we can infer which ROIs are important to\n",
      "identify a certain disease based on the node pooling scores calculated by the\n",
      "pooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN),\n",
      "encourages reasonable ROI-selection and provides flexibility to preserve either\n",
      "individual- or group-level patterns. We apply the PR-GNN framework on a\n",
      "Biopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different\n",
      "choices of the hyperparameters and show that PR-GNN outperforms baseline\n",
      "methods in terms of classification accuracy. The salient ROI detection results\n",
      "show high correspondence with the previous neuroimaging-derived biomarkers for\n",
      "ASD.\n",
      "\n",
      "**Paper Id :2010.13924 \n",
      "Title :Benchmarking Deep Learning Interpretability in Time Series Predictions\n",
      "  Saliency methods are used extensively to highlight the importance of input\n",
      "features in model predictions. These methods are mostly used in vision and\n",
      "language tasks, and their applications to time series data is relatively\n",
      "unexplored. In this paper, we set out to extensively compare the performance of\n",
      "various saliency-based interpretability methods across diverse neural\n",
      "architectures, including Recurrent Neural Network, Temporal Convolutional\n",
      "Networks, and Transformers in a new benchmark of synthetic time series data. We\n",
      "propose and report multiple metrics to empirically evaluate the performance of\n",
      "saliency methods for detecting feature importance over time using both\n",
      "precision (i.e., whether identified features contain meaningful signals) and\n",
      "recall (i.e., the number of features with signal identified as important).\n",
      "Through several experiments, we show that (i) in general, network architectures\n",
      "and saliency methods fail to reliably and accurately identify feature\n",
      "importance over time in time series data, (ii) this failure is mainly due to\n",
      "the conflation of time and feature domains, and (iii) the quality of saliency\n",
      "maps can be improved substantially by using our proposed two-step temporal\n",
      "saliency rescaling (TSR) approach that first calculates the importance of each\n",
      "time step before calculating the importance of each feature at a time step.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.14622 \n",
      "Title :Approaches to Fraud Detection on Credit Card Transactions Using\n",
      "  Artificial Intelligence Methods\n",
      "  Credit card fraud is an ongoing problem for almost all industries in the\n",
      "world, and it raises millions of dollars to the global economy each year.\n",
      "Therefore, there is a number of research either completed or proceeding in\n",
      "order to detect these kinds of frauds in the industry. These researches\n",
      "generally use rule-based or novel artificial intelligence approaches to find\n",
      "eligible solutions. The ultimate goal of this paper is to summarize\n",
      "state-of-the-art approaches to fraud detection using artificial intelligence\n",
      "and machine learning techniques. While summarizing, we will categorize the\n",
      "common problems such as imbalanced dataset, real time working scenarios, and\n",
      "feature engineering challenges that almost all research works encounter, and\n",
      "identify general approaches to solve them. The imbalanced dataset problem\n",
      "occurs because the number of legitimate transactions is much higher than the\n",
      "fraudulent ones whereas applying the right feature engineering is substantial\n",
      "as the features obtained from the industries are limited, and applying feature\n",
      "engineering methods and reforming the dataset is crucial. Also, adapting the\n",
      "detection system to real time scenarios is a challenge since the number of\n",
      "credit card transactions in a limited time period is very high. In addition, we\n",
      "will discuss how evaluation metrics and machine learning methods differentiate\n",
      "among each research.\n",
      "\n",
      "**Paper Id :2005.02595 \n",
      "Title :Approaches and Applications of Early Classification of Time Series: A\n",
      "  Review\n",
      "  Early classification of time series has been extensively studied for\n",
      "minimizing class prediction delay in time-sensitive applications such as\n",
      "healthcare and finance. A primary task of an early classification approach is\n",
      "to classify an incomplete time series as soon as possible with some desired\n",
      "level of accuracy. Recent years have witnessed several approaches for early\n",
      "classification of time series. As most of the approaches have solved the early\n",
      "classification problem with different aspects, it becomes very important to\n",
      "make a thorough review of the existing solutions to know the current status of\n",
      "the area. These solutions have demonstrated reasonable performance in a wide\n",
      "range of applications including human activity recognition, gene expression\n",
      "based health diagnostic, industrial monitoring, and so on. In this paper, we\n",
      "present a systematic review of current literature on early classification\n",
      "approaches for both univariate and multivariate time series. We divide various\n",
      "existing approaches into four exclusive categories based on their proposed\n",
      "solution strategies. The four categories include prefix based, shapelet based,\n",
      "model based, and miscellaneous approaches. The authors also discuss the\n",
      "applications of early classification in many areas including industrial\n",
      "monitoring, intelligent transportation, and medical. Finally, we provide a\n",
      "quick summary of the current literature with future research directions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.14777 \n",
      "Title :PDCOVIDNet: A Parallel-Dilated Convolutional Neural Network Architecture\n",
      "  for Detecting COVID-19 from Chest X-Ray Images\n",
      "  The COVID-19 pandemic continues to severely undermine the prosperity of the\n",
      "global health system. To combat this pandemic, effective screening techniques\n",
      "for infected patients are indispensable. There is no doubt that the use of\n",
      "chest X-ray images for radiological assessment is one of the essential\n",
      "screening techniques. Some of the early studies revealed that the patient's\n",
      "chest X-ray images showed abnormalities, which is natural for patients infected\n",
      "with COVID-19. In this paper, we proposed a parallel-dilated convolutional\n",
      "neural network (CNN) based COVID-19 detection system from chest x-ray images,\n",
      "named as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available\n",
      "chest X-ray collection fully preloaded and enhanced, and then classified by the\n",
      "proposed method. Differing convolution dilation rate in a parallel form\n",
      "demonstrates the proof-of-principle for using PDCOVIDNet to extract\n",
      "radiological features for COVID-19 detection. Accordingly, we have assisted our\n",
      "method with two visualization methods, which are specifically designed to\n",
      "increase understanding of the key components associated with COVID-19\n",
      "infection. Both visualization methods compute gradients for a given image\n",
      "category related to feature maps of the last convolutional layer to create a\n",
      "class-discriminative region. In our experiment, we used a total of 2,905 chest\n",
      "X-ray images, comprising three cases (such as COVID-19, normal, and viral\n",
      "pneumonia), and empirical evaluations revealed that the proposed method\n",
      "extracted more significant features expeditiously related to the suspected\n",
      "disease. The experimental results demonstrate that our proposed method\n",
      "significantly improves performance metrics: accuracy, precision, recall, and F1\n",
      "scores reach 96.58%, 96.58%, 96.59%, and 96.58%, respectively, which is\n",
      "comparable or enhanced compared with the state-of-the-art methods.\n",
      "\n",
      "**Paper Id :2006.12229 \n",
      "Title :Improving performance of CNN to predict likelihood of COVID-19 using\n",
      "  chest X-ray images with preprocessing algorithms\n",
      "  As the rapid spread of coronavirus disease (COVID-19) worldwide, chest X-ray\n",
      "radiography has also been used to detect COVID-19 infected pneumonia and assess\n",
      "its severity or monitor its prognosis in the hospitals due to its low cost, low\n",
      "radiation dose, and wide accessibility. However, how to more accurately and\n",
      "efficiently detect COVID-19 infected pneumonia and distinguish it from other\n",
      "community-acquired pneumonia remains a challenge. In order to address this\n",
      "challenge, we in this study develop and test a new computer-aided diagnosis\n",
      "(CAD) scheme. It includes several image pre-processing algorithms to remove\n",
      "diaphragms, normalize image contrast-to-noise ratio, and generate three input\n",
      "images, then links to a transfer learning based convolutional neural network (a\n",
      "VGG16 based CNN model) to classify chest X-ray images into three classes of\n",
      "COVID-19 infected pneumonia, other community-acquired pneumonia and normal\n",
      "(non-pneumonia) cases. To this purpose, a publicly available dataset of 8,474\n",
      "chest X-ray images is used, which includes 415 confirmed COVID-19 infected\n",
      "pneumonia, 5,179 community-acquired pneumonia, and 2,880 non-pneumonia cases.\n",
      "The dataset is divided into two subsets with 90% and 10% of images in each\n",
      "subset to train and test the CNN-based CAD scheme. The testing results achieve\n",
      "94.0% of overall accuracy in classifying three classes and 98.6% accuracy in\n",
      "detecting Covid-19 infected cases. Thus, the study demonstrates the feasibility\n",
      "of developing a CAD scheme of chest X-ray images and providing radiologists\n",
      "useful decision-making supporting tools in detecting and diagnosis of COVID-19\n",
      "infected pneumonia.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.15422 \n",
      "Title :Few shot domain adaptation for in situ macromolecule structural\n",
      "  classification in cryo-electron tomograms\n",
      "  Motivation: Cryo-Electron Tomography (cryo-ET) visualizes structure and\n",
      "spatial organization of macromolecules and their interactions with other\n",
      "subcellular components inside single cells in the close-to-native state at\n",
      "sub-molecular resolution. Such information is critical for the accurate\n",
      "understanding of cellular processes. However, subtomogram classification\n",
      "remains one of the major challenges for the systematic recognition and recovery\n",
      "of the macromolecule structures in cryo-ET because of imaging limits and data\n",
      "quantity. Recently, deep learning has significantly improved the throughput and\n",
      "accuracy of large-scale subtomogram classification. However often it is\n",
      "difficult to get enough high-quality annotated subtomogram data for supervised\n",
      "training due to the enormous expense of labeling. To tackle this problem, it is\n",
      "beneficial to utilize another already annotated dataset to assist the training\n",
      "process. However, due to the discrepancy of image intensity distribution\n",
      "between source domain and target domain, the model trained on subtomograms in\n",
      "source domainmay perform poorly in predicting subtomogram classes in the target\n",
      "domain.\n",
      "  Results: In this paper, we adapt a few shot domain adaptation method for deep\n",
      "learning based cross-domain subtomogram classification. The essential idea of\n",
      "our method consists of two parts: 1) take full advantage of the distribution of\n",
      "plentiful unlabeled target domain data, and 2) exploit the correlation between\n",
      "the whole source domain dataset and few labeled target domain data. Experiments\n",
      "conducted on simulated and real datasets show that our method achieves\n",
      "significant improvement on cross domain subtomogram classification compared\n",
      "with baseline methods.\n",
      "\n",
      "**Paper Id :2009.04153 \n",
      "Title :One-shot Text Field Labeling using Attention and Belief Propagation for\n",
      "  Structure Information Extraction\n",
      "  Structured information extraction from document images usually consists of\n",
      "three steps: text detection, text recognition, and text field labeling. While\n",
      "text detection and text recognition have been heavily studied and improved a\n",
      "lot in literature, text field labeling is less explored and still faces many\n",
      "challenges. Existing learning based methods for text labeling task usually\n",
      "require a large amount of labeled examples to train a specific model for each\n",
      "type of document. However, collecting large amounts of document images and\n",
      "labeling them is difficult and sometimes impossible due to privacy issues.\n",
      "Deploying separate models for each type of document also consumes a lot of\n",
      "resources. Facing these challenges, we explore one-shot learning for the text\n",
      "field labeling task. Existing one-shot learning methods for the task are mostly\n",
      "rule-based and have difficulty in labeling fields in crowded regions with few\n",
      "landmarks and fields consisting of multiple separate text regions. To alleviate\n",
      "these problems, we proposed a novel deep end-to-end trainable approach for\n",
      "one-shot text field labeling, which makes use of attention mechanism to\n",
      "transfer the layout information between document images. We further applied\n",
      "conditional random field on the transferred layout information for the\n",
      "refinement of field labeling. We collected and annotated a real-world one-shot\n",
      "field labeling dataset with a large variety of document types and conducted\n",
      "extensive experiments to examine the effectiveness of the proposed model. To\n",
      "stimulate research in this direction, the collected dataset and the one-shot\n",
      "model will be released1.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.15474 \n",
      "Title :Music FaderNets: Controllable Music Generation Based On High-Level\n",
      "  Features via Low-Level Feature Modelling\n",
      "  High-level musical qualities (such as emotion) are often abstract,\n",
      "subjective, and hard to quantify. Given these difficulties, it is not easy to\n",
      "learn good feature representations with supervised learning techniques, either\n",
      "because of the insufficiency of labels, or the subjectiveness (and hence large\n",
      "variance) in human-annotated labels. In this paper, we present a framework that\n",
      "can learn high-level feature representations with a limited amount of data, by\n",
      "first modelling their corresponding quantifiable low-level attributes. We refer\n",
      "to our proposed framework as Music FaderNets, which is inspired by the fact\n",
      "that low-level attributes can be continuously manipulated by separate \"sliding\n",
      "faders\" through feature disentanglement and latent regularization techniques.\n",
      "High-level features are then inferred from the low-level representations\n",
      "through semi-supervised clustering using Gaussian Mixture Variational\n",
      "Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we\n",
      "show that the \"faders\" of our model are disentangled and change linearly w.r.t.\n",
      "the modelled low-level attributes of the generated output music. Furthermore,\n",
      "we demonstrate that the model successfully learns the intrinsic relationship\n",
      "between arousal and its corresponding low-level attributes (rhythm and note\n",
      "density), with only 1% of the training set being labelled. Finally, using the\n",
      "learnt high-level feature representations, we explore the application of our\n",
      "framework in style transfer tasks across different arousal states. The\n",
      "effectiveness of this approach is verified through a subjective listening test.\n",
      "\n",
      "**Paper Id :2006.14293 \n",
      "Title :Neural Decomposition: Functional ANOVA with Variational Autoencoders\n",
      "  Variational Autoencoders (VAEs) have become a popular approach for\n",
      "dimensionality reduction. However, despite their ability to identify latent\n",
      "low-dimensional structures embedded within high-dimensional data, these latent\n",
      "representations are typically hard to interpret on their own. Due to the\n",
      "black-box nature of VAEs, their utility for healthcare and genomics\n",
      "applications has been limited. In this paper, we focus on characterising the\n",
      "sources of variation in Conditional VAEs. Our goal is to provide a\n",
      "feature-level variance decomposition, i.e. to decompose variation in the data\n",
      "by separating out the marginal additive effects of latent variables z and fixed\n",
      "inputs c from their non-linear interactions. We propose to achieve this through\n",
      "what we call Neural Decomposition - an adaptation of the well-known concept of\n",
      "functional ANOVA variance decomposition from classical statistics to deep\n",
      "learning models. We show how identifiability can be achieved by training models\n",
      "subject to constraints on the marginal properties of the decoder networks. We\n",
      "demonstrate the utility of our Neural Decomposition on a series of synthetic\n",
      "examples as well as high-dimensional genomics data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.15793 \n",
      "Title :App-Aware Response Synthesis for User Reviews\n",
      "  Responding to user reviews promptly and satisfactorily improves application\n",
      "ratings, which is key to application popularity and success. The proliferation\n",
      "of such reviews makes it virtually impossible for developers to keep up with\n",
      "responding manually. To address this challenge, recent work has shown the\n",
      "possibility of automatic response generation. However, because the training\n",
      "review-response pairs are aggregated from many different apps, it remains\n",
      "challenging for such models to generate app-specific responses, which, on the\n",
      "other hand, are often desirable as apps have different features and concerns.\n",
      "Solving the challenge by simply building a model per app (i.e., training with\n",
      "review-response pairs of a single app) may be insufficient because individual\n",
      "apps have limited review-response pairs, and such pairs typically lack the\n",
      "relevant information needed to respond to a new review. To enable app-specific\n",
      "response generation, this work proposes AARSynth: an app-aware response\n",
      "synthesis system. The key idea behind AARSynth is to augment the seq2seq model\n",
      "with information specific to a given app. Given a new user review, it first\n",
      "retrieves the top-K most relevant app reviews and the most relevant snippet\n",
      "from the app description. The retrieved information and the new user review are\n",
      "then fed into a fused machine learning model that integrates the seq2seq model\n",
      "with a machine reading comprehension model. The latter helps digest the\n",
      "retrieved reviews and app description. Finally, the fused model generates a\n",
      "response that is customized to the given app. We evaluated AARSynth using a\n",
      "large corpus of reviews and responses from Google Play. The results show that\n",
      "AARSynth outperforms the state-of-the-art system by 22.2% on BLEU-4 score.\n",
      "Furthermore, our human study shows that AARSynth produces a statistically\n",
      "significant improvement in response quality compared to the state-of-the-art\n",
      "system.\n",
      "\n",
      "**Paper Id :1910.05493 \n",
      "Title :Deep Transfer Learning for Source Code Modeling\n",
      "  In recent years, deep learning models have shown great potential in source\n",
      "code modeling and analysis. Generally, deep learning-based approaches are\n",
      "problem-specific and data-hungry. A challenging issue of these approaches is\n",
      "that they require training from starch for a different related problem. In this\n",
      "work, we propose a transfer learning-based approach that significantly improves\n",
      "the performance of deep learning-based source code models. In contrast to\n",
      "traditional learning paradigms, transfer learning can transfer the knowledge\n",
      "learned in solving one problem into another related problem. First, we present\n",
      "two recurrent neural network-based models RNN and GRU for the purpose of\n",
      "transfer learning in the domain of source code modeling. Next, via transfer\n",
      "learning, these pre-trained (RNN and GRU) models are used as feature\n",
      "extractors. Then, these extracted features are combined into attention learner\n",
      "for different downstream tasks. The attention learner leverages from the\n",
      "learned knowledge of pre-trained models and fine-tunes them for a specific\n",
      "downstream task. We evaluate the performance of the proposed approach with\n",
      "extensive experiments with the source code suggestion task. The results\n",
      "indicate that the proposed approach outperforms the state-of-the-art models in\n",
      "terms of accuracy, precision, recall, and F-measure without training the models\n",
      "from scratch.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2007.16011 \n",
      "Title :Neural Machine Translation model for University Email Application\n",
      "  Machine translation has many applications such as news translation, email\n",
      "translation, official letter translation etc. Commercial translators, e.g.\n",
      "Google Translation lags in regional vocabulary and are unable to learn the\n",
      "bilingual text in the source and target languages within the input. In this\n",
      "paper, a regional vocabulary-based application-oriented Neural Machine\n",
      "Translation (NMT) model is proposed over the data set of emails used at the\n",
      "University for communication over a period of three years. A state-of-the-art\n",
      "Sequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is\n",
      "compared with Google Translate using Gated Recurrent Unit Recurrent Neural\n",
      "Network machine translation model with attention decoder. The low BLEU score of\n",
      "Google Translation in comparison to our model indicates that the application\n",
      "based regional models are better. The low BLEU score of EN -> ML of our model\n",
      "and Google Translation indicates that the Malay Language has complex language\n",
      "features corresponding to English.\n",
      "\n",
      "**Paper Id :2004.00588 \n",
      "Title :Better Sign Language Translation with STMC-Transformer\n",
      "  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\n",
      "system to extract sign language glosses from videos. Then, a translation system\n",
      "generates spoken language translations from the sign language glosses. This\n",
      "paper focuses on the translation system and introduces the STMC-Transformer\n",
      "which improves on the current state-of-the-art by over 5 and 7 BLEU\n",
      "respectively on gloss-to-text and video-to-text translation of the\n",
      "PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\n",
      "of over 16 BLEU.\n",
      "  We also demonstrate the problem in current methods that rely on gloss\n",
      "supervision. The video-to-text translation of our STMC-Transformer outperforms\n",
      "translation of GT glosses. This contradicts previous claims that GT gloss\n",
      "translation acts as an upper bound for SLT performance and reveals that glosses\n",
      "are an inefficient representation of sign language. For future SLT research, we\n",
      "therefore suggest an end-to-end training of the recognition and translation\n",
      "models, or using a different sign language annotation scheme.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.00029 \n",
      "Title :Cold Posteriors and Aleatoric Uncertainty\n",
      "  Recent work has observed that one can outperform exact inference in Bayesian\n",
      "neural networks by tuning the \"temperature\" of the posterior on a validation\n",
      "set (the \"cold posterior\" effect). To help interpret this phenomenon, we argue\n",
      "that commonly used priors in Bayesian neural networks can significantly\n",
      "overestimate the aleatoric uncertainty in the labels on many classification\n",
      "datasets. This problem is particularly pronounced in academic benchmarks like\n",
      "MNIST or CIFAR, for which the quality of the labels is high. For the special\n",
      "case of Gaussian process regression, any positive temperature corresponds to a\n",
      "valid posterior under a modified prior, and tuning this temperature is directly\n",
      "analogous to empirical Bayes. On classification tasks, there is no direct\n",
      "equivalence between modifying the prior and tuning the temperature, however\n",
      "reducing the temperature can lead to models which better reflect our belief\n",
      "that one gains little information by relabeling existing examples in the\n",
      "training set. Therefore although cold posteriors do not always correspond to an\n",
      "exact inference procedure, we believe they may often better reflect our true\n",
      "prior beliefs.\n",
      "\n",
      "**Paper Id :1910.03231 \n",
      "Title :Peer Loss Functions: Learning from Noisy Labels without Knowing Noise\n",
      "  Rates\n",
      "  Learning with noisy labels is a common challenge in supervised learning.\n",
      "Existing approaches often require practitioners to specify noise rates, i.e., a\n",
      "set of parameters controlling the severity of label noises in the problem, and\n",
      "the specifications are either assumed to be given or estimated using additional\n",
      "steps. In this work, we introduce a new family of loss functions that we name\n",
      "as peer loss functions, which enables learning from noisy labels and does not\n",
      "require a priori specification of the noise rates. Peer loss functions work\n",
      "within the standard empirical risk minimization (ERM) framework. We show that,\n",
      "under mild conditions, performing ERM with peer loss functions on the noisy\n",
      "dataset leads to the optimal or a near-optimal classifier as if performing ERM\n",
      "over the clean training data, which we do not have access to. We pair our\n",
      "results with an extensive set of experiments. Peer loss provides a way to\n",
      "simplify model development when facing potentially noisy training labels, and\n",
      "can be promoted as a robust candidate loss function in such situations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.00217 \n",
      "Title :Efficient Adversarial Attacks for Visual Object Tracking\n",
      "  Visual object tracking is an important task that requires the tracker to find\n",
      "the objects quickly and accurately. The existing state-ofthe-art object\n",
      "trackers, i.e., Siamese based trackers, use DNNs to attain high accuracy.\n",
      "However, the robustness of visual tracking models is seldom explored. In this\n",
      "paper, we analyze the weakness of object trackers based on the Siamese network\n",
      "and then extend adversarial examples to visual object tracking. We present an\n",
      "end-to-end network FAN (Fast Attack Network) that uses a novel drift loss\n",
      "combined with the embedded feature loss to attack the Siamese network based\n",
      "trackers. Under a single GPU, FAN is efficient in the training speed and has a\n",
      "strong attack performance. The FAN can generate an adversarial example at 10ms,\n",
      "achieve effective targeted attack (at least 40% drop rate on OTB) and\n",
      "untargeted attack (at least 70% drop rate on OTB).\n",
      "\n",
      "**Paper Id :2002.09821 \n",
      "Title :A Multi-view CNN-based Acoustic Classification System for Automatic\n",
      "  Animal Species Identification\n",
      "  Automatic identification of animal species by their vocalization is an\n",
      "important and challenging task. Although many kinds of audio monitoring system\n",
      "have been proposed in the literature, they suffer from several disadvantages\n",
      "such as non-trivial feature selection, accuracy degradation because of\n",
      "environmental noise or intensive local computation. In this paper, we propose a\n",
      "deep learning based acoustic classification framework for Wireless Acoustic\n",
      "Sensor Network (WASN). The proposed framework is based on cloud architecture\n",
      "which relaxes the computational burden on the wireless sensor node. To improve\n",
      "the recognition accuracy, we design a multi-view Convolution Neural Network\n",
      "(CNN) to extract the short-, middle-, and long-term dependencies in parallel.\n",
      "The evaluation on two real datasets shows that the proposed architecture can\n",
      "achieve high accuracy and outperforms traditional classification systems\n",
      "significantly when the environmental noise dominate the audio signal (low SNR).\n",
      "Moreover, we implement and deploy the proposed system on a testbed and analyse\n",
      "the system performance in real-world environments. Both simulation and\n",
      "real-world evaluation demonstrate the accuracy and robustness of the proposed\n",
      "acoustic classification system in distinguishing species of animals.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.00323 \n",
      "Title :Convergence of Sparse Variational Inference in Gaussian Processes\n",
      "  Regression\n",
      "  Gaussian processes are distributions over functions that are versatile and\n",
      "mathematically convenient priors in Bayesian modelling. However, their use is\n",
      "often impeded for data with large numbers of observations, $N$, due to the\n",
      "cubic (in $N$) cost of matrix operations used in exact inference. Many\n",
      "solutions have been proposed that rely on $M \\ll N$ inducing variables to form\n",
      "an approximation at a cost of $\\mathcal{O}(NM^2)$. While the computational cost\n",
      "appears linear in $N$, the true complexity depends on how $M$ must scale with\n",
      "$N$ to ensure a certain quality of the approximation. In this work, we\n",
      "investigate upper and lower bounds on how $M$ needs to grow with $N$ to ensure\n",
      "high quality approximations. We show that we can make the KL-divergence between\n",
      "the approximate model and the exact posterior arbitrarily small for a\n",
      "Gaussian-noise regression model with $M\\ll N$. Specifically, for the popular\n",
      "squared exponential kernel and $D$-dimensional Gaussian distributed covariates,\n",
      "$M=\\mathcal{O}((\\log N)^D)$ suffice and a method with an overall computational\n",
      "cost of $\\mathcal{O}(N(\\log N)^{2D}(\\log\\log N)^2)$ can be used to perform\n",
      "inference.\n",
      "\n",
      "**Paper Id :1905.04559 \n",
      "Title :ForestDSH: A Universal Hash Design for Discrete Probability\n",
      "  Distributions\n",
      "  In this paper, we consider the problem of classification of $M$ high\n",
      "dimensional queries $y^1,\\cdots,y^M\\in B^S$ to $N$ high dimensional classes\n",
      "$x^1,\\cdots,x^N\\in A^S$ where $A$ and $B$ are discrete alphabets and the\n",
      "probabilistic model that relates data to the classes $P(x,y)$ is known. This\n",
      "problem has applications in various fields including the database search\n",
      "problem in mass spectrometry. The problem is analogous to the nearest neighbor\n",
      "search problem, where the goal is to find the data point in a database that is\n",
      "the most similar to a query point. The state of the art method for solving an\n",
      "approximate version of the nearest neighbor search problem in high dimensions\n",
      "is locality sensitive hashing (LSH). LSH is based on designing hash functions\n",
      "that map near points to the same buckets with a probability higher than random\n",
      "(far) points. To solve our high dimensional classification problem, we\n",
      "introduce distribution sensitive hashes that map jointly generated pairs\n",
      "$(x,y)\\sim P$ to the same bucket with probability higher than random pairs\n",
      "$x\\sim P^A$ and $y\\sim P^B$, where $P^A$ and $P^B$ are the marginal probability\n",
      "distributions of $P$. We design distribution sensitive hashes using a forest of\n",
      "decision trees and we show that the complexity of search grows with\n",
      "$O(N^{\\lambda^*(P)})$ where $\\lambda^*(P)$ is expressed in an analytical form.\n",
      "We further show that the proposed hashes perform faster than state of the art\n",
      "approximate nearest neighbor search methods for a range of probability\n",
      "distributions, in both theory and simulations. Finally, we apply our method to\n",
      "the spectral library search problem in mass spectrometry, and show that it is\n",
      "an order of magnitude faster than the state of the art methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.01564 \n",
      "Title :LXPER Index: a curriculum-specific text readability assessment model for\n",
      "  EFL students in Korea\n",
      "  Automatic readability assessment is one of the most important applications of\n",
      "Natural Language Processing (NLP) in education. Since automatic readability\n",
      "assessment allows the fast selection of appropriate reading material for\n",
      "readers at all levels of proficiency, it can be particularly useful for the\n",
      "English education of English as Foreign Language (EFL) students around the\n",
      "world. Most readability assessment models are developed for the native readers\n",
      "of English and have low accuracy for texts in the non-native English Language\n",
      "Training (ELT) curriculum. We introduce LXPER Index, which is a readability\n",
      "assessment model for non-native EFL readers in the ELT curriculum of Korea. Our\n",
      "experiments show that our new model, trained with CoKEC-text (Text Corpus of\n",
      "the Korean ELT Curriculum), significantly improves the accuracy of automatic\n",
      "readability assessment for texts in the Korean ELT curriculum.\n",
      "\n",
      "**Paper Id :2006.04016 \n",
      "Title :A Multitask Learning Approach for Diacritic Restoration\n",
      "  In many languages like Arabic, diacritics are used to specify pronunciations\n",
      "as well as meanings. Such diacritics are often omitted in written text,\n",
      "increasing the number of possible pronunciations and meanings for a word. This\n",
      "results in a more ambiguous text making computational processing on such text\n",
      "more difficult. Diacritic restoration is the task of restoring missing\n",
      "diacritics in the written text. Most state-of-the-art diacritic restoration\n",
      "models are built on character level information which helps generalize the\n",
      "model to unseen data, but presumably lose useful information at the word level.\n",
      "Thus, to compensate for this loss, we investigate the use of multi-task\n",
      "learning to jointly optimize diacritic restoration with related NLP problems\n",
      "namely word segmentation, part-of-speech tagging, and syntactic diacritization.\n",
      "We use Arabic as a case study since it has sufficient data resources for tasks\n",
      "that we consider in our joint modeling. Our joint models significantly\n",
      "outperform the baselines and are comparable to the state-of-the-art models that\n",
      "are more complex relying on morphological analyzers and/or a lot more data\n",
      "(e.g. dialectal data).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.01916 \n",
      "Title :More Than Privacy: Applying Differential Privacy in Key Areas of\n",
      "  Artificial Intelligence\n",
      "  Artificial Intelligence (AI) has attracted a great deal of attention in\n",
      "recent years. However, alongside all its advancements, problems have also\n",
      "emerged, such as privacy violations, security issues and model fairness.\n",
      "Differential privacy, as a promising mathematical model, has several attractive\n",
      "properties that can help solve these problems, making it quite a valuable tool.\n",
      "For this reason, differential privacy has been broadly applied in AI but to\n",
      "date, no study has documented which differential privacy mechanisms can or have\n",
      "been leveraged to overcome its issues or the properties that make this\n",
      "possible. In this paper, we show that differential privacy can do more than\n",
      "just privacy preservation. It can also be used to improve security, stabilize\n",
      "learning, build fair models, and impose composition in selected areas of AI.\n",
      "With a focus on regular machine learning, distributed machine learning, deep\n",
      "learning, and multi-agent systems, the purpose of this article is to deliver a\n",
      "new view on many possibilities for improving AI performance with differential\n",
      "privacy techniques.\n",
      "\n",
      "**Paper Id :2002.10433 \n",
      "Title :From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the\n",
      "  World of AI\n",
      "  This paper reviews the field of Game AI, which not only deals with creating\n",
      "agents that can play a certain game, but also with areas as diverse as creating\n",
      "game content automatically, game analytics, or player modelling. While Game AI\n",
      "was for a long time not very well recognized by the larger scientific\n",
      "community, it has established itself as a research area for developing and\n",
      "testing the most advanced forms of AI algorithms and articles covering advances\n",
      "in mastering video games such as StarCraft 2 and Quake III appear in the most\n",
      "prestigious journals. Because of the growth of the field, a single review\n",
      "cannot cover it completely. Therefore, we put a focus on important recent\n",
      "developments, including that advances in Game AI are starting to be extended to\n",
      "areas outside of games, such as robotics or the synthesis of chemicals. In this\n",
      "article, we review the algorithms and methods that have paved the way for these\n",
      "breakthroughs, report on the other important areas of Game AI research, and\n",
      "also point out exciting directions for the future of Game AI.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.02366 \n",
      "Title :A robot that counts like a child: a developmental model of counting and\n",
      "  pointing\n",
      "  In this paper, a novel neuro-robotics model capable of counting real items is\n",
      "introduced. The model allows us to investigate the interaction between\n",
      "embodiment and numerical cognition. This is composed of a deep neural network\n",
      "capable of image processing and sequential tasks performance, and a robotic\n",
      "platform providing the embodiment - the iCub humanoid robot. The network is\n",
      "trained using images from the robot's cameras and proprioceptive signals from\n",
      "its joints. The trained model is able to count a set of items and at the same\n",
      "time points to them. We investigate the influence of pointing on the counting\n",
      "process and compare our results with those from studies with children. Several\n",
      "training approaches are presented in this paper all of them uses pre-training\n",
      "routine allowing the network to gain the ability of pointing and number\n",
      "recitation (from 1 to 10) prior to counting training. The impact of the counted\n",
      "set size and distance to the objects are investigated. The obtained results on\n",
      "counting performance show similarities with those from human studies.\n",
      "\n",
      "**Paper Id :2006.13546 \n",
      "Title :Crossmodal Language Grounding in an Embodied Neurocognitive Model\n",
      "  Human infants are able to acquire natural language seemingly easily at an\n",
      "early age. Their language learning seems to occur simultaneously with learning\n",
      "other cognitive functions as well as with playful interactions with the\n",
      "environment and caregivers. From a neuroscientific perspective, natural\n",
      "language is embodied, grounded in most, if not all, sensory and sensorimotor\n",
      "modalities, and acquired by means of crossmodal integration. However,\n",
      "characterising the underlying mechanisms in the brain is difficult and\n",
      "explaining the grounding of language in crossmodal perception and action\n",
      "remains challenging. In this paper, we present a neurocognitive model for\n",
      "language grounding which reflects bio-inspired mechanisms such as an implicit\n",
      "adaptation of timescales as well as end-to-end multimodal abstraction. It\n",
      "addresses developmental robotic interaction and extends its learning\n",
      "capabilities using larger-scale knowledge-based data. In our scenario, we\n",
      "utilise the humanoid robot NICO in obtaining the EMIL data collection, in which\n",
      "the cognitive robot interacts with objects in a children's playground\n",
      "environment while receiving linguistic labels from a caregiver. The model\n",
      "analysis shows that crossmodally integrated representations are sufficient for\n",
      "acquiring language merely from sensory input through interaction with objects\n",
      "in an environment. The representations self-organise hierarchically and embed\n",
      "temporal and spatial information through composition and decomposition. This\n",
      "model can also provide the basis for further crossmodal integration of\n",
      "perceptually grounded cognitive representations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.02492 \n",
      "Title :Zero-Shot Multi-View Indoor Localization via Graph Location Networks\n",
      "  Indoor localization is a fundamental problem in location-based applications.\n",
      "Current approaches to this problem typically rely on Radio Frequency\n",
      "technology, which requires not only supporting infrastructures but human\n",
      "efforts to measure and calibrate the signal. Moreover, data collection for all\n",
      "locations is indispensable in existing methods, which in turn hinders their\n",
      "large-scale deployment. In this paper, we propose a novel neural network based\n",
      "architecture Graph Location Networks (GLN) to perform infrastructure-free,\n",
      "multi-view image based indoor localization. GLN makes location predictions\n",
      "based on robust location representations extracted from images through\n",
      "message-passing networks. Furthermore, we introduce a novel zero-shot indoor\n",
      "localization setting and tackle it by extending the proposed GLN to a dedicated\n",
      "zero-shot version, which exploits a novel mechanism Map2Vec to train\n",
      "location-aware embeddings and make predictions on novel unseen locations. Our\n",
      "extensive experiments show that the proposed approach outperforms\n",
      "state-of-the-art methods in the standard setting, and achieves promising\n",
      "accuracy even in the zero-shot setting where data for half of the locations are\n",
      "not available. The source code and datasets are publicly available at\n",
      "https://github.com/coldmanck/zero-shot-indoor-localization-release.\n",
      "\n",
      "**Paper Id :2011.00359 \n",
      "Title :TartanVO: A Generalizable Learning-based VO\n",
      "  We present the first learning-based visual odometry (VO) model, which\n",
      "generalizes to multiple datasets and real-world scenarios and outperforms\n",
      "geometry-based methods in challenging scenes. We achieve this by leveraging the\n",
      "SLAM dataset TartanAir, which provides a large amount of diverse synthetic data\n",
      "in challenging environments. Furthermore, to make our VO model generalize\n",
      "across datasets, we propose an up-to-scale loss function and incorporate the\n",
      "camera intrinsic parameters into the model. Experiments show that a single\n",
      "model, TartanVO, trained only on synthetic data, without any finetuning, can be\n",
      "generalized to real-world datasets such as KITTI and EuRoC, demonstrating\n",
      "significant advantages over the geometry-based methods on challenging\n",
      "trajectories. Our code is available at https://github.com/castacks/tartanvo.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.02837 \n",
      "Title :aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF,\n",
      "  and Transfer Learning\n",
      "  We describe our system for SemEval-2020 Task 11 on Detection of Propaganda\n",
      "Techniques in News Articles. We developed ensemble models using RoBERTa-based\n",
      "neural architectures, additional CRF layers, transfer learning between the two\n",
      "subtasks, and advanced post-processing to handle the multi-label nature of the\n",
      "task, the consistency between nested spans, repetitions, and labels from\n",
      "similar spans in training. We achieved sizable improvements over baseline\n",
      "fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd\n",
      "(almost tied with the 2nd) out of 36 teams on the span identification subtask\n",
      "with an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams\n",
      "on the technique classification subtask with an F1 score of 0.62.\n",
      "\n",
      "**Paper Id :2006.09526 \n",
      "Title :Cross-lingual Retrieval for Iterative Self-Supervised Training\n",
      "  Recent studies have demonstrated the cross-lingual alignment ability of\n",
      "multilingual pretrained language models. In this work, we found that the\n",
      "cross-lingual alignment can be further improved by training seq2seq models on\n",
      "sentence pairs mined using their own encoder outputs. We utilized these\n",
      "findings to develop a new approach -- cross-lingual retrieval for iterative\n",
      "self-supervised training (CRISS), where mining and training processes are\n",
      "applied iteratively, improving cross-lingual alignment and translation ability\n",
      "at the same time. Using this method, we achieved state-of-the-art unsupervised\n",
      "machine translation results on 9 language directions with an average\n",
      "improvement of 2.4 BLEU, and on the Tatoeba sentence retrieval task in the\n",
      "XTREME benchmark on 16 languages with an average improvement of 21.5% in\n",
      "absolute accuracy. Furthermore, CRISS also brings an additional 1.8 BLEU\n",
      "improvement on average compared to mBART, when finetuned on supervised machine\n",
      "translation downstream tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.02897 \n",
      "Title :Iterative Compression of End-to-End ASR Model using AutoML\n",
      "  Increasing demand for on-device Automatic Speech Recognition (ASR) systems\n",
      "has resulted in renewed interests in developing automatic model compression\n",
      "techniques. Past research have shown that AutoML-based Low Rank Factorization\n",
      "(LRF) technique, when applied to an end-to-end Encoder-Attention-Decoder style\n",
      "ASR model, can achieve a speedup of up to 3.7x, outperforming laborious manual\n",
      "rank-selection approaches. However, we show that current AutoML-based search\n",
      "techniques only work up to a certain compression level, beyond which they fail\n",
      "to produce compressed models with acceptable word error rates (WER). In this\n",
      "work, we propose an iterative AutoML-based LRF approach that achieves over 5x\n",
      "compression without degrading the WER, thereby advancing the state-of-the-art\n",
      "in ASR compression.\n",
      "\n",
      "**Paper Id :2008.05011 \n",
      "Title :Compact Speaker Embedding: lrx-vector\n",
      "  Deep neural networks (DNN) have recently been widely used in speaker\n",
      "recognition systems, achieving state-of-the-art performance on various\n",
      "benchmarks. The x-vector architecture is especially popular in this research\n",
      "community, due to its excellent performance and manageable computational\n",
      "complexity. In this paper, we present the lrx-vector system, which is the\n",
      "low-rank factorized version of the x-vector embedding network. The primary\n",
      "objective of this topology is to further reduce the memory requirement of the\n",
      "speaker recognition system. We discuss the deployment of knowledge distillation\n",
      "for training the lrx-vector system and compare against low-rank factorization\n",
      "with SVD. On the VOiCES 2019 far-field corpus we were able to reduce the\n",
      "weights by 28% compared to the full-rank x-vector system while keeping the\n",
      "recognition rate constant (1.83% EER).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.02952 \n",
      "Title :Few Shot Learning Framework to Reduce Inter-observer Variability in\n",
      "  Medical Images\n",
      "  Most computer aided pathology detection systems rely on large volumes of\n",
      "quality annotated data to aid diagnostics and follow up procedures. However,\n",
      "quality assuring large volumes of annotated medical image data can be\n",
      "subjective and expensive. In this work we present a novel standardization\n",
      "framework that implements three few-shot learning (FSL) models that can be\n",
      "iteratively trained by atmost 5 images per 3D stack to generate multiple\n",
      "regional proposals (RPs) per test image. These FSL models include a novel\n",
      "parallel echo state network (ParESN) framework and an augmented U-net model.\n",
      "Additionally, we propose a novel target label selection algorithm (TLSA) that\n",
      "measures relative agreeability between RPs and the manually annotated target\n",
      "labels to detect the \"best\" quality annotation per image. Using the FSL models,\n",
      "our system achieves 0.28-0.64 Dice coefficient across vendor image stacks for\n",
      "intra-retinal cyst segmentation. Additionally, the TLSA is capable of\n",
      "automatically classifying high quality target labels from their noisy\n",
      "counterparts for 60-97% of the images while ensuring manual supervision on\n",
      "remaining images. Also, the proposed framework with ParESN model minimizes\n",
      "manual annotation checking to 12-28% of the total number of images. The TLSA\n",
      "metrics further provide confidence scores for the automated annotation quality\n",
      "assurance. Thus, the proposed framework is flexible to extensions for quality\n",
      "image annotation curation of other image stacks as well.\n",
      "\n",
      "**Paper Id :2005.06835 \n",
      "Title :RegQCNET: Deep Quality Control for Image-to-template Brain MRI Affine\n",
      "  Registration\n",
      "  Affine registration of one or several brain image(s) onto a common reference\n",
      "space is a necessary prerequisite for many image processing tasks, such as\n",
      "brain segmentation or functional analysis. Manual assessment of registration\n",
      "quality is a tedious and time-consuming task, especially in studies comprising\n",
      "a large amount of data. An automated and reliable quality control (QC) becomes\n",
      "mandatory. Moreover, the computation time of the QC must be also compatible\n",
      "with the processing of massive datasets. Therefore, an automated deep neural\n",
      "network approaches appear as a method of choice to automatically assess\n",
      "registration quality.\n",
      "  In the current study, a compact 3D convolutional neural network (CNN),\n",
      "referred to as RegQCNET, is introduced to quantitatively predict the amplitude\n",
      "of an affine registration mismatch between a registered image and a reference\n",
      "template. This quantitative estimation of registration error is expressed using\n",
      "metric unit system. Therefore, a meaningful task-specific threshold can be\n",
      "manually or automatically defined in order to distinguish usable and non-usable\n",
      "images.\n",
      "  The robustness of the proposed RegQCNET is first analyzed on lifespan brain\n",
      "images undergoing various simulated spatial transformations and intensity\n",
      "variations between training and testing. Secondly, the potential of RegQCNET to\n",
      "classify images as usable or non-usable is evaluated using both manual and\n",
      "automatic thresholds. During our experiments, automatic thresholds are\n",
      "estimated using several computer-assisted classification models through\n",
      "cross-validation. To this end we used expert's visual quality control estimated\n",
      "on a lifespan cohort of 3953 brains. Finally, the RegQCNET accuracy is compared\n",
      "to usual image features.\n",
      "  Results show that the proposed deep learning QC is robust, fast and accurate\n",
      "to estimate affine registration error in processing pipeline.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.03209 \n",
      "Title :Investigating maximum likelihood based training of infinite mixtures for\n",
      "  uncertainty quantification\n",
      "  Uncertainty quantification in neural networks gained a lot of attention in\n",
      "the past years. The most popular approaches, Bayesian neural networks (BNNs),\n",
      "Monte Carlo dropout, and deep ensembles have one thing in common: they are all\n",
      "based on some kind of mixture model. While the BNNs build infinite mixture\n",
      "models and are derived via variational inference, the latter two build finite\n",
      "mixtures trained with the maximum likelihood method. In this work we\n",
      "investigate the effect of training an infinite mixture distribution with the\n",
      "maximum likelihood method instead of variational inference. We find that the\n",
      "proposed objective leads to stochastic networks with an increased predictive\n",
      "variance, which improves uncertainty based identification of\n",
      "miss-classification and robustness against adversarial attacks in comparison to\n",
      "a standard BNN with equivalent network structure. The new model also displays\n",
      "higher entropy on out-of-distribution data.\n",
      "\n",
      "**Paper Id :1907.00865 \n",
      "Title :Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale\n",
      "  Bayesian Deep Learning\n",
      "  We propose Radial Bayesian Neural Networks (BNNs): a variational approximate\n",
      "posterior for BNNs which scales well to large models while maintaining a\n",
      "distribution over weight-space with full support. Other scalable Bayesian deep\n",
      "learning methods, like MC dropout or deep ensembles, have discrete support-they\n",
      "assign zero probability to almost all of the weight-space. Unlike these\n",
      "discrete support methods, Radial BNNs' full support makes them suitable for use\n",
      "as a prior for sequential inference. In addition, they solve the conceptual\n",
      "challenges with the a priori implausibility of weight distributions with\n",
      "discrete support. The Radial BNN is motivated by avoiding a sampling problem in\n",
      "'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble'\n",
      "pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are\n",
      "robust to hyperparameters and can be efficiently applied to a challenging\n",
      "real-world medical application without needing ad-hoc tweaks and intensive\n",
      "tuning. In fact, in this setting Radial BNNs out-perform discrete-support\n",
      "methods like MC dropout. Lastly, by using Radial BNNs as a theoretically\n",
      "principled, robust alternative to MFVI we make significant strides in a\n",
      "Bayesian continual learning evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.03637 \n",
      "Title :MODEL: Motif-based Deep Feature Learning for Link Prediction\n",
      "  Link prediction plays an important role in network analysis and applications.\n",
      "Recently, approaches for link prediction have evolved from traditional\n",
      "similarity-based algorithms into embedding-based algorithms. However, most\n",
      "existing approaches fail to exploit the fact that real-world networks are\n",
      "different from random networks. In particular, real-world networks are known to\n",
      "contain motifs, natural network building blocks reflecting the underlying\n",
      "network-generating processes. In this paper, we propose a novel embedding\n",
      "algorithm that incorporates network motifs to capture higher-order structures\n",
      "in the network. To evaluate its effectiveness for link prediction, experiments\n",
      "were conducted on three types of networks: social networks, biological\n",
      "networks, and academic networks. The results demonstrate that our algorithm\n",
      "outperforms both the traditional similarity-based algorithms by 20% and the\n",
      "state-of-the-art embedding-based algorithms by 19%.\n",
      "\n",
      "**Paper Id :1901.01718 \n",
      "Title :Deep Network Embedding for Graph Representation Learning in Signed\n",
      "  Networks\n",
      "  Network embedding has attracted an increasing attention over the past few\n",
      "years. As an effective approach to solve graph mining problems, network\n",
      "embedding aims to learn a low-dimensional feature vector representation for\n",
      "each node of a given network. The vast majority of existing network embedding\n",
      "algorithms, however, are only designed for unsigned networks, and the signed\n",
      "networks containing both positive and negative links, have pretty distinct\n",
      "properties from the unsigned counterpart. In this paper, we propose a deep\n",
      "network embedding model to learn the low-dimensional node vector\n",
      "representations with structural balance preservation for the signed networks.\n",
      "The model employs a semi-supervised stacked auto-encoder to reconstruct the\n",
      "adjacency connections of a given signed network. As the adjacency connections\n",
      "are overwhelmingly positive in the real-world signed networks, we impose a\n",
      "larger penalty to make the auto-encoder focus more on reconstructing the scarce\n",
      "negative links than the abundant positive links. In addition, to preserve the\n",
      "structural balance property of signed networks, we design the pairwise\n",
      "constraints to make the positively connected nodes much closer than the\n",
      "negatively connected nodes in the embedding space. Based on the network\n",
      "representations learned by the proposed model, we conduct link sign prediction\n",
      "and community detection in signed networks. Extensive experimental results in\n",
      "real-world datasets demonstrate the superiority of the proposed model over the\n",
      "state-of-the-art network embedding algorithms for graph representation learning\n",
      "in signed networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.03639 \n",
      "Title :Random Walks: A Review of Algorithms and Applications\n",
      "  A random walk is known as a random process which describes a path including a\n",
      "succession of random steps in the mathematical space. It has increasingly been\n",
      "popular in various disciplines such as mathematics and computer science.\n",
      "Furthermore, in quantum mechanics, quantum walks can be regarded as quantum\n",
      "analogues of classical random walks. Classical random walks and quantum walks\n",
      "can be used to calculate the proximity between nodes and extract the topology\n",
      "in the network. Various random walk related models can be applied in different\n",
      "fields, which is of great significance to downstream tasks such as link\n",
      "prediction, recommendation, computer vision, semi-supervised learning, and\n",
      "network embedding. In this paper, we aim to provide a comprehensive review of\n",
      "classical random walks and quantum walks. We first review the knowledge of\n",
      "classical random walks and quantum walks, including basic concepts and some\n",
      "typical algorithms. We also compare the algorithms based on quantum walks and\n",
      "classical random walks from the perspective of time complexity. Then we\n",
      "introduce their applications in the field of computer science. Finally we\n",
      "discuss the open issues from the perspectives of efficiency, main-memory\n",
      "volume, and computing time of existing algorithms. This study aims to\n",
      "contribute to this growing area of research by exploring random walks and\n",
      "quantum walks together.\n",
      "\n",
      "**Paper Id :1911.00890 \n",
      "Title :Mean-field inference methods for neural networks\n",
      "  Machine learning algorithms relying on deep neural networks recently allowed\n",
      "a great leap forward in artificial intelligence. Despite the popularity of\n",
      "their applications, the efficiency of these algorithms remains largely\n",
      "unexplained from a theoretical point of view. The mathematical description of\n",
      "learning problems involves very large collections of interacting random\n",
      "variables, difficult to handle analytically as well as numerically. This\n",
      "complexity is precisely the object of study of statistical physics. Its\n",
      "mission, originally pointed towards natural systems, is to understand how\n",
      "macroscopic behaviors arise from microscopic laws. Mean-field methods are one\n",
      "type of approximation strategy developed in this view. We review a selection of\n",
      "classical mean-field methods and recent progress relevant for inference in\n",
      "neural networks. In particular, we remind the principles of derivations of\n",
      "high-temperature expansions, the replica method and message passing algorithms,\n",
      "highlighting their equivalences and complementarities. We also provide\n",
      "references for past and current directions of research on neural networks\n",
      "relying on mean-field methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.03654 \n",
      "Title :Multivariate Relations Aggregation Learning in Social Networks\n",
      "  Multivariate relations are general in various types of networks, such as\n",
      "biological networks, social networks, transportation networks, and academic\n",
      "networks. Due to the principle of ternary closures and the trend of group\n",
      "formation, the multivariate relationships in social networks are complex and\n",
      "rich. Therefore, in graph learning tasks of social networks, the identification\n",
      "and utilization of multivariate relationship information are more important.\n",
      "Existing graph learning methods are based on the neighborhood information\n",
      "diffusion mechanism, which often leads to partial omission or even lack of\n",
      "multivariate relationship information, and ultimately affects the accuracy and\n",
      "execution efficiency of the task. To address these challenges, this paper\n",
      "proposes the multivariate relationship aggregation learning (MORE) method,\n",
      "which can effectively capture the multivariate relationship information in the\n",
      "network environment. By aggregating node attribute features and structural\n",
      "features, MORE achieves higher accuracy and faster convergence speed. We\n",
      "conducted experiments on one citation network and five social networks. The\n",
      "experimental results show that the MORE model has higher accuracy than the GCN\n",
      "(Graph Convolutional Network) model in node classification tasks, and can\n",
      "significantly reduce time cost.\n",
      "\n",
      "**Paper Id :2001.05313 \n",
      "Title :Tensor Graph Convolutional Networks for Text Classification\n",
      "  Compared to sequential learning models, graph-based neural networks exhibit\n",
      "some excellent properties, such as ability capturing global information. In\n",
      "this paper, we investigate graph-based neural networks for text classification\n",
      "problem. A new framework TensorGCN (tensor graph convolutional networks), is\n",
      "presented for this task. A text graph tensor is firstly constructed to describe\n",
      "semantic, syntactic, and sequential contextual information. Then, two kinds of\n",
      "propagation learning perform on the text graph tensor. The first is intra-graph\n",
      "propagation used for aggregating information from neighborhood nodes in a\n",
      "single graph. The second is inter-graph propagation used for harmonizing\n",
      "heterogeneous information between graphs. Extensive experiments are conducted\n",
      "on benchmark datasets, and the results illustrate the effectiveness of our\n",
      "proposed framework. Our proposed TensorGCN presents an effective way to\n",
      "harmonize and integrate heterogeneous information from different kinds of\n",
      "graphs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.03687 \n",
      "Title :LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition\n",
      "  Speech synthesis (text to speech, TTS) and recognition (automatic speech\n",
      "recognition, ASR) are important speech tasks, and require a large amount of\n",
      "text and speech pairs for model training. However, there are more than 6,000\n",
      "languages in the world and most languages are lack of speech training data,\n",
      "which poses significant challenges when building TTS and ASR systems for\n",
      "extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and\n",
      "ASR system under the extremely low-resource setting, which can support rare\n",
      "languages with low data cost. LRSpeech consists of three key techniques: 1)\n",
      "pre-training on rich-resource languages and fine-tuning on low-resource\n",
      "languages; 2) dual transformation between TTS and ASR to iteratively boost the\n",
      "accuracy of each other; 3) knowledge distillation to customize the TTS model on\n",
      "a high-quality target-speaker voice and improve the ASR model on multiple\n",
      "voices. We conduct experiments on an experimental language (English) and a\n",
      "truly low-resource language (Lithuanian) to verify the effectiveness of\n",
      "LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for\n",
      "TTS in terms of both intelligibility (more than 98% intelligibility rate) and\n",
      "naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech,\n",
      "which satisfy the requirements for industrial deployment, 2) achieves promising\n",
      "recognition accuracy for ASR, and 3) last but not least, uses extremely\n",
      "low-resource training data. We also conduct comprehensive analyses on LRSpeech\n",
      "with different amounts of data resources, and provide valuable insights and\n",
      "guidances for industrial deployment. We are currently deploying LRSpeech into a\n",
      "commercialized cloud speech service to support TTS on more rare languages.\n",
      "\n",
      "**Paper Id :2010.02477 \n",
      "Title :A Unified Deep Learning Framework for Short-Duration Speaker\n",
      "  Verification in Adverse Environments\n",
      "  Speaker verification (SV) has recently attracted considerable research\n",
      "interest due to the growing popularity of virtual assistants. At the same time,\n",
      "there is an increasing requirement for an SV system: it should be robust to\n",
      "short speech segments, especially in noisy and reverberant environments. In\n",
      "this paper, we consider one more important requirement for practical\n",
      "applications: the system should be robust to an audio stream containing long\n",
      "non-speech segments, where a voice activity detection (VAD) is not applied. To\n",
      "meet these two requirements, we introduce feature pyramid module (FPM)-based\n",
      "multi-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present\n",
      "the FPM-based MSA to deal with short speech segments in noisy and reverberant\n",
      "environments. Also, we use the SAS-VAD to increase the robustness to long\n",
      "non-speech segments. To further improve the robustness to acoustic distortions\n",
      "(i.e., noise and reverberation), we apply a masking-based speech enhancement\n",
      "(SE) method. We combine SV, VAD, and SE models in a unified deep learning\n",
      "framework and jointly train the entire network in an end-to-end manner. To the\n",
      "best of our knowledge, this is the first work combining these three models in a\n",
      "deep learning framework. We conduct experiments on Korean indoor (KID) and\n",
      "VoxCeleb datasets, which are corrupted by noise and reverberation. The results\n",
      "show that the proposed method is effective for SV in the challenging conditions\n",
      "and performs better than the baseline i-vector and deep speaker embedding\n",
      "systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.03787 \n",
      "Title :Neural Manipulation Planning on Constraint Manifolds\n",
      "  The presence of task constraints imposes a significant challenge to motion\n",
      "planning. Despite all recent advancements, existing algorithms are still\n",
      "computationally expensive for most planning problems. In this paper, we present\n",
      "Constrained Motion Planning Networks (CoMPNet), the first neural planner for\n",
      "multimodal kinematic constraints. Our approach comprises the following\n",
      "components: i) constraint and environment perception encoders; ii) neural robot\n",
      "configuration generator that outputs configurations on/near the constraint\n",
      "manifold(s), and iii) a bidirectional planning algorithm that takes the\n",
      "generated configurations to create a feasible robot motion trajectory. We show\n",
      "that CoMPNet solves practical motion planning tasks involving both\n",
      "unconstrained and constrained problems. Furthermore, it generalizes to new\n",
      "unseen locations of the objects, i.e., not seen during training, in the given\n",
      "environments with high success rates. When compared to the state-of-the-art\n",
      "constrained motion planning algorithms, CoMPNet outperforms by order of\n",
      "magnitude improvement in computational speed with a significantly lower\n",
      "variance.\n",
      "\n",
      "**Paper Id :2007.13866 \n",
      "Title :se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n",
      "  Residuals in Synthetic Domains\n",
      "  Tracking the 6D pose of objects in video sequences is important for robot\n",
      "manipulation. This task, however, introduces multiple challenges: (i) robot\n",
      "manipulation involves significant occlusions; (ii) data and annotations are\n",
      "troublesome and difficult to collect for 6D poses, which complicates machine\n",
      "learning solutions, and (iii) incremental error drift often accumulates in long\n",
      "term tracking to necessitate re-initialization of the object's pose. This work\n",
      "proposes a data-driven optimization approach for long-term, 6D pose tracking.\n",
      "It aims to identify the optimal relative pose given the current RGB-D\n",
      "observation and a synthetic image conditioned on the previous best estimate and\n",
      "the object's model. The key contribution in this context is a novel neural\n",
      "network architecture, which appropriately disentangles the feature encoding to\n",
      "help reduce domain shift, and an effective 3D orientation representation via\n",
      "Lie Algebra. Consequently, even when the network is trained only with synthetic\n",
      "data can work effectively over real images. Comprehensive experiments over\n",
      "benchmarks - existing ones as well as a new dataset with significant occlusions\n",
      "related to object manipulation - show that the proposed approach achieves\n",
      "consistently robust estimates and outperforms alternatives, even though they\n",
      "have been trained with real images. The approach is also the most\n",
      "computationally efficient among the alternatives and achieves a tracking\n",
      "frequency of 90.9Hz.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.04222 \n",
      "Title :Accuracy of neural networks for the simulation of chaotic dynamics:\n",
      "  precision of training data vs precision of the algorithm\n",
      "  We explore the influence of precision of the data and the algorithm for the\n",
      "simulation of chaotic dynamics by neural networks techniques. For this purpose,\n",
      "we simulate the Lorenz system with different precisions using three different\n",
      "neural network techniques adapted to time series, namely reservoir computing\n",
      "(using ESN), LSTM and TCN, for both short and long time predictions, and assess\n",
      "their efficiency and accuracy. Our results show that the ESN network is better\n",
      "at predicting accurately the dynamics of the system, and that in all cases the\n",
      "precision of the algorithm is more important than the precision of the training\n",
      "data for the accuracy of the predictions. This result gives support to the idea\n",
      "that neural networks can perform time-series predictions in many practical\n",
      "applications for which data are necessarily of limited precision, in line with\n",
      "recent results. It also suggests that for a given set of data the reliability\n",
      "of the predictions can be significantly improved by using a network with higher\n",
      "precision than the one of the data.\n",
      "\n",
      "**Paper Id :2001.01520 \n",
      "Title :Combining data assimilation and machine learning to emulate a dynamical\n",
      "  model from sparse and noisy observations: a case study with the Lorenz 96\n",
      "  model\n",
      "  A novel method, based on the combination of data assimilation and machine\n",
      "learning is introduced. The new hybrid approach is designed for a two-fold\n",
      "scope: (i) emulating hidden, possibly chaotic, dynamics and (ii) predicting\n",
      "their future states. The method consists in applying iteratively a data\n",
      "assimilation step, here an ensemble Kalman filter, and a neural network. Data\n",
      "assimilation is used to optimally combine a surrogate model with sparse noisy\n",
      "data. The output analysis is spatially complete and is used as a training set\n",
      "by the neural network to update the surrogate model. The two steps are then\n",
      "repeated iteratively. Numerical experiments have been carried out using the\n",
      "chaotic 40-variables Lorenz 96 model, proving both convergence and statistical\n",
      "skill of the proposed hybrid approach. The surrogate model shows short-term\n",
      "forecast skill up to two Lyapunov times, the retrieval of positive Lyapunov\n",
      "exponents as well as the more energetic frequencies of the power density\n",
      "spectrum. The sensitivity of the method to critical setup parameters is also\n",
      "presented: the forecast skill decreases smoothly with increased observational\n",
      "noise but drops abruptly if less than half of the model domain is observed. The\n",
      "successful synergy between data assimilation and machine learning, proven here\n",
      "with a low-dimensional system, encourages further investigation of such hybrids\n",
      "with more sophisticated dynamics.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.04852 \n",
      "Title :GeLaTO: Generative Latent Textured Objects\n",
      "  Accurate modeling of 3D objects exhibiting transparency, reflections and thin\n",
      "structures is an extremely challenging problem. Inspired by billboards and\n",
      "geometric proxies used in computer graphics, this paper proposes Generative\n",
      "Latent Textured Objects (GeLaTO), a compact representation that combines a set\n",
      "of coarse shape proxies defining low frequency geometry with learned neural\n",
      "textures, to encode both medium and fine scale geometry as well as\n",
      "view-dependent appearance. To generate the proxies' textures, we learn a joint\n",
      "latent space allowing category-level appearance and geometry interpolation. The\n",
      "proxies are independently rasterized with their corresponding neural texture\n",
      "and composited using a U-Net, which generates an output photorealistic image\n",
      "including an alpha map. We demonstrate the effectiveness of our approach by\n",
      "reconstructing complex objects from a sparse set of views. We show results on a\n",
      "dataset of real images of eyeglasses frames, which are particularly challenging\n",
      "to reconstruct using classical methods. We also demonstrate that these coarse\n",
      "proxies can be handcrafted when the underlying object geometry is easy to\n",
      "model, like eyeglasses, or generated using a neural network for more complex\n",
      "categories, such as cars.\n",
      "\n",
      "**Paper Id :1905.13209 \n",
      "Title :AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n",
      "  Architectures\n",
      "  Learning to represent videos is a very challenging task both algorithmically\n",
      "and computationally. Standard video CNN architectures have been designed by\n",
      "directly extending architectures devised for image understanding to include the\n",
      "time dimension, using modules such as 3D convolutions, or by using two-stream\n",
      "design to capture both appearance and motion in videos. We interpret a video\n",
      "CNN as a collection of multi-stream convolutional blocks connected to each\n",
      "other, and propose the approach of automatically finding neural architectures\n",
      "with better connectivity and spatio-temporal interactions for video\n",
      "understanding. This is done by evolving a population of overly-connected\n",
      "architectures guided by connection weight learning. Architectures combining\n",
      "representations that abstract different input types (i.e., RGB and optical\n",
      "flow) at multiple temporal resolutions are searched for, allowing different\n",
      "types or sources of information to interact with each other. Our method,\n",
      "referred to as AssembleNet, outperforms prior approaches on public video\n",
      "datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n",
      "34.27% accuracy on Moments-in-Time.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.04878 \n",
      "Title :Hardware-Centric AutoML for Mixed-Precision Quantization\n",
      "  Model quantization is a widely used technique to compress and accelerate deep\n",
      "neural network (DNN) inference. Emergent DNN hardware accelerators begin to\n",
      "support mixed precision (1-8 bits) to further improve the computation\n",
      "efficiency, which raises a great challenge to find the optimal bitwidth for\n",
      "each layer: it requires domain experts to explore the vast design space trading\n",
      "off among accuracy, latency, energy, and model size, which is both\n",
      "time-consuming and sub-optimal. Conventional quantization algorithm ignores the\n",
      "different hardware architectures and quantizes all the layers in a uniform way.\n",
      "In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\n",
      "framework which leverages the reinforcement learning to automatically determine\n",
      "the quantization policy, and we take the hardware accelerator's feedback in the\n",
      "design loop. Rather than relying on proxy signals such as FLOPs and model size,\n",
      "we employ a hardware simulator to generate direct feedback signals (latency and\n",
      "energy) to the RL agent. Compared with conventional methods, our framework is\n",
      "fully automated and can specialize the quantization policy for different neural\n",
      "network architectures and hardware architectures. Our framework effectively\n",
      "reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with\n",
      "negligible loss of accuracy compared with the fixed bitwidth (8 bits)\n",
      "quantization. Our framework reveals that the optimal policies on different\n",
      "hardware architectures (i.e., edge and cloud architectures) under different\n",
      "resource constraints (i.e., latency, energy, and model size) are drastically\n",
      "different. We interpreted the implication of different quantization policies,\n",
      "which offer insights for both neural network architecture design and hardware\n",
      "architecture design.\n",
      "\n",
      "**Paper Id :1907.06870 \n",
      "Title :Light Multi-segment Activation for Model Compression\n",
      "  Model compression has become necessary when applying neural networks (NN)\n",
      "into many real application tasks that can accept slightly-reduced model\n",
      "accuracy with strict tolerance to model complexity. Recently, Knowledge\n",
      "Distillation, which distills the knowledge from well-trained and highly complex\n",
      "teacher model into a compact student model, has been widely used for model\n",
      "compression. However, under the strict requirement on the resource cost, it is\n",
      "quite challenging to achieve comparable performance with the teacher model,\n",
      "essentially due to the drastically-reduced expressiveness ability of the\n",
      "compact student model. Inspired by the nature of the expressiveness ability in\n",
      "Neural Networks, we propose to use multi-segment activation, which can\n",
      "significantly improve the expressiveness ability with very little cost, in the\n",
      "compact student model. Specifically, we propose a highly efficient\n",
      "multi-segment activation, called Light Multi-segment Activation (LMA), which\n",
      "can rapidly produce multiple linear regions with very few parameters by\n",
      "leveraging the statistical information. With using LMA, the compact student\n",
      "model is capable of achieving much better performance effectively and\n",
      "efficiently, than the ReLU-equipped one with same model scale. Furthermore, the\n",
      "proposed method is compatible with other model compression techniques, such as\n",
      "quantization, which means they can be used jointly for better compression\n",
      "performance. Experiments on state-of-the-art NN architectures over the\n",
      "real-world tasks demonstrate the effectiveness and extensibility of the LMA.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.04968 \n",
      "Title :Campus3D: A Photogrammetry Point Cloud Benchmark for Hierarchical\n",
      "  Understanding of Outdoor Scene\n",
      "  Learning on 3D scene-based point cloud has received extensive attention as\n",
      "its promising application in many fields, and well-annotated and multisource\n",
      "datasets can catalyze the development of those data-driven approaches. To\n",
      "facilitate the research of this area, we present a richly-annotated 3D point\n",
      "cloud dataset for multiple outdoor scene understanding tasks and also an\n",
      "effective learning framework for its hierarchical segmentation task. The\n",
      "dataset was generated via the photogrammetric processing on unmanned aerial\n",
      "vehicle (UAV) images of the National University of Singapore (NUS) campus, and\n",
      "has been point-wisely annotated with both hierarchical and instance-based\n",
      "labels. Based on it, we formulate a hierarchical learning problem for 3D point\n",
      "cloud segmentation and propose a measurement evaluating consistency across\n",
      "various hierarchies. To solve this problem, a two-stage method including\n",
      "multi-task (MT) learning and hierarchical ensemble (HE) with consistency\n",
      "consideration is proposed. Experimental results demonstrate the superiority of\n",
      "the proposed method and potential advantages of our hierarchical annotations.\n",
      "In addition, we benchmark results of semantic and instance segmentation, which\n",
      "is accessible online at https://3d.dataset.site with the dataset and all source\n",
      "codes.\n",
      "\n",
      "**Paper Id :2007.09590 \n",
      "Title :AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation\n",
      "  In this paper, we propose an adaptive weighting regression (AWR) method to\n",
      "leverage the advantages of both detection-based and regression-based methods.\n",
      "Hand joint coordinates are estimated as discrete integration of all pixels in\n",
      "dense representation, guided by adaptive weight maps. This learnable\n",
      "aggregation process introduces both dense and joint supervision that allows\n",
      "end-to-end training and brings adaptability to weight maps, making the network\n",
      "more accurate and robust. Comprehensive exploration experiments are conducted\n",
      "to validate the effectiveness and generality of AWR under various experimental\n",
      "settings, especially its usefulness for different types of dense representation\n",
      "and input modality. Our method outperforms other state-of-the-art methods on\n",
      "four publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017\n",
      "dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.05011 \n",
      "Title :Compact Speaker Embedding: lrx-vector\n",
      "  Deep neural networks (DNN) have recently been widely used in speaker\n",
      "recognition systems, achieving state-of-the-art performance on various\n",
      "benchmarks. The x-vector architecture is especially popular in this research\n",
      "community, due to its excellent performance and manageable computational\n",
      "complexity. In this paper, we present the lrx-vector system, which is the\n",
      "low-rank factorized version of the x-vector embedding network. The primary\n",
      "objective of this topology is to further reduce the memory requirement of the\n",
      "speaker recognition system. We discuss the deployment of knowledge distillation\n",
      "for training the lrx-vector system and compare against low-rank factorization\n",
      "with SVD. On the VOiCES 2019 far-field corpus we were able to reduce the\n",
      "weights by 28% compared to the full-rank x-vector system while keeping the\n",
      "recognition rate constant (1.83% EER).\n",
      "\n",
      "**Paper Id :1906.01493 \n",
      "Title :Constructing Energy-efficient Mixed-precision Neural Networks through\n",
      "  Principal Component Analysis for Edge Intelligence\n",
      "  The `Internet of Things' has brought increased demand for AI-based edge\n",
      "computing in applications ranging from healthcare monitoring systems to\n",
      "autonomous vehicles. Quantization is a powerful tool to address the growing\n",
      "computational cost of such applications, and yields significant compression\n",
      "over full-precision networks. However, quantization can result in substantial\n",
      "loss of performance for complex image classification tasks. To address this, we\n",
      "propose a Principal Component Analysis (PCA) driven methodology to identify the\n",
      "important layers of a binary network, and design mixed-precision networks. The\n",
      "proposed Hybrid-Net achieves a more than 10% improvement in classification\n",
      "accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures\n",
      "on CIFAR-100 and ImageNet datasets while still achieving up to 94% of the\n",
      "energy-efficiency of XNOR-Nets. This work furthers the feasibility of using\n",
      "highly compressed neural networks for energy-efficient neural computing in edge\n",
      "devices.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.05459 \n",
      "Title :Analyzing Upper Bounds on Mean Absolute Errors for Deep Neural Network\n",
      "  Based Vector-to-Vector Regression\n",
      "  In this paper, we show that, in vector-to-vector regression utilizing deep\n",
      "neural networks (DNNs), a generalized loss of mean absolute error (MAE) between\n",
      "the predicted and expected feature vectors is upper bounded by the sum of an\n",
      "approximation error, an estimation error, and an optimization error. Leveraging\n",
      "upon error decomposition techniques in statistical learning theory and\n",
      "non-convex optimization theory, we derive upper bounds for each of the three\n",
      "aforementioned errors and impose necessary constraints on DNN models. Moreover,\n",
      "we assess our theoretical results through a set of image de-noising and speech\n",
      "enhancement experiments. Our proposed upper bounds of MAE for DNN based\n",
      "vector-to-vector regression are corroborated by the experimental results and\n",
      "the upper bounds are valid with and without the \"over-parametrization\"\n",
      "technique.\n",
      "\n",
      "**Paper Id :2008.07281 \n",
      "Title :On Mean Absolute Error for Deep Neural Network Based Vector-to-Vector\n",
      "  Regression\n",
      "  In this paper, we exploit the properties of mean absolute error (MAE) as a\n",
      "loss function for the deep neural network (DNN) based vector-to-vector\n",
      "regression. The goal of this work is two-fold: (i) presenting performance\n",
      "bounds of MAE, and (ii) demonstrating new properties of MAE that make it more\n",
      "appropriate than mean squared error (MSE) as a loss function for DNN based\n",
      "vector-to-vector regression. First, we show that a generalized upper-bound for\n",
      "DNN-based vector- to-vector regression can be ensured by leveraging the known\n",
      "Lipschitz continuity property of MAE. Next, we derive a new generalized upper\n",
      "bound in the presence of additive noise. Finally, in contrast to conventional\n",
      "MSE commonly adopted to approximate Gaussian errors for regression, we show\n",
      "that MAE can be interpreted as an error modeled by Laplacian distribution.\n",
      "Speech enhancement experiments are conducted to corroborate our proposed\n",
      "theorems and validate the performance advantages of MAE over MSE for DNN based\n",
      "regression.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.06376 \n",
      "Title :MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages\n",
      "  and Modalities\n",
      "  In this paper, we introduce the MLM (Multiple Languages and Modalities)\n",
      "dataset - a new resource to train and evaluate multitask systems on samples in\n",
      "multiple modalities and three languages. The generation process and inclusion\n",
      "of semantic data provide a resource that further tests the ability for\n",
      "multitask systems to learn relationships between entities. The dataset is\n",
      "designed for researchers and developers who build applications that perform\n",
      "multiple tasks on data encountered on the web and in digital archives. A second\n",
      "version of MLM provides a geo-representative subset of the data with weighted\n",
      "samples for countries of the European Union. We demonstrate the value of the\n",
      "resource in developing novel applications in the digital humanities with a\n",
      "motivating use case and specify a benchmark set of tasks to retrieve modalities\n",
      "and locate entities in the dataset. Evaluation of baseline multitask and single\n",
      "task systems on the full and geo-representative versions of MLM demonstrate the\n",
      "challenges of generalising on diverse data. In addition to the digital\n",
      "humanities, we expect the resource to contribute to research in multimodal\n",
      "representation learning, location estimation, and scene understanding.\n",
      "\n",
      "**Paper Id :2001.11268 \n",
      "Title :Data Mining in Clinical Trial Text: Transformers for Classification and\n",
      "  Question Answering Tasks\n",
      "  This research on data extraction methods applies recent advances in natural\n",
      "language processing to evidence synthesis based on medical texts. Texts of\n",
      "interest include abstracts of clinical trials in English and in multilingual\n",
      "contexts. The main focus is on information characterized via the Population,\n",
      "Intervention, Comparator, and Outcome (PICO) framework, but data extraction is\n",
      "not limited to these fields. Recent neural network architectures based on\n",
      "transformers show capacities for transfer learning and increased performance on\n",
      "downstream natural language processing tasks such as universal reading\n",
      "comprehension, brought forward by this architecture's use of contextualized\n",
      "word embeddings and self-attention mechanisms. This paper contributes to\n",
      "solving problems related to ambiguity in PICO sentence prediction tasks, as\n",
      "well as highlighting how annotations for training named entity recognition\n",
      "systems are used to train a high-performing, but nevertheless flexible\n",
      "architecture for question answering in systematic review automation.\n",
      "Additionally, it demonstrates how the problem of insufficient amounts of\n",
      "training annotations for PICO entity extraction is tackled by augmentation. All\n",
      "models in this paper were created with the aim to support systematic review\n",
      "(semi)automation. They achieve high F1 scores, and demonstrate the feasibility\n",
      "of applying transformer-based classification methods to support data mining in\n",
      "the biomedical literature.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.06415 \n",
      "Title :Orbital Graph Convolutional Neural Network for Material Property\n",
      "  Prediction\n",
      "  Material representations that are compatible with machine learning models\n",
      "play a key role in developing models that exhibit high accuracy for property\n",
      "prediction. Atomic orbital interactions are one of the important factors that\n",
      "govern the properties of crystalline materials, from which the local chemical\n",
      "environments of atoms is inferred. Therefore, to develop robust machine\n",
      "learningmodels for material properties prediction, it is imperative to include\n",
      "features representing such chemical attributes. Here, we propose the Orbital\n",
      "Graph Convolutional Neural Network (OGCNN), a crystal graph convolutional\n",
      "neural network framework that includes atomic orbital interaction features that\n",
      "learns material properties in a robust way. In addition, we embedded an\n",
      "encoder-decoder network into the OGCNN enabling it to learn important features\n",
      "among basic atomic (elemental features), orbital-orbital interactions, and\n",
      "topological features. We examined the performance of this model on a broad\n",
      "range of crystalline material data to predict different properties. We\n",
      "benchmarked the performance of the OGCNN model with that of: 1) the crystal\n",
      "graph convolutional neural network (CGCNN), 2) other state-of-the-art\n",
      "descriptors for material representations including Many-body Tensor\n",
      "Representation (MBTR) and the Smooth Overlap of Atomic Positions (SOAP), and 3)\n",
      "other conventional regression machine learning algorithms where different\n",
      "crystal featurization methods have been used. We find that OGCNN significantly\n",
      "outperforms them. The OGCNN model with high predictive accuracy can be used to\n",
      "discover new materials among the immense phase and compound spaces of materials\n",
      "\n",
      "**Paper Id :2003.13425 \n",
      "Title :Predicting Elastic Properties of Materials from Electronic Charge\n",
      "  Density Using 3D Deep Convolutional Neural Networks\n",
      "  Materials representation plays a key role in machine learning based\n",
      "prediction of materials properties and new materials discovery. Currently both\n",
      "graph and 3D voxel representation methods are based on the heterogeneous\n",
      "elements of the crystal structures. Here, we propose to use electronic charge\n",
      "density (ECD) as a generic unified 3D descriptor for materials property\n",
      "prediction with the advantage of possessing close relation with the physical\n",
      "and chemical properties of materials. We developed an ECD based 3D\n",
      "convolutional neural networks (CNNs) for predicting elastic properties of\n",
      "materials, in which CNNs can learn effective hierarchical features with\n",
      "multiple convolving and pooling operations. Extensive benchmark experiments\n",
      "over 2,170 Fm-3m face-centered-cubic (FCC) materials show that our ECD based\n",
      "CNNs can achieve good performance for elasticity prediction. Especially, our\n",
      "CNN models based on the fusion of elemental Magpie features and ECD descriptors\n",
      "achieved the best 5-fold cross-validation performance. More importantly, we\n",
      "showed that our ECD based CNN models can achieve significantly better\n",
      "extrapolation performance when evaluated over non-redundant datasets where\n",
      "there are few neighbor training samples around test samples. As additional\n",
      "validation, we evaluated the predictive performance of our models on 329\n",
      "materials of space group Fm-3m by comparing to DFT calculated values, which\n",
      "shows better prediction power of our model for bulk modulus than shear modulus.\n",
      "Due to the unified representation power of ECD, it is expected that our ECD\n",
      "based CNN approach can also be applied to predict other physical and chemical\n",
      "properties of crystalline materials.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.06831 \n",
      "Title :DeepSampling: Selectivity Estimation with Predicted Error and Response\n",
      "  Time\n",
      "  The rapid growth of spatial data urges the research community to find\n",
      "efficient processing techniques for interactive queries on large volumes of\n",
      "data. Approximate Query Processing (AQP) is the most prominent technique that\n",
      "can provide real-time answer for ad-hoc queries based on a random sample.\n",
      "Unfortunately, existing AQP methods provide an answer without providing any\n",
      "accuracy metrics due to the complex relationship between the sample size, the\n",
      "query parameters, the data distribution, and the result accuracy. This paper\n",
      "proposes DeepSampling, a deep-learning-based model that predicts the accuracy\n",
      "of a sample-based AQP algorithm, specially selectivity estimation, given the\n",
      "sample size, the input distribution, and query parameters. The model can also\n",
      "be reversed to measure the sample size that would produce a desired accuracy.\n",
      "DeepSampling is the first system that provides a reliable tool for existing\n",
      "spatial databases to control the accuracy of AQP.\n",
      "\n",
      "**Paper Id :1912.03927 \n",
      "Title :Large deviations for the perceptron model and consequences for active\n",
      "  learning\n",
      "  Active learning is a branch of machine learning that deals with problems\n",
      "where unlabeled data is abundant yet obtaining labels is expensive. The\n",
      "learning algorithm has the possibility of querying a limited number of samples\n",
      "to obtain the corresponding labels, subsequently used for supervised learning.\n",
      "In this work, we consider the task of choosing the subset of samples to be\n",
      "labeled from a fixed finite pool of samples. We assume the pool of samples to\n",
      "be a random matrix and the ground truth labels to be generated by a\n",
      "single-layer teacher random neural network. We employ replica methods to\n",
      "analyze the large deviations for the accuracy achieved after supervised\n",
      "learning on a subset of the original pool. These large deviations then provide\n",
      "optimal achievable performance boundaries for any active learning algorithm. We\n",
      "show that the optimal learning performance can be efficiently approached by\n",
      "simple message-passing active learning algorithms. We also provide a comparison\n",
      "with the performance of some other popular active learning strategies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.07029 \n",
      "Title :Uncertainty aware Search Framework for Multi-Objective Bayesian\n",
      "  Optimization with Constraints\n",
      "  We consider the problem of constrained multi-objective (MO) blackbox\n",
      "optimization using expensive function evaluations, where the goal is to\n",
      "approximate the true Pareto set of solutions satisfying a set of constraints\n",
      "while minimizing the number of function evaluations. We propose a novel\n",
      "framework named Uncertainty-aware Search framework for Multi-Objective\n",
      "Optimization with Constraints (USeMOC) to efficiently select the sequence of\n",
      "inputs for evaluation to solve this problem. The selection method of USeMOC\n",
      "consists of solving a cheap constrained MO optimization problem via surrogate\n",
      "models of the true functions to identify the most promising candidates and\n",
      "picking the best candidate based on a measure of uncertainty. We applied this\n",
      "framework to optimize the design of a multi-output switched-capacitor voltage\n",
      "regulator via expensive simulations. Our experimental results show that USeMOC\n",
      "is able to achieve more than 90 % reduction in the number of simulations needed\n",
      "to uncover optimized circuits.\n",
      "\n",
      "**Paper Id :2006.03963 \n",
      "Title :Combinatorial Black-Box Optimization with Expert Advice\n",
      "  We consider the problem of black-box function optimization over the boolean\n",
      "hypercube. Despite the vast literature on black-box function optimization over\n",
      "continuous domains, not much attention has been paid to learning models for\n",
      "optimization over combinatorial domains until recently. However, the\n",
      "computational complexity of the recently devised algorithms are prohibitive\n",
      "even for moderate numbers of variables; drawing one sample using the existing\n",
      "algorithms is more expensive than a function evaluation for many black-box\n",
      "functions of interest. To address this problem, we propose a computationally\n",
      "efficient model learning algorithm based on multilinear polynomials and\n",
      "exponential weight updates. In the proposed algorithm, we alternate between\n",
      "simulated annealing with respect to the current polynomial representation and\n",
      "updating the weights using monomial experts' advice. Numerical experiments on\n",
      "various datasets in both unconstrained and sum-constrained boolean optimization\n",
      "indicate the competitive performance of the proposed algorithm, while improving\n",
      "the computational time up to several orders of magnitude compared to\n",
      "state-of-the-art algorithms in the literature.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.07092 \n",
      "Title :Understanding Brain Dynamics for Color Perception using Wearable EEG\n",
      "  headband\n",
      "  The perception of color is an important cognitive feature of the human brain.\n",
      "The variety of colors that impinge upon the human eye can trigger changes in\n",
      "brain activity which can be captured using electroencephalography (EEG). In\n",
      "this work, we have designed a multiclass classification model to detect the\n",
      "primary colors from the features of raw EEG signals. In contrast to previous\n",
      "research, our method employs spectral power features, statistical features as\n",
      "well as correlation features from the signal band power obtained from\n",
      "continuous Morlet wavelet transform instead of raw EEG, for the classification\n",
      "task. We have applied dimensionality reduction techniques such as Forward\n",
      "Feature Selection and Stacked Autoencoders to reduce the dimension of data\n",
      "eventually increasing the model's efficiency. Our proposed methodology using\n",
      "Forward Selection and Random Forest Classifier gave the best overall accuracy\n",
      "of 80.6\\% for intra-subject classification. Our approach shows promise in\n",
      "developing techniques for cognitive tasks using color cues such as controlling\n",
      "Internet of Thing (IoT) devices by looking at primary colors for individuals\n",
      "with restricted motor abilities.\n",
      "\n",
      "**Paper Id :2011.02195 \n",
      "Title :Correlation based Multi-phasal models for improved imagined speech EEG\n",
      "  recognition\n",
      "  Translation of imagined speech electroencephalogram(EEG) into human\n",
      "understandable commands greatly facilitates the design of naturalistic brain\n",
      "computer interfaces. To achieve improved imagined speech unit classification,\n",
      "this work aims to profit from the parallel information contained in\n",
      "multi-phasal EEG data recorded while speaking, imagining and performing\n",
      "articulatory movements corresponding to specific speech units. A bi-phase\n",
      "common representation learning module using neural networks is designed to\n",
      "model the correlation and reproducibility between an analysis phase and a\n",
      "support phase. The trained Correlation Network is then employed to extract\n",
      "discriminative features of the analysis phase. These features are further\n",
      "classified into five binary phonological categories using machine learning\n",
      "models such as Gaussian mixture based hidden Markov model and deep neural\n",
      "networks. The proposed approach further handles the non-availability of\n",
      "multi-phasal data during decoding. Topographic visualizations along with\n",
      "result-based inferences suggest that the multi-phasal correlation modelling\n",
      "approach proposed in the paper enhances imagined-speech EEG recognition\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.07118 \n",
      "Title :PIANOTREE VAE: Structured Representation Learning for Polyphonic Music\n",
      "  The dominant approach for music representation learning involves the deep\n",
      "unsupervised model family variational autoencoder (VAE). However, most, if not\n",
      "all, viable attempts on this problem have largely been limited to monophonic\n",
      "music. Normally composed of richer modality and more complex musical\n",
      "structures, the polyphonic counterpart has yet to be addressed in the context\n",
      "of music representation learning. In this work, we propose the PianoTree VAE, a\n",
      "novel tree-structure extension upon VAE aiming to fit the polyphonic music\n",
      "learning. The experiments prove the validity of the PianoTree VAE via\n",
      "(i)-semantically meaningful latent code for polyphonic segments; (ii)-more\n",
      "satisfiable reconstruction aside of decent geometry learned in the latent\n",
      "space; (iii)-this model's benefits to the variety of the downstream music\n",
      "generation.\n",
      "\n",
      "**Paper Id :2008.07122 \n",
      "Title :Learning Interpretable Representation for Controllable Polyphonic Music\n",
      "  Generation\n",
      "  While deep generative models have become the leading methods for algorithmic\n",
      "composition, it remains a challenging problem to control the generation process\n",
      "because the latent variables of most deep-learning models lack good\n",
      "interpretability. Inspired by the content-style disentanglement idea, we design\n",
      "a novel architecture, under the VAE framework, that effectively learns two\n",
      "interpretable latent factors of polyphonic music: chord and texture. The\n",
      "current model focuses on learning 8-beat long piano composition segments. We\n",
      "show that such chord-texture disentanglement provides a controllable generation\n",
      "pathway leading to a wide spectrum of applications, including compositional\n",
      "style transfer, texture variation, and accompaniment arrangement. Both\n",
      "objective and subjective evaluations show that our method achieves a successful\n",
      "disentanglement and high quality controlled music generation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.07122 \n",
      "Title :Learning Interpretable Representation for Controllable Polyphonic Music\n",
      "  Generation\n",
      "  While deep generative models have become the leading methods for algorithmic\n",
      "composition, it remains a challenging problem to control the generation process\n",
      "because the latent variables of most deep-learning models lack good\n",
      "interpretability. Inspired by the content-style disentanglement idea, we design\n",
      "a novel architecture, under the VAE framework, that effectively learns two\n",
      "interpretable latent factors of polyphonic music: chord and texture. The\n",
      "current model focuses on learning 8-beat long piano composition segments. We\n",
      "show that such chord-texture disentanglement provides a controllable generation\n",
      "pathway leading to a wide spectrum of applications, including compositional\n",
      "style transfer, texture variation, and accompaniment arrangement. Both\n",
      "objective and subjective evaluations show that our method achieves a successful\n",
      "disentanglement and high quality controlled music generation.\n",
      "\n",
      "**Paper Id :2001.08255 \n",
      "Title :A Probabilistic Framework for Imitating Human Race Driver Behavior\n",
      "  Understanding and modeling human driver behavior is crucial for advanced\n",
      "vehicle development. However, unique driving styles, inconsistent behavior, and\n",
      "complex decision processes render it a challenging task, and existing\n",
      "approaches often lack variability or robustness. To approach this problem, we\n",
      "propose Probabilistic Modeling of Driver behavior (ProMoD), a modular framework\n",
      "which splits the task of driver behavior modeling into multiple modules. A\n",
      "global target trajectory distribution is learned with Probabilistic Movement\n",
      "Primitives, clothoids are utilized for local path generation, and the\n",
      "corresponding choice of actions is performed by a neural network. Experiments\n",
      "in a simulated car racing setting show considerable advantages in imitation\n",
      "accuracy and robustness compared to other imitation learning algorithms. The\n",
      "modular architecture of the proposed framework facilitates straightforward\n",
      "extensibility in driving line adaptation and sequencing of multiple movement\n",
      "primitives for future research.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.07142 \n",
      "Title :POP909: A Pop-song Dataset for Music Arrangement Generation\n",
      "  Music arrangement generation is a subtask of automatic music generation,\n",
      "which involves reconstructing and re-conceptualizing a piece with new\n",
      "compositional techniques. Such a generation process inevitably requires\n",
      "reference from the original melody, chord progression, or other structural\n",
      "information. Despite some promising models for arrangement, they lack more\n",
      "refined data to achieve better evaluations and more practical results. In this\n",
      "paper, we propose POP909, a dataset which contains multiple versions of the\n",
      "piano arrangements of 909 popular songs created by professional musicians. The\n",
      "main body of the dataset contains the vocal melody, the lead instrument melody,\n",
      "and the piano accompaniment for each song in MIDI format, which are aligned to\n",
      "the original audio files. Furthermore, we provide the annotations of tempo,\n",
      "beat, key, and chords, where the tempo curves are hand-labeled and others are\n",
      "done by MIR algorithms. Finally, we conduct several baseline experiments with\n",
      "this dataset using standard deep music generation algorithms.\n",
      "\n",
      "**Paper Id :2002.05511 \n",
      "Title :Deep Autotuner: a Pitch Correcting Network for Singing Performances\n",
      "  We introduce a data-driven approach to automatic pitch correction of solo\n",
      "singing performances. The proposed approach predicts note-wise pitch shifts\n",
      "from the relationship between the respective spectrograms of the singing and\n",
      "accompaniment. This approach differs from commercial systems, where vocal track\n",
      "notes are usually shifted to be centered around pitches in a user-defined\n",
      "score, or mapped to the closest pitch among the twelve equal-tempered scale\n",
      "degrees. The proposed system treats pitch as a continuous value rather than\n",
      "relying on a set of discretized notes found in musical scores, thus allowing\n",
      "for improvisation and harmonization in the singing performance. We train our\n",
      "neural network model using a dataset of 4,702 amateur karaoke performances\n",
      "selected for good intonation. Our model is trained on both incorrect\n",
      "intonation, for which it learns a correction, and intentional pitch variation,\n",
      "which it learns to preserve. The proposed deep neural network with gated\n",
      "recurrent units on top of convolutional layers shows promising performance on\n",
      "the real-world score-free singing pitch correction task of autotuning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.07281 \n",
      "Title :On Mean Absolute Error for Deep Neural Network Based Vector-to-Vector\n",
      "  Regression\n",
      "  In this paper, we exploit the properties of mean absolute error (MAE) as a\n",
      "loss function for the deep neural network (DNN) based vector-to-vector\n",
      "regression. The goal of this work is two-fold: (i) presenting performance\n",
      "bounds of MAE, and (ii) demonstrating new properties of MAE that make it more\n",
      "appropriate than mean squared error (MSE) as a loss function for DNN based\n",
      "vector-to-vector regression. First, we show that a generalized upper-bound for\n",
      "DNN-based vector- to-vector regression can be ensured by leveraging the known\n",
      "Lipschitz continuity property of MAE. Next, we derive a new generalized upper\n",
      "bound in the presence of additive noise. Finally, in contrast to conventional\n",
      "MSE commonly adopted to approximate Gaussian errors for regression, we show\n",
      "that MAE can be interpreted as an error modeled by Laplacian distribution.\n",
      "Speech enhancement experiments are conducted to corroborate our proposed\n",
      "theorems and validate the performance advantages of MAE over MSE for DNN based\n",
      "regression.\n",
      "\n",
      "**Paper Id :2008.05459 \n",
      "Title :Analyzing Upper Bounds on Mean Absolute Errors for Deep Neural Network\n",
      "  Based Vector-to-Vector Regression\n",
      "  In this paper, we show that, in vector-to-vector regression utilizing deep\n",
      "neural networks (DNNs), a generalized loss of mean absolute error (MAE) between\n",
      "the predicted and expected feature vectors is upper bounded by the sum of an\n",
      "approximation error, an estimation error, and an optimization error. Leveraging\n",
      "upon error decomposition techniques in statistical learning theory and\n",
      "non-convex optimization theory, we derive upper bounds for each of the three\n",
      "aforementioned errors and impose necessary constraints on DNN models. Moreover,\n",
      "we assess our theoretical results through a set of image de-noising and speech\n",
      "enhancement experiments. Our proposed upper bounds of MAE for DNN based\n",
      "vector-to-vector regression are corroborated by the experimental results and\n",
      "the upper bounds are valid with and without the \"over-parametrization\"\n",
      "technique.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.08072 \n",
      "Title :AssembleNet++: Assembling Modality Representations via Attention\n",
      "  Connections\n",
      "  We create a family of powerful video models which are able to: (i) learn\n",
      "interactions between semantic object information and raw appearance and motion\n",
      "features, and (ii) deploy attention in order to better learn the importance of\n",
      "features at each convolutional block of the network. A new network component\n",
      "named peer-attention is introduced, which dynamically learns the attention\n",
      "weights using another block or input modality. Even without pre-training, our\n",
      "models outperform the previous work on standard public activity recognition\n",
      "datasets with continuous videos, establishing new state-of-the-art. We also\n",
      "confirm that our findings of having neural connections from the object modality\n",
      "and the use of peer-attention is generally applicable for different existing\n",
      "architectures, improving their performances. We name our model explicitly as\n",
      "AssembleNet++. The code will be available at:\n",
      "https://sites.google.com/corp/view/assemblenet/\n",
      "\n",
      "**Paper Id :2005.05930 \n",
      "Title :Localized convolutional neural networks for geospatial wind forecasting\n",
      "  Convolutional Neural Networks (CNN) possess many positive qualities when it\n",
      "comes to spatial raster data. Translation invariance enables CNNs to detect\n",
      "features regardless of their position in the scene. However, in some domains,\n",
      "like geospatial, not all locations are exactly equal. In this work, we propose\n",
      "localized convolutional neural networks that enable convolutional architectures\n",
      "to learn local features in addition to the global ones. We investigate their\n",
      "instantiations in the form of learnable inputs, local weights, and a more\n",
      "general form. They can be added to any convolutional layers, easily end-to-end\n",
      "trained, introduce minimal additional complexity, and let CNNs retain most of\n",
      "their benefits to the extent that they are needed. In this work we address\n",
      "spatio-temporal prediction: test the effectiveness of our methods on a\n",
      "synthetic benchmark dataset and tackle three real-world wind prediction\n",
      "datasets. For one of them, we propose a method to spatially order the unordered\n",
      "data. We compare the recent state-of-the-art spatio-temporal prediction models\n",
      "on the same data. Models that use convolutional layers can be and are extended\n",
      "with our localizations. In all these cases our extensions improve the results,\n",
      "and thus often the state-of-the-art. We share all the code at a public\n",
      "repository.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.08424 \n",
      "Title :AutoSimulate: (Quickly) Learning Synthetic Data Generation\n",
      "  Simulation is increasingly being used for generating large labelled datasets\n",
      "in many machine learning problems. Recent methods have focused on adjusting\n",
      "simulator parameters with the goal of maximising accuracy on a validation task,\n",
      "usually relying on REINFORCE-like gradient estimators. However these approaches\n",
      "are very expensive as they treat the entire data generation, model training,\n",
      "and validation pipeline as a black-box and require multiple costly objective\n",
      "evaluations at each iteration. We propose an efficient alternative for optimal\n",
      "synthetic data generation, based on a novel differentiable approximation of the\n",
      "objective. This allows us to optimize the simulator, which may be\n",
      "non-differentiable, requiring only one objective evaluation at each iteration\n",
      "with a little overhead. We demonstrate on a state-of-the-art photorealistic\n",
      "renderer that the proposed method finds the optimal data distribution faster\n",
      "(up to $50\\times$), with significantly reduced training data generation (up to\n",
      "$30\\times$) and better accuracy ($+8.7\\%$) on real-world test datasets than\n",
      "previous methods.\n",
      "\n",
      "**Paper Id :2010.12438 \n",
      "Title :Transferable Graph Optimizers for ML Compilers\n",
      "  Most compilers for machine learning (ML) frameworks need to solve many\n",
      "correlated optimization problems to generate efficient machine code. Current ML\n",
      "compilers rely on heuristics based algorithms to solve these optimization\n",
      "problems one at a time. However, this approach is not only hard to maintain but\n",
      "often leads to sub-optimal solutions especially for newer model architectures.\n",
      "Existing learning based approaches in the literature are sample inefficient,\n",
      "tackle a single optimization problem, and do not generalize to unseen graphs\n",
      "making them infeasible to be deployed in practice. To address these\n",
      "limitations, we propose an end-to-end, transferable deep reinforcement learning\n",
      "method for computational graph optimization (GO), based on a scalable\n",
      "sequential attention mechanism over an inductive graph neural network. GO\n",
      "generates decisions on the entire graph rather than on each individual node\n",
      "autoregressively, drastically speeding up the search compared to prior methods.\n",
      "Moreover, we propose recurrent attention layers to jointly optimize dependent\n",
      "graph optimization tasks and demonstrate 33%-60% speedup on three graph\n",
      "optimization tasks compared to TensorFlow default optimization. On a diverse\n",
      "set of representative graphs consisting of up to 80,000 nodes, including\n",
      "Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21%\n",
      "improvement over human experts and 18% improvement over the prior state of the\n",
      "art with 15x faster convergence, on a device placement task evaluated in real\n",
      "systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.09020 \n",
      "Title :Training Sensitivity in Graph Isomorphism Network\n",
      "  Graph neural network (GNN) is a popular tool to learn the lower-dimensional\n",
      "representation of a graph. It facilitates the applicability of machine learning\n",
      "tasks on graphs by incorporating domain-specific features. There are various\n",
      "options for underlying procedures (such as optimization functions, activation\n",
      "functions, etc.) that can be considered in the implementation of GNN. However,\n",
      "most of the existing tools are confined to one approach without any analysis.\n",
      "Thus, this emerging field lacks a robust implementation ignoring the highly\n",
      "irregular structure of the real-world graphs. In this paper, we attempt to fill\n",
      "this gap by studying various alternative functions for a respective module\n",
      "using a diverse set of benchmark datasets. Our empirical results suggest that\n",
      "the generally used underlying techniques do not always perform well to capture\n",
      "the overall structure from a set of graphs.\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.09043 \n",
      "Title :Considerations, Good Practices, Risks and Pitfalls in Developing AI\n",
      "  Solutions Against COVID-19\n",
      "  The COVID-19 pandemic has been a major challenge to humanity, with 12.7\n",
      "million confirmed cases as of July 13th, 2020 [1]. In previous work, we\n",
      "described how Artificial Intelligence can be used to tackle the pandemic with\n",
      "applications at the molecular, clinical, and societal scales [2]. In the\n",
      "present follow-up article, we review these three research directions, and\n",
      "assess the level of maturity and feasibility of the approaches used, as well as\n",
      "their potential for operationalization. We also summarize some commonly\n",
      "encountered risks and practical pitfalls, as well as guidelines and best\n",
      "practices for formulating and deploying AI applications at different scales.\n",
      "\n",
      "**Paper Id :1905.12762 \n",
      "Title :Securing Connected & Autonomous Vehicles: Challenges Posed by\n",
      "  Adversarial Machine Learning and The Way Forward\n",
      "  Connected and autonomous vehicles (CAVs) will form the backbone of future\n",
      "next-generation intelligent transportation systems (ITS) providing travel\n",
      "comfort, road safety, along with a number of value-added services. Such a\n",
      "transformation---which will be fuelled by concomitant advances in technologies\n",
      "for machine learning (ML) and wireless communications---will enable a future\n",
      "vehicular ecosystem that is better featured and more efficient. However, there\n",
      "are lurking security problems related to the use of ML in such a critical\n",
      "setting where an incorrect ML decision may not only be a nuisance but can lead\n",
      "to loss of precious lives. In this paper, we present an in-depth overview of\n",
      "the various challenges associated with the application of ML in vehicular\n",
      "networks. In addition, we formulate the ML pipeline of CAVs and present various\n",
      "potential security issues associated with the adoption of ML methods. In\n",
      "particular, we focus on the perspective of adversarial ML attacks on CAVs and\n",
      "outline a solution to defend against adversarial attacks in multiple settings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.09733 \n",
      "Title :Fatigue-aware Bandits for Dependent Click Models\n",
      "  As recommender systems send a massive amount of content to keep users\n",
      "engaged, users may experience fatigue which is contributed by 1) an\n",
      "overexposure to irrelevant content, 2) boredom from seeing too many similar\n",
      "recommendations. To address this problem, we consider an online learning\n",
      "setting where a platform learns a policy to recommend content that takes user\n",
      "fatigue into account. We propose an extension of the Dependent Click Model\n",
      "(DCM) to describe users' behavior. We stipulate that for each piece of content,\n",
      "its attractiveness to a user depends on its intrinsic relevance and a discount\n",
      "factor which measures how many similar contents have been shown. Users view the\n",
      "recommended content sequentially and click on the ones that they find\n",
      "attractive. Users may leave the platform at any time, and the probability of\n",
      "exiting is higher when they do not like the content. Based on user's feedback,\n",
      "the platform learns the relevance of the underlying content as well as the\n",
      "discounting effect due to content fatigue. We refer to this learning task as\n",
      "\"fatigue-aware DCM Bandit\" problem. We consider two learning scenarios\n",
      "depending on whether the discounting effect is known. For each scenario, we\n",
      "propose a learning algorithm which simultaneously explores and exploits, and\n",
      "characterize its regret bound.\n",
      "\n",
      "**Paper Id :2007.02334 \n",
      "Title :Multi-Manifold Learning for Large-scale Targeted Advertising System\n",
      "  Messenger advertisements (ads) give direct and personal user experience\n",
      "yielding high conversion rates and sales. However, people are skeptical about\n",
      "ads and sometimes perceive them as spam, which eventually leads to a decrease\n",
      "in user satisfaction. Targeted advertising, which serves ads to individuals who\n",
      "may exhibit interest in a particular advertising message, is strongly required.\n",
      "The key to the success of precise user targeting lies in learning the accurate\n",
      "user and ad representation in the embedding space. Most of the previous studies\n",
      "have limited the representation learning in the Euclidean space, but recent\n",
      "studies have suggested hyperbolic manifold learning for the distinct projection\n",
      "of complex network properties emerging from real-world datasets such as social\n",
      "networks, recommender systems, and advertising. We propose a framework that can\n",
      "effectively learn the hierarchical structure in users and ads on the hyperbolic\n",
      "space, and extend to the Multi-Manifold Learning. Our method constructs\n",
      "multiple hyperbolic manifolds with learnable curvatures and maps the\n",
      "representation of user and ad to each manifold. The origin of each manifold is\n",
      "set as the centroid of each user cluster. The user preference for each ad is\n",
      "estimated using the distance between two entities in the hyperbolic space, and\n",
      "the final prediction is determined by aggregating the values calculated from\n",
      "the learned multiple manifolds. We evaluate our method on public benchmark\n",
      "datasets and a large-scale commercial messenger system LINE, and demonstrate\n",
      "its effectiveness through improved performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.09983 \n",
      "Title :Leveraging Organizational Resources to Adapt Models to New Data\n",
      "  Modalities\n",
      "  As applications in large organizations evolve, the machine learning (ML)\n",
      "models that power them must adapt the same predictive tasks to newly arising\n",
      "data modalities (e.g., a new video content launch in a social media application\n",
      "requires existing text or image models to extend to video). To solve this\n",
      "problem, organizations typically create ML pipelines from scratch. However,\n",
      "this fails to utilize the domain expertise and data they have cultivated from\n",
      "developing tasks for existing modalities. We demonstrate how organizational\n",
      "resources, in the form of aggregate statistics, knowledge bases, and existing\n",
      "services that operate over related tasks, enable teams to construct a common\n",
      "feature space that connects new and existing data modalities. This allows teams\n",
      "to apply methods for training data curation (e.g., weak supervision and label\n",
      "propagation) and model training (e.g., forms of multi-modal learning) across\n",
      "these different data modalities. We study how this use of organizational\n",
      "resources composes at production scale in over 5 classification tasks at\n",
      "Google, and demonstrate how it reduces the time needed to develop models for\n",
      "new modalities from months to weeks to days.\n",
      "\n",
      "**Paper Id :2009.10259 \n",
      "Title :ALICE: Active Learning with Contrastive Natural Language Explanations\n",
      "  Training a supervised neural network classifier typically requires many\n",
      "annotated training samples. Collecting and annotating a large number of data\n",
      "points are costly and sometimes even infeasible. Traditional annotation process\n",
      "uses a low-bandwidth human-machine communication interface: classification\n",
      "labels, each of which only provides several bits of information. We propose\n",
      "Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\n",
      "training framework that utilizes contrastive natural language explanations to\n",
      "improve data efficiency in learning. ALICE learns to first use active learning\n",
      "to select the most informative pairs of label classes to elicit contrastive\n",
      "natural language explanations from experts. Then it extracts knowledge from\n",
      "these explanations using a semantic parser. Finally, it incorporates the\n",
      "extracted knowledge through dynamically changing the learning model's\n",
      "structure. We applied ALICE in two visual recognition tasks, bird species\n",
      "classification and social relationship classification. We found by\n",
      "incorporating contrastive explanations, our models outperform baseline models\n",
      "that are trained with 40-100% more training data. We found that adding 1\n",
      "explanation leads to similar performance gain as adding 13-30 labeled training\n",
      "data points.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.09994 \n",
      "Title :Discriminative Residual Analysis for Image Set Classification with\n",
      "  Posture and Age Variations\n",
      "  Image set recognition has been widely applied in many practical problems like\n",
      "real-time video retrieval and image caption tasks. Due to its superior\n",
      "performance, it has grown into a significant topic in recent years. However,\n",
      "images with complicated variations, e.g., postures and human ages, are\n",
      "difficult to address, as these variations are continuous and gradual with\n",
      "respect to image appearance. Consequently, the crucial point of image set\n",
      "recognition is to mine the intrinsic connection or structural information from\n",
      "the image batches with variations. In this work, a Discriminant Residual\n",
      "Analysis (DRA) method is proposed to improve the classification performance by\n",
      "discovering discriminant features in related and unrelated groups.\n",
      "Specifically, DRA attempts to obtain a powerful projection which casts the\n",
      "residual representations into a discriminant subspace. Such a projection\n",
      "subspace is expected to magnify the useful information of the input space as\n",
      "much as possible, then the relation between the training set and the test set\n",
      "described by the given metric or distance will be more precise in the\n",
      "discriminant subspace. We also propose a nonfeasance strategy by defining\n",
      "another approach to construct the unrelated groups, which help to reduce\n",
      "furthermore the cost of sampling errors. Two regularization approaches are used\n",
      "to deal with the probable small sample size problem. Extensive experiments are\n",
      "conducted on benchmark databases, and the results show superiority and\n",
      "efficiency of the new methods.\n",
      "\n",
      "**Paper Id :2005.03227 \n",
      "Title :Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent\n",
      "  Multi-View Representation Learning\n",
      "  Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread\n",
      "rapidly across the world. Due to the large number of affected patients and\n",
      "heavy labor for doctors, computer-aided diagnosis with machine learning\n",
      "algorithm is urgently needed, and could largely reduce the efforts of\n",
      "clinicians and accelerate the diagnosis process. Chest computed tomography (CT)\n",
      "has been recognized as an informative tool for diagnosis of the disease. In\n",
      "this study, we propose to conduct the diagnosis of COVID-19 with a series of\n",
      "features extracted from CT images. To fully explore multiple features\n",
      "describing CT images from different views, a unified latent representation is\n",
      "learned which can completely encode information from different aspects of\n",
      "features and is endowed with promising class structure for separability.\n",
      "Specifically, the completeness is guaranteed with a group of backward neural\n",
      "networks (each for one type of features), while by using class labels the\n",
      "representation is enforced to be compact within COVID-19/community-acquired\n",
      "pneumonia (CAP) and also a large margin is guaranteed between different types\n",
      "of pneumonia. In this way, our model can well avoid overfitting compared to the\n",
      "case of directly projecting highdimensional features into classes. Extensive\n",
      "experimental results show that the proposed method outperforms all comparison\n",
      "methods, and rather stable performances are observed when varying the numbers\n",
      "of training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.11400 \n",
      "Title :Joint Modelling of Cyber Activities and Physical Context to Improve\n",
      "  Prediction of Visitor Behaviors\n",
      "  This paper investigates the Cyber-Physical behavior of users in a large\n",
      "indoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and\n",
      "browsing logs recorded by the mall operators. Our analysis shows that many\n",
      "users exhibit a high correlation between their cyber activities and their\n",
      "physical context. To find this correlation, we propose a mechanism to\n",
      "semantically label a physical space with rich categorical information from\n",
      "DBPedia concepts and compute a contextual similarity that represents a user's\n",
      "activities with the mall context. We demonstrate the application of\n",
      "cyber-physical contextual similarity in two situations: user visit intent\n",
      "classification and future location prediction. The experimental results\n",
      "demonstrate that exploitation of contextual similarity significantly improves\n",
      "the accuracy of such applications.\n",
      "\n",
      "**Paper Id :1909.04421 \n",
      "Title :Privacy-Preserving Bandits\n",
      "  Contextual bandit algorithms~(CBAs) often rely on personal data to provide\n",
      "recommendations. Centralized CBA agents utilize potentially sensitive data from\n",
      "recent interactions to provide personalization to end-users. Keeping the\n",
      "sensitive data locally, by running a local agent on the user's device, protects\n",
      "the user's privacy, however, the agent requires longer to produce useful\n",
      "recommendations, as it does not leverage feedback from other users. This paper\n",
      "proposes a technique we call Privacy-Preserving Bandits (P2B); a system that\n",
      "updates local agents by collecting feedback from other local agents in a\n",
      "differentially-private manner. Comparisons of our proposed approach with a\n",
      "non-private, as well as a fully-private (local) system, show competitive\n",
      "performance on both synthetic benchmarks and real-world data. Specifically, we\n",
      "observed only a decrease of 2.6% and 3.6% in multi-label classification\n",
      "accuracy, and a CTR increase of 0.0025 in online advertising for a privacy\n",
      "budget $\\epsilon \\approx 0.693$. These results suggest P2B is an effective\n",
      "approach to challenges arising in on-device privacy-preserving personalization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.11426 \n",
      "Title :Disentangled Adversarial Autoencoder for Subject-Invariant Physiological\n",
      "  Feature Extraction\n",
      "  Recent developments in biosignal processing have enabled users to exploit\n",
      "their physiological status for manipulating devices in a reliable and safe\n",
      "manner. One major challenge of physiological sensing lies in the variability of\n",
      "biosignals across different users and tasks. To address this issue, we propose\n",
      "an adversarial feature extractor for transfer learning to exploit disentangled\n",
      "universal representations. We consider the trade-off between task-relevant\n",
      "features and user-discriminative information by introducing additional\n",
      "adversary and nuisance networks in order to manipulate the latent\n",
      "representations such that the learned feature extractor is applicable to\n",
      "unknown users and various tasks. Results on cross-subject transfer evaluations\n",
      "exhibit the benefits of the proposed framework, with up to 8.8% improvement in\n",
      "average accuracy of classification, and demonstrate adaptability to a broader\n",
      "range of subjects.\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.11432 \n",
      "Title :Time-Aware Music Recommender Systems: Modeling the Evolution of Implicit\n",
      "  User Preferences and User Listening Habits in A Collaborative Filtering\n",
      "  Approach\n",
      "  Online streaming services have become the most popular way of listening to\n",
      "music. The majority of these services are endowed with recommendation\n",
      "mechanisms that help users to discover songs and artists that may interest them\n",
      "from the vast amount of music available. However, many are not reliable as they\n",
      "may not take into account contextual aspects or the ever-evolving user\n",
      "behavior. Therefore, it is necessary to develop systems that consider these\n",
      "aspects. In the field of music, time is one of the most important factors\n",
      "influencing user preferences and managing its effects, and is the motivation\n",
      "behind the work presented in this paper. Here, the temporal information\n",
      "regarding when songs are played is examined. The purpose is to model both the\n",
      "evolution of user preferences in the form of evolving implicit ratings and user\n",
      "listening behavior. In the collaborative filtering method proposed in this\n",
      "work, daily listening habits are captured in order to characterize users and\n",
      "provide them with more reliable recommendations. The results of the validation\n",
      "prove that this approach outperforms other methods in generating both\n",
      "context-aware and context-free recommendations\n",
      "\n",
      "**Paper Id :2004.12733 \n",
      "Title :Personalized Recommendation of PoIs to People with Autism\n",
      "  The suggestion of Points of Interest to people with Autism Spectrum Disorder\n",
      "(ASD) challenges recommender systems research because these users' perception\n",
      "of places is influenced by idiosyncratic sensory aversions which can mine their\n",
      "experience by causing stress and anxiety. Therefore, managing individual\n",
      "preferences is not enough to provide these people with suitable\n",
      "recommendations. In order to address this issue, we propose a Top-N\n",
      "recommendation model that combines the user's idiosyncratic aversions with\n",
      "her/his preferences in a personalized way to suggest the most compatible and\n",
      "likable Points of Interest for her/him. We are interested in finding a\n",
      "user-specific balance of compatibility and interest within a recommendation\n",
      "model that integrates heterogeneous evaluation criteria to appropriately take\n",
      "these aspects into account. We tested our model on both ASD and \"neurotypical\"\n",
      "people. The evaluation results show that, on both groups, our model outperforms\n",
      "in accuracy and ranking capability the recommender systems based on item\n",
      "compatibility, on user preferences, or which integrate these two aspects by\n",
      "means of a uniform evaluation model.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.11450 \n",
      "Title :Training Multimodal Systems for Classification with Multiple Objectives\n",
      "  We learn about the world from a diverse range of sensory information.\n",
      "Automated systems lack this ability as investigation has centred on processing\n",
      "information presented in a single form. Adapting architectures to learn from\n",
      "multiple modalities creates the potential to learn rich representations of the\n",
      "world - but current multimodal systems only deliver marginal improvements on\n",
      "unimodal approaches. Neural networks learn sampling noise during training with\n",
      "the result that performance on unseen data is degraded. This research\n",
      "introduces a second objective over the multimodal fusion process learned with\n",
      "variational inference. Regularisation methods are implemented in the inner\n",
      "training loop to control variance and the modular structure stabilises\n",
      "performance as additional neurons are added to layers. This framework is\n",
      "evaluated on a multilabel classification task with textual and visual inputs to\n",
      "demonstrate the potential for multiple objectives and probabilistic methods to\n",
      "lower variance and improve generalisation.\n",
      "\n",
      "**Paper Id :2004.06874 \n",
      "Title :Understanding Aesthetic Evaluation using Deep Learning\n",
      "  A bottleneck in any evolutionary art system is aesthetic evaluation. Many\n",
      "different methods have been proposed to automate the evaluation of aesthetics,\n",
      "including measures of symmetry, coherence, complexity, contrast and grouping.\n",
      "The interactive genetic algorithm (IGA) relies on human-in-the-loop, subjective\n",
      "evaluation of aesthetics, but limits possibilities for large search due to user\n",
      "fatigue and small population sizes. In this paper we look at how recent\n",
      "advances in deep learning can assist in automating personal aesthetic\n",
      "judgement. Using a leading artist's computer art dataset, we use dimensionality\n",
      "reduction methods to visualise both genotype and phenotype space in order to\n",
      "support the exploration of new territory in any generative system.\n",
      "Convolutional Neural Networks trained on the user's prior aesthetic evaluations\n",
      "are used to suggest new possibilities similar or between known high quality\n",
      "genotype-phenotype mappings.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.11752 \n",
      "Title :Appropriateness of Performance Indices for Imbalanced Data\n",
      "  Classification: An Analysis\n",
      "  Indices quantifying the performance of classifiers under class-imbalance,\n",
      "often suffer from distortions depending on the constitution of the test set or\n",
      "the class-specific classification accuracy, creating difficulties in assessing\n",
      "the merit of the classifier. We identify two fundamental conditions that a\n",
      "performance index must satisfy to be respectively resilient to altering number\n",
      "of testing instances from each class and the number of classes in the test set.\n",
      "In light of these conditions, under the effect of class imbalance, we\n",
      "theoretically analyze four indices commonly used for evaluating binary\n",
      "classifiers and five popular indices for multi-class classifiers. For indices\n",
      "violating any of the conditions, we also suggest remedial modification and\n",
      "normalization. We further investigate the capability of the indices to retain\n",
      "information about the classification performance over all the classes, even\n",
      "when the classifier exhibits extreme performance on some classes. Simulation\n",
      "studies are performed on high dimensional deep representations of subset of the\n",
      "ImageNet dataset using four state-of-the-art classifiers tailored for handling\n",
      "class imbalance. Finally, based on our theoretical findings and empirical\n",
      "evidence, we recommend the appropriate indices that should be used to evaluate\n",
      "the performance of classifiers in presence of class-imbalance.\n",
      "\n",
      "**Paper Id :1802.02558 \n",
      "Title :Intentional Control of Type I Error over Unconscious Data Distortion: a\n",
      "  Neyman-Pearson Approach to Text Classification\n",
      "  This paper addresses the challenges in classifying textual data obtained from\n",
      "open online platforms, which are vulnerable to distortion. Most existing\n",
      "classification methods minimize the overall classification error and may yield\n",
      "an undesirably large type I error (relevant textual messages are classified as\n",
      "irrelevant), particularly when available data exhibit an asymmetry between\n",
      "relevant and irrelevant information. Data distortion exacerbates this situation\n",
      "and often leads to fallacious prediction. To deal with inestimable data\n",
      "distortion, we propose the use of the Neyman-Pearson (NP) classification\n",
      "paradigm, which minimizes type II error under a user-specified type I error\n",
      "constraint. Theoretically, we show that the NP oracle is unaffected by data\n",
      "distortion when the class conditional distributions remain the same.\n",
      "Empirically, we study a case of classifying posts about worker strikes obtained\n",
      "from a leading Chinese microblogging platform, which are frequently prone to\n",
      "extensive, unpredictable and inestimable censorship. We demonstrate that, even\n",
      "though the training and test data are susceptible to different distortion and\n",
      "therefore potentially follow different distributions, our proposed NP methods\n",
      "control the type I error on test data at the targeted level. The methods and\n",
      "implementation pipeline proposed in our case study are applicable to many other\n",
      "problems involving data distortion.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2008.12080 \n",
      "Title :Identifying and Tracking Solar Magnetic Flux Elements with Deep Learning\n",
      "  Deep learning has drawn a lot of interest in recent years due to its\n",
      "effectiveness in processing big and complex observational data gathered from\n",
      "diverse instruments. Here we propose a new deep learning method, called\n",
      "SolarUnet, to identify and track solar magnetic flux elements or features in\n",
      "observed vector magnetograms based on the Southwest Automatic Magnetic\n",
      "Identification Suite (SWAMIS). Our method consists of a data pre-processing\n",
      "component that prepares training data from the SWAMIS tool, a deep learning\n",
      "model implemented as a U-shaped convolutional neural network for fast and\n",
      "accurate image segmentation, and a post-processing component that prepares\n",
      "tracking results. SolarUnet is applied to data from the 1.6 meter Goode Solar\n",
      "Telescope at the Big Bear Solar Observatory. When compared to the widely used\n",
      "SWAMIS tool, SolarUnet is faster while agreeing mostly with SWAMIS on feature\n",
      "size and flux distributions, and complementing SWAMIS in tracking long-lifetime\n",
      "features. Thus, the proposed physics-guided deep learning-based tool can be\n",
      "considered as an alternative method for solar magnetic tracking.\n",
      "\n",
      "**Paper Id :2005.03945 \n",
      "Title :Inferring Vector Magnetic Fields from Stokes Profiles of GST/NIRIS Using\n",
      "  a Convolutional Neural Network\n",
      "  We propose a new machine learning approach to Stokes inversion based on a\n",
      "convolutional neural network (CNN) and the Milne-Eddington (ME) method. The\n",
      "Stokes measurements used in this study were taken by the Near InfraRed Imaging\n",
      "Spectropolarimeter (NIRIS) on the 1.6 m Goode Solar Telescope (GST) at the Big\n",
      "Bear Solar Observatory. By learning the latent patterns in the training data\n",
      "prepared by the physics-based ME tool, the proposed CNN method is able to infer\n",
      "vector magnetic fields from the Stokes profiles of GST/NIRIS. Experimental\n",
      "results show that our CNN method produces smoother and cleaner magnetic maps\n",
      "than the widely used ME method. Furthermore, the CNN method is 4~6 times faster\n",
      "than the ME method, and is able to produce vector magnetic fields in near\n",
      "real-time, which is essential to space weather forecasting. Specifically, it\n",
      "takes ~50 seconds for the CNN method to process an image of 720 x 720 pixels\n",
      "comprising Stokes profiles of GST/NIRIS. Finally, the CNN-inferred results are\n",
      "highly correlated to the ME-calculated results and are closer to the ME's\n",
      "results with the Pearson product-moment correlation coefficient (PPMCC) being\n",
      "closer to 1 on average than those from other machine learning algorithms such\n",
      "as multiple support vector regression and multilayer perceptrons (MLP). In\n",
      "particular, the CNN method outperforms the current best machine learning method\n",
      "(MLP) by 2.6% on average in PPMCC according to our experimental study. Thus,\n",
      "the proposed physics-assisted deep learning-based CNN tool can be considered as\n",
      "an alternative, efficient method for Stokes inversion for high resolution\n",
      "polarimetric observations obtained by GST/NIRIS.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.00611 \n",
      "Title :Identifying Documents In-Scope of a Collection from Web Archives\n",
      "  Web archive data usually contains high-quality documents that are very useful\n",
      "for creating specialized collections of documents, e.g., scientific digital\n",
      "libraries and repositories of technical reports. In doing so, there is a\n",
      "substantial need for automatic approaches that can distinguish the documents of\n",
      "interest for a collection out of the huge number of documents collected by web\n",
      "archiving institutions. In this paper, we explore different learning models and\n",
      "feature representations to determine the best performing ones for identifying\n",
      "the documents of interest from the web archived data. Specifically, we study\n",
      "both machine learning and deep learning models and \"bag of words\" (BoW)\n",
      "features extracted from the entire document or from specific portions of the\n",
      "document, as well as structural features that capture the structure of\n",
      "documents. We focus our evaluation on three datasets that we created from three\n",
      "different Web archives. Our experimental results show that the BoW classifiers\n",
      "that focus only on specific portions of the documents (rather than the full\n",
      "text) outperform all compared methods on all three datasets.\n",
      "\n",
      "**Paper Id :2007.06390 \n",
      "Title :A Feature Analysis for Multimodal News Retrieval\n",
      "  Content-based information retrieval is based on the information contained in\n",
      "documents rather than using metadata such as keywords. Most information\n",
      "retrieval methods are either based on text or image. In this paper, we\n",
      "investigate the usefulness of multimodal features for cross-lingual news search\n",
      "in various domains: politics, health, environment, sport, and finance. To this\n",
      "end, we consider five feature types for image and text and compare the\n",
      "performance of the retrieval system using different combinations. Experimental\n",
      "results show that retrieval results can be improved when considering both\n",
      "visual and textual information. In addition, it is observed that among textual\n",
      "features entity overlap outperforms word embeddings, while geolocation\n",
      "embeddings achieve better performance among visual features in the retrieval\n",
      "task.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.00666 \n",
      "Title :Robust, Accurate Stochastic Optimization for Variational Inference\n",
      "  We consider the problem of fitting variational posterior approximations using\n",
      "stochastic optimization methods. The performance of these approximations\n",
      "depends on (1) how well the variational family matches the true posterior\n",
      "distribution,(2) the choice of divergence, and (3) the optimization of the\n",
      "variational objective. We show that even in the best-case scenario when the\n",
      "exact posterior belongs to the assumed variational family, common stochastic\n",
      "optimization methods lead to poor variational approximations if the problem\n",
      "dimension is moderately large. We also demonstrate that these methods are not\n",
      "robust across diverse model types. Motivated by these findings, we develop a\n",
      "more robust and accurate stochastic optimization framework by viewing the\n",
      "underlying optimization algorithm as producing a Markov chain. Our approach is\n",
      "theoretically motivated and includes a diagnostic for convergence and a novel\n",
      "stopping rule, both of which are robust to noisy evaluations of the objective\n",
      "function. We show empirically that the proposed framework works well on a\n",
      "diverse set of models: it can automatically detect stochastic optimization\n",
      "failure or inaccurate variational approximation\n",
      "\n",
      "**Paper Id :2002.07217 \n",
      "Title :Decision-Making with Auto-Encoding Variational Bayes\n",
      "  To make decisions based on a model fit with auto-encoding variational Bayes\n",
      "(AEVB), practitioners often let the variational distribution serve as a\n",
      "surrogate for the posterior distribution. This approach yields biased estimates\n",
      "of the expected risk, and therefore leads to poor decisions for two reasons.\n",
      "First, the model fit with AEVB may not equal the underlying data distribution.\n",
      "Second, the variational distribution may not equal the posterior distribution\n",
      "under the fitted model. We explore how fitting the variational distribution\n",
      "based on several objective functions other than the ELBO, while continuing to\n",
      "fit the generative model based on the ELBO, affects the quality of downstream\n",
      "decisions. For the probabilistic principal component analysis model, we\n",
      "investigate how importance sampling error, as well as the bias of the model\n",
      "parameter estimates, varies across several approximate posteriors when used as\n",
      "proposal distributions. Our theoretical results suggest that a posterior\n",
      "approximation distinct from the variational distribution should be used for\n",
      "making decisions. Motivated by these theoretical results, we propose learning\n",
      "several approximate proposals for the best model and combining them using\n",
      "multiple importance sampling for decision-making. In addition to toy examples,\n",
      "we present a full-fledged case study of single-cell RNA sequencing. In this\n",
      "challenging instance of multiple hypothesis testing, our proposed approach\n",
      "surpasses the current state of the art.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.00804 \n",
      "Title :Architectural Implications of Graph Neural Networks\n",
      "  Graph neural networks (GNN) represent an emerging line of deep learning\n",
      "models that operate on graph structures. It is becoming more and more popular\n",
      "due to its high accuracy achieved in many graph-related tasks. However, GNN is\n",
      "not as well understood in the system and architecture community as its\n",
      "counterparts such as multi-layer perceptrons and convolutional neural networks.\n",
      "This work tries to introduce the GNN to our community. In contrast to prior\n",
      "work that only presents characterizations of GCNs, our work covers a large\n",
      "portion of the varieties for GNN workloads based on a general GNN description\n",
      "framework. By constructing the models on top of two widely-used libraries, we\n",
      "characterize the GNN computation at inference stage concerning general-purpose\n",
      "and application-specific architectures and hope our work can foster more system\n",
      "and architecture research for GNNs.\n",
      "\n",
      "**Paper Id :1902.02181 \n",
      "Title :Attention in Natural Language Processing\n",
      "  Attention is an increasingly popular mechanism used in a wide range of neural\n",
      "architectures. The mechanism itself has been realized in a variety of formats.\n",
      "However, because of the fast-paced advances in this domain, a systematic\n",
      "overview of attention is still missing. In this article, we define a unified\n",
      "model for attention architectures in natural language processing, with a focus\n",
      "on those designed to work with vector representations of the textual data. We\n",
      "propose a taxonomy of attention models according to four dimensions: the\n",
      "representation of the input, the compatibility function, the distribution\n",
      "function, and the multiplicity of the input and/or output. We present the\n",
      "examples of how prior information can be exploited in attention models and\n",
      "discuss ongoing research efforts and open challenges in the area, providing the\n",
      "first extensive categorization of the vast body of literature in this exciting\n",
      "domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.01987 \n",
      "Title :A Hybrid Deep Learning Model for Arabic Text Recognition\n",
      "  Arabic text recognition is a challenging task because of the cursive nature\n",
      "of Arabic writing system, its joint writing scheme, the large number of\n",
      "ligatures and many other challenges. Deep Learning DL models achieved\n",
      "significant progress in numerous domains including computer vision and sequence\n",
      "modelling. This paper presents a model that can recognize Arabic text that was\n",
      "printed using multiple font types including fonts that mimic Arabic handwritten\n",
      "scripts. The proposed model employs a hybrid DL network that can recognize\n",
      "Arabic printed text without the need for character segmentation. The model was\n",
      "tested on a custom dataset comprised of over two million word samples that were\n",
      "generated using 18 different Arabic font types. The objective of the testing\n",
      "process was to assess the model capability in recognizing a diverse set of\n",
      "Arabic fonts representing a varied cursive styles. The model achieved good\n",
      "results in recognizing characters and words and it also achieved promising\n",
      "results in recognizing characters when it was tested on unseen data. The\n",
      "prepared model, the custom datasets and the toolkit for generating similar\n",
      "datasets are made publicly available, these tools can be used to prepare models\n",
      "for recognizing other font types as well as to further extend and enhance the\n",
      "performance of the proposed model.\n",
      "\n",
      "**Paper Id :2008.06376 \n",
      "Title :MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages\n",
      "  and Modalities\n",
      "  In this paper, we introduce the MLM (Multiple Languages and Modalities)\n",
      "dataset - a new resource to train and evaluate multitask systems on samples in\n",
      "multiple modalities and three languages. The generation process and inclusion\n",
      "of semantic data provide a resource that further tests the ability for\n",
      "multitask systems to learn relationships between entities. The dataset is\n",
      "designed for researchers and developers who build applications that perform\n",
      "multiple tasks on data encountered on the web and in digital archives. A second\n",
      "version of MLM provides a geo-representative subset of the data with weighted\n",
      "samples for countries of the European Union. We demonstrate the value of the\n",
      "resource in developing novel applications in the digital humanities with a\n",
      "motivating use case and specify a benchmark set of tasks to retrieve modalities\n",
      "and locate entities in the dataset. Evaluation of baseline multitask and single\n",
      "task systems on the full and geo-representative versions of MLM demonstrate the\n",
      "challenges of generalising on diverse data. In addition to the digital\n",
      "humanities, we expect the resource to contribute to research in multimodal\n",
      "representation learning, location estimation, and scene understanding.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.02256 \n",
      "Title :Interactive Visual Study of Multiple Attributes Learning Model of X-Ray\n",
      "  Scattering Images\n",
      "  Existing interactive visualization tools for deep learning are mostly applied\n",
      "to the training, debugging, and refinement of neural network models working on\n",
      "natural images. However, visual analytics tools are lacking for the specific\n",
      "application of x-ray image classification with multiple structural attributes.\n",
      "In this paper, we present an interactive system for domain scientists to\n",
      "visually study the multiple attributes learning models applied to x-ray\n",
      "scattering images. It allows domain scientists to interactively explore this\n",
      "important type of scientific images in embedded spaces that are defined on the\n",
      "model prediction output, the actual labels, and the discovered feature space of\n",
      "neural networks. Users are allowed to flexibly select instance images, their\n",
      "clusters, and compare them regarding the specified visual representation of\n",
      "attributes. The exploration is guided by the manifestation of model performance\n",
      "related to mutual relationships among attributes, which often affect the\n",
      "learning accuracy and effectiveness. The system thus supports domain scientists\n",
      "to improve the training dataset and model, find questionable attributes labels,\n",
      "and identify outlier images or spurious data clusters. Case studies and\n",
      "scientists feedback demonstrate its functionalities and usefulness.\n",
      "\n",
      "**Paper Id :1910.09477 \n",
      "Title :Toward automatic comparison of visualization techniques: Application to\n",
      "  graph visualization\n",
      "  Many end-user evaluations of data visualization techniques have been run\n",
      "during the last decades. Their results are cornerstones to build efficient\n",
      "visualization systems. However, designing such an evaluation is always complex\n",
      "and time-consuming and may end in a lack of statistical evidence and\n",
      "reproducibility. We believe that modern and efficient computer vision\n",
      "techniques, such as deep convolutional neural networks (CNNs), may help\n",
      "visualization researchers to build and/or adjust their evaluation hypothesis.\n",
      "The basis of our idea is to train machine learning models on several\n",
      "visualization techniques to solve a specific task. Our assumption is that it is\n",
      "possible to compare the efficiency of visualization techniques based on the\n",
      "performance of their corresponding model. As current machine learning models\n",
      "are not able to strictly reflect human capabilities, including their\n",
      "imperfections, such results should be interpreted with caution. However, we\n",
      "think that using machine learning-based pre-evaluation, as a pre-process of\n",
      "standard user evaluations, should help researchers to perform a more exhaustive\n",
      "study of their design space. Thus, it should improve their final user\n",
      "evaluation by providing it better test cases. In this paper, we present the\n",
      "results of two experiments we have conducted to assess how correlated the\n",
      "performance of users and computer vision techniques can be. That study compares\n",
      "two mainstream graph visualization techniques: node-link (\\NL) and\n",
      "adjacency-matrix (\\MD) diagrams. Using two well-known deep convolutional neural\n",
      "networks, we partially reproduced user evaluations from Ghoniem \\textit{et al.}\n",
      "and from Okoe \\textit{et al.}. These experiments showed that some user\n",
      "evaluation results can be reproduced automatically.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.02267 \n",
      "Title :Zero-Bias Deep Learning for Accurate Identification of Internet of\n",
      "  Things (IoT) Devices\n",
      "  The Internet of Things (IoT) provides applications and services that would\n",
      "otherwise not be possible. However, the open nature of IoT make it vulnerable\n",
      "to cybersecurity threats. Especially, identity spoofing attacks, where an\n",
      "adversary passively listens to existing radio communications and then mimic the\n",
      "identity of legitimate devices to conduct malicious activities. Existing\n",
      "solutions employ cryptographic signatures to verify the trustworthiness of\n",
      "received information. In prevalent IoT, secret keys for cryptography can\n",
      "potentially be disclosed and disable the verification mechanism.\n",
      "Non-cryptographic device verification is needed to ensure trustworthy IoT. In\n",
      "this paper, we propose an enhanced deep learning framework for IoT device\n",
      "identification using physical layer signals. Specifically, we enable our\n",
      "framework to report unseen IoT devices and introduce the zero-bias layer to\n",
      "deep neural networks to increase robustness and interpretability. We have\n",
      "evaluated the effectiveness of the proposed framework using real data from\n",
      "ADS-B (Automatic Dependent Surveillance-Broadcast), an application of IoT in\n",
      "aviation. The proposed framework has the potential to be applied to accurate\n",
      "identification of IoT devices in a variety of IoT applications and services.\n",
      "Codes and data are available in IEEE Dataport.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.02708 \n",
      "Title :Deep Learning for the Analysis of Disruption Precursors based on Plasma\n",
      "  Tomography\n",
      "  The JET baseline scenario is being developed to achieve high fusion\n",
      "performance and sustained fusion power. However, with higher plasma current and\n",
      "higher input power, an increase in pulse disruptivity is being observed.\n",
      "Although there is a wide range of possible disruption causes, the present\n",
      "disruptions seem to be closely related to radiative phenomena such as impurity\n",
      "accumulation, core radiation, and radiative collapse. In this work, we focus on\n",
      "bolometer tomography to reconstruct the plasma radiation profile and, on top of\n",
      "it, we apply anomaly detection to identify the radiation patterns that precede\n",
      "major disruptions. The approach makes extensive use of machine learning. First,\n",
      "we train a surrogate model for plasma tomography based on matrix\n",
      "multiplication, which provides a fast method to compute the plasma radiation\n",
      "profiles across the full extent of any given pulse. Then, we train a\n",
      "variational autoencoder to reproduce the radiation profiles by encoding them\n",
      "into a latent distribution and subsequently decoding them. As an anomaly\n",
      "detector, the variational autoencoder struggles to reproduce unusual behaviors,\n",
      "which includes not only the actual disruptions but their precursors as well.\n",
      "These precursors are identified based on an analysis of the anomaly score\n",
      "across all baseline pulses in two recent campaigns at JET.\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.02931 \n",
      "Title :Team Alex at CLEF CheckThat! 2020: Identifying Check-Worthy Tweets With\n",
      "  Transformer Models\n",
      "  While misinformation and disinformation have been thriving in social media\n",
      "for years, with the emergence of the COVID-19 pandemic, the political and the\n",
      "health misinformation merged, thus elevating the problem to a whole new level\n",
      "and giving rise to the first global infodemic. The fight against this infodemic\n",
      "has many aspects, with fact-checking and debunking false and misleading claims\n",
      "being among the most important ones. Unfortunately, manual fact-checking is\n",
      "time-consuming and automatic fact-checking is resource-intense, which means\n",
      "that we need to pre-filter the input social media posts and to throw out those\n",
      "that do not appear to be check-worthy. With this in mind, here we propose a\n",
      "model for detecting check-worthy tweets about COVID-19, which combines deep\n",
      "contextualized text representations with modeling the social context of the\n",
      "tweet. We further describe a number of additional experiments and comparisons,\n",
      "which we believe should be useful for future research as they provide some\n",
      "indication about what techniques are effective for the task. Our official\n",
      "submission to the English version of CLEF-2020 CheckThat! Task 1, system\n",
      "Team_Alex, was ranked second with a MAP score of 0.8034, which is almost tied\n",
      "with the wining system, lagging behind by just 0.003 MAP points absolute.\n",
      "\n",
      "**Paper Id :2005.06058 \n",
      "Title :That is a Known Lie: Detecting Previously Fact-Checked Claims\n",
      "  The recent proliferation of \"fake news\" has triggered a number of responses,\n",
      "most notably the emergence of several manual fact-checking initiatives. As a\n",
      "result and over time, a large number of fact-checked claims have been\n",
      "accumulated, which increases the likelihood that a new claim in social media or\n",
      "a new statement by a politician might have already been fact-checked by some\n",
      "trusted fact-checking organization, as viral claims often come back after a\n",
      "while in social media, and politicians like to repeat their favorite\n",
      "statements, true or false, over and over again. As manual fact-checking is very\n",
      "time-consuming (and fully automatic fact-checking has credibility issues), it\n",
      "is important to try to save this effort and to avoid wasting time on claims\n",
      "that have already been fact-checked. Interestingly, despite the importance of\n",
      "the task, it has been largely ignored by the research community so far. Here,\n",
      "we aim to bridge this gap. In particular, we formulate the task and we discuss\n",
      "how it relates to, but also differs from, previous work. We further create a\n",
      "specialized dataset, which we release to the research community. Finally, we\n",
      "present learning-to-rank experiments that demonstrate sizable improvements over\n",
      "state-of-the-art retrieval and textual similarity approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.03219 \n",
      "Title :Active deep learning method for the discovery of objects of interest in\n",
      "  large spectroscopic surveys\n",
      "  Current archives of the LAMOST telescope contain millions of\n",
      "pipeline-processed spectra that have probably never been seen by human eyes.\n",
      "Most of the rare objects with interesting physical properties, however, can\n",
      "only be identified by visual analysis of their characteristic spectral\n",
      "features. A proper combination of interactive visualisation with modern machine\n",
      "learning techniques opens new ways to discover such objects. We apply active\n",
      "learning classification supported by deep convolutional networks to\n",
      "automatically identify complex emission-line shapes in multi-million spectra\n",
      "archives.\n",
      "  We used the pool-based uncertainty sampling active learning driven by a\n",
      "custom-designed deep convolutional neural network with 12 layers inspired by\n",
      "VGGNet, AlexNet, and ZFNet, but adapted for one-dimensional feature vectors.\n",
      "The unlabelled pool set is represented by 4.1 million spectra from the LAMOST\n",
      "DR2 survey. The initial training of the network was performed on a labelled set\n",
      "of about 13000 spectra obtained in the region around H$\\alpha$ by the 2m Perek\n",
      "telescope of the Ond\\v{r}ejov observatory, which mostly contains spectra of Be\n",
      "and related early-type stars. The differences between the Ond\\v{r}ejov\n",
      "intermediate-resolution and the LAMOST low-resolution spectrographs were\n",
      "compensated for by Gaussian blurring.\n",
      "  After several iterations, the network was able to successfully identify\n",
      "emission-line stars with an error smaller than 6.5%. Using the technology of\n",
      "the Virtual Observatory to visualise the results, we discovered 1013 spectra of\n",
      "948 new candidates of emission-line objects in addition to 664 spectra of 549\n",
      "objects that are listed in SIMBAD and 2644 spectra of 2291 objects identified\n",
      "in an earlier paper of a Chinese group led by Wen Hou. The most interesting\n",
      "objects with unusual spectral properties are discussed in detail.\n",
      "\n",
      "**Paper Id :2005.00220 \n",
      "Title :Automatic Catalog of RRLyrae from $\\sim$ 14 million VVV Light Curves:\n",
      "  How far can we go with traditional machine-learning?\n",
      "  The creation of a 3D map of the bulge using RRLyrae (RRL) is one of the main\n",
      "goals of the VVV(X) surveys. The overwhelming number of sources under analysis\n",
      "request the use of automatic procedures. In this context, previous works\n",
      "introduced the use of Machine Learning (ML) methods for the variable star\n",
      "classification. Our goal is the development and analysis of an automatic\n",
      "procedure, based on ML, for the identification of RRLs in the VVV Survey. This\n",
      "procedure will be use to generate reliable catalogs integrated over several\n",
      "tiles in the survey. After the reconstruction of light-curves, we extract a set\n",
      "of period and intensity-based features. We use for the first time a new subset\n",
      "of pseudo color features. We discuss all the appropriate steps needed to define\n",
      "our automatic pipeline: selection of quality measures; sampling procedures;\n",
      "classifier setup and model selection. As final result, we construct an ensemble\n",
      "classifier with an average Recall of 0.48 and average Precision of 0.86 over 15\n",
      "tiles. We also make available our processed datasets and a catalog of candidate\n",
      "RRLs. Perhaps most interestingly, from a classification perspective based on\n",
      "photometric broad-band data, is that our results indicate that Color is an\n",
      "informative feature type of the RRL that should be considered for automatic\n",
      "classification methods via ML. We also argue that Recall and Precision in both\n",
      "tables and curves are high quality metrics for this highly imbalanced problem.\n",
      "Furthermore, we show for our VVV data-set that to have good estimates it is\n",
      "important to use the original distribution more than reduced samples with an\n",
      "artificial balance. Finally, we show that the use of ensemble classifiers helps\n",
      "resolve the crucial model selection step, and that most errors in the\n",
      "identification of RRLs are related to low quality observations of some sources\n",
      "or to the difficulty to resolve the RRL-C type given the date.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.03323 \n",
      "Title :Modeling protoplanetary disk SEDs with artificial neural networks:\n",
      "  Revisiting the viscous disk model and updated disk masses\n",
      "  We model the spectral energy distributions (SEDs) of 23 protoplanetary disks\n",
      "in the Taurus-Auriga star-forming region using detailed disk models and a\n",
      "Bayesian approach. This is made possible by combining these models with\n",
      "artificial neural networks to drastically speed up their performance. Such a\n",
      "setup allows us to confront $\\alpha$-disk models with observations while\n",
      "accounting for several uncertainties and degeneracies. Our results yield high\n",
      "viscosities and accretion rates for many sources, which is not consistent with\n",
      "recent measurements of low turbulence levels in disks. This inconsistency could\n",
      "imply that viscosity is not the main mechanism for angular momentum transport\n",
      "in disks, and that alternatives such as disk winds play an important role in\n",
      "this process. We also find that our SED-derived disk masses are systematically\n",
      "higher than those obtained solely from (sub)mm fluxes, suggesting that part of\n",
      "the disk emission could still be optically thick at (sub)mm wavelengths. This\n",
      "effect is particularly relevant for disk population studies and alleviates\n",
      "previous observational tensions between the masses of protoplanetary disks and\n",
      "exoplanetary systems.\n",
      "\n",
      "**Paper Id :2007.06585 \n",
      "Title :Gravitational-wave selection effects using neural-network classifiers\n",
      "  We present a novel machine-learning approach to estimate selection effects in\n",
      "gravitational-wave observations. Using techniques similar to those commonly\n",
      "employed in image classification and pattern recognition, we train a series of\n",
      "neural-network classifiers to predict the LIGO/Virgo detectability of\n",
      "gravitational-wave signals from compact-binary mergers. We include the effect\n",
      "of spin precession, higher-order modes, and multiple detectors and show that\n",
      "their omission, as it is common in large population studies, tends to\n",
      "overestimate the inferred merger rate in selected regions of the parameter\n",
      "space. Although here we train our classifiers using a simple signal-to-noise\n",
      "ratio threshold, our approach is ready to be used in conjunction with full\n",
      "pipeline injections, thus paving the way toward including actual distributions\n",
      "of astrophysical and noise triggers into gravitational-wave population\n",
      "analyses.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.03816 \n",
      "Title :PSO-PS: Parameter Synchronization with Particle Swarm Optimization for\n",
      "  Distributed Training of Deep Neural Networks\n",
      "  Parameter updating is an important stage in parallelism-based distributed\n",
      "deep learning. Synchronous methods are widely used in distributed training the\n",
      "Deep Neural Networks (DNNs). To reduce the communication and synchronization\n",
      "overhead of synchronous methods, decreasing the synchronization frequency\n",
      "(e.g., every $n$ mini-batches) is a straightforward approach. However, it often\n",
      "suffers from poor convergence. In this paper, we propose a new algorithm of\n",
      "integrating Particle Swarm Optimization (PSO) into the distributed training\n",
      "process of DNNs to automatically compute new parameters. In the proposed\n",
      "algorithm, a computing work is encoded by a particle, the weights of DNNs and\n",
      "the training loss are modeled by the particle attributes. At each\n",
      "synchronization stage, the weights are updated by PSO from the sub weights\n",
      "gathered from all workers, instead of averaging the weights or the gradients.\n",
      "To verify the performance of the proposed algorithm, the experiments are\n",
      "performed on two commonly used image classification benchmarks: MNIST and\n",
      "CIFAR10, and compared with the peer competitors at multiple different\n",
      "synchronization configurations. The experimental results demonstrate the\n",
      "competitiveness of the proposed algorithm.\n",
      "\n",
      "**Paper Id :1904.10900 \n",
      "Title :Learning big Gaussian Bayesian networks: partition, estimation, and\n",
      "  fusion\n",
      "  Structure learning of Bayesian networks has always been a challenging\n",
      "problem. Nowadays, massive-size networks with thousands or more of nodes but\n",
      "fewer samples frequently appear in many areas. We develop a divide-and-conquer\n",
      "framework, called partition-estimation-fusion (PEF), for structure learning of\n",
      "such big networks. The proposed method first partitions nodes into clusters,\n",
      "then learns a subgraph on each cluster of nodes, and finally fuses all learned\n",
      "subgraphs into one Bayesian network. The PEF method is designed in a flexible\n",
      "way so that any structure learning method may be used in the second step to\n",
      "learn a subgraph structure as either a DAG or a CPDAG. In the clustering step,\n",
      "we adapt the hierarchical clustering method to automatically choose a proper\n",
      "number of clusters. In the fusion step, we propose a novel hybrid method that\n",
      "sequentially add edges between subgraphs. Extensive numerical experiments\n",
      "demonstrate the competitive performance of our PEF method, in terms of both\n",
      "speed and accuracy compared to existing methods. Our method can improve the\n",
      "accuracy of structure learning by 20% or more, while reducing running time up\n",
      "to two orders-of-magnitude.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.04153 \n",
      "Title :One-shot Text Field Labeling using Attention and Belief Propagation for\n",
      "  Structure Information Extraction\n",
      "  Structured information extraction from document images usually consists of\n",
      "three steps: text detection, text recognition, and text field labeling. While\n",
      "text detection and text recognition have been heavily studied and improved a\n",
      "lot in literature, text field labeling is less explored and still faces many\n",
      "challenges. Existing learning based methods for text labeling task usually\n",
      "require a large amount of labeled examples to train a specific model for each\n",
      "type of document. However, collecting large amounts of document images and\n",
      "labeling them is difficult and sometimes impossible due to privacy issues.\n",
      "Deploying separate models for each type of document also consumes a lot of\n",
      "resources. Facing these challenges, we explore one-shot learning for the text\n",
      "field labeling task. Existing one-shot learning methods for the task are mostly\n",
      "rule-based and have difficulty in labeling fields in crowded regions with few\n",
      "landmarks and fields consisting of multiple separate text regions. To alleviate\n",
      "these problems, we proposed a novel deep end-to-end trainable approach for\n",
      "one-shot text field labeling, which makes use of attention mechanism to\n",
      "transfer the layout information between document images. We further applied\n",
      "conditional random field on the transferred layout information for the\n",
      "refinement of field labeling. We collected and annotated a real-world one-shot\n",
      "field labeling dataset with a large variety of document types and conducted\n",
      "extensive experiments to examine the effectiveness of the proposed model. To\n",
      "stimulate research in this direction, the collected dataset and the one-shot\n",
      "model will be released1.\n",
      "\n",
      "**Paper Id :2003.09831 \n",
      "Title :Prior Knowledge Driven Label Embedding for Slot Filling in Natural\n",
      "  Language Understanding\n",
      "  Traditional slot filling in natural language understanding (NLU) predicts a\n",
      "one-hot vector for each word. This form of label representation lacks semantic\n",
      "correlation modelling, which leads to severe data sparsity problem, especially\n",
      "when adapting an NLU model to a new domain. To address this issue, a novel\n",
      "label embedding based slot filling framework is proposed in this paper. Here,\n",
      "distributed label embedding is constructed for each slot using prior knowledge.\n",
      "Three encoding methods are investigated to incorporate different kinds of prior\n",
      "knowledge about slots: atomic concepts, slot descriptions, and slot exemplars.\n",
      "The proposed label embeddings tend to share text patterns and reuses data with\n",
      "different slot labels. This makes it useful for adaptive NLU with limited data.\n",
      "Also, since label embedding is independent of NLU model, it is compatible with\n",
      "almost all deep learning based slot filling models. The proposed approaches are\n",
      "evaluated on three datasets. Experiments on single domain and domain adaptation\n",
      "tasks show that label embedding achieves significant performance improvement\n",
      "over traditional one-hot label representation as well as advanced zero-shot\n",
      "approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.04160 \n",
      "Title :Revealing Lung Affections from CTs. A Comparative Analysis of Various\n",
      "  Deep Learning Approaches for Dealing with Volumetric Data\n",
      "  The paper presents and comparatively analyses several deep learning\n",
      "approaches to automatically detect tuberculosis related lesions in lung CTs, in\n",
      "the context of the ImageClef 2020 Tuberculosis task. Three classes of methods,\n",
      "different with respect to the way the volumetric data is given as input to\n",
      "neural network-based classifiers are discussed and evaluated. All these come\n",
      "with a rich experimental analysis comprising a variety of neural network\n",
      "architectures, various segmentation algorithms and data augmentation schemes.\n",
      "The reported work belongs to the SenticLab.UAIC team, which obtained the best\n",
      "results in the competition.\n",
      "\n",
      "**Paper Id :1907.10132 \n",
      "Title :Domain specific cues improve robustness of deep learning based\n",
      "  segmentation of ct volumes\n",
      "  Machine Learning has considerably improved medical image analysis in the past\n",
      "years. Although data-driven approaches are intrinsically adaptive and thus,\n",
      "generic, they often do not perform the same way on data from different imaging\n",
      "modalities. In particular Computed tomography (CT) data poses many challenges\n",
      "to medical image segmentation based on convolutional neural networks (CNNs),\n",
      "mostly due to the broad dynamic range of intensities and the varying number of\n",
      "recorded slices of CT volumes. In this paper, we address these issues with a\n",
      "framework that combines domain-specific data preprocessing and augmentation\n",
      "with state-of-the-art CNN architectures. The focus is not limited to optimise\n",
      "the score, but also to stabilise the prediction performance since this is a\n",
      "mandatory requirement for use in automated and semi-automated workflows in the\n",
      "clinical environment.\n",
      "  The framework is validated with an architecture comparison to show CNN\n",
      "architecture-independent effects of our framework functionality. We compare a\n",
      "modified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\n",
      "dilated convolutions for parallel multi-scale processing to the U-Net approach\n",
      "based on traditional scaling operations. Finally, we propose an ensemble model\n",
      "combining the strengths of different individual methods. The framework performs\n",
      "well on a range of tasks such as liver and kidney segmentation, without\n",
      "significant differences in prediction performance on strongly differing volume\n",
      "sizes and varying slice thickness. Thus our framework is an essential step\n",
      "towards performing robust segmentation of unknown real-world samples.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.04450 \n",
      "Title :Map-Adaptive Goal-Based Trajectory Prediction\n",
      "  We present a new method for multi-modal, long-term vehicle trajectory\n",
      "prediction. Our approach relies on using lane centerlines captured in rich maps\n",
      "of the environment to generate a set of proposed goal paths for each vehicle.\n",
      "Using these paths -- which are generated at run time and therefore dynamically\n",
      "adapt to the scene -- as spatial anchors, we predict a set of goal-based\n",
      "trajectories along with a categorical distribution over the goals. This\n",
      "approach allows us to directly model the goal-directed behavior of traffic\n",
      "actors, which unlocks the potential for more accurate long-term prediction. Our\n",
      "experimental results on both a large-scale internal driving dataset and on the\n",
      "public nuScenes dataset show that our model outperforms state-of-the-art\n",
      "approaches for vehicle trajectory prediction over a 6-second horizon. We also\n",
      "empirically demonstrate that our model is better able to generalize to road\n",
      "scenes from a completely new city than existing methods.\n",
      "\n",
      "**Paper Id :2005.10872 \n",
      "Title :Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n",
      "  Model-Based Strategies for Sample-Efficient Policy Learning\n",
      "  Traditional robotic approaches rely on an accurate model of the environment,\n",
      "a detailed description of how to perform the task, and a robust perception\n",
      "system to keep track of the current state. On the other hand, reinforcement\n",
      "learning approaches can operate directly from raw sensory inputs with only a\n",
      "reward signal to describe the task, but are extremely sample-inefficient and\n",
      "brittle. In this work, we combine the strengths of model-based methods with the\n",
      "flexibility of learning-based methods to obtain a general method that is able\n",
      "to overcome inaccuracies in the robotics perception/actuation pipeline, while\n",
      "requiring minimal interactions with the environment. This is achieved by\n",
      "leveraging uncertainty estimates to divide the space in regions where the given\n",
      "model-based policy is reliable, and regions where it may have flaws or not be\n",
      "well defined. In these uncertain regions, we show that a locally learned-policy\n",
      "can be used directly with raw sensory inputs. We test our algorithm, Guided\n",
      "Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing\n",
      "peg insertion. Videos are available at https://sites.google.com/view/guapo-rl\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.04519 \n",
      "Title :Extrapolating continuous color emotions through deep learning\n",
      "  By means of an experimental dataset, we use deep learning to implement an RGB\n",
      "extrapolation of emotions associated to color, and do a mathematical study of\n",
      "the results obtained through this neural network. In particular, we see that\n",
      "males typically associate a given emotion with darker colors while females with\n",
      "brighter colors. A similar trend was observed with older people and\n",
      "associations to lighter colors. Moreover, through our classification matrix, we\n",
      "identify which colors have weak associations to emotions and which colors are\n",
      "typically confused with other colors.\n",
      "\n",
      "**Paper Id :2010.03204 \n",
      "Title :Cardiac Arrhythmia Detection from ECG with Convolutional Recurrent\n",
      "  Neural Networks\n",
      "  Except for a few specific types, cardiac arrhythmias are not immediately\n",
      "life-threatening. However, if not treated appropriately, they can cause serious\n",
      "complications. In particular, atrial fibrillation, which is characterized by\n",
      "fast and irregular heart beats, increases the risk of stroke. We propose three\n",
      "neural network architectures to detect abnormal rhythms from single-lead ECG\n",
      "signals. These architectures combine convolutional layers to extract high-level\n",
      "features pertinent for arrhythmia detection from sliding windows and recurrent\n",
      "layers to aggregate these features over signals of varying durations. We\n",
      "applied the neural networks to the dataset used for the challenge of Computing\n",
      "in Cardiology 2017 and a dataset built by joining three databases available on\n",
      "PhysioNet. Our architectures achieved an accuracy of 86.23% on the first\n",
      "dataset, similar to the winning entries of the challenge, and an accuracy of\n",
      "92.02% on the second dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "**Paper Id :2004.04917 \n",
      "Title :Multimodal Categorization of Crisis Events in Social Media\n",
      "  Recent developments in image classification and natural language processing,\n",
      "coupled with the rapid growth in social media usage, have enabled fundamental\n",
      "advances in detecting breaking events around the world in real-time. Emergency\n",
      "response is one such area that stands to gain from these advances. By\n",
      "processing billions of texts and images a minute, events can be automatically\n",
      "detected to enable emergency response workers to better assess rapidly evolving\n",
      "situations and deploy resources accordingly. To date, most event detection\n",
      "techniques in this area have focused on image-only or text-only approaches,\n",
      "limiting detection performance and impacting the quality of information\n",
      "delivered to crisis response teams. In this paper, we present a new multimodal\n",
      "fusion method that leverages both images and texts as input. In particular, we\n",
      "introduce a cross-attention module that can filter uninformative and misleading\n",
      "components from weak modalities on a sample by sample basis. In addition, we\n",
      "employ a multimodal graph-based approach to stochastically transition between\n",
      "embeddings of different multimodal pairs during training to better regularize\n",
      "the learning process as well as dealing with limited training data by\n",
      "constructing new matched pairs from different samples. We show that our method\n",
      "outperforms the unimodal approaches and strong multimodal baselines by a large\n",
      "margin on three crisis-related tasks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.05752 \n",
      "Title :Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial\n",
      "  Networks\n",
      "  Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common\n",
      "procedure for the identification of many respiratory diseases compared to MRI,\n",
      "CT, and PET scans. This paper presents the use of generative adversarial\n",
      "networks (GAN) to perform the task of lung segmentation on a given CXR. GANs\n",
      "are popular to generate realistic data by learning the mapping from one domain\n",
      "to another. In our work, the generator of the GAN is trained to generate a\n",
      "segmented mask of a given input CXR. The discriminator distinguishes between a\n",
      "ground truth and the generated mask, and updates the generator through the\n",
      "adversarial loss measure. The objective is to generate masks for the input CXR,\n",
      "which are as realistic as possible compared to the ground truth masks. The\n",
      "model is trained and evaluated using four different discriminators referred to\n",
      "as D1, D2, D3, and D4, respectively. Experimental results on three different\n",
      "CXR datasets reveal that the proposed model is able to achieve a dice-score of\n",
      "0.9740, and IOU score of 0.943, which are better than other reported\n",
      "state-of-the art results.\n",
      "\n",
      "**Paper Id :2003.00682 \n",
      "Title :Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images\n",
      "  Lung disease is common throughout the world. These include chronic\n",
      "obstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc.\n",
      "Timely diagnosis of lung disease is essential. Many image processing and\n",
      "machine learning models have been developed for this purpose. Different forms\n",
      "of existing deep learning techniques including convolutional neural network\n",
      "(CNN), vanilla neural network, visual geometry group based neural network\n",
      "(VGG), and capsule network are applied for lung disease prediction.The basic\n",
      "CNN has poor performance for rotated, tilted, or other abnormal image\n",
      "orientation. Therefore, we propose a new hybrid deep learning framework by\n",
      "combining VGG, data augmentation and spatial transformer network (STN) with\n",
      "CNN. This new hybrid method is termed here as VGG Data STN with CNN (VDSNet).\n",
      "As implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The\n",
      "new model is applied to NIH chest X-ray image dataset collected from Kaggle\n",
      "repository. Full and sample versions of the dataset are considered. For both\n",
      "full and sample datasets, VDSNet outperforms existing methods in terms of a\n",
      "number of metrics including precision, recall, F0.5 score and validation\n",
      "accuracy. For the case of full dataset, VDSNet exhibits a validation accuracy\n",
      "of 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified\n",
      "capsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%,\n",
      "respectively. When sample dataset rather than full dataset is used, VDSNet\n",
      "requires much lower training time at the expense of a slightly lower validation\n",
      "accuracy. Hence, the proposed VDSNet framework will simplify the detection of\n",
      "lung disease for experts as well as for doctors.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.06349 \n",
      "Title :Play MNIST For Me! User Studies on the Effects of Post-Hoc,\n",
      "  Example-Based Explanations & Error Rates on Debugging a Deep Learning,\n",
      "  Black-Box Classifier\n",
      "  This paper reports two experiments (N=349) on the impact of post hoc\n",
      "explanations by example and error rates on peoples perceptions of a black box\n",
      "classifier. Both experiments show that when people are given case based\n",
      "explanations, from an implemented ANN CBR twin system, they perceive miss\n",
      "classifications to be more correct. They also show that as error rates increase\n",
      "above 4%, people trust the classifier less and view it as being less correct,\n",
      "less reasonable and less trustworthy. The implications of these results for XAI\n",
      "are discussed.\n",
      "\n",
      "**Paper Id :2010.02458 \n",
      "Title :Identifying Spurious Correlations for Robust Text Classification\n",
      "  The predictions of text classifiers are often driven by spurious correlations\n",
      "-- e.g., the term `Spielberg' correlates with positively reviewed movies, even\n",
      "though the term itself does not semantically convey a positive sentiment. In\n",
      "this paper, we propose a method to distinguish spurious and genuine\n",
      "correlations in text classification. We treat this as a supervised\n",
      "classification problem, using features derived from treatment effect estimators\n",
      "to distinguish spurious correlations from \"genuine\" ones. Due to the generic\n",
      "nature of these features and their small dimensionality, we find that the\n",
      "approach works well even with limited training examples, and that it is\n",
      "possible to transport the word classifier to new domains. Experiments on four\n",
      "datasets (sentiment classification and toxicity detection) suggest that using\n",
      "this approach to inform feature selection also leads to more robust\n",
      "classification, as measured by improved worst-case accuracy on the samples\n",
      "affected by spurious correlations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.07560 \n",
      "Title :Similarity-based data mining for online domain adaptation of a sonar ATR\n",
      "  system\n",
      "  Due to the expensive nature of field data gathering, the lack of training\n",
      "data often limits the performance of Automatic Target Recognition (ATR)\n",
      "systems. This problem is often addressed with domain adaptation techniques,\n",
      "however the currently existing methods fail to satisfy the constraints of\n",
      "resource and time-limited underwater systems. We propose to address this issue\n",
      "via an online fine-tuning of the ATR algorithm using a novel data-selection\n",
      "method. Our proposed data-mining approach relies on visual similarity and\n",
      "outperforms the traditionally employed hard-mining methods. We present a\n",
      "comparative performance analysis in a wide range of simulated environments and\n",
      "highlight the benefits of using our method for the rapid adaptation to\n",
      "previously unseen environments.\n",
      "\n",
      "**Paper Id :2005.05550 \n",
      "Title :High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n",
      "  Fine-Tuning of Physics-Based Neural Networks\n",
      "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\n",
      "has emerged as a powerful means for accelerated MRI reconstruction by providing\n",
      "data-driven regularizers that are directly learned from data. These data-driven\n",
      "priors typically remain unchanged for future data in the testing phase once\n",
      "they are learned during training. In this study, we propose to use a transfer\n",
      "learning approach to fine-tune these regularizers for new subjects using a\n",
      "self-supervision approach. While the proposed approach can compromise the\n",
      "extremely fast reconstruction time of deep learning MRI methods, our results on\n",
      "knee MRI indicate that such adaptation can substantially reduce the remaining\n",
      "artifacts in reconstructed images. In addition, the proposed approach has the\n",
      "potential to reduce the risks of generalization to rare pathological\n",
      "conditions, which may be unavailable in the training data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.07608 \n",
      "Title :Deep Learning in Photoacoustic Tomography: Current approaches and future\n",
      "  directions\n",
      "  Biomedical photoacoustic tomography, which can provide high resolution 3D\n",
      "soft tissue images based on the optical absorption, has advanced to the stage\n",
      "at which translation from the laboratory to clinical settings is becoming\n",
      "possible. The need for rapid image formation and the practical restrictions on\n",
      "data acquisition that arise from the constraints of a clinical workflow are\n",
      "presenting new image reconstruction challenges. There are many classical\n",
      "approaches to image reconstruction, but ameliorating the effects of incomplete\n",
      "or imperfect data through the incorporation of accurate priors is challenging\n",
      "and leads to slow algorithms. Recently, the application of Deep Learning, or\n",
      "deep neural networks, to this problem has received a great deal of attention.\n",
      "This paper reviews the literature on learned image reconstruction, summarising\n",
      "the current trends, and explains how these new approaches fit within, and to\n",
      "some extent have arisen from, a framework that encompasses classical\n",
      "reconstruction methods. In particular, it shows how these new techniques can be\n",
      "understood from a Bayesian perspective, providing useful insights. The paper\n",
      "also provides a concise tutorial demonstration of three prototypical approaches\n",
      "to learned image reconstruction. The code and data sets for these\n",
      "demonstrations are available to researchers. It is anticipated that it is in in\n",
      "vivo applications - where data may be sparse, fast imaging critical and priors\n",
      "difficult to construct by hand - that Deep Learning will have the most impact.\n",
      "With this in mind, the paper concludes with some indications of possible future\n",
      "research directions.\n",
      "\n",
      "**Paper Id :2005.06540 \n",
      "Title :Deep Learning for Political Science\n",
      "  Political science, and social science in general, have traditionally been\n",
      "using computational methods to study areas such as voting behavior, policy\n",
      "making, international conflict, and international development. More recently,\n",
      "increasingly available quantities of data are being combined with improved\n",
      "algorithms and affordable computational resources to predict, learn, and\n",
      "discover new insights from data that is large in volume and variety. New\n",
      "developments in the areas of machine learning, deep learning, natural language\n",
      "processing (NLP), and, more generally, artificial intelligence (AI) are opening\n",
      "up new opportunities for testing theories and evaluating the impact of\n",
      "interventions and programs in a more dynamic and effective way. Applications\n",
      "using large volumes of structured and unstructured data are becoming common in\n",
      "government and industry, and increasingly also in social science research. This\n",
      "chapter offers an introduction to such methods drawing examples from political\n",
      "science. Focusing on the areas where the strengths of the methods coincide with\n",
      "challenges in these fields, the chapter first presents an introduction to AI\n",
      "and its core technology - machine learning, with its rapidly developing\n",
      "subfield of deep learning. The discussion of deep neural networks is\n",
      "illustrated with the NLP tasks that are relevant to political science. The\n",
      "latest advances in deep learning methods for NLP are also reviewed, together\n",
      "with their potential for improving information extraction and pattern\n",
      "recognition from political science texts.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.08720 \n",
      "Title :Contextual Semantic Interpretability\n",
      "  Convolutional neural networks (CNN) are known to learn an image\n",
      "representation that captures concepts relevant to the task, but do so in an\n",
      "implicit way that hampers model interpretability. However, one could argue that\n",
      "such a representation is hidden in the neurons and can be made explicit by\n",
      "teaching the model to recognize semantically interpretable attributes that are\n",
      "present in the scene. We call such an intermediate layer a \\emph{semantic\n",
      "bottleneck}. Once the attributes are learned, they can be re-combined to reach\n",
      "the final decision and provide both an accurate prediction and an explicit\n",
      "reasoning behind the CNN decision. In this paper, we look into semantic\n",
      "bottlenecks that capture context: we want attributes to be in groups of a few\n",
      "meaningful elements and participate jointly to the final decision. We use a\n",
      "two-layer semantic bottleneck that gathers attributes into interpretable,\n",
      "sparse groups, allowing them contribute differently to the final output\n",
      "depending on the context. We test our contextual semantic interpretable\n",
      "bottleneck (CSIB) on the task of landscape scenicness estimation and train the\n",
      "semantic interpretable bottleneck using an auxiliary database (SUN Attributes).\n",
      "Our model yields in predictions as accurate as a non-interpretable baseline\n",
      "when applied to a real-world test set of Flickr images, all while providing\n",
      "clear and interpretable explanations for each prediction.\n",
      "\n",
      "**Paper Id :2009.10259 \n",
      "Title :ALICE: Active Learning with Contrastive Natural Language Explanations\n",
      "  Training a supervised neural network classifier typically requires many\n",
      "annotated training samples. Collecting and annotating a large number of data\n",
      "points are costly and sometimes even infeasible. Traditional annotation process\n",
      "uses a low-bandwidth human-machine communication interface: classification\n",
      "labels, each of which only provides several bits of information. We propose\n",
      "Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\n",
      "training framework that utilizes contrastive natural language explanations to\n",
      "improve data efficiency in learning. ALICE learns to first use active learning\n",
      "to select the most informative pairs of label classes to elicit contrastive\n",
      "natural language explanations from experts. Then it extracts knowledge from\n",
      "these explanations using a semantic parser. Finally, it incorporates the\n",
      "extracted knowledge through dynamically changing the learning model's\n",
      "structure. We applied ALICE in two visual recognition tasks, bird species\n",
      "classification and social relationship classification. We found by\n",
      "incorporating contrastive explanations, our models outperform baseline models\n",
      "that are trained with 40-100% more training data. We found that adding 1\n",
      "explanation leads to similar performance gain as adding 13-30 labeled training\n",
      "data points.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.09140 \n",
      "Title :Introspective Learning by Distilling Knowledge from Online\n",
      "  Self-explanation\n",
      "  In recent years, many explanation methods have been proposed to explain\n",
      "individual classifications of deep neural networks. However, how to leverage\n",
      "the created explanations to improve the learning process has been less\n",
      "explored. As the privileged information, the explanations of a model can be\n",
      "used to guide the learning process of the model itself. In the community,\n",
      "another intensively investigated privileged information used to guide the\n",
      "training of a model is the knowledge from a powerful teacher model. The goal of\n",
      "this work is to leverage the self-explanation to improve the learning process\n",
      "by borrowing ideas from knowledge distillation. We start by investigating the\n",
      "effective components of the knowledge transferred from the teacher network to\n",
      "the student network. Our investigation reveals that both the responses in\n",
      "non-ground-truth classes and class-similarity information in teacher's outputs\n",
      "contribute to the success of the knowledge distillation. Motivated by the\n",
      "conclusion, we propose an implementation of introspective learning by\n",
      "distilling knowledge from online self-explanations. The models trained with the\n",
      "introspective learning procedure outperform the ones trained with the standard\n",
      "learning procedure, as well as the ones trained with different regularization\n",
      "methods. When compared to the models learned from peer networks or teacher\n",
      "networks, our models also show competitive performance and requires neither\n",
      "peers nor teachers.\n",
      "\n",
      "**Paper Id :1909.11723 \n",
      "Title :Revisit Knowledge Distillation: a Teacher-free Framework\n",
      "  Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome\n",
      "teacher model into a lightweight student model. Its success is generally\n",
      "attributed to the privileged information on similarities among categories\n",
      "provided by the teacher model, and in this sense, only strong teacher models\n",
      "are deployed to teach weaker students in practice. In this work, we challenge\n",
      "this common belief by following experimental observations: 1) beyond the\n",
      "acknowledgment that the teacher can improve the student, the student can also\n",
      "enhance the teacher significantly by reversing the KD procedure; 2) a\n",
      "poorly-trained teacher with much lower accuracy than the student can still\n",
      "improve the latter significantly. To explain these observations, we provide a\n",
      "theoretical analysis of the relationships between KD and label smoothing\n",
      "regularization. We prove that 1) KD is a type of learned label smoothing\n",
      "regularization and 2) label smoothing regularization provides a virtual teacher\n",
      "model for KD. From these results, we argue that the success of KD is not fully\n",
      "due to the similarity information between categories, but also to the\n",
      "regularization of soft targets, which is equally or even more important.\n",
      "  Based on these analyses, we further propose a novel Teacher-free Knowledge\n",
      "Distillation (Tf-KD) framework, where a student model learns from itself or\n",
      "manually-designed regularization distribution. The Tf-KD achieves comparable\n",
      "performance with normal KD from a superior teacher, which is well applied when\n",
      "teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly\n",
      "deployed for training deep neural networks. Without any extra computation cost,\n",
      "Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established\n",
      "baseline models, which is superior to label smoothing regularization. The codes\n",
      "are in: \\url{https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation}\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.09781 \n",
      "Title :Rethinking Supervised Learning and Reinforcement Learning in\n",
      "  Task-Oriented Dialogue Systems\n",
      "  Dialogue policy learning for task-oriented dialogue systems has enjoyed great\n",
      "progress recently mostly through employing reinforcement learning methods.\n",
      "However, these approaches have become very sophisticated. It is time to\n",
      "re-evaluate it. Are we really making progress developing dialogue agents only\n",
      "based on reinforcement learning? We demonstrate how (1)~traditional supervised\n",
      "learning together with (2)~a simulator-free adversarial learning method can be\n",
      "used to achieve performance comparable to state-of-the-art RL-based methods.\n",
      "First, we introduce a simple dialogue action decoder to predict the appropriate\n",
      "actions. Then, the traditional multi-label classification solution for dialogue\n",
      "policy learning is extended by adding dense layers to improve the dialogue\n",
      "agent performance. Finally, we employ the Gumbel-Softmax estimator to\n",
      "alternatively train the dialogue agent and the dialogue reward model without\n",
      "using reinforcement learning. Based on our extensive experimentation, we can\n",
      "conclude the proposed methods can achieve more stable and higher performance\n",
      "with fewer efforts, such as the domain knowledge required to design a user\n",
      "simulator and the intractable parameter tuning in reinforcement learning. Our\n",
      "main goal is not to beat reinforcement learning with supervised learning, but\n",
      "to demonstrate the value of rethinking the role of reinforcement learning and\n",
      "supervised learning in optimizing task-oriented dialogue systems.\n",
      "\n",
      "**Paper Id :2004.03267 \n",
      "Title :Guided Dialog Policy Learning without Adversarial Learning in the Loop\n",
      "  Reinforcement Learning (RL) methods have emerged as a popular choice for\n",
      "training an efficient and effective dialogue policy. However, these methods\n",
      "suffer from sparse and unstable reward signals returned by a user simulator\n",
      "only when a dialogue finishes. Besides, the reward signal is manually designed\n",
      "by human experts, which requires domain knowledge. Recently, a number of\n",
      "adversarial learning methods have been proposed to learn the reward function\n",
      "together with the dialogue policy. However, to alternatively update the\n",
      "dialogue policy and the reward model on the fly, we are limited to\n",
      "policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the\n",
      "alternating training of a dialogue agent and the reward model can easily get\n",
      "stuck in local optima or result in mode collapse. To overcome the listed\n",
      "issues, we propose to decompose the adversarial training into two steps. First,\n",
      "we train the discriminator with an auxiliary dialogue generator and then\n",
      "incorporate a derived reward model into a common RL method to guide the\n",
      "dialogue policy learning. This approach is applicable to both on-policy and\n",
      "off-policy RL methods. Based on our extensive experimentation, we can conclude\n",
      "the proposed method: (1) achieves a remarkable task success rate using both\n",
      "on-policy and off-policy RL methods; and (2) has the potential to transfer\n",
      "knowledge from existing domains to a new domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.10259 \n",
      "Title :ALICE: Active Learning with Contrastive Natural Language Explanations\n",
      "  Training a supervised neural network classifier typically requires many\n",
      "annotated training samples. Collecting and annotating a large number of data\n",
      "points are costly and sometimes even infeasible. Traditional annotation process\n",
      "uses a low-bandwidth human-machine communication interface: classification\n",
      "labels, each of which only provides several bits of information. We propose\n",
      "Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\n",
      "training framework that utilizes contrastive natural language explanations to\n",
      "improve data efficiency in learning. ALICE learns to first use active learning\n",
      "to select the most informative pairs of label classes to elicit contrastive\n",
      "natural language explanations from experts. Then it extracts knowledge from\n",
      "these explanations using a semantic parser. Finally, it incorporates the\n",
      "extracted knowledge through dynamically changing the learning model's\n",
      "structure. We applied ALICE in two visual recognition tasks, bird species\n",
      "classification and social relationship classification. We found by\n",
      "incorporating contrastive explanations, our models outperform baseline models\n",
      "that are trained with 40-100% more training data. We found that adding 1\n",
      "explanation leads to similar performance gain as adding 13-30 labeled training\n",
      "data points.\n",
      "\n",
      "**Paper Id :2005.11014 \n",
      "Title :Intent Mining from past conversations for Conversational Agent\n",
      "  Conversational systems are of primary interest in the AI community. Chatbots\n",
      "are increasingly being deployed to provide round-the-clock support and to\n",
      "increase customer engagement. Many of the commercial bot building frameworks\n",
      "follow a standard approach that requires one to build and train an intent model\n",
      "to recognize a user input. Intent models are trained in a supervised setting\n",
      "with a collection of textual utterance and intent label pairs. Gathering a\n",
      "substantial and wide coverage of training data for different intent is a\n",
      "bottleneck in the bot building process. Moreover, the cost of labeling a\n",
      "hundred to thousands of conversations with intent is a time consuming and\n",
      "laborious job. In this paper, we present an intent discovery framework that\n",
      "involves 4 primary steps: Extraction of textual utterances from a conversation\n",
      "using a pre-trained domain agnostic Dialog Act Classifier (Data Extraction),\n",
      "automatic clustering of similar user utterances (Clustering), manual annotation\n",
      "of clusters with an intent label (Labeling) and propagation of intent labels to\n",
      "the utterances from the previous step, which are not mapped to any cluster\n",
      "(Label Propagation); to generate intent training data from raw conversations.\n",
      "We have introduced a novel density-based clustering algorithm ITER-DBSCAN for\n",
      "unbalanced data clustering. Subject Matter Expert (Annotators with domain\n",
      "expertise) manually looks into the clustered user utterances and provides an\n",
      "intent label for discovery. We conducted user studies to validate the\n",
      "effectiveness of the trained intent model generated in terms of coverage of\n",
      "intents, accuracy and time saving concerning manual annotation. Although the\n",
      "system is developed for building an intent model for the conversational system,\n",
      "this framework can also be used for a short text clustering or as a labeling\n",
      "framework.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.10269 \n",
      "Title :An Incentive Mechanism for Federated Learning in Wireless Cellular\n",
      "  network: An Auction Approach\n",
      "  Federated Learning (FL) is a distributed learning framework that can deal\n",
      "with the distributed issue in machine learning and still guarantee high\n",
      "learning performance. However, it is impractical that all users will sacrifice\n",
      "their resources to join the FL algorithm. This motivates us to study the\n",
      "incentive mechanism design for FL. In this paper, we consider a FL system that\n",
      "involves one base station (BS) and multiple mobile users. The mobile users use\n",
      "their own data to train the local machine learning model, and then send the\n",
      "trained models to the BS, which generates the initial model, collects local\n",
      "models and constructs the global model. Then, we formulate the incentive\n",
      "mechanism between the BS and mobile users as an auction game where the BS is an\n",
      "auctioneer and the mobile users are the sellers. In the proposed game, each\n",
      "mobile user submits its bids according to the minimal energy cost that the\n",
      "mobile users experiences in participating in FL. To decide winners in the\n",
      "auction and maximize social welfare, we propose the primal-dual greedy auction\n",
      "mechanism. The proposed mechanism can guarantee three economic properties,\n",
      "namely, truthfulness, individual rationality and efficiency. Finally, numerical\n",
      "results are shown to demonstrate the performance effectiveness of our proposed\n",
      "mechanism.\n",
      "\n",
      "**Paper Id :1905.07210 \n",
      "Title :Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using\n",
      "  Non-IID Data\n",
      "  This paper proposes a cooperative mechanism for mitigating the performance\n",
      "degradation due to non-independent-and-identically-distributed (non-IID) data\n",
      "in collaborative machine learning (ML), namely federated learning (FL), which\n",
      "trains an ML model using the rich data and computational resources of mobile\n",
      "clients without gathering their data to central systems. The data of mobile\n",
      "clients is typically non-IID owing to diversity among mobile clients' interests\n",
      "and usage, and FL with non-IID data could degrade the model performance.\n",
      "Therefore, to mitigate the degradation induced by non-IID data, we assume that\n",
      "a limited number (e.g., less than 1%) of clients allow their data to be\n",
      "uploaded to a server, and we propose a hybrid learning mechanism referred to as\n",
      "Hybrid-FL, wherein the server updates the model using the data gathered from\n",
      "the clients and aggregates the model with the models trained by clients. The\n",
      "Hybrid-FL solves both client- and data-selection problems via heuristic\n",
      "algorithms, which try to select the optimal sets of clients who train models\n",
      "with their own data, clients who upload their data to the server, and data\n",
      "uploaded to the server. The algorithms increase the number of clients\n",
      "participating in FL and make more data gather in the server IID, thereby\n",
      "improving the prediction accuracy of the aggregated model. Evaluations, which\n",
      "consist of network simulations and ML experiments, demonstrate that the\n",
      "proposed scheme achieves a 13.5% higher classification accuracy than those of\n",
      "the previously proposed schemes for the non-IID case.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.10343 \n",
      "Title :Gamma distribution-based sampling for imbalanced data\n",
      "  Imbalanced class distribution is a common problem in a number of fields\n",
      "including medical diagnostics, fraud detection, and others. It causes bias in\n",
      "classification algorithms leading to poor performance on the minority class\n",
      "data. In this paper, we propose a novel method for balancing the class\n",
      "distribution in data through intelligent resampling of the minority class\n",
      "instances. The proposed method is based on generating new minority instances in\n",
      "the neighborhood of the existing minority points via a gamma distribution. Our\n",
      "method offers a natural and coherent approach to balancing the data. We conduct\n",
      "a comprehensive numerical analysis of the new sampling technique. The\n",
      "experimental results show that the proposed method outperforms the existing\n",
      "state-of-the-art methods for imbalanced data. Concretely, the new sampling\n",
      "technique produces the best results on 12 out of 24 real life as well as\n",
      "synthetic datasets. For comparison, the SMOTE method achieves the top score on\n",
      "only 1 dataset. We conclude that the new technique offers a simple yet\n",
      "effective sampling approach to balance data.\n",
      "\n",
      "**Paper Id :1909.03681 \n",
      "Title :Outlier Detection in High Dimensional Data\n",
      "  High-dimensional data poses unique challenges in outlier detection process.\n",
      "Most of the existing algorithms fail to properly address the issues stemming\n",
      "from a large number of features. In particular, outlier detection algorithms\n",
      "perform poorly on data set of small size with a large number of features. In\n",
      "this paper, we propose a novel outlier detection algorithm based on principal\n",
      "component analysis and kernel density estimation. The proposed method is\n",
      "designed to address the challenges of dealing with high-dimensional data by\n",
      "projecting the original data onto a smaller space and using the innate\n",
      "structure of the data to calculate anomaly scores for each data point.\n",
      "Numerical experiments on synthetic and real-life data show that our method\n",
      "performs well on high-dimensional data. In particular, the proposed method\n",
      "outperforms the benchmark methods as measured by the $F_1$-score. Our method\n",
      "also produces better-than-average execution times compared to the benchmark\n",
      "methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.10619 \n",
      "Title :An Exponential Factorization Machine with Percentage Error Minimization\n",
      "  to Retail Sales Forecasting\n",
      "  This paper proposes a new approach to sales forecasting for new products with\n",
      "long lead time but short product life cycle. These SKUs are usually sold for\n",
      "one season only, without any replenishments. An exponential factorization\n",
      "machine (EFM) sales forecast model is developed to solve this problem which not\n",
      "only considers SKU attributes, but also pairwise interactions. The EFM model is\n",
      "significantly different from the original Factorization Machines (FM) from\n",
      "two-fold: (1) the attribute-level formulation for explanatory variables and (2)\n",
      "exponential formulation for the positive response variable. The attribute-level\n",
      "formation excludes infeasible intra-attribute interactions and results in more\n",
      "efficient feature engineering comparing with the conventional one-hot encoding,\n",
      "while the exponential formulation is demonstrated more effective than the\n",
      "log-transformation for the positive but not skewed distributed responses. In\n",
      "order to estimate the parameters, percentage error squares (PES) and error\n",
      "squares (ES) are minimized by a proposed adaptive batch gradient descent method\n",
      "over the training set. Real-world data provided by a footwear retailer in\n",
      "Singapore is used for testing the proposed approach. The forecasting\n",
      "performance in terms of both mean absolute percentage error (MAPE) and mean\n",
      "absolute error (MAE) compares favourably with not only off-the-shelf models but\n",
      "also results reported by extant sales and demand forecasting studies. The\n",
      "effectiveness of the proposed approach is also demonstrated by two external\n",
      "public datasets. Moreover, we prove the theoretical relationships between PES\n",
      "and ES minimization, and present an important property of the PES minimization\n",
      "for regression models; that it trains models to underestimate data. This\n",
      "property fits the situation of sales forecasting where unit-holding cost is\n",
      "much greater than the unit-shortage cost.\n",
      "\n",
      "**Paper Id :2004.07210 \n",
      "Title :On Box-Cox Transformation for Image Normality and Pattern Classification\n",
      "  A unique member of the power transformation family is known as the Box-Cox\n",
      "transformation. The latter can be seen as a mathematical operation that leads\n",
      "to finding the optimum lambda ({\\lambda}) value that maximizes the\n",
      "log-likelihood function to transform a data to a normal distribution and to\n",
      "reduce heteroscedasticity. In data analytics, a normality assumption underlies\n",
      "a variety of statistical test models. This technique, however, is best known in\n",
      "statistical analysis to handle one-dimensional data. Herein, this paper\n",
      "revolves around the utility of such a tool as a pre-processing step to\n",
      "transform two-dimensional data, namely, digital images and to study its effect.\n",
      "Moreover, to reduce time complexity, it suffices to estimate the parameter\n",
      "lambda in real-time for large two-dimensional matrices by merely considering\n",
      "their probability density function as a statistical inference of the underlying\n",
      "data distribution. We compare the effect of this light-weight Box-Cox\n",
      "transformation with well-established state-of-the-art low light image\n",
      "enhancement techniques. We also demonstrate the effectiveness of our approach\n",
      "through several test-bed data sets for generic improvement of visual appearance\n",
      "of images and for ameliorating the performance of a colour pattern\n",
      "classification algorithm as an example application. Results with and without\n",
      "the proposed approach, are compared using the AlexNet (transfer deep learning)\n",
      "pretrained model. To the best of our knowledge, this is the first time that the\n",
      "Box-Cox transformation is extended to digital images by exploiting histogram\n",
      "transformation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.12583 \n",
      "Title :Small Data, Big Decisions: Model Selection in the Small-Data Regime\n",
      "  Highly overparametrized neural networks can display curiously strong\n",
      "generalization performance - a phenomenon that has recently garnered a wealth\n",
      "of theoretical and empirical research in order to better understand it. In\n",
      "contrast to most previous work, which typically considers the performance as a\n",
      "function of the model size, in this paper we empirically study the\n",
      "generalization performance as the size of the training set varies over multiple\n",
      "orders of magnitude. These systematic experiments lead to some interesting and\n",
      "potentially very useful observations; perhaps most notably that training on\n",
      "smaller subsets of the data can lead to more reliable model selection decisions\n",
      "whilst simultaneously enjoying smaller computational costs. Our experiments\n",
      "furthermore allow us to estimate Minimum Description Lengths for common\n",
      "datasets given modern neural network architectures, thereby paving the way for\n",
      "principled model selection taking into account Occams-razor.\n",
      "\n",
      "**Paper Id :1905.04305 \n",
      "Title :Spectral Reconstruction with Deep Neural Networks\n",
      "  We explore artificial neural networks as a tool for the reconstruction of\n",
      "spectral functions from imaginary time Green's functions, a classic\n",
      "ill-conditioned inverse problem. Our ansatz is based on a supervised learning\n",
      "framework in which prior knowledge is encoded in the training data and the\n",
      "inverse transformation manifold is explicitly parametrised through a neural\n",
      "network. We systematically investigate this novel reconstruction approach,\n",
      "providing a detailed analysis of its performance on physically motivated mock\n",
      "data, and compare it to established methods of Bayesian inference. The\n",
      "reconstruction accuracy is found to be at least comparable, and potentially\n",
      "superior in particular at larger noise levels. We argue that the use of\n",
      "labelled training data in a supervised setting and the freedom in defining an\n",
      "optimisation objective are inherent advantages of the present approach and may\n",
      "lead to significant improvements over state-of-the-art methods in the future.\n",
      "Potential directions for further research are discussed in detail.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.12735 \n",
      "Title :Modeling Topical Relevance for Multi-Turn Dialogue Generation\n",
      "  Topic drift is a common phenomenon in multi-turn dialogue. Therefore, an\n",
      "ideal dialogue generation models should be able to capture the topic\n",
      "information of each context, detect the relevant context, and produce\n",
      "appropriate responses accordingly. However, existing models usually use word or\n",
      "sentence level similarities to detect the relevant contexts, which fail to well\n",
      "capture the topical level relevance. In this paper, we propose a new model,\n",
      "named STAR-BTM, to tackle this problem. Firstly, the Biterm Topic Model is\n",
      "pre-trained on the whole training dataset. Then, the topic level attention\n",
      "weights are computed based on the topic representation of each context.\n",
      "Finally, the attention weights and the topic distribution are utilized in the\n",
      "decoding process to generate the corresponding responses. Experimental results\n",
      "on both Chinese customer services data and English Ubuntu dialogue data show\n",
      "that STAR-BTM significantly outperforms several state-of-the-art methods, in\n",
      "terms of both metric-based and human evaluations.\n",
      "\n",
      "**Paper Id :2003.11644 \n",
      "Title :Multi-Label Text Classification using Attention-based Graph Neural\n",
      "  Network\n",
      "  In Multi-Label Text Classification (MLTC), one sample can belong to more than\n",
      "one class. It is observed that most MLTC tasks, there are dependencies or\n",
      "correlations among labels. Existing methods tend to ignore the relationship\n",
      "among labels. In this paper, a graph attention network-based model is proposed\n",
      "to capture the attentive dependency structure among the labels. The graph\n",
      "attention network uses a feature matrix and a correlation matrix to capture and\n",
      "explore the crucial dependencies between the labels and generate classifiers\n",
      "for the task. The generated classifiers are applied to sentence feature vectors\n",
      "obtained from the text feature extraction network (BiLSTM) to enable end-to-end\n",
      "training. Attention allows the system to assign different weights to neighbor\n",
      "nodes per label, thus allowing it to learn the dependencies among labels\n",
      "implicitly. The results of the proposed model are validated on five real-world\n",
      "MLTC datasets. The proposed model achieves similar or better performance\n",
      "compared to the previous state-of-the-art models.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.13003 \n",
      "Title :On Efficient Constructions of Checkpoints\n",
      "  Efficient construction of checkpoints/snapshots is a critical tool for\n",
      "training and diagnosing deep learning models. In this paper, we propose a lossy\n",
      "compression scheme for checkpoint constructions (called LC-Checkpoint).\n",
      "LC-Checkpoint simultaneously maximizes the compression rate and optimizes the\n",
      "recovery speed, under the assumption that SGD is used to train the model.\n",
      "LC-Checkpointuses quantization and priority promotion to store the most crucial\n",
      "information for SGD to recover, and then uses a Huffman coding to leverage the\n",
      "non-uniform distribution of the gradient scales. Our extensive experiments show\n",
      "that LC-Checkpoint achieves a compression rate up to $28\\times$ and recovery\n",
      "speedup up to $5.77\\times$ over a state-of-the-art algorithm (SCAR).\n",
      "\n",
      "**Paper Id :1911.02945 \n",
      "Title :J-MoDL: Joint Model-Based Deep Learning for Optimized Sampling and\n",
      "  Reconstruction\n",
      "  Modern MRI schemes, which rely on compressed sensing or deep learning\n",
      "algorithms to recover MRI data from undersampled multichannel Fourier\n",
      "measurements, are widely used to reduce scan time. The image quality of these\n",
      "approaches is heavily dependent on the sampling pattern. We introduce a\n",
      "continuous strategy to jointly optimize the sampling pattern and network\n",
      "parameters. We use a multichannel forward model, consisting of a non-uniform\n",
      "Fourier transform with continuously defined sampling locations, to realize the\n",
      "data consistency block within a model-based deep learning image reconstruction\n",
      "scheme. This approach facilitates the joint and continuous optimization of the\n",
      "sampling pattern and the CNN parameters to improve image quality. We observe\n",
      "that the joint optimization of the sampling patterns and the reconstruction\n",
      "module significantly improves the performance of most deep learning\n",
      "reconstruction algorithms. The source code of the proposed joint learning\n",
      "framework is available at https://github.com/hkaggarwal/J-MoDL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.13033 \n",
      "Title :Where Does the Robustness Come from? A Study of the Transformation-based\n",
      "  Ensemble Defence\n",
      "  This paper aims to provide a thorough study on the effectiveness of the\n",
      "transformation-based ensemble defence for image classification and its reasons.\n",
      "It has been empirically shown that they can enhance the robustness against\n",
      "evasion attacks, while there is little analysis on the reasons. In particular,\n",
      "it is not clear whether the robustness improvement is a result of\n",
      "transformation or ensemble. In this paper, we design two adaptive attacks to\n",
      "better evaluate the transformation-based ensemble defence. We conduct\n",
      "experiments to show that 1) the transferability of adversarial examples exists\n",
      "among the models trained on data records after different reversible\n",
      "transformations; 2) the robustness gained through transformation-based ensemble\n",
      "is limited; 3) this limited robustness is mainly from the irreversible\n",
      "transformations rather than the ensemble of a number of models; and 4) blindly\n",
      "increasing the number of sub-models in a transformation-based ensemble does not\n",
      "bring extra robustness gain.\n",
      "\n",
      "**Paper Id :2008.09994 \n",
      "Title :Discriminative Residual Analysis for Image Set Classification with\n",
      "  Posture and Age Variations\n",
      "  Image set recognition has been widely applied in many practical problems like\n",
      "real-time video retrieval and image caption tasks. Due to its superior\n",
      "performance, it has grown into a significant topic in recent years. However,\n",
      "images with complicated variations, e.g., postures and human ages, are\n",
      "difficult to address, as these variations are continuous and gradual with\n",
      "respect to image appearance. Consequently, the crucial point of image set\n",
      "recognition is to mine the intrinsic connection or structural information from\n",
      "the image batches with variations. In this work, a Discriminant Residual\n",
      "Analysis (DRA) method is proposed to improve the classification performance by\n",
      "discovering discriminant features in related and unrelated groups.\n",
      "Specifically, DRA attempts to obtain a powerful projection which casts the\n",
      "residual representations into a discriminant subspace. Such a projection\n",
      "subspace is expected to magnify the useful information of the input space as\n",
      "much as possible, then the relation between the training set and the test set\n",
      "described by the given metric or distance will be more precise in the\n",
      "discriminant subspace. We also propose a nonfeasance strategy by defining\n",
      "another approach to construct the unrelated groups, which help to reduce\n",
      "furthermore the cost of sampling errors. Two regularization approaches are used\n",
      "to deal with the probable small sample size problem. Extensive experiments are\n",
      "conducted on benchmark databases, and the results show superiority and\n",
      "efficiency of the new methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.13299 \n",
      "Title :Learning to Match Jobs with Resumes from Sparse Interaction Data using\n",
      "  Multi-View Co-Teaching Network\n",
      "  With the ever-increasing growth of online recruitment data, job-resume\n",
      "matching has become an important task to automatically match jobs with suitable\n",
      "resumes. This task is typically casted as a supervised text matching problem.\n",
      "Supervised learning is powerful when the labeled data is sufficient. However,\n",
      "on online recruitment platforms, job-resume interaction data is sparse and\n",
      "noisy, which affects the performance of job-resume match algorithms. To\n",
      "alleviate these problems, in this paper, we propose a novel multi-view\n",
      "co-teaching network from sparse interaction data for job-resume matching. Our\n",
      "network consists of two major components, namely text-based matching model and\n",
      "relation-based matching model. The two parts capture semantic compatibility in\n",
      "two different views, and complement each other. In order to address the\n",
      "challenges from sparse and noisy data, we design two specific strategies to\n",
      "combine the two components. First, two components share the learned parameters\n",
      "or representations, so that the original representations of each component can\n",
      "be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the\n",
      "influence of noise in training data. The core idea is to let the two components\n",
      "help each other by selecting more reliable training instances. The two\n",
      "strategies focus on representation enhancement and data enhancement,\n",
      "respectively. Compared with pure text-based matching models, the proposed\n",
      "approach is able to learn better data representations from limited or even\n",
      "sparse interaction data, which is more resistible to noise in training data.\n",
      "Experiment results have demonstrated that our model is able to outperform\n",
      "state-of-the-art methods for job-resume matching.\n",
      "\n",
      "**Paper Id :2002.07366 \n",
      "Title :Adversarial Deep Network Embedding for Cross-network Node Classification\n",
      "  In this paper, the task of cross-network node classification, which leverages\n",
      "the abundant labeled nodes from a source network to help classify unlabeled\n",
      "nodes in a target network, is studied. The existing domain adaptation\n",
      "algorithms generally fail to model the network structural information, and the\n",
      "current network embedding models mainly focus on single-network applications.\n",
      "Thus, both of them cannot be directly applied to solve the cross-network node\n",
      "classification problem. This motivates us to propose an adversarial\n",
      "cross-network deep network embedding (ACDNE) model to integrate adversarial\n",
      "domain adaptation with deep network embedding so as to learn network-invariant\n",
      "node representations that can also well preserve the network structural\n",
      "information. In ACDNE, the deep network embedding module utilizes two feature\n",
      "extractors to jointly preserve attributed affinity and topological proximities\n",
      "between nodes. In addition, a node classifier is incorporated to make node\n",
      "representations label-discriminative. Moreover, an adversarial domain\n",
      "adaptation technique is employed to make node representations\n",
      "network-invariant. Extensive experimental results demonstrate that the proposed\n",
      "ACDNE model achieves the state-of-the-art performance in cross-network node\n",
      "classification.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2009.13984 \n",
      "Title :The design and implementation of Language Learning Chatbot with XAI\n",
      "  using Ontology and Transfer Learning\n",
      "  In this paper, we proposed a transfer learning-based English language\n",
      "learning chatbot, whose output generated by GPT-2 can be explained by\n",
      "corresponding ontology graph rooted by fine-tuning dataset. We design three\n",
      "levels for systematically English learning, including phonetics level for\n",
      "speech recognition and pronunciation correction, semantic level for specific\n",
      "domain conversation, and the simulation of free-style conversation in English -\n",
      "the highest level of language chatbot communication as free-style conversation\n",
      "agent. For academic contribution, we implement the ontology graph to explain\n",
      "the performance of free-style conversation, following the concept of XAI\n",
      "(Explainable Artificial Intelligence) to visualize the connections of neural\n",
      "network in bionics, and explain the output sentence from language model. From\n",
      "implementation perspective, our Language Learning agent integrated the\n",
      "mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer\n",
      "learning as back-end to interpret the responses by ontology graph.\n",
      "\n",
      "**Paper Id :2007.16011 \n",
      "Title :Neural Machine Translation model for University Email Application\n",
      "  Machine translation has many applications such as news translation, email\n",
      "translation, official letter translation etc. Commercial translators, e.g.\n",
      "Google Translation lags in regional vocabulary and are unable to learn the\n",
      "bilingual text in the source and target languages within the input. In this\n",
      "paper, a regional vocabulary-based application-oriented Neural Machine\n",
      "Translation (NMT) model is proposed over the data set of emails used at the\n",
      "University for communication over a period of three years. A state-of-the-art\n",
      "Sequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is\n",
      "compared with Google Translate using Gated Recurrent Unit Recurrent Neural\n",
      "Network machine translation model with attention decoder. The low BLEU score of\n",
      "Google Translation in comparison to our model indicates that the application\n",
      "based regional models are better. The low BLEU score of EN -> ML of our model\n",
      "and Google Translation indicates that the Malay Language has complex language\n",
      "features corresponding to English.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.00081 \n",
      "Title :Stage-wise Conservative Linear Bandits\n",
      "  We study stage-wise conservative linear stochastic bandits: an instance of\n",
      "bandit optimization, which accounts for (unknown) safety constraints that\n",
      "appear in applications such as online advertising and medical trials. At each\n",
      "stage, the learner must choose actions that not only maximize cumulative reward\n",
      "across the entire time horizon but further satisfy a linear baseline constraint\n",
      "that takes the form of a lower bound on the instantaneous reward. For this\n",
      "problem, we present two novel algorithms, stage-wise conservative linear\n",
      "Thompson Sampling (SCLTS) and stage-wise conservative linear UCB (SCLUCB), that\n",
      "respect the baseline constraints and enjoy probabilistic regret bounds of order\n",
      "O(\\sqrt{T} \\log^{3/2}T) and O(\\sqrt{T} \\log T), respectively. Notably, the\n",
      "proposed algorithms can be adjusted with only minor modifications to tackle\n",
      "different problem variations, such as constraints with bandit-feedback, or an\n",
      "unknown sequence of baseline actions. We discuss these and other improvements\n",
      "over the state-of-the-art. For instance, compared to existing solutions, we\n",
      "show that SCLTS plays the (non-optimal) baseline action at most O(\\log{T})\n",
      "times (compared to O(\\sqrt{T})). Finally, we make connections to another\n",
      "studied form of safety constraints that takes the form of an upper bound on the\n",
      "instantaneous reward. While this incurs additional complexity to the learning\n",
      "process as the optimal action is not guaranteed to belong to the safe set at\n",
      "each round, we show that SCLUCB can properly adjust in this setting via a\n",
      "simple modification.\n",
      "\n",
      "**Paper Id :1912.03517 \n",
      "Title :No-Regret Exploration in Goal-Oriented Reinforcement Learning\n",
      "  Many popular reinforcement learning problems (e.g., navigation in a maze,\n",
      "some Atari games, mountain car) are instances of the episodic setting under its\n",
      "stochastic shortest path (SSP) formulation, where an agent has to achieve a\n",
      "goal state while minimizing the cumulative cost. Despite the popularity of this\n",
      "setting, the exploration-exploitation dilemma has been sparsely studied in\n",
      "general SSP problems, with most of the theoretical literature focusing on\n",
      "different problems (i.e., fixed-horizon and infinite-horizon) or making the\n",
      "restrictive loop-free SSP assumption (i.e., no state can be visited twice\n",
      "during an episode). In this paper, we study the general SSP problem with no\n",
      "assumption on its dynamics (some policies may actually never reach the goal).\n",
      "We introduce UC-SSP, the first no-regret algorithm in this setting, and prove a\n",
      "regret bound scaling as $\\displaystyle \\widetilde{\\mathcal{O}}( D S \\sqrt{ A D\n",
      "K})$ after $K$ episodes for any unknown SSP with $S$ states, $A$ actions,\n",
      "positive costs and SSP-diameter $D$, defined as the smallest expected hitting\n",
      "time from any starting state to the goal. We achieve this result by crafting a\n",
      "novel stopping rule, such that UC-SSP may interrupt the current policy if it is\n",
      "taking too long to achieve the goal and switch to alternative policies that are\n",
      "designed to rapidly terminate the episode.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.00730 \n",
      "Title :Modifying the Symbolic Aggregate Approximation Method to Capture Segment\n",
      "  Trend Information\n",
      "  The Symbolic Aggregate approXimation (SAX) is a very popular symbolic\n",
      "dimensionality reduction technique of time series data, as it has several\n",
      "advantages over other dimensionality reduction techniques. One of its major\n",
      "advantages is its efficiency, as it uses precomputed distances. The other main\n",
      "advantage is that in SAX the distance measure defined on the reduced space\n",
      "lower bounds the distance measure defined on the original space. This enables\n",
      "SAX to return exact results in query-by-content tasks. Yet SAX has an inherent\n",
      "drawback, which is its inability to capture segment trend information. Several\n",
      "researchers have attempted to enhance SAX by proposing modifications to include\n",
      "trend information. However, this comes at the expense of giving up on one or\n",
      "more of the advantages of SAX. In this paper we investigate three modifications\n",
      "of SAX to add trend capturing ability to it. These modifications retain the\n",
      "same features of SAX in terms of simplicity, efficiency, as well as the exact\n",
      "results it returns. They are simple procedures based on a different\n",
      "segmentation of the time series than that used in classic-SAX. We test the\n",
      "performance of these three modifications on 45 time series datasets of\n",
      "different sizes, dimensions, and nature, on a classification task and we\n",
      "compare it to that of classic-SAX. The results we obtained show that one of\n",
      "these modifications manages to outperform classic-SAX and that another one\n",
      "slightly gives better results than classic-SAX.\n",
      "\n",
      "**Paper Id :2010.00732 \n",
      "Title :Extreme-SAX: Extreme Points Based Symbolic Representation for Time\n",
      "  Series Classification\n",
      "  Time series classification is an important problem in data mining with\n",
      "several applications in different domains. Because time series data are usually\n",
      "high dimensional, dimensionality reduction techniques have been proposed as an\n",
      "efficient approach to lower their dimensionality. One of the most popular\n",
      "dimensionality reduction techniques of time series data is the Symbolic\n",
      "Aggregate Approximation (SAX), which is inspired by algorithms from text mining\n",
      "and bioinformatics. SAX is simple and efficient because it uses precomputed\n",
      "distances. The disadvantage of SAX is its inability to accurately represent\n",
      "important points in the time series. In this paper we present Extreme-SAX\n",
      "(E-SAX), which uses only the extreme points of each segment to represent the\n",
      "time series. E-SAX has exactly the same simplicity and efficiency of the\n",
      "original SAX, yet it gives better results in time series classification than\n",
      "the original SAX, as we show in extensive experiments on a variety of time\n",
      "series datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.00732 \n",
      "Title :Extreme-SAX: Extreme Points Based Symbolic Representation for Time\n",
      "  Series Classification\n",
      "  Time series classification is an important problem in data mining with\n",
      "several applications in different domains. Because time series data are usually\n",
      "high dimensional, dimensionality reduction techniques have been proposed as an\n",
      "efficient approach to lower their dimensionality. One of the most popular\n",
      "dimensionality reduction techniques of time series data is the Symbolic\n",
      "Aggregate Approximation (SAX), which is inspired by algorithms from text mining\n",
      "and bioinformatics. SAX is simple and efficient because it uses precomputed\n",
      "distances. The disadvantage of SAX is its inability to accurately represent\n",
      "important points in the time series. In this paper we present Extreme-SAX\n",
      "(E-SAX), which uses only the extreme points of each segment to represent the\n",
      "time series. E-SAX has exactly the same simplicity and efficiency of the\n",
      "original SAX, yet it gives better results in time series classification than\n",
      "the original SAX, as we show in extensive experiments on a variety of time\n",
      "series datasets.\n",
      "\n",
      "**Paper Id :2010.00730 \n",
      "Title :Modifying the Symbolic Aggregate Approximation Method to Capture Segment\n",
      "  Trend Information\n",
      "  The Symbolic Aggregate approXimation (SAX) is a very popular symbolic\n",
      "dimensionality reduction technique of time series data, as it has several\n",
      "advantages over other dimensionality reduction techniques. One of its major\n",
      "advantages is its efficiency, as it uses precomputed distances. The other main\n",
      "advantage is that in SAX the distance measure defined on the reduced space\n",
      "lower bounds the distance measure defined on the original space. This enables\n",
      "SAX to return exact results in query-by-content tasks. Yet SAX has an inherent\n",
      "drawback, which is its inability to capture segment trend information. Several\n",
      "researchers have attempted to enhance SAX by proposing modifications to include\n",
      "trend information. However, this comes at the expense of giving up on one or\n",
      "more of the advantages of SAX. In this paper we investigate three modifications\n",
      "of SAX to add trend capturing ability to it. These modifications retain the\n",
      "same features of SAX in terms of simplicity, efficiency, as well as the exact\n",
      "results it returns. They are simple procedures based on a different\n",
      "segmentation of the time series than that used in classic-SAX. We test the\n",
      "performance of these three modifications on 45 time series datasets of\n",
      "different sizes, dimensions, and nature, on a classification task and we\n",
      "compare it to that of classic-SAX. The results we obtained show that one of\n",
      "these modifications manages to outperform classic-SAX and that another one\n",
      "slightly gives better results than classic-SAX.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.00844 \n",
      "Title :Linear Classifier Combination via Multiple Potential Functions\n",
      "  A vital aspect of the classification based model construction process is the\n",
      "calibration of the scoring function. One of the weaknesses of the calibration\n",
      "process is that it does not take into account the information about the\n",
      "relative positions of the recognized objects in the feature space. To alleviate\n",
      "this limitation, in this paper, we propose a novel concept of calculating a\n",
      "scoring function based on the distance of the object from the decision boundary\n",
      "and its distance to the class centroid. An important property is that the\n",
      "proposed score function has the same nature for all linear base classifiers,\n",
      "which means that outputs of these classifiers are equally represented and have\n",
      "the same meaning. The proposed approach is compared with other ensemble\n",
      "algorithms and experiments on multiple Keel datasets demonstrate the\n",
      "effectiveness of our method. To discuss the results of our experiments, we use\n",
      "multiple classification performance measures and statistical analysis.\n",
      "\n",
      "**Paper Id :2003.11804 \n",
      "Title :Active Learning Approach to Optimization of Experimental Control\n",
      "  In this work we present a general machine learning based scheme to optimize\n",
      "experimental control. The method utilizes the neural network to learn the\n",
      "relation between the control parameters and the control goal, with which the\n",
      "optimal control parameters can be obtained. The main challenge of this approach\n",
      "is that the labeled data obtained from experiments are not abundant. The\n",
      "central idea of our scheme is to use the active learning to overcome this\n",
      "difficulty. As a demonstration example, we apply our method to control\n",
      "evaporative cooling experiments in cold atoms. We have first tested our method\n",
      "with simulated data and then applied our method to real experiments. We\n",
      "demonstrate that our method can successfully reach the best performance within\n",
      "hundreds of experimental runs. Our method does not require knowledge of the\n",
      "experimental system as a prior and is universal for experimental control in\n",
      "different systems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.01309 \n",
      "Title :Personality Trait Detection Using Bagged SVM over BERT Word Embedding\n",
      "  Ensembles\n",
      "  Recently, the automatic prediction of personality traits has received\n",
      "increasing attention and has emerged as a hot topic within the field of\n",
      "affective computing. In this work, we present a novel deep learning-based\n",
      "approach for automated personality detection from text. We leverage state of\n",
      "the art advances in natural language understanding, namely the BERT language\n",
      "model to extract contextualized word embeddings from textual data for automated\n",
      "author personality detection. Our primary goal is to develop a computationally\n",
      "efficient, high-performance personality prediction model which can be easily\n",
      "used by a large number of people without access to huge computation resources.\n",
      "Our extensive experiments with this ideology in mind, led us to develop a novel\n",
      "model which feeds contextualized embeddings along with psycholinguistic\n",
      "features toa Bagged-SVM classifier for personality trait prediction. Our model\n",
      "outperforms the previous state of the art by 1.04% and, at the same time is\n",
      "significantly more computationally efficient to train. We report our results on\n",
      "the famous gold standard Essays dataset for personality detection.\n",
      "\n",
      "**Paper Id :1904.08064 \n",
      "Title :Forecasting with time series imaging\n",
      "  Feature-based time series representations have attracted substantial\n",
      "attention in a wide range of time series analysis methods. Recently, the use of\n",
      "time series features for forecast model averaging has been an emerging research\n",
      "focus in the forecasting community. Nonetheless, most of the existing\n",
      "approaches depend on the manual choice of an appropriate set of features.\n",
      "Exploiting machine learning methods to extract features from time series\n",
      "automatically becomes crucial in state-of-the-art time series analysis. In this\n",
      "paper, we introduce an automated approach to extract time series features based\n",
      "on time series imaging. We first transform time series into recurrence plots,\n",
      "from which local features can be extracted using computer vision algorithms.\n",
      "The extracted features are used for forecast model averaging. Our experiments\n",
      "show that forecasting based on automatically extracted features, with less\n",
      "human intervention and a more comprehensive view of the raw time series data,\n",
      "yields highly comparable performances with the best methods in the largest\n",
      "forecasting competition dataset (M4) and outperforms the top methods in the\n",
      "Tourism forecasting competition dataset.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.01342 \n",
      "Title :End-to-End Training of CNN Ensembles for Person Re-Identification\n",
      "  We propose an end-to-end ensemble method for person re-identification (ReID)\n",
      "to address the problem of overfitting in discriminative models. These models\n",
      "are known to converge easily, but they are biased to the training data in\n",
      "general and may produce a high model variance, which is known as overfitting.\n",
      "The ReID task is more prone to this problem due to the large discrepancy\n",
      "between training and test distributions. To address this problem, our proposed\n",
      "ensemble learning framework produces several diverse and accurate base learners\n",
      "in a single DenseNet. Since most of the costly dense blocks are shared, our\n",
      "method is computationally efficient, which makes it favorable compared to the\n",
      "conventional ensemble models. Experiments on several benchmark datasets\n",
      "demonstrate that our method achieves state-of-the-art results. Noticeable\n",
      "performance improvements, especially on relatively small datasets, indicate\n",
      "that the proposed method deals with the overfitting problem effectively.\n",
      "\n",
      "**Paper Id :1911.03437 \n",
      "Title :SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language\n",
      "  Models through Principled Regularized Optimization\n",
      "  Transfer learning has fundamentally changed the landscape of natural language\n",
      "processing (NLP) research. Many existing state-of-the-art models are first\n",
      "pre-trained on a large text corpus and then fine-tuned on downstream tasks.\n",
      "However, due to limited data resources from downstream tasks and the extremely\n",
      "large capacity of pre-trained models, aggressive fine-tuning often causes the\n",
      "adapted model to overfit the data of downstream tasks and forget the knowledge\n",
      "of the pre-trained model. To address the above issue in a more principled\n",
      "manner, we propose a new computational framework for robust and efficient\n",
      "fine-tuning for pre-trained language models. Specifically, our proposed\n",
      "framework contains two important ingredients: 1. Smoothness-inducing\n",
      "regularization, which effectively manages the capacity of the model; 2. Bregman\n",
      "proximal point optimization, which is a class of trust-region methods and can\n",
      "prevent knowledge forgetting. Our experiments demonstrate that our proposed\n",
      "method achieves the state-of-the-art performance on multiple NLP benchmarks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.01478 \n",
      "Title :Explanation Ontology in Action: A Clinical Use-Case\n",
      "  We addressed the problem of a lack of semantic representation for\n",
      "user-centric explanations and different explanation types in our Explanation\n",
      "Ontology (https://purl.org/heals/eo). Such a representation is increasingly\n",
      "necessary as explainability has become an important problem in Artificial\n",
      "Intelligence with the emergence of complex methods and an uptake in\n",
      "high-precision and user-facing settings. In this submission, we provide\n",
      "step-by-step guidance for system designers to utilize our ontology, introduced\n",
      "in our resource track paper, to plan and model for explanations during the\n",
      "design of their Artificial Intelligence systems. We also provide a detailed\n",
      "example with our utilization of this guidance in a clinical setting.\n",
      "\n",
      "**Paper Id :2010.01479 \n",
      "Title :Explanation Ontology: A Model of Explanations for User-Centered AI\n",
      "  Explainability has been a goal for Artificial Intelligence (AI) systems since\n",
      "their conception, with the need for explainability growing as more complex AI\n",
      "models are increasingly used in critical, high-stakes settings such as\n",
      "healthcare. Explanations have often added to an AI system in a non-principled,\n",
      "post-hoc manner. With greater adoption of these systems and emphasis on\n",
      "user-centric explainability, there is a need for a structured representation\n",
      "that treats explainability as a primary consideration, mapping end user needs\n",
      "to specific explanation types and the system's AI capabilities. We design an\n",
      "explanation ontology to model both the role of explanations, accounting for the\n",
      "system and user attributes in the process, and the range of different\n",
      "literature-derived explanation types. We indicate how the ontology can support\n",
      "user requirements for explanations in the domain of healthcare. We evaluate our\n",
      "ontology with a set of competency questions geared towards a system designer\n",
      "who might use our ontology to decide which explanation types to include, given\n",
      "a combination of users' needs and a system's capabilities, both in system\n",
      "design settings and in real-time operations. Through the use of this ontology,\n",
      "system designers will be able to make informed choices on which explanations AI\n",
      "systems can and should provide.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.01479 \n",
      "Title :Explanation Ontology: A Model of Explanations for User-Centered AI\n",
      "  Explainability has been a goal for Artificial Intelligence (AI) systems since\n",
      "their conception, with the need for explainability growing as more complex AI\n",
      "models are increasingly used in critical, high-stakes settings such as\n",
      "healthcare. Explanations have often added to an AI system in a non-principled,\n",
      "post-hoc manner. With greater adoption of these systems and emphasis on\n",
      "user-centric explainability, there is a need for a structured representation\n",
      "that treats explainability as a primary consideration, mapping end user needs\n",
      "to specific explanation types and the system's AI capabilities. We design an\n",
      "explanation ontology to model both the role of explanations, accounting for the\n",
      "system and user attributes in the process, and the range of different\n",
      "literature-derived explanation types. We indicate how the ontology can support\n",
      "user requirements for explanations in the domain of healthcare. We evaluate our\n",
      "ontology with a set of competency questions geared towards a system designer\n",
      "who might use our ontology to decide which explanation types to include, given\n",
      "a combination of users' needs and a system's capabilities, both in system\n",
      "design settings and in real-time operations. Through the use of this ontology,\n",
      "system designers will be able to make informed choices on which explanations AI\n",
      "systems can and should provide.\n",
      "\n",
      "**Paper Id :2010.01478 \n",
      "Title :Explanation Ontology in Action: A Clinical Use-Case\n",
      "  We addressed the problem of a lack of semantic representation for\n",
      "user-centric explanations and different explanation types in our Explanation\n",
      "Ontology (https://purl.org/heals/eo). Such a representation is increasingly\n",
      "necessary as explainability has become an important problem in Artificial\n",
      "Intelligence with the emergence of complex methods and an uptake in\n",
      "high-precision and user-facing settings. In this submission, we provide\n",
      "step-by-step guidance for system designers to utilize our ontology, introduced\n",
      "in our resource track paper, to plan and model for explanations during the\n",
      "design of their Artificial Intelligence systems. We also provide a detailed\n",
      "example with our utilization of this guidance in a clinical setting.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.01804 \n",
      "Title :Graph Cross Networks with Vertex Infomax Pooling\n",
      "  We propose a novel graph cross network (GXN) to achieve comprehensive feature\n",
      "learning from multiple scales of a graph. Based on trainable hierarchical\n",
      "representations of a graph, GXN enables the interchange of intermediate\n",
      "features across scales to promote information flow. Two key ingredients of GXN\n",
      "include a novel vertex infomax pooling (VIPool), which creates multiscale\n",
      "graphs in a trainable manner, and a novel feature-crossing layer, enabling\n",
      "feature interchange across scales. The proposed VIPool selects the most\n",
      "informative subset of vertices based on the neural estimation of mutual\n",
      "information between vertex features and neighborhood features. The intuition\n",
      "behind is that a vertex is informative when it can maximally reflect its\n",
      "neighboring information. The proposed feature-crossing layer fuses intermediate\n",
      "features between two scales for mutual enhancement by improving information\n",
      "flow and enriching multiscale features at hidden layers. The cross shape of the\n",
      "feature-crossing layer distinguishes GXN from many other multiscale\n",
      "architectures. Experimental results show that the proposed GXN improves the\n",
      "classification accuracy by 2.12% and 1.15% on average for graph classification\n",
      "and vertex classification, respectively. Based on the same network, the\n",
      "proposed VIPool consistently outperforms other graph-pooling methods.\n",
      "\n",
      "**Paper Id :2001.05313 \n",
      "Title :Tensor Graph Convolutional Networks for Text Classification\n",
      "  Compared to sequential learning models, graph-based neural networks exhibit\n",
      "some excellent properties, such as ability capturing global information. In\n",
      "this paper, we investigate graph-based neural networks for text classification\n",
      "problem. A new framework TensorGCN (tensor graph convolutional networks), is\n",
      "presented for this task. A text graph tensor is firstly constructed to describe\n",
      "semantic, syntactic, and sequential contextual information. Then, two kinds of\n",
      "propagation learning perform on the text graph tensor. The first is intra-graph\n",
      "propagation used for aggregating information from neighborhood nodes in a\n",
      "single graph. The second is inter-graph propagation used for harmonizing\n",
      "heterogeneous information between graphs. Extensive experiments are conducted\n",
      "on benchmark datasets, and the results illustrate the effectiveness of our\n",
      "proposed framework. Our proposed TensorGCN presents an effective way to\n",
      "harmonize and integrate heterogeneous information from different kinds of\n",
      "graphs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.02004 \n",
      "Title :Assessing Robustness of Text Classification through Maximal Safe Radius\n",
      "  Computation\n",
      "  Neural network NLP models are vulnerable to small modifications of the input\n",
      "that maintain the original meaning but result in a different prediction. In\n",
      "this paper, we focus on robustness of text classification against word\n",
      "substitutions, aiming to provide guarantees that the model prediction does not\n",
      "change if a word is replaced with a plausible alternative, such as a synonym.\n",
      "As a measure of robustness, we adopt the notion of the maximal safe radius for\n",
      "a given input text, which is the minimum distance in the embedding space to the\n",
      "decision boundary. Since computing the exact maximal safe radius is not\n",
      "feasible in practice, we instead approximate it by computing a lower and upper\n",
      "bound. For the upper bound computation, we employ Monte Carlo Tree Search in\n",
      "conjunction with syntactic filtering to analyse the effect of single and\n",
      "multiple word substitutions. The lower bound computation is achieved through an\n",
      "adaptation of the linear bounding techniques implemented in tools CNN-Cert and\n",
      "POPQORN, respectively for convolutional and recurrent network models. We\n",
      "evaluate the methods on sentiment analysis and news classification models for\n",
      "four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and\n",
      "provide an analysis of robustness trends. We also apply our framework to\n",
      "interpretability analysis and compare it with LIME.\n",
      "\n",
      "**Paper Id :2007.03511 \n",
      "Title :Estimating Generalization under Distribution Shifts via Domain-Invariant\n",
      "  Representations\n",
      "  When machine learning models are deployed on a test distribution different\n",
      "from the training distribution, they can perform poorly, but overestimate their\n",
      "performance. In this work, we aim to better estimate a model's performance\n",
      "under distribution shift, without supervision. To do so, we use a set of\n",
      "domain-invariant predictors as a proxy for the unknown, true target labels.\n",
      "Since the error of the resulting risk estimate depends on the target risk of\n",
      "the proxy model, we study generalization of domain-invariant representations\n",
      "and show that the complexity of the latent representation has a significant\n",
      "influence on the target risk. Empirically, our approach (1) enables self-tuning\n",
      "of domain adaptation models, and (2) accurately estimates the target error of\n",
      "given models under distribution shift. Other applications include model\n",
      "selection, deciding early stopping and error detection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.02168 \n",
      "Title :Identification of Anomalous Diffusion Sources by Unsupervised Learning\n",
      "  Fractional Brownian motion (fBm) is a ubiquitous diffusion process in which\n",
      "the memory effects of the stochastic transport result in the mean squared\n",
      "particle displacement following a power law, $\\langle {\\Delta r}^2 \\rangle \\sim\n",
      "t^{\\alpha}$, where the diffusion exponent $\\alpha$ characterizes whether the\n",
      "transport is subdiffusive, ($\\alpha<1$), diffusive ($\\alpha = 1$), or\n",
      "superdiffusive, ($\\alpha >1$). Due to the abundance of fBm processes in nature,\n",
      "significant efforts have been devoted to the identification and\n",
      "characterization of fBm sources in various phenomena. In practice, the\n",
      "identification of the fBm sources often relies on solving a complex and\n",
      "ill-posed inverse problem based on limited observed data. In the general case,\n",
      "the detected signals are formed by an unknown number of release sources,\n",
      "located at different locations and with different strengths, that act\n",
      "simultaneously. This means that the observed data is composed of mixtures of\n",
      "releases from an unknown number of sources, which makes the traditional inverse\n",
      "modeling approaches unreliable. Here, we report an unsupervised learning\n",
      "method, based on Nonnegative Matrix Factorization, that enables the\n",
      "identification of the unknown number of release sources as well the anomalous\n",
      "diffusion characteristics based on limited observed data and the general form\n",
      "of the corresponding fBm Green's function. We show that our method performs\n",
      "accurately for different types of sources and configurations with a\n",
      "predetermined number of sources with specific characteristics and introduced\n",
      "noise.\n",
      "\n",
      "**Paper Id :1910.06948 \n",
      "Title :Data-Driven Deep Learning of Partial Differential Equations in Modal\n",
      "  Space\n",
      "  We present a framework for recovering/approximating unknown time-dependent\n",
      "partial differential equation (PDE) using its solution data. Instead of\n",
      "identifying the terms in the underlying PDE, we seek to approximate the\n",
      "evolution operator of the underlying PDE numerically. The evolution operator of\n",
      "the PDE, defined in infinite-dimensional space, maps the solution from a\n",
      "current time to a future time and completely characterizes the solution\n",
      "evolution of the underlying unknown PDE. Our recovery strategy relies on\n",
      "approximation of the evolution operator in a properly defined modal space,\n",
      "i.e., generalized Fourier space, in order to reduce the problem to finite\n",
      "dimensions. The finite dimensional approximation is then accomplished by\n",
      "training a deep neural network structure, which is based on residual network\n",
      "(ResNet), using the given data. Error analysis is provided to illustrate the\n",
      "predictive accuracy of the proposed method. A set of examples of different\n",
      "types of PDEs, including inviscid Burgers' equation that develops discontinuity\n",
      "in its solution, are presented to demonstrate the effectiveness of the proposed\n",
      "method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.02322 \n",
      "Title :SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup\n",
      "  Active learning is an important technique for low-resource sequence labeling\n",
      "tasks. However, current active sequence labeling methods use the queried\n",
      "samples alone in each iteration, which is an inefficient way of leveraging\n",
      "human annotations. We propose a simple but effective data augmentation method\n",
      "to improve the label efficiency of active sequence labeling. Our method,\n",
      "SeqMix, simply augments the queried samples by generating extra labeled\n",
      "sequences in each iteration. The key difficulty is to generate plausible\n",
      "sequences along with token-level labels. In SeqMix, we address this challenge\n",
      "by performing mixup for both sequences and token-level labels of the queried\n",
      "samples. Furthermore, we design a discriminator during sequence mixup, which\n",
      "judges whether the generated sequences are plausible or not. Our experiments on\n",
      "Named Entity Recognition and Event Detection tasks show that SeqMix can improve\n",
      "the standard active sequence labeling method by $2.27\\%$--$3.75\\%$ in terms of\n",
      "$F_1$ scores. The code and data for SeqMix can be found at\n",
      "https://github.com/rz-zhang/SeqMix\n",
      "\n",
      "**Paper Id :2003.09831 \n",
      "Title :Prior Knowledge Driven Label Embedding for Slot Filling in Natural\n",
      "  Language Understanding\n",
      "  Traditional slot filling in natural language understanding (NLU) predicts a\n",
      "one-hot vector for each word. This form of label representation lacks semantic\n",
      "correlation modelling, which leads to severe data sparsity problem, especially\n",
      "when adapting an NLU model to a new domain. To address this issue, a novel\n",
      "label embedding based slot filling framework is proposed in this paper. Here,\n",
      "distributed label embedding is constructed for each slot using prior knowledge.\n",
      "Three encoding methods are investigated to incorporate different kinds of prior\n",
      "knowledge about slots: atomic concepts, slot descriptions, and slot exemplars.\n",
      "The proposed label embeddings tend to share text patterns and reuses data with\n",
      "different slot labels. This makes it useful for adaptive NLU with limited data.\n",
      "Also, since label embedding is independent of NLU model, it is compatible with\n",
      "almost all deep learning based slot filling models. The proposed approaches are\n",
      "evaluated on three datasets. Experiments on single domain and domain adaptation\n",
      "tasks show that label embedding achieves significant performance improvement\n",
      "over traditional one-hot label representation as well as advanced zero-shot\n",
      "approaches.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.02458 \n",
      "Title :Identifying Spurious Correlations for Robust Text Classification\n",
      "  The predictions of text classifiers are often driven by spurious correlations\n",
      "-- e.g., the term `Spielberg' correlates with positively reviewed movies, even\n",
      "though the term itself does not semantically convey a positive sentiment. In\n",
      "this paper, we propose a method to distinguish spurious and genuine\n",
      "correlations in text classification. We treat this as a supervised\n",
      "classification problem, using features derived from treatment effect estimators\n",
      "to distinguish spurious correlations from \"genuine\" ones. Due to the generic\n",
      "nature of these features and their small dimensionality, we find that the\n",
      "approach works well even with limited training examples, and that it is\n",
      "possible to transport the word classifier to new domains. Experiments on four\n",
      "datasets (sentiment classification and toxicity detection) suggest that using\n",
      "this approach to inform feature selection also leads to more robust\n",
      "classification, as measured by improved worst-case accuracy on the samples\n",
      "affected by spurious correlations.\n",
      "\n",
      "**Paper Id :1911.06147 \n",
      "Title :t-SS3: a text classifier with dynamic n-grams for early risk detection\n",
      "  over text streams\n",
      "  A recently introduced classifier, called SS3, has shown to be well suited to\n",
      "deal with early risk detection (ERD) problems on text streams. It obtained\n",
      "state-of-the-art performance on early depression and anorexia detection on\n",
      "Reddit in the CLEF's eRisk open tasks. SS3 was created to deal with ERD\n",
      "problems naturally since: it supports incremental training and classification\n",
      "over text streams, and it can visually explain its rationale. However, SS3\n",
      "processes the input using a bag-of-word model lacking the ability to recognize\n",
      "important word sequences. This aspect could negatively affect the\n",
      "classification performance and also reduces the descriptiveness of visual\n",
      "explanations. In the standard document classification field, it is very common\n",
      "to use word n-grams to try to overcome some of these limitations.\n",
      "Unfortunately, when working with text streams, using n-grams is not trivial\n",
      "since the system must learn and recognize which n-grams are important \"on the\n",
      "fly\". This paper introduces t-SS3, an extension of SS3 that allows it to\n",
      "recognize useful patterns over text streams dynamically. We evaluated our model\n",
      "in the eRisk 2017 and 2018 tasks on early depression and anorexia detection.\n",
      "Experimental results suggest that t-SS3 is able to improve both current results\n",
      "and the richness of visual explanations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.02477 \n",
      "Title :A Unified Deep Learning Framework for Short-Duration Speaker\n",
      "  Verification in Adverse Environments\n",
      "  Speaker verification (SV) has recently attracted considerable research\n",
      "interest due to the growing popularity of virtual assistants. At the same time,\n",
      "there is an increasing requirement for an SV system: it should be robust to\n",
      "short speech segments, especially in noisy and reverberant environments. In\n",
      "this paper, we consider one more important requirement for practical\n",
      "applications: the system should be robust to an audio stream containing long\n",
      "non-speech segments, where a voice activity detection (VAD) is not applied. To\n",
      "meet these two requirements, we introduce feature pyramid module (FPM)-based\n",
      "multi-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present\n",
      "the FPM-based MSA to deal with short speech segments in noisy and reverberant\n",
      "environments. Also, we use the SAS-VAD to increase the robustness to long\n",
      "non-speech segments. To further improve the robustness to acoustic distortions\n",
      "(i.e., noise and reverberation), we apply a masking-based speech enhancement\n",
      "(SE) method. We combine SV, VAD, and SE models in a unified deep learning\n",
      "framework and jointly train the entire network in an end-to-end manner. To the\n",
      "best of our knowledge, this is the first work combining these three models in a\n",
      "deep learning framework. We conduct experiments on Korean indoor (KID) and\n",
      "VoxCeleb datasets, which are corrupted by noise and reverberation. The results\n",
      "show that the proposed method is effective for SV in the challenging conditions\n",
      "and performs better than the baseline i-vector and deep speaker embedding\n",
      "systems.\n",
      "\n",
      "**Paper Id :2008.03687 \n",
      "Title :LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition\n",
      "  Speech synthesis (text to speech, TTS) and recognition (automatic speech\n",
      "recognition, ASR) are important speech tasks, and require a large amount of\n",
      "text and speech pairs for model training. However, there are more than 6,000\n",
      "languages in the world and most languages are lack of speech training data,\n",
      "which poses significant challenges when building TTS and ASR systems for\n",
      "extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and\n",
      "ASR system under the extremely low-resource setting, which can support rare\n",
      "languages with low data cost. LRSpeech consists of three key techniques: 1)\n",
      "pre-training on rich-resource languages and fine-tuning on low-resource\n",
      "languages; 2) dual transformation between TTS and ASR to iteratively boost the\n",
      "accuracy of each other; 3) knowledge distillation to customize the TTS model on\n",
      "a high-quality target-speaker voice and improve the ASR model on multiple\n",
      "voices. We conduct experiments on an experimental language (English) and a\n",
      "truly low-resource language (Lithuanian) to verify the effectiveness of\n",
      "LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for\n",
      "TTS in terms of both intelligibility (more than 98% intelligibility rate) and\n",
      "naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech,\n",
      "which satisfy the requirements for industrial deployment, 2) achieves promising\n",
      "recognition accuracy for ASR, and 3) last but not least, uses extremely\n",
      "low-resource training data. We also conduct comprehensive analyses on LRSpeech\n",
      "with different amounts of data resources, and provide valuable insights and\n",
      "guidances for industrial deployment. We are currently deploying LRSpeech into a\n",
      "commercialized cloud speech service to support TTS on more rare languages.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.02675 \n",
      "Title :Recovering Causal Structures from Low-Order Conditional Independencies\n",
      "  One of the common obstacles for learning causal models from data is that\n",
      "high-order conditional independence (CI) relationships between random variables\n",
      "are difficult to estimate. Since CI tests with conditioning sets of low order\n",
      "can be performed accurately even for a small number of observations, a\n",
      "reasonable approach to determine casual structures is to base merely on the\n",
      "low-order CIs. Recent research has confirmed that, e.g. in the case of sparse\n",
      "true causal models, structures learned even from zero- and first-order\n",
      "conditional independencies yield good approximations of the models. However, a\n",
      "challenging task here is to provide methods that faithfully explain a given set\n",
      "of low-order CIs. In this paper, we propose an algorithm which, for a given set\n",
      "of conditional independencies of order less or equal to $k$, where $k$ is a\n",
      "small fixed number, computes a faithful graphical representation of the given\n",
      "set. Our results complete and generalize the previous work on learning from\n",
      "pairwise marginal independencies. Moreover, they enable to improve upon the 0-1\n",
      "graph model which, e.g. is heavily used in the estimation of genome networks.\n",
      "\n",
      "**Paper Id :1911.01931 \n",
      "Title :Online matrix factorization for Markovian data and applications to\n",
      "  Network Dictionary Learning\n",
      "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\n",
      "learning problems, giving an approximate representation of complex data sets in\n",
      "terms of a reduced number of extracted features. Convergence guarantees for\n",
      "most of the OMF algorithms in the literature assume independence between data\n",
      "matrices, and the case of dependent data streams remains largely unexplored. In\n",
      "this paper, we show that a non-convex generalization of the well-known OMF\n",
      "algorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\n",
      "almost surely to the set of critical points of the expected loss function, even\n",
      "when the data matrices are functions of some underlying Markov chain satisfying\n",
      "a mild mixing condition. This allows one to extract features more efficiently\n",
      "from dependent data streams, as there is no need to subsample the data sequence\n",
      "to approximately satisfy the independence assumption. As the main application,\n",
      "by combining online non-negative matrix factorization and a recent MCMC\n",
      "algorithm for sampling motifs from networks, we propose a novel framework of\n",
      "Network Dictionary Learning, which extracts ``network dictionary patches' from\n",
      "a given network in an online manner that encodes main features of the network.\n",
      "We demonstrate this technique and its application to network denoising problems\n",
      "on real-world network data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.03094 \n",
      "Title :Correlated Differential Privacy: Feature Selection in Machine Learning\n",
      "  Privacy preserving in machine learning is a crucial issue in industry\n",
      "informatics since data used for training in industries usually contain\n",
      "sensitive information. Existing differentially private machine learning\n",
      "algorithms have not considered the impact of data correlation, which may lead\n",
      "to more privacy leakage than expected in industrial applications. For example,\n",
      "data collected for traffic monitoring may contain some correlated records due\n",
      "to temporal correlation or user correlation. To fill this gap, we propose a\n",
      "correlation reduction scheme with differentially private feature selection\n",
      "considering the issue of privacy loss when data have correlation in machine\n",
      "learning tasks. %The key to the proposed scheme is to describe the data\n",
      "correlation and select features which leads to less data correlation across the\n",
      "whole dataset. The proposed scheme involves five steps with the goal of\n",
      "managing the extent of data correlation, preserving the privacy, and supporting\n",
      "accuracy in the prediction results. In this way, the impact of data correlation\n",
      "is relieved with the proposed feature selection scheme, and moreover, the\n",
      "privacy issue of data correlation in learning is guaranteed. The proposed\n",
      "method can be widely used in machine learning algorithms which provide services\n",
      "in industrial areas. Experiments show that the proposed scheme can produce\n",
      "better prediction results with machine learning tasks and fewer mean square\n",
      "errors for data queries compared to existing schemes.\n",
      "\n",
      "**Paper Id :1911.10143 \n",
      "Title :Adversarial Learning of Privacy-Preserving and Task-Oriented\n",
      "  Representations\n",
      "  Data privacy has emerged as an important issue as data-driven deep learning\n",
      "has been an essential component of modern machine learning systems. For\n",
      "instance, there could be a potential privacy risk of machine learning systems\n",
      "via the model inversion attack, whose goal is to reconstruct the input data\n",
      "from the latent representation of deep networks. Our work aims at learning a\n",
      "privacy-preserving and task-oriented representation to defend against such\n",
      "model inversion attacks. Specifically, we propose an adversarial reconstruction\n",
      "learning framework that prevents the latent representations decoded into\n",
      "original input data. By simulating the expected behavior of adversary, our\n",
      "framework is realized by minimizing the negative pixel reconstruction loss or\n",
      "the negative feature reconstruction (i.e., perceptual distance) loss. We\n",
      "validate the proposed method on face attribute prediction, showing that our\n",
      "method allows protecting visual privacy with a small decrease in utility\n",
      "performance. In addition, we show the utility-privacy trade-off with different\n",
      "choices of hyperparameter for negative perceptual distance loss at training,\n",
      "allowing service providers to determine the right level of privacy-protection\n",
      "with a certain utility performance. Moreover, we provide an extensive study\n",
      "with different selections of features, tasks, and the data to further analyze\n",
      "their influence on privacy protection.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.03204 \n",
      "Title :Cardiac Arrhythmia Detection from ECG with Convolutional Recurrent\n",
      "  Neural Networks\n",
      "  Except for a few specific types, cardiac arrhythmias are not immediately\n",
      "life-threatening. However, if not treated appropriately, they can cause serious\n",
      "complications. In particular, atrial fibrillation, which is characterized by\n",
      "fast and irregular heart beats, increases the risk of stroke. We propose three\n",
      "neural network architectures to detect abnormal rhythms from single-lead ECG\n",
      "signals. These architectures combine convolutional layers to extract high-level\n",
      "features pertinent for arrhythmia detection from sliding windows and recurrent\n",
      "layers to aggregate these features over signals of varying durations. We\n",
      "applied the neural networks to the dataset used for the challenge of Computing\n",
      "in Cardiology 2017 and a dataset built by joining three databases available on\n",
      "PhysioNet. Our architectures achieved an accuracy of 86.23% on the first\n",
      "dataset, similar to the winning entries of the challenge, and an accuracy of\n",
      "92.02% on the second dataset.\n",
      "\n",
      "**Paper Id :1904.01949 \n",
      "Title :Automatic diagnosis of the 12-lead ECG using a deep neural network\n",
      "  The role of automatic electrocardiogram (ECG) analysis in clinical practice\n",
      "is limited by the accuracy of existing models. Deep Neural Networks (DNNs) are\n",
      "models composed of stacked transformations that learn tasks by examples. This\n",
      "technology has recently achieved striking success in a variety of task and\n",
      "there are great expectations on how it might improve clinical practice. Here we\n",
      "present a DNN model trained in a dataset with more than 2 million labeled exams\n",
      "analyzed by the Telehealth Network of Minas Gerais and collected under the\n",
      "scope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The\n",
      "DNN outperform cardiology resident medical doctors in recognizing 6 types of\n",
      "abnormalities in 12-lead ECG recordings, with F1 scores above 80% and\n",
      "specificity over 99%. These results indicate ECG analysis based on DNNs,\n",
      "previously studied in a single-lead setup, generalizes well to 12-lead exams,\n",
      "taking the technology closer to the standard clinical practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.03561 \n",
      "Title :Ensembling geophysical models with Bayesian Neural Networks\n",
      "  Ensembles of geophysical models improve projection accuracy and express\n",
      "uncertainties. We develop a novel data-driven ensembling strategy for combining\n",
      "geophysical models using Bayesian Neural Networks, which infers\n",
      "spatiotemporally varying model weights and bias while accounting for\n",
      "heteroscedastic uncertainties in the observations. This produces more accurate\n",
      "and uncertainty-aware projections without sacrificing interpretability. Applied\n",
      "to the prediction of total column ozone from an ensemble of 15\n",
      "chemistry-climate models, we find that the Bayesian neural network ensemble\n",
      "(BayNNE) outperforms existing ensembling methods, achieving a 49.4% reduction\n",
      "in RMSE for temporal extrapolation, and a 67.4% reduction in RMSE for polar\n",
      "data voids, compared to a weighted mean. Uncertainty is also\n",
      "well-characterized, with 90.6% of the data points in our extrapolation\n",
      "validation dataset lying within 2 standard deviations and 98.5% within 3\n",
      "standard deviations.\n",
      "\n",
      "**Paper Id :2011.02838 \n",
      "Title :Real-time parameter inference in reduced-order flame models with\n",
      "  heteroscedastic Bayesian neural network ensembles\n",
      "  The estimation of model parameters with uncertainties from observed data is a\n",
      "ubiquitous inverse problem in science and engineering. In this paper, we\n",
      "suggest an inexpensive and easy to implement parameter estimation technique\n",
      "that uses a heteroscedastic Bayesian Neural Network trained using anchored\n",
      "ensembling. The heteroscedastic aleatoric error of the network models the\n",
      "irreducible uncertainty due to parameter degeneracies in our inverse problem,\n",
      "while the epistemic uncertainty of the Bayesian model captures uncertainties\n",
      "which may arise from an input observation's out-of-distribution nature. We use\n",
      "this tool to perform real-time parameter inference in a 6 parameter G-equation\n",
      "model of a ducted, premixed flame from observations of acoustically excited\n",
      "flames. We train our networks on a library of 2.1 million simulated flame\n",
      "videos. Results on the test dataset of simulated flames show that the network\n",
      "recovers flame model parameters, with the correlation coefficient between\n",
      "predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated\n",
      "uncertainty estimates. The trained neural networks are then used to infer model\n",
      "parameters from real videos of a premixed Bunsen flame captured using a\n",
      "high-speed camera in our lab. Re-simulation using inferred parameters shows\n",
      "excellent agreement between the real and simulated flames. Compared to Ensemble\n",
      "Kalman Filter-based tools that have been proposed for this problem in the\n",
      "combustion literature, our neural network ensemble achieves better\n",
      "data-efficiency and our sub-millisecond inference times represent a savings on\n",
      "computational costs by several orders of magnitude. This allows us to calibrate\n",
      "our reduced-order flame model in real-time and predict the thermoacoustic\n",
      "instability behaviour of the flame more accurately.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.04017 \n",
      "Title :DiffTune: Optimizing CPU Simulator Parameters with Learned\n",
      "  Differentiable Surrogates\n",
      "  CPU simulators are useful tools for modeling CPU execution behavior. However,\n",
      "they suffer from inaccuracies due to the cost and complexity of setting their\n",
      "fine-grained parameters, such as the latencies of individual instructions. This\n",
      "complexity arises from the expertise required to design benchmarks and\n",
      "measurement frameworks that can precisely measure the values of parameters at\n",
      "such fine granularity. In some cases, these parameters do not necessarily have\n",
      "a physical realization and are therefore fundamentally approximate, or even\n",
      "unmeasurable.\n",
      "  In this paper we present DiffTune, a system for learning the parameters of\n",
      "x86 basic block CPU simulators from coarse-grained end-to-end measurements.\n",
      "Given a simulator, DiffTune learns its parameters by first replacing the\n",
      "original simulator with a differentiable surrogate, another function that\n",
      "approximates the original function; by making the surrogate differentiable,\n",
      "DiffTune is then able to apply gradient-based optimization techniques even when\n",
      "the original function is non-differentiable, such as is the case with CPU\n",
      "simulators. With this differentiable surrogate, DiffTune then applies\n",
      "gradient-based optimization to produce values of the simulator's parameters\n",
      "that minimize the simulator's error on a dataset of ground truth end-to-end\n",
      "performance measurements. Finally, the learned parameters are plugged back into\n",
      "the original simulator.\n",
      "  DiffTune is able to automatically learn the entire set of\n",
      "microarchitecture-specific parameters within the Intel x86 simulation model of\n",
      "llvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling\n",
      "model. DiffTune's learned parameters lead llvm-mca to an average error that not\n",
      "only matches but lowers that of its original, expert-provided parameter values.\n",
      "\n",
      "**Paper Id :2009.05440 \n",
      "Title :ODIN: Automated Drift Detection and Recovery in Video Analytics\n",
      "  Recent advances in computer vision have led to a resurgence of interest in\n",
      "visual data analytics. Researchers are developing systems for effectively and\n",
      "efficiently analyzing visual data at scale. A significant challenge that these\n",
      "systems encounter lies in the drift in real-world visual data. For instance, a\n",
      "model for self-driving vehicles that is not trained on images containing snow\n",
      "does not work well when it encounters them in practice. This drift phenomenon\n",
      "limits the accuracy of models employed for visual data analytics. In this\n",
      "paper, we present a visual data analytics system, called ODIN, that\n",
      "automatically detects and recovers from drift. ODIN uses adversarial\n",
      "autoencoders to learn the distribution of high-dimensional images. We present\n",
      "an unsupervised algorithm for detecting drift by comparing the distributions of\n",
      "the given data against that of previously seen data. When ODIN detects drift,\n",
      "it invokes a drift recovery algorithm to deploy specialized models tailored\n",
      "towards the novel data points. These specialized models outperform their\n",
      "non-specialized counterpart on accuracy, performance, and memory footprint.\n",
      "Lastly, we present a model selection algorithm for picking an ensemble of\n",
      "best-fit specialized models to process a given input. We evaluate the efficacy\n",
      "and efficiency of ODIN on high-resolution dashboard camera videos captured\n",
      "under diverse environments from the Berkeley DeepDrive dataset. We demonstrate\n",
      "that ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\n",
      "smaller memory footprint compared to a baseline system without automated drift\n",
      "detection and recovery.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.04303 \n",
      "Title :How Can Self-Attention Networks Recognize Dyck-n Languages?\n",
      "  We focus on the recognition of Dyck-n ($\\mathcal{D}_n$) languages with\n",
      "self-attention (SA) networks, which has been deemed to be a difficult task for\n",
      "these networks. We compare the performance of two variants of SA, one with a\n",
      "starting symbol (SA$^+$) and one without (SA$^-$). Our results show that SA$^+$\n",
      "is able to generalize to longer sequences and deeper dependencies. For\n",
      "$\\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences\n",
      "whereas the accuracy of SA$^+$ is 58.82$\\%$. We find attention maps learned by\n",
      "$\\text{SA}{^+}$ to be amenable to interpretation and compatible with a\n",
      "stack-based language recognizer. Surprisingly, the performance of SA networks\n",
      "is at par with LSTMs, which provides evidence on the ability of SA to learn\n",
      "hierarchies without recursion.\n",
      "\n",
      "**Paper Id :1909.04719 \n",
      "Title :Neural Belief Reasoner\n",
      "  This paper proposes a new generative model called neural belief reasoner\n",
      "(NBR). It differs from previous models in that it specifies a belief function\n",
      "rather than a probability distribution. Its implementation consists of neural\n",
      "networks, fuzzy-set operations and belief-function operations, and\n",
      "query-answering, sample-generation and training algorithms are presented. This\n",
      "paper studies NBR in two tasks. The first is a synthetic unsupervised-learning\n",
      "task, which demonstrates NBR's ability to perform multi-hop reasoning,\n",
      "reasoning with uncertainty and reasoning about conflicting information. The\n",
      "second is supervised learning: a robust MNIST classifier for 4 and 9, which is\n",
      "the most challenging pair of digits. This classifier needs no adversarial\n",
      "training, and it substantially exceeds the state of the art in adversarial\n",
      "robustness as measured by the L2 metric, while at the same time maintains 99.1%\n",
      "accuracy on natural images.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.04420 \n",
      "Title :Prognosis Prediction in Covid-19 Patients from Lab Tests and X-ray Data\n",
      "  through Randomized Decision Trees\n",
      "  AI and Machine Learning can offer powerful tools to help in the fight against\n",
      "Covid-19. In this paper we present a study and a concrete tool based on machine\n",
      "learning to predict the prognosis of hospitalised patients with Covid-19. In\n",
      "particular we address the task of predicting the risk of death of a patient at\n",
      "different times of the hospitalisation, on the base of some demographic\n",
      "information, chest X-ray scores and several laboratory findings. Our machine\n",
      "learning models use ensembles of decision trees trained and tested using data\n",
      "from more than 2000 patients. An experimental evaluation of the models shows\n",
      "good performance in solving the addressed task.\n",
      "\n",
      "**Paper Id :2004.00959 \n",
      "Title :Neural network based country wise risk prediction of COVID-19\n",
      "  The recent worldwide outbreak of the novel coronavirus (COVID-19) has opened\n",
      "up new challenges to the research community. Artificial intelligence (AI)\n",
      "driven methods can be useful to predict the parameters, risks, and effects of\n",
      "such an epidemic. Such predictions can be helpful to control and prevent the\n",
      "spread of such diseases. The main challenges of applying AI is the small volume\n",
      "of data and the uncertain nature. Here, we propose a shallow long short-term\n",
      "memory (LSTM) based neural network to predict the risk category of a country.\n",
      "We have used a Bayesian optimization framework to optimize and automatically\n",
      "design country-specific networks. The results show that the proposed pipeline\n",
      "outperforms state-of-the-art methods for data of 180 countries and can be a\n",
      "useful tool for such risk categorization. We have also experimented with the\n",
      "trend data and weather data combined for the prediction. The outcome shows that\n",
      "the weather does not have a significant role. The tool can be used to predict\n",
      "long-duration outbreak of such an epidemic such that we can take preventive\n",
      "steps earlier\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.04541 \n",
      "Title :Upper Esophageal Sphincter Opening Segmentation with Convolutional\n",
      "  Recurrent Neural Networks in High Resolution Cervical Auscultation\n",
      "  Upper esophageal sphincter is an important anatomical landmark of the\n",
      "swallowing process commonly observed through the kinematic analysis of\n",
      "radiographic examinations that are vulnerable to subjectivity and clinical\n",
      "feasibility issues. Acting as the doorway of esophagus, upper esophageal\n",
      "sphincter allows the transition of ingested materials from pharyngeal into\n",
      "esophageal stages of swallowing and a reduced duration of opening can lead to\n",
      "penetration/aspiration and/or pharyngeal residue. Therefore, in this study we\n",
      "consider a non-invasive high resolution cervical auscultation-based screening\n",
      "tool to approximate the human ratings of upper esophageal sphincter opening and\n",
      "closure. Swallows were collected from 116 patients and a deep neural network\n",
      "was trained to produce a mask that demarcates the duration of upper esophageal\n",
      "sphincter opening. The proposed method achieved more than 90\\% accuracy and\n",
      "similar values of sensitivity and specificity when compared to human ratings\n",
      "even when tested over swallows from an independent clinical experiment.\n",
      "Moreover, the predicted opening and closure moments surprisingly fell within an\n",
      "inter-human comparable error of their human rated counterparts which\n",
      "demonstrates the clinical significance of high resolution cervical auscultation\n",
      "in replacing ionizing radiation-based evaluation of swallowing kinematics.\n",
      "\n",
      "**Paper Id :2011.05025 \n",
      "Title :Deep correction of breathing-related artifacts in MR-thermometry\n",
      "  Real-time MR-imaging has been clinically adapted for monitoring thermal\n",
      "therapies since it can provide on-the-fly temperature maps simultaneously with\n",
      "anatomical information. However, proton resonance frequency based thermometry\n",
      "of moving targets remains challenging since temperature artifacts are induced\n",
      "by the respiratory as well as physiological motion. If left uncorrected, these\n",
      "artifacts lead to severe errors in temperature estimates and impair therapy\n",
      "guidance. In this study, we evaluated deep learning for on-line correction of\n",
      "motion related errors in abdominal MR-thermometry. For this, a convolutional\n",
      "neural network (CNN) was designed to learn the apparent temperature\n",
      "perturbation from images acquired during a preparative learning stage prior to\n",
      "hyperthermia. The input of the designed CNN is the most recent magnitude image\n",
      "and no surrogate of motion is needed. During the subsequent hyperthermia\n",
      "procedure, the recent magnitude image is used as an input for the CNN-model in\n",
      "order to generate an on-line correction for the current temperature map. The\n",
      "method's artifact suppression performance was evaluated on 12 free breathing\n",
      "volunteers and was found robust and artifact-free in all examined cases.\n",
      "Furthermore, thermometric precision and accuracy was assessed for in vivo\n",
      "ablation using high intensity focused ultrasound. All calculations involved at\n",
      "the different stages of the proposed workflow were designed to be compatible\n",
      "with the clinical time constraints of a therapeutic procedure.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.04950 \n",
      "Title :A Model Compression Method with Matrix Product Operators for Speech\n",
      "  Enhancement\n",
      "  The deep neural network (DNN) based speech enhancement approaches have\n",
      "achieved promising performance. However, the number of parameters involved in\n",
      "these methods is usually enormous for the real applications of speech\n",
      "enhancement on the device with the limited resources. This seriously restricts\n",
      "the applications. To deal with this issue, model compression techniques are\n",
      "being widely studied. In this paper, we propose a model compression method\n",
      "based on matrix product operators (MPO) to substantially reduce the number of\n",
      "parameters in DNN models for speech enhancement. In this method, the weight\n",
      "matrices in the linear transformations of neural network model are replaced by\n",
      "the MPO decomposition format before training. In experiment, this process is\n",
      "applied to the causal neural network models, such as the feedforward multilayer\n",
      "perceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM\n",
      "models with/without compression are then utilized to estimate the ideal ratio\n",
      "mask for monaural speech enhancement. The experimental results show that our\n",
      "proposed MPO-based method outperforms the widely-used pruning method for speech\n",
      "enhancement under various compression rates, and further improvement can be\n",
      "achieved with respect to low compression rates. Our proposal provides an\n",
      "effective model compression method for speech enhancement, especially in\n",
      "cloud-free application.\n",
      "\n",
      "**Paper Id :1907.06870 \n",
      "Title :Light Multi-segment Activation for Model Compression\n",
      "  Model compression has become necessary when applying neural networks (NN)\n",
      "into many real application tasks that can accept slightly-reduced model\n",
      "accuracy with strict tolerance to model complexity. Recently, Knowledge\n",
      "Distillation, which distills the knowledge from well-trained and highly complex\n",
      "teacher model into a compact student model, has been widely used for model\n",
      "compression. However, under the strict requirement on the resource cost, it is\n",
      "quite challenging to achieve comparable performance with the teacher model,\n",
      "essentially due to the drastically-reduced expressiveness ability of the\n",
      "compact student model. Inspired by the nature of the expressiveness ability in\n",
      "Neural Networks, we propose to use multi-segment activation, which can\n",
      "significantly improve the expressiveness ability with very little cost, in the\n",
      "compact student model. Specifically, we propose a highly efficient\n",
      "multi-segment activation, called Light Multi-segment Activation (LMA), which\n",
      "can rapidly produce multiple linear regions with very few parameters by\n",
      "leveraging the statistical information. With using LMA, the compact student\n",
      "model is capable of achieving much better performance effectively and\n",
      "efficiently, than the ReLU-equipped one with same model scale. Furthermore, the\n",
      "proposed method is compatible with other model compression techniques, such as\n",
      "quantization, which means they can be used jointly for better compression\n",
      "performance. Experiments on state-of-the-art NN architectures over the\n",
      "real-world tasks demonstrate the effectiveness and extensibility of the LMA.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.04963 \n",
      "Title :Block-term Tensor Neural Networks\n",
      "  Deep neural networks (DNNs) have achieved outstanding performance in a wide\n",
      "range of applications, e.g., image classification, natural language processing,\n",
      "etc. Despite the good performance, the huge number of parameters in DNNs brings\n",
      "challenges to efficient training of DNNs and also their deployment in low-end\n",
      "devices with limited computing resources. In this paper, we explore the\n",
      "correlations in the weight matrices, and approximate the weight matrices with\n",
      "the low-rank block-term tensors. We name the new corresponding structure as\n",
      "block-term tensor layers (BT-layers), which can be easily adapted to neural\n",
      "network models, such as CNNs and RNNs. In particular, the inputs and the\n",
      "outputs in BT-layers are reshaped into low-dimensional high-order tensors with\n",
      "a similar or improved representation power. Sufficient experiments have\n",
      "demonstrated that BT-layers in CNNs and RNNs can achieve a very large\n",
      "compression ratio on the number of parameters while preserving or improving the\n",
      "representation power of the original DNNs.\n",
      "\n",
      "**Paper Id :2010.04950 \n",
      "Title :A Model Compression Method with Matrix Product Operators for Speech\n",
      "  Enhancement\n",
      "  The deep neural network (DNN) based speech enhancement approaches have\n",
      "achieved promising performance. However, the number of parameters involved in\n",
      "these methods is usually enormous for the real applications of speech\n",
      "enhancement on the device with the limited resources. This seriously restricts\n",
      "the applications. To deal with this issue, model compression techniques are\n",
      "being widely studied. In this paper, we propose a model compression method\n",
      "based on matrix product operators (MPO) to substantially reduce the number of\n",
      "parameters in DNN models for speech enhancement. In this method, the weight\n",
      "matrices in the linear transformations of neural network model are replaced by\n",
      "the MPO decomposition format before training. In experiment, this process is\n",
      "applied to the causal neural network models, such as the feedforward multilayer\n",
      "perceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM\n",
      "models with/without compression are then utilized to estimate the ideal ratio\n",
      "mask for monaural speech enhancement. The experimental results show that our\n",
      "proposed MPO-based method outperforms the widely-used pruning method for speech\n",
      "enhancement under various compression rates, and further improvement can be\n",
      "achieved with respect to low compression rates. Our proposal provides an\n",
      "effective model compression method for speech enhancement, especially in\n",
      "cloud-free application.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.05290 \n",
      "Title :Distributed Resource Allocation with Multi-Agent Deep Reinforcement\n",
      "  Learning for 5G-V2V Communication\n",
      "  We consider the distributed resource selection problem in Vehicle-to-vehicle\n",
      "(V2V) communication in the absence of a base station. Each vehicle autonomously\n",
      "selects transmission resources from a pool of shared resources to disseminate\n",
      "Cooperative Awareness Messages (CAMs). This is a consensus problem where each\n",
      "vehicle has to select a unique resource. The problem becomes more challenging\n",
      "when---due to mobility---the number of vehicles in vicinity of each other is\n",
      "changing dynamically. In a congested scenario, allocation of unique resources\n",
      "for each vehicle becomes infeasible and a congested resource allocation\n",
      "strategy has to be developed. The standardized approach in 5G, namely\n",
      "semi-persistent scheduling (SPS) suffers from effects caused by spatial\n",
      "distribution of the vehicles. In our approach, we turn this into an advantage.\n",
      "We propose a novel DIstributed Resource Allocation mechanism using multi-agent\n",
      "reinforcement Learning (DIRAL) which builds on a unique state representation.\n",
      "One challenging issue is to cope with the non-stationarity introduced by\n",
      "concurrently learning agents which causes convergence problems in multi-agent\n",
      "learning systems. We aimed to tackle non-stationarity with unique state\n",
      "representation. Specifically, we deploy view-based positional distribution as a\n",
      "state representation to tackle non-stationarity and perform complex joint\n",
      "behavior in a distributed fashion. Our results showed that DIRAL improves PRR\n",
      "by 20% compared to SPS in challenging congested scenarios.\n",
      "\n",
      "**Paper Id :2001.10505 \n",
      "Title :Data-driven control of micro-climate in buildings: an event-triggered\n",
      "  reinforcement learning approach\n",
      "  Smart buildings have great potential for shaping an energy-efficient,\n",
      "sustainable, and more economic future for our planet as buildings account for\n",
      "approximately 40% of the global energy consumption. Future of the smart\n",
      "buildings lies in using sensory data for adaptive decision making and control\n",
      "that is currently gloomed by the key challenge of learning a good control\n",
      "policy in a short period of time in an online and continuing fashion. To tackle\n",
      "this challenge, an event-triggered -- as opposed to classic time-triggered --\n",
      "paradigm, is proposed in which learning and control decisions are made when\n",
      "events occur and enough information is collected. Events are characterized by\n",
      "certain design conditions and they occur when the conditions are met, for\n",
      "instance, when a certain state threshold is reached. By systematically\n",
      "adjusting the time of learning and control decisions, the proposed framework\n",
      "can potentially reduce the variance in learning, and consequently, improve the\n",
      "control process. We formulate the micro-climate control problem based on\n",
      "semi-Markov decision processes that allow for variable-time state transitions\n",
      "and decision making. Using extended policy gradient theorems and temporal\n",
      "difference methods in a reinforcement learning set-up, we propose two learning\n",
      "algorithms for event-triggered control of micro-climate in buildings. We show\n",
      "the efficacy of our proposed approach via designing a smart learning thermostat\n",
      "that simultaneously optimizes energy consumption and occupants' comfort in a\n",
      "test building.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.05388 \n",
      "Title :AI Song Contest: Human-AI Co-Creation in Songwriting\n",
      "  Machine learning is challenging the way we make music. Although research in\n",
      "deep generative models has dramatically improved the capability and fluency of\n",
      "music models, recent work has shown that it can be challenging for humans to\n",
      "partner with this new class of algorithms. In this paper, we present findings\n",
      "on what 13 musician/developer teams, a total of 61 users, needed when\n",
      "co-creating a song with AI, the challenges they faced, and how they leveraged\n",
      "and repurposed existing characteristics of AI to overcome some of these\n",
      "challenges. Many teams adopted modular approaches, such as independently\n",
      "running multiple smaller models that align with the musical building blocks of\n",
      "a song, before re-combining their results. As ML models are not easily\n",
      "steerable, teams also generated massive numbers of samples and curated them\n",
      "post-hoc, or used a range of strategies to direct the generation, or\n",
      "algorithmically ranked the samples. Ultimately, teams not only had to manage\n",
      "the \"flare and focus\" aspects of the creative process, but also juggle them\n",
      "with a parallel process of exploring and curating multiple ML models and\n",
      "outputs. These findings reflect a need to design machine learning-powered music\n",
      "interfaces that are more decomposable, steerable, interpretable, and adaptive,\n",
      "which in return will enable artists to more effectively explore how AI can\n",
      "extend their personal expression.\n",
      "\n",
      "**Paper Id :2008.09983 \n",
      "Title :Leveraging Organizational Resources to Adapt Models to New Data\n",
      "  Modalities\n",
      "  As applications in large organizations evolve, the machine learning (ML)\n",
      "models that power them must adapt the same predictive tasks to newly arising\n",
      "data modalities (e.g., a new video content launch in a social media application\n",
      "requires existing text or image models to extend to video). To solve this\n",
      "problem, organizations typically create ML pipelines from scratch. However,\n",
      "this fails to utilize the domain expertise and data they have cultivated from\n",
      "developing tasks for existing modalities. We demonstrate how organizational\n",
      "resources, in the form of aggregate statistics, knowledge bases, and existing\n",
      "services that operate over related tasks, enable teams to construct a common\n",
      "feature space that connects new and existing data modalities. This allows teams\n",
      "to apply methods for training data curation (e.g., weak supervision and label\n",
      "propagation) and model training (e.g., forms of multi-modal learning) across\n",
      "these different data modalities. We study how this use of organizational\n",
      "resources composes at production scale in over 5 classification tasks at\n",
      "Google, and demonstrate how it reduces the time needed to develop models for\n",
      "new modalities from months to weeks to days.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.05609 \n",
      "Title :Load What You Need: Smaller Versions of Multilingual BERT\n",
      "  Pre-trained Transformer-based models are achieving state-of-the-art results\n",
      "on a variety of Natural Language Processing data sets. However, the size of\n",
      "these models is often a drawback for their deployment in real production\n",
      "applications. In the case of multilingual models, most of the parameters are\n",
      "located in the embeddings layer. Therefore, reducing the vocabulary size should\n",
      "have an important impact on the total number of parameters. In this paper, we\n",
      "propose to generate smaller models that handle fewer number of languages\n",
      "according to the targeted corpora. We present an evaluation of smaller versions\n",
      "of multilingual BERT on the XNLI data set, but we believe that this method may\n",
      "be applied to other multilingual transformers. The obtained results confirm\n",
      "that we can generate smaller models that keep comparable results, while\n",
      "reducing up to 45% of the total number of parameters. We compared our models\n",
      "with DistilmBERT (a distilled version of multilingual BERT) and showed that\n",
      "unlike language reduction, distillation induced a 1.7% to 6% drop in the\n",
      "overall accuracy on the XNLI data set. The presented models and code are\n",
      "publicly available.\n",
      "\n",
      "**Paper Id :2005.03812 \n",
      "Title :Comparative Analysis of Word Embeddings for Capturing Word Similarities\n",
      "  Distributed language representation has become the most widely used technique\n",
      "for language representation in various natural language processing tasks. Most\n",
      "of the natural language processing models that are based on deep learning\n",
      "techniques use already pre-trained distributed word representations, commonly\n",
      "called word embeddings. Determining the most qualitative word embeddings is of\n",
      "crucial importance for such models. However, selecting the appropriate word\n",
      "embeddings is a perplexing task since the projected embedding space is not\n",
      "intuitive to humans. In this paper, we explore different approaches for\n",
      "creating distributed word representations. We perform an intrinsic evaluation\n",
      "of several state-of-the-art word embedding methods. Their performance on\n",
      "capturing word similarities is analysed with existing benchmark datasets for\n",
      "word pairs similarities. The research in this paper conducts a correlation\n",
      "analysis between ground truth word similarities and similarities obtained by\n",
      "different word embedding methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.05635 \n",
      "Title :Inferring Causal Direction from Observational Data: A Complexity\n",
      "  Approach\n",
      "  At the heart of causal structure learning from observational data lies a\n",
      "deceivingly simple question: given two statistically dependent random\n",
      "variables, which one has a causal effect on the other? This is impossible to\n",
      "answer using statistical dependence testing alone and requires that we make\n",
      "additional assumptions. We propose several fast and simple criteria for\n",
      "distinguishing cause and effect in pairs of discrete or continuous random\n",
      "variables. The intuition behind them is that predicting the effect variable\n",
      "using the cause variable should be `simpler' than the reverse -- different\n",
      "notions of `simplicity' giving rise to different criteria. We demonstrate the\n",
      "accuracy of the criteria on synthetic data generated under a broad family of\n",
      "causal mechanisms and types of noise.\n",
      "\n",
      "**Paper Id :2010.02675 \n",
      "Title :Recovering Causal Structures from Low-Order Conditional Independencies\n",
      "  One of the common obstacles for learning causal models from data is that\n",
      "high-order conditional independence (CI) relationships between random variables\n",
      "are difficult to estimate. Since CI tests with conditioning sets of low order\n",
      "can be performed accurately even for a small number of observations, a\n",
      "reasonable approach to determine casual structures is to base merely on the\n",
      "low-order CIs. Recent research has confirmed that, e.g. in the case of sparse\n",
      "true causal models, structures learned even from zero- and first-order\n",
      "conditional independencies yield good approximations of the models. However, a\n",
      "challenging task here is to provide methods that faithfully explain a given set\n",
      "of low-order CIs. In this paper, we propose an algorithm which, for a given set\n",
      "of conditional independencies of order less or equal to $k$, where $k$ is a\n",
      "small fixed number, computes a faithful graphical representation of the given\n",
      "set. Our results complete and generalize the previous work on learning from\n",
      "pairwise marginal independencies. Moreover, they enable to improve upon the 0-1\n",
      "graph model which, e.g. is heavily used in the estimation of genome networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.05895 \n",
      "Title :BayReL: Bayesian Relational Learning for Multi-omics Data Integration\n",
      "  High-throughput molecular profiling technologies have produced\n",
      "high-dimensional multi-omics data, enabling systematic understanding of living\n",
      "systems at the genome scale. Studying molecular interactions across different\n",
      "data types helps reveal signal transduction mechanisms across different classes\n",
      "of molecules. In this paper, we develop a novel Bayesian representation\n",
      "learning method that infers the relational interactions across multi-omics data\n",
      "types. Our method, Bayesian Relational Learning (BayReL) for multi-omics data\n",
      "integration, takes advantage of a priori known relationships among the same\n",
      "class of molecules, modeled as a graph at each corresponding view, to learn\n",
      "view-specific latent variables as well as a multi-partite graph that encodes\n",
      "the interactions across views. Our experiments on several real-world datasets\n",
      "demonstrate enhanced performance of BayReL in inferring meaningful interactions\n",
      "compared to existing baselines.\n",
      "\n",
      "**Paper Id :1912.03366 \n",
      "Title :Med2Meta: Learning Representations of Medical Concepts with\n",
      "  Meta-Embeddings\n",
      "  Distributed representations of medical concepts have been used to support\n",
      "downstream clinical tasks recently. Electronic Health Records (EHR) capture\n",
      "different aspects of patients' hospital encounters and serve as a rich source\n",
      "for augmenting clinical decision making by learning robust medical concept\n",
      "embeddings. However, the same medical concept can be recorded in different\n",
      "modalities (e.g., clinical notes, lab results)-with each capturing salient\n",
      "information unique to that modality-and a holistic representation calls for\n",
      "relevant feature ensemble from all information sources. We hypothesize that\n",
      "representations learned from heterogeneous data types would lead to performance\n",
      "enhancement on various clinical informatics and predictive modeling tasks. To\n",
      "this end, our proposed approach makes use of meta-embeddings, embeddings\n",
      "aggregated from learned embeddings. Firstly, modality-specific embeddings for\n",
      "each medical concept is learned with graph autoencoders. The ensemble of all\n",
      "the embeddings is then modeled as a meta-embedding learning problem to\n",
      "incorporate their correlating and complementary information through a joint\n",
      "reconstruction. Empirical results of our model on both quantitative and\n",
      "qualitative clinical evaluations have shown improvements over state-of-the-art\n",
      "embedding models, thus validating our hypothesis.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.06145 \n",
      "Title :Customer Support Ticket Escalation Prediction using Feature Engineering\n",
      "  Understanding and keeping the customer happy is a central tenet of\n",
      "requirements engineering. Strategies to gather, analyze, and negotiate\n",
      "requirements are complemented by efforts to manage customer input after\n",
      "products have been deployed. For the latter, support tickets are key in\n",
      "allowing customers to submit their issues, bug reports, and feature requests.\n",
      "If insufficient attention is given to support issues, however, their escalation\n",
      "to management becomes time-consuming and expensive, especially for large\n",
      "organizations managing hundreds of customers and thousands of support tickets.\n",
      "Our work provides a step towards simplifying the job of support analysts and\n",
      "managers, particularly in predicting the risk of escalating support tickets. In\n",
      "a field study at our large industrial partner, IBM, we used a design science\n",
      "research methodology to characterize the support process and data available to\n",
      "IBM analysts in managing escalations. We then implemented these features into a\n",
      "machine learning model to predict support ticket escalations. We trained and\n",
      "evaluated our machine learning model on over 2.5 million support tickets and\n",
      "10,000 escalations, obtaining a recall of 87.36% and an 88.23% reduction in the\n",
      "workload for support analysts looking to identify support tickets at risk of\n",
      "escalation. Finally, in addition to these research evaluation activities, we\n",
      "compared the performance of our support ticket model with that of a model\n",
      "developed with no feature engineering; the support ticket model features\n",
      "outperformed the non-engineered model. The artifacts created in this research\n",
      "are designed to serve as a starting place for organizations interested in\n",
      "predicting support ticket escalations, and for future researchers to build on\n",
      "to advance research in escalation prediction.\n",
      "\n",
      "**Paper Id :1911.01225 \n",
      "Title :Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale\n",
      "  Service Environment\n",
      "  Root cause analysis in a large-scale production environment is challenging\n",
      "due to the complexity of services running across global data centers. Due to\n",
      "the distributed nature of a large-scale system, the various hardware, software,\n",
      "and tooling logs are often maintained separately, making it difficult to review\n",
      "the logs jointly for understanding production issues. Another challenge in\n",
      "reviewing the logs for identifying issues is the scale - there could easily be\n",
      "millions of entities, each described by hundreds of features. In this paper we\n",
      "present a fast dimensional analysis framework that automates the root cause\n",
      "analysis on structured logs with improved scalability.\n",
      "  We first explore item-sets, i.e. combinations of feature values, that could\n",
      "identify groups of samples with sufficient support for the target failures\n",
      "using the Apriori algorithm and a subsequent improvement, FP-Growth. These\n",
      "algorithms were designed for frequent item-set mining and association rule\n",
      "learning over transactional databases. After applying them on structured logs,\n",
      "we select the item-sets that are most unique to the target failures based on\n",
      "lift. We propose pre-processing steps with the use of a large-scale real-time\n",
      "database and post-processing techniques and parallelism to further speed up the\n",
      "analysis and improve interpretability, and demonstrate that such optimization\n",
      "is necessary for handling large-scale production datasets. We have successfully\n",
      "rolled out this approach for root cause investigation purposes in a large-scale\n",
      "infrastructure. We also present the setup and results from multiple production\n",
      "use cases in this paper.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.06460 \n",
      "Title :Deep Reinforcement Learning for Real-Time Optimization of Pumps in Water\n",
      "  Distribution Systems\n",
      "  Real-time control of pumps can be an infeasible task in water distribution\n",
      "systems (WDSs) because the calculation to find the optimal pump speeds is\n",
      "resource-intensive. The computational need cannot be lowered even with the\n",
      "capabilities of smart water networks when conventional optimization techniques\n",
      "are used. Deep reinforcement learning (DRL) is presented here as a controller\n",
      "of pumps in two WDSs. An agent based on a dueling deep q-network is trained to\n",
      "maintain the pump speeds based on instantaneous nodal pressure data. General\n",
      "optimization techniques (e.g., Nelder-Mead method, differential evolution)\n",
      "serve as baselines. The total efficiency achieved by the DRL agent compared to\n",
      "the best performing baseline is above 0.98, whereas the speedup is around 2x\n",
      "compared to that. The main contribution of the presented approach is that the\n",
      "agent can run the pumps in real-time because it depends only on measurement\n",
      "data. If the WDS is replaced with a hydraulic simulation, the agent still\n",
      "outperforms conventional techniques in search speed.\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.06545 \n",
      "Title :Toward Few-step Adversarial Training from a Frequency Perspective\n",
      "  We investigate adversarial-sample generation methods from a frequency domain\n",
      "perspective and extend standard $l_{\\infty}$ Projected Gradient Descent (PGD)\n",
      "to the frequency domain. The resulting method, which we call Spectral Projected\n",
      "Gradient Descent (SPGD), has better success rate compared to PGD during early\n",
      "steps of the method. Adversarially training models using SPGD achieves greater\n",
      "adversarial accuracy compared to PGD when holding the number of attack steps\n",
      "constant. The use of SPGD can, therefore, reduce the overhead of adversarial\n",
      "training when utilizing adversarial generation with a smaller number of steps.\n",
      "However, we also prove that SPGD is equivalent to a variant of the PGD\n",
      "ordinarily used for the $l_{\\infty}$ threat model. This PGD variant omits the\n",
      "sign function which is ordinarily applied to the gradient. SPGD can, therefore,\n",
      "be performed without explicitly transforming into the frequency domain.\n",
      "Finally, we visualize the perturbations SPGD generates and find they use both\n",
      "high and low-frequency components, which suggests that removing either\n",
      "high-frequency components or low-frequency components is not an effective\n",
      "defense.\n",
      "\n",
      "**Paper Id :1906.01827 \n",
      "Title :Coresets for Data-efficient Training of Machine Learning Models\n",
      "  Incremental gradient (IG) methods, such as stochastic gradient descent and\n",
      "its variants are commonly used for large scale optimization in machine\n",
      "learning. Despite the sustained effort to make IG methods more data-efficient,\n",
      "it remains an open question how to select a training data subset that can\n",
      "theoretically and practically perform on par with the full dataset. Here we\n",
      "develop CRAIG, a method to select a weighted subset (or coreset) of training\n",
      "data that closely estimates the full gradient by maximizing a submodular\n",
      "function. We prove that applying IG to this subset is guaranteed to converge to\n",
      "the (near)optimal solution with the same convergence rate as that of IG for\n",
      "convex optimization. As a result, CRAIG achieves a speedup that is inversely\n",
      "proportional to the size of the subset. To our knowledge, this is the first\n",
      "rigorous method for data-efficient training of general machine learning models.\n",
      "Our extensive set of experiments show that CRAIG, while achieving practically\n",
      "the same solution, speeds up various IG methods by up to 6x for logistic\n",
      "regression and 3x for training deep neural networks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.06659 \n",
      "Title :Towards Data-efficient Modeling for Wake Word Spotting\n",
      "  Wake word (WW) spotting is challenging in far-field not only because of the\n",
      "interference in signal transmission but also the complexity in acoustic\n",
      "environments. Traditional WW model training requires large amount of in-domain\n",
      "WW-specific data with substantial human annotations therefore it is hard to\n",
      "build WW models without such data. In this paper we present data-efficient\n",
      "solutions to address the challenges in WW modeling, such as domain-mismatch,\n",
      "noisy conditions, limited annotation, etc. Our proposed system is composed of a\n",
      "multi-condition training pipeline with a stratified data augmentation, which\n",
      "improves the model robustness to a variety of predefined acoustic conditions,\n",
      "together with a semi-supervised learning pipeline to accurately extract the WW\n",
      "and confusable examples from untranscribed speech corpus. Starting from only 10\n",
      "hours of domain-mismatched WW audio, we are able to enlarge and enrich the\n",
      "training dataset by 20-100 times to capture the acoustic complexity. Our\n",
      "experiments on real user data show that the proposed solutions can achieve\n",
      "comparable performance of a production-grade model by saving 97\\% of the amount\n",
      "of WW-specific data collection and 86\\% of the bandwidth for annotation.\n",
      "\n",
      "**Paper Id :2009.12735 \n",
      "Title :Modeling Topical Relevance for Multi-Turn Dialogue Generation\n",
      "  Topic drift is a common phenomenon in multi-turn dialogue. Therefore, an\n",
      "ideal dialogue generation models should be able to capture the topic\n",
      "information of each context, detect the relevant context, and produce\n",
      "appropriate responses accordingly. However, existing models usually use word or\n",
      "sentence level similarities to detect the relevant contexts, which fail to well\n",
      "capture the topical level relevance. In this paper, we propose a new model,\n",
      "named STAR-BTM, to tackle this problem. Firstly, the Biterm Topic Model is\n",
      "pre-trained on the whole training dataset. Then, the topic level attention\n",
      "weights are computed based on the topic representation of each context.\n",
      "Finally, the attention weights and the topic distribution are utilized in the\n",
      "decoding process to generate the corresponding responses. Experimental results\n",
      "on both Chinese customer services data and English Ubuntu dialogue data show\n",
      "that STAR-BTM significantly outperforms several state-of-the-art methods, in\n",
      "terms of both metric-based and human evaluations.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.07078 \n",
      "Title :Differentiable Implicit Layers\n",
      "  In this paper, we introduce an efficient backpropagation scheme for\n",
      "non-constrained implicit functions. These functions are parametrized by a set\n",
      "of learnable weights and may optionally depend on some input; making them\n",
      "perfectly suitable as a learnable layer in a neural network. We demonstrate our\n",
      "scheme on different applications: (i) neural ODEs with the implicit Euler\n",
      "method, and (ii) system identification in model predictive control.\n",
      "\n",
      "**Paper Id :1910.02333 \n",
      "Title :The Role of Neural Network Activation Functions\n",
      "  A wide variety of activation functions have been proposed for neural\n",
      "networks. The Rectified Linear Unit (ReLU) is especially popular today. There\n",
      "are many practical reasons that motivate the use of the ReLU. This paper\n",
      "provides new theoretical characterizations that support the use of the ReLU,\n",
      "its variants such as the leaky ReLU, as well as other activation functions in\n",
      "the case of univariate, single-hidden layer feedforward neural networks. Our\n",
      "results also explain the importance of commonly used strategies in the design\n",
      "and training of neural networks such as \"weight decay\" and \"path-norm\"\n",
      "regularization, and provide a new justification for the use of \"skip\n",
      "connections\" in network architectures. These new insights are obtained through\n",
      "the lens of spline theory. In particular, we show how neural network training\n",
      "problems are related to infinite-dimensional optimizations posed over Banach\n",
      "spaces of functions whose solutions are well-known to be fractional and\n",
      "polynomial splines, where the particular Banach space (which controls the order\n",
      "of the spline) depends on the choice of activation function.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.07468 \n",
      "Title :AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed\n",
      "  Gradients\n",
      "  Most popular optimizers for deep learning can be broadly categorized as\n",
      "adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient\n",
      "descent (SGD) with momentum). For many models such as convolutional neural\n",
      "networks (CNNs), adaptive methods typically converge faster but generalize\n",
      "worse compared to SGD; for complex settings such as generative adversarial\n",
      "networks (GANs), adaptive methods are typically the default because of their\n",
      "stability.We propose AdaBelief to simultaneously achieve three goals: fast\n",
      "convergence as in adaptive methods, good generalization as in SGD, and training\n",
      "stability. The intuition for AdaBelief is to adapt the stepsize according to\n",
      "the \"belief\" in the current gradient direction. Viewing the exponential moving\n",
      "average (EMA) of the noisy gradient as the prediction of the gradient at the\n",
      "next time step, if the observed gradient greatly deviates from the prediction,\n",
      "we distrust the current observation and take a small step; if the observed\n",
      "gradient is close to the prediction, we trust it and take a large step. We\n",
      "validate AdaBelief in extensive experiments, showing that it outperforms other\n",
      "methods with fast convergence and high accuracy on image classification and\n",
      "language modeling. Specifically, on ImageNet, AdaBelief achieves comparable\n",
      "accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief\n",
      "demonstrates high stability and improves the quality of generated samples\n",
      "compared to a well-tuned Adam optimizer. Code is available at\n",
      "https://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\n",
      "**Paper Id :2003.04296 \n",
      "Title :Propagating Asymptotic-Estimated Gradients for Low Bitwidth Quantized\n",
      "  Neural Networks\n",
      "  The quantized neural networks (QNNs) can be useful for neural network\n",
      "acceleration and compression, but during the training process they pose a\n",
      "challenge: how to propagate the gradient of loss function through the graph\n",
      "flow with a derivative of 0 almost everywhere. In response to this\n",
      "non-differentiable situation, we propose a novel Asymptotic-Quantized Estimator\n",
      "(AQE) to estimate the gradient. In particular, during back-propagation, the\n",
      "graph that relates inputs to output remains smoothness and differentiability.\n",
      "At the end of training, the weights and activations have been quantized to\n",
      "low-precision because of the asymptotic behaviour of AQE. Meanwhile, we propose\n",
      "a M-bit Inputs and N-bit Weights Network (MINW-Net) trained by AQE, a quantized\n",
      "neural network with 1-3 bits weights and activations. In the inference phase,\n",
      "we can use XNOR or SHIFT operations instead of convolution operations to\n",
      "accelerate the MINW-Net. Our experiments on CIFAR datasets demonstrate that our\n",
      "AQE is well defined, and the QNNs with AQE perform better than that with\n",
      "Straight-Through Estimator (STE). For example, in the case of the same ConvNet\n",
      "that has 1-bit weights and activations, our MINW-Net with AQE can achieve a\n",
      "prediction accuracy 1.5\\% higher than the Binarized Neural Network (BNN) with\n",
      "STE. The MINW-Net, which is trained from scratch by AQE, can achieve comparable\n",
      "classification accuracy as 32-bit counterparts on CIFAR test sets. Extensive\n",
      "experimental results on ImageNet dataset show great superiority of the proposed\n",
      "AQE and our MINW-Net achieves comparable results with other state-of-the-art\n",
      "QNNs.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.08865 \n",
      "Title :HABERTOR: An Efficient and Effective Deep Hatespeech Detector\n",
      "  We present our HABERTOR model for detecting hatespeech in large scale\n",
      "user-generated content. Inspired by the recent success of the BERT model, we\n",
      "propose several modifications to BERT to enhance the performance on the\n",
      "downstream hatespeech classification task. HABERTOR inherits BERT's\n",
      "architecture, but is different in four aspects: (i) it generates its own\n",
      "vocabularies and is pre-trained from the scratch using the largest scale\n",
      "hatespeech dataset; (ii) it consists of Quaternion-based factorized components,\n",
      "resulting in a much smaller number of parameters, faster training and\n",
      "inferencing, as well as less memory usage; (iii) it uses our proposed\n",
      "multi-source ensemble heads with a pooling layer for separate input sources, to\n",
      "further enhance its effectiveness; and (iv) it uses a regularized adversarial\n",
      "training with our proposed fine-grained and adaptive noise magnitude to enhance\n",
      "its robustness. Through experiments on the large-scale real-world hatespeech\n",
      "dataset with 1.4M annotated comments, we show that HABERTOR works better than\n",
      "15 state-of-the-art hatespeech detection methods, including fine-tuning\n",
      "Language Models. In particular, comparing with BERT, our HABERTOR is 4~5 times\n",
      "faster in the training/inferencing phase, uses less than 1/3 of the memory, and\n",
      "has better performance, even though we pre-train it by using less than 1% of\n",
      "the number of words. Our generalizability analysis shows that HABERTOR\n",
      "transfers well to other unseen hatespeech datasets and is a more efficient and\n",
      "effective alternative to BERT for the hatespeech classification.\n",
      "\n",
      "**Paper Id :1911.03437 \n",
      "Title :SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language\n",
      "  Models through Principled Regularized Optimization\n",
      "  Transfer learning has fundamentally changed the landscape of natural language\n",
      "processing (NLP) research. Many existing state-of-the-art models are first\n",
      "pre-trained on a large text corpus and then fine-tuned on downstream tasks.\n",
      "However, due to limited data resources from downstream tasks and the extremely\n",
      "large capacity of pre-trained models, aggressive fine-tuning often causes the\n",
      "adapted model to overfit the data of downstream tasks and forget the knowledge\n",
      "of the pre-trained model. To address the above issue in a more principled\n",
      "manner, we propose a new computational framework for robust and efficient\n",
      "fine-tuning for pre-trained language models. Specifically, our proposed\n",
      "framework contains two important ingredients: 1. Smoothness-inducing\n",
      "regularization, which effectively manages the capacity of the model; 2. Bregman\n",
      "proximal point optimization, which is a class of trust-region methods and can\n",
      "prevent knowledge forgetting. Our experiments demonstrate that our proposed\n",
      "method achieves the state-of-the-art performance on multiple NLP benchmarks.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.09355 \n",
      "Title :SHREC 2020 track: 6D Object Pose Estimation\n",
      "  6D pose estimation is crucial for augmented reality, virtual reality, robotic\n",
      "manipulation and visual navigation. However, the problem is challenging due to\n",
      "the variety of objects in the real world. They have varying 3D shape and their\n",
      "appearances in captured images are affected by sensor noise, changing lighting\n",
      "conditions and occlusions between objects. Different pose estimation methods\n",
      "have different strengths and weaknesses, depending on feature representations\n",
      "and scene contents. At the same time, existing 3D datasets that are used for\n",
      "data-driven methods to estimate 6D poses have limited view angles and low\n",
      "resolution. To address these issues, we organize the Shape Retrieval Challenge\n",
      "benchmark on 6D pose estimation and create a physically accurate simulator that\n",
      "is able to generate photo-realistic color-and-depth image pairs with\n",
      "corresponding ground truth 6D poses. From captured color and depth images, we\n",
      "use this simulator to generate a 3D dataset which has 400 photo-realistic\n",
      "synthesized color-and-depth image pairs with various view angles for training,\n",
      "and another 100 captured and synthetic images for testing. Five research groups\n",
      "register in this track and two of them submitted their results. Data-driven\n",
      "methods are the current trend in 6D object pose estimation and our evaluation\n",
      "results show that approaches which fully exploit the color and geometric\n",
      "features are more robust for 6D pose estimation of reflective and texture-less\n",
      "objects and occlusion. This benchmark and comparative evaluation results have\n",
      "the potential to further enrich and boost the research of 6D object pose\n",
      "estimation and its applications.\n",
      "\n",
      "**Paper Id :2003.01456 \n",
      "Title :Implicit Functions in Feature Space for 3D Shape Reconstruction and\n",
      "  Completion\n",
      "  While many works focus on 3D reconstruction from images, in this paper, we\n",
      "focus on 3D shape reconstruction and completion from a variety of 3D inputs,\n",
      "which are deficient in some respect: low and high resolution voxels, sparse and\n",
      "dense point clouds, complete or incomplete. Processing of such 3D inputs is an\n",
      "increasingly important problem as they are the output of 3D scanners, which are\n",
      "becoming more accessible, and are the intermediate output of 3D computer vision\n",
      "algorithms. Recently, learned implicit functions have shown great promise as\n",
      "they produce continuous reconstructions. However, we identified two limitations\n",
      "in reconstruction from 3D inputs: 1) details present in the input data are not\n",
      "retained, and 2) poor reconstruction of articulated humans. To solve this, we\n",
      "propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,\n",
      "can handle multiple topologies, and complete shapes for missing or sparse input\n",
      "data retaining the nice properties of recent learned implicit functions, but\n",
      "critically they can also retain detail when it is present in the input data,\n",
      "and can reconstruct articulated humans. Our work differs from prior work in two\n",
      "crucial aspects. First, instead of using a single vector to encode a 3D shape,\n",
      "we extract a learnable 3-dimensional multi-scale tensor of deep features, which\n",
      "is aligned with the original Euclidean space embedding the shape. Second,\n",
      "instead of classifying x-y-z point coordinates directly, we classify deep\n",
      "features extracted from the tensor at a continuous query point. We show that\n",
      "this forces our model to make decisions based on global and local shape\n",
      "structure, as opposed to point coordinates, which are arbitrary under Euclidean\n",
      "transformations. Experiments demonstrate that IF-Nets clearly outperform prior\n",
      "work in 3D object reconstruction in ShapeNet, and obtain significantly more\n",
      "accurate 3D human reconstructions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.09468 \n",
      "Title :Chance-Constrained Control with Lexicographic Deep Reinforcement\n",
      "  Learning\n",
      "  This paper proposes a lexicographic Deep Reinforcement Learning\n",
      "(DeepRL)-based approach to chance-constrained Markov Decision Processes, in\n",
      "which the controller seeks to ensure that the probability of satisfying the\n",
      "constraint is above a given threshold. Standard DeepRL approaches require i)\n",
      "the constraints to be included as additional weighted terms in the cost\n",
      "function, in a multi-objective fashion, and ii) the tuning of the introduced\n",
      "weights during the training phase of the Deep Neural Network (DNN) according to\n",
      "the probability thresholds. The proposed approach, instead, requires to\n",
      "separately train one constraint-free DNN and one DNN associated to each\n",
      "constraint and then, at each time-step, to select which DNN to use depending on\n",
      "the system observed state. The presented solution does not require any\n",
      "hyper-parameter tuning besides the standard DNN ones, even if the probability\n",
      "thresholds changes. A lexicographic version of the well-known DeepRL algorithm\n",
      "DQN is also proposed and validated via simulations.\n",
      "\n",
      "**Paper Id :2003.09844 \n",
      "Title :Tune smarter not harder: A principled approach to tuning learning rates\n",
      "  for shallow nets\n",
      "  Effective hyper-parameter tuning is essential to guarantee the performance\n",
      "that neural networks have come to be known for. In this work, a principled\n",
      "approach to choosing the learning rate is proposed for shallow feedforward\n",
      "neural networks. We associate the learning rate with the gradient Lipschitz\n",
      "constant of the objective to be minimized while training. An upper bound on the\n",
      "mentioned constant is derived and a search algorithm, which always results in\n",
      "non-divergent traces, is proposed to exploit the derived bound. It is shown\n",
      "through simulations that the proposed search method significantly outperforms\n",
      "the existing tuning methods such as Tree Parzen Estimators (TPE). The proposed\n",
      "method is applied to three different existing applications: a) channel\n",
      "estimation in OFDM systems, b) prediction of the exchange currency rates and c)\n",
      "offset estimation in OFDM receivers, and it is shown to pick better learning\n",
      "rates than the existing methods using the same or lesser compute power.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.09483 \n",
      "Title :Mycorrhiza: Genotype Assignment usingPhylogenetic Networks\n",
      "  Motivation The genotype assignment problem consists of predicting, from the\n",
      "genotype of an individual, which of a known set of populations it originated\n",
      "from. The problem arises in a variety of contexts, including wildlife\n",
      "forensics, invasive species detection and biodiversity monitoring. Existing\n",
      "approaches perform well under ideal conditions but are sensitive to a variety\n",
      "of common violations of the assumptions they rely on. Results In this article,\n",
      "we introduce Mycorrhiza, a machine learning approach for the genotype\n",
      "assignment problem. Our algorithm makes use of phylogenetic networks to\n",
      "engineer features that encode the evolutionary relationships among samples.\n",
      "Those features are then used as input to a Random Forests classifier. The\n",
      "classification accuracy was assessed on multiple published empirical SNP,\n",
      "microsatellite or consensus sequence datasets with wide ranges of size,\n",
      "geographical distribution and population structure and on simulated datasets.\n",
      "It compared favorably against widely used assessment tests or mixture analysis\n",
      "methods such as STRUCTURE and Admixture, and against another machine-learning\n",
      "based approach using principal component analysis for dimensionality reduction.\n",
      "Mycorrhiza yields particularly significant gains on datasets with a large\n",
      "average fixation index (FST) or deviation from the Hardy-Weinberg equilibrium.\n",
      "Moreover, the phylogenetic network approach estimates mixture proportions with\n",
      "good accuracy.\n",
      "\n",
      "**Paper Id :2004.14764 \n",
      "Title :Hierarchical clustering of bipartite data sets based on the statistical\n",
      "  significance of coincidences\n",
      "  When some 'entities' are related by the 'features' they share they are\n",
      "amenable to a bipartite network representation. Plant-pollinator ecological\n",
      "communities, co-authorship of scientific papers, customers and purchases, or\n",
      "answers in a poll, are but a few examples. Analyzing clustering of such\n",
      "entities in the network is a useful tool with applications in many fields, like\n",
      "internet technology, recommender systems, or detection of diseases. The\n",
      "algorithms most widely applied to find clusters in bipartite networks are\n",
      "variants of modularity optimization. Here we provide an hierarchical clustering\n",
      "algorithm based on a dissimilarity between entities that quantifies the\n",
      "probability that the features shared by two entities is due to mere chance. The\n",
      "algorithm performance is $O(n^2)$ when applied to a set of n entities, and its\n",
      "outcome is a dendrogram exhibiting the connections of those entities. Through\n",
      "the introduction of a 'susceptibility' measure we can provide an 'optimal'\n",
      "choice for the clustering as well as quantify its quality. The dendrogram\n",
      "reveals further useful structural information though -- like the existence of\n",
      "sub-clusters within clusters or of nodes that do not fit in any cluster. We\n",
      "illustrate the algorithm by applying it first to a set of synthetic networks,\n",
      "and then to a selection of examples. We also illustrate how to transform our\n",
      "algorithm into a valid alternative for one-mode networks as well, and show that\n",
      "it performs at least as well as the standard, modularity-based algorithms --\n",
      "with a higher numerical performance. We provide an implementation of the\n",
      "algorithm in Python freely accessible from GitHub.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.09680 \n",
      "Title :A Survey of Machine Learning Techniques in Adversarial Image Forensics\n",
      "  Image forensic plays a crucial role in both criminal investigations (e.g.,\n",
      "dissemination of fake images to spread racial hate or false narratives about\n",
      "specific ethnicity groups) and civil litigation (e.g., defamation).\n",
      "Increasingly, machine learning approaches are also utilized in image forensics.\n",
      "However, there are also a number of limitations and vulnerabilities associated\n",
      "with machine learning-based approaches, for example how to detect adversarial\n",
      "(image) examples, with real-world consequences (e.g., inadmissible evidence, or\n",
      "wrongful conviction). Therefore, with a focus on image forensics, this paper\n",
      "surveys techniques that can be used to enhance the robustness of machine\n",
      "learning-based binary manipulation detectors in various adversarial scenarios.\n",
      "\n",
      "**Paper Id :2005.06618 \n",
      "Title :Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective\n",
      "  Learning\n",
      "  Human society had a long history of suffering from cognitive biases leading\n",
      "to social prejudices and mass injustice. The prevalent existence of cognitive\n",
      "biases in large volumes of historical data can pose a threat of being\n",
      "manifested as unethical and seemingly inhuman predictions as outputs of AI\n",
      "systems trained on such data. To alleviate this problem, we propose a\n",
      "bias-aware multi-objective learning framework that given a set of identity\n",
      "attributes (e.g. gender, ethnicity etc.) and a subset of sensitive categories\n",
      "of the possible classes of prediction outputs, learns to reduce the frequency\n",
      "of predicting certain combinations of them, e.g. predicting stereotypes such as\n",
      "`most blacks use abusive language', or `fear is a virtue of women'. Our\n",
      "experiments conducted on an emotion prediction task with balanced class priors\n",
      "shows that a set of baseline bias-agnostic models exhibit cognitive biases with\n",
      "respect to gender, such as women are prone to be afraid whereas men are more\n",
      "prone to be angry. In contrast, our proposed bias-aware multi-objective\n",
      "learning methodology is shown to reduce such biases in the predictied emotions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.10817 \n",
      "Title :Using the Full-text Content of Academic Articles to Identify and\n",
      "  Evaluate Algorithm Entities in the Domain of Natural Language Processing\n",
      "  In the era of big data, the advancement, improvement, and application of\n",
      "algorithms in academic research have played an important role in promoting the\n",
      "development of different disciplines. Academic papers in various disciplines,\n",
      "especially computer science, contain a large number of algorithms. Identifying\n",
      "the algorithms from the full-text content of papers can determine popular or\n",
      "classical algorithms in a specific field and help scholars gain a comprehensive\n",
      "understanding of the algorithms and even the field. To this end, this article\n",
      "takes the field of natural language processing (NLP) as an example and\n",
      "identifies algorithms from academic papers in the field. A dictionary of\n",
      "algorithms is constructed by manually annotating the contents of papers, and\n",
      "sentences containing algorithms in the dictionary are extracted through\n",
      "dictionary-based matching. The number of articles mentioning an algorithm is\n",
      "used as an indicator to analyze the influence of that algorithm. Our results\n",
      "reveal the algorithm with the highest influence in NLP papers and show that\n",
      "classification algorithms represent the largest proportion among the\n",
      "high-impact algorithms. In addition, the evolution of the influence of\n",
      "algorithms reflects the changes in research tasks and topics in the field, and\n",
      "the changes in the influence of different algorithms show different trends. As\n",
      "a preliminary exploration, this paper conducts an analysis of the impact of\n",
      "algorithms mentioned in the academic text, and the results can be used as\n",
      "training data for the automatic extraction of large-scale algorithms in the\n",
      "future. The methodology in this paper is domain-independent and can be applied\n",
      "to other domains.\n",
      "\n",
      "**Paper Id :2007.14622 \n",
      "Title :Approaches to Fraud Detection on Credit Card Transactions Using\n",
      "  Artificial Intelligence Methods\n",
      "  Credit card fraud is an ongoing problem for almost all industries in the\n",
      "world, and it raises millions of dollars to the global economy each year.\n",
      "Therefore, there is a number of research either completed or proceeding in\n",
      "order to detect these kinds of frauds in the industry. These researches\n",
      "generally use rule-based or novel artificial intelligence approaches to find\n",
      "eligible solutions. The ultimate goal of this paper is to summarize\n",
      "state-of-the-art approaches to fraud detection using artificial intelligence\n",
      "and machine learning techniques. While summarizing, we will categorize the\n",
      "common problems such as imbalanced dataset, real time working scenarios, and\n",
      "feature engineering challenges that almost all research works encounter, and\n",
      "identify general approaches to solve them. The imbalanced dataset problem\n",
      "occurs because the number of legitimate transactions is much higher than the\n",
      "fraudulent ones whereas applying the right feature engineering is substantial\n",
      "as the features obtained from the industries are limited, and applying feature\n",
      "engineering methods and reforming the dataset is crucial. Also, adapting the\n",
      "detection system to real time scenarios is a challenge since the number of\n",
      "credit card transactions in a limited time period is very high. In addition, we\n",
      "will discuss how evaluation metrics and machine learning methods differentiate\n",
      "among each research.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.11010 \n",
      "Title :Complex data labeling with deep learning methods: Lessons from fisheries\n",
      "  acoustics\n",
      "  Quantitative and qualitative analysis of acoustic backscattered signals from\n",
      "the seabed bottom to the sea surface is used worldwide for fish stocks\n",
      "assessment and marine ecosystem monitoring. Huge amounts of raw data are\n",
      "collected yet require tedious expert labeling. This paper focuses on a case\n",
      "study where the ground truth labels are non-obvious: echograms labeling, which\n",
      "is time-consuming and critical for the quality of fisheries and ecological\n",
      "analysis. We investigate how these tasks can benefit from supervised learning\n",
      "algorithms and demonstrate that convolutional neural networks trained with\n",
      "non-stationary datasets can be used to stress parts of a new dataset needing\n",
      "human expert correction. Further development of this approach paves the way\n",
      "toward a standardization of the labeling process in fisheries acoustics and is\n",
      "a good case study for non-obvious data labeling processes.\n",
      "\n",
      "**Paper Id :2006.15969 \n",
      "Title :Interpretation of 3D CNNs for Brain MRI Data Classification\n",
      "  Deep learning shows high potential for many medical image analysis tasks.\n",
      "Neural networks can work with full-size data without extensive preprocessing\n",
      "and feature generation and, thus, information loss. Recent work has shown that\n",
      "the morphological difference in specific brain regions can be found on MRI with\n",
      "the means of Convolution Neural Networks (CNN). However, interpretation of the\n",
      "existing models is based on a region of interest and can not be extended to\n",
      "voxel-wise image interpretation on a whole image. In the current work, we\n",
      "consider the classification task on a large-scale open-source dataset of young\n",
      "healthy subjects -- an exploration of brain differences between men and women.\n",
      "In this paper, we extend the previous findings in gender differences from\n",
      "diffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D\n",
      "CNN interpretation comparing the results of three interpretation methods:\n",
      "Meaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute\n",
      "with the open-source library.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.11370 \n",
      "Title :A Multi-Componential Approach to Emotion Recognition and the Effect of\n",
      "  Personality\n",
      "  Emotions are an inseparable part of human nature affecting our behavior in\n",
      "response to the outside world. Although most empirical studies have been\n",
      "dominated by two theoretical models including discrete categories of emotion\n",
      "and dichotomous dimensions, results from neuroscience approaches suggest a\n",
      "multi-processes mechanism underpinning emotional experience with a large\n",
      "overlap across different emotions. While these findings are consistent with the\n",
      "influential theories of emotion in psychology that emphasize a role for\n",
      "multiple component processes to generate emotion episodes, few studies have\n",
      "systematically investigated the relationship between discrete emotions and a\n",
      "full componential view. This paper applies a componential framework with a\n",
      "data-driven approach to characterize emotional experiences evoked during movie\n",
      "watching. The results suggest that differences between various emotions can be\n",
      "captured by a few (at least 6) latent dimensions, each defined by features\n",
      "associated with component processes, including appraisal, expression,\n",
      "physiology, motivation, and feeling. In addition, the link between discrete\n",
      "emotions and component model is explored and results show that a componential\n",
      "model with a limited number of descriptors is still able to predict the level\n",
      "of experienced discrete emotion(s) to a satisfactory level. Finally, as\n",
      "appraisals may vary according to individual dispositions and biases, we also\n",
      "study the relationship between personality traits and emotions in our\n",
      "computational framework and show that the role of personality on discrete\n",
      "emotion differences can be better justified using the component model.\n",
      "\n",
      "**Paper Id :1902.02181 \n",
      "Title :Attention in Natural Language Processing\n",
      "  Attention is an increasingly popular mechanism used in a wide range of neural\n",
      "architectures. The mechanism itself has been realized in a variety of formats.\n",
      "However, because of the fast-paced advances in this domain, a systematic\n",
      "overview of attention is still missing. In this article, we define a unified\n",
      "model for attention architectures in natural language processing, with a focus\n",
      "on those designed to work with vector representations of the textual data. We\n",
      "propose a taxonomy of attention models according to four dimensions: the\n",
      "representation of the input, the compatibility function, the distribution\n",
      "function, and the multiplicity of the input and/or output. We present the\n",
      "examples of how prior information can be exploited in attention models and\n",
      "discuss ongoing research efforts and open challenges in the area, providing the\n",
      "first extensive categorization of the vast body of literature in this exciting\n",
      "domain.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.11635 \n",
      "Title :Continual Learning in Low-rank Orthogonal Subspaces\n",
      "  In continual learning (CL), a learner is faced with a sequence of tasks,\n",
      "arriving one after the other, and the goal is to remember all the tasks once\n",
      "the continual learning experience is finished. The prior art in CL uses\n",
      "episodic memory, parameter regularization or extensible network structures to\n",
      "reduce interference among tasks, but in the end, all the approaches learn\n",
      "different tasks in a joint vector space. We believe this invariably leads to\n",
      "interference among different tasks. We propose to learn tasks in different\n",
      "(low-rank) vector subspaces that are kept orthogonal to each other in order to\n",
      "minimize interference. Further, to keep the gradients of different tasks coming\n",
      "from these subspaces orthogonal to each other, we learn isometric mappings by\n",
      "posing network training as an optimization problem over the Stiefel manifold.\n",
      "To the best of our understanding, we report, for the first time, strong results\n",
      "over experience-replay baseline with and without memory on standard\n",
      "classification benchmarks in continual learning. The code is made publicly\n",
      "available.\n",
      "\n",
      "**Paper Id :1905.01907 \n",
      "Title :Missing Data Imputation with Adversarially-trained Graph Convolutional\n",
      "  Networks\n",
      "  Missing data imputation (MDI) is a fundamental problem in many scientific\n",
      "disciplines. Popular methods for MDI use global statistics computed from the\n",
      "entire data set (e.g., the feature-wise medians), or build predictive models\n",
      "operating independently on every instance. In this paper we propose a more\n",
      "general framework for MDI, leveraging recent work in the field of graph neural\n",
      "networks (GNNs). We formulate the MDI task in terms of a graph denoising\n",
      "autoencoder, where each edge of the graph encodes the similarity between two\n",
      "patterns. A GNN encoder learns to build intermediate representations for each\n",
      "example by interleaving classical projection layers and locally combining\n",
      "information between neighbors, while another decoding GNN learns to reconstruct\n",
      "the full imputed data set from this intermediate embedding. In order to\n",
      "speed-up training and improve the performance, we use a combination of multiple\n",
      "losses, including an adversarial loss implemented with the Wasserstein metric\n",
      "and a gradient penalty. We also explore a few extensions to the basic\n",
      "architecture involving the use of residual connections between layers, and of\n",
      "global statistics computed from the data set to improve the accuracy. On a\n",
      "large experimental evaluation, we show that our method robustly outperforms\n",
      "state-of-the-art approaches for MDI, especially for large percentages of\n",
      "missing values.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.11727 \n",
      "Title :Vision-Based Layout Detection from Scientific Literature using Recurrent\n",
      "  Convolutional Neural Networks\n",
      "  We present an approach for adapting convolutional neural networks for object\n",
      "recognition and classification to scientific literature layout detection\n",
      "(SLLD), a shared subtask of several information extraction problems. Scientific\n",
      "publications contain multiple types of information sought by researchers in\n",
      "various disciplines, organized into an abstract, bibliography, and sections\n",
      "documenting related work, experimental methods, and results; however, there is\n",
      "no effective way to extract this information due to their diverse layout. In\n",
      "this paper, we present a novel approach to developing an end-to-end learning\n",
      "framework to segment and classify major regions of a scientific document. We\n",
      "consider scientific document layout analysis as an object detection task over\n",
      "digital images, without any additional text features that need to be added into\n",
      "the network during the training process. Our technical objective is to\n",
      "implement transfer learning via fine-tuning of pre-trained networks and thereby\n",
      "demonstrate that this deep learning architecture is suitable for tasks that\n",
      "lack very large document corpora for training ab initio. As part of the\n",
      "experimental test bed for empirical evaluation of this approach, we created a\n",
      "merged multi-corpus data set for scientific publication layout detection tasks.\n",
      "Our results show good improvement with fine-tuning of a pre-trained base\n",
      "network using this merged data set, compared to the baseline convolutional\n",
      "neural network architecture.\n",
      "\n",
      "**Paper Id :1910.08978 \n",
      "Title :Attention Enriched Deep Learning Model for Breast Tumor Segmentation in\n",
      "  Ultrasound Images\n",
      "  Incorporating human domain knowledge for breast tumor diagnosis is\n",
      "challenging, since shape, boundary, curvature, intensity, or other common\n",
      "medical priors vary significantly across patients and cannot be employed. This\n",
      "work proposes a new approach for integrating visual saliency into a deep\n",
      "learning model for breast tumor segmentation in ultrasound images. Visual\n",
      "saliency refers to image maps containing regions that are more likely to\n",
      "attract radiologists visual attention. The proposed approach introduces\n",
      "attention blocks into a U-Net architecture, and learns feature representations\n",
      "that prioritize spatial regions with high saliency levels. The validation\n",
      "results demonstrate increased accuracy for tumor segmentation relative to\n",
      "models without salient attention layers. The approach achieved a Dice\n",
      "similarity coefficient of 90.5 percent on a dataset of 510 images. The salient\n",
      "attention model has potential to enhance accuracy and robustness in processing\n",
      "medical images of other organs, by providing a means to incorporate\n",
      "task-specific knowledge into deep learning architectures.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.12088 \n",
      "Title :Adversarial Robustness of Supervised Sparse Coding\n",
      "  Several recent results provide theoretical insights into the phenomena of\n",
      "adversarial examples. Existing results, however, are often limited due to a gap\n",
      "between the simplicity of the models studied and the complexity of those\n",
      "deployed in practice. In this work, we strike a better balance by considering a\n",
      "model that involves learning a representation while at the same time giving a\n",
      "precise generalization bound and a robustness certificate. We focus on the\n",
      "hypothesis class obtained by combining a sparsity-promoting encoder coupled\n",
      "with a linear classifier, and show an interesting interplay between the\n",
      "expressivity and stability of the (supervised) representation map and a notion\n",
      "of margin in the feature space. We bound the robust risk (to $\\ell_2$-bounded\n",
      "perturbations) of hypotheses parameterized by dictionaries that achieve a mild\n",
      "encoder gap on training data. Furthermore, we provide a robustness certificate\n",
      "for end-to-end classification. We demonstrate the applicability of our analysis\n",
      "by computing certified accuracy on real data, and compare with other\n",
      "alternatives for certified robustness.\n",
      "\n",
      "**Paper Id :2010.02004 \n",
      "Title :Assessing Robustness of Text Classification through Maximal Safe Radius\n",
      "  Computation\n",
      "  Neural network NLP models are vulnerable to small modifications of the input\n",
      "that maintain the original meaning but result in a different prediction. In\n",
      "this paper, we focus on robustness of text classification against word\n",
      "substitutions, aiming to provide guarantees that the model prediction does not\n",
      "change if a word is replaced with a plausible alternative, such as a synonym.\n",
      "As a measure of robustness, we adopt the notion of the maximal safe radius for\n",
      "a given input text, which is the minimum distance in the embedding space to the\n",
      "decision boundary. Since computing the exact maximal safe radius is not\n",
      "feasible in practice, we instead approximate it by computing a lower and upper\n",
      "bound. For the upper bound computation, we employ Monte Carlo Tree Search in\n",
      "conjunction with syntactic filtering to analyse the effect of single and\n",
      "multiple word substitutions. The lower bound computation is achieved through an\n",
      "adaptation of the linear bounding techniques implemented in tools CNN-Cert and\n",
      "POPQORN, respectively for convolutional and recurrent network models. We\n",
      "evaluate the methods on sentiment analysis and news classification models for\n",
      "four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and\n",
      "provide an analysis of robustness trends. We also apply our framework to\n",
      "interpretability analysis and compare it with LIME.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.12438 \n",
      "Title :Transferable Graph Optimizers for ML Compilers\n",
      "  Most compilers for machine learning (ML) frameworks need to solve many\n",
      "correlated optimization problems to generate efficient machine code. Current ML\n",
      "compilers rely on heuristics based algorithms to solve these optimization\n",
      "problems one at a time. However, this approach is not only hard to maintain but\n",
      "often leads to sub-optimal solutions especially for newer model architectures.\n",
      "Existing learning based approaches in the literature are sample inefficient,\n",
      "tackle a single optimization problem, and do not generalize to unseen graphs\n",
      "making them infeasible to be deployed in practice. To address these\n",
      "limitations, we propose an end-to-end, transferable deep reinforcement learning\n",
      "method for computational graph optimization (GO), based on a scalable\n",
      "sequential attention mechanism over an inductive graph neural network. GO\n",
      "generates decisions on the entire graph rather than on each individual node\n",
      "autoregressively, drastically speeding up the search compared to prior methods.\n",
      "Moreover, we propose recurrent attention layers to jointly optimize dependent\n",
      "graph optimization tasks and demonstrate 33%-60% speedup on three graph\n",
      "optimization tasks compared to TensorFlow default optimization. On a diverse\n",
      "set of representative graphs consisting of up to 80,000 nodes, including\n",
      "Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21%\n",
      "improvement over human experts and 18% improvement over the prior state of the\n",
      "art with 15x faster convergence, on a device placement task evaluated in real\n",
      "systems.\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.12455 \n",
      "Title :Primal-Dual Mesh Convolutional Neural Networks\n",
      "  Recent works in geometric deep learning have introduced neural networks that\n",
      "allow performing inference tasks on three-dimensional geometric data by\n",
      "defining convolution, and sometimes pooling, operations on triangle meshes.\n",
      "These methods, however, either consider the input mesh as a graph, and do not\n",
      "exploit specific geometric properties of meshes for feature aggregation and\n",
      "downsampling, or are specialized for meshes, but rely on a rigid definition of\n",
      "convolution that does not properly capture the local topology of the mesh. We\n",
      "propose a method that combines the advantages of both types of approaches,\n",
      "while addressing their limitations: we extend a primal-dual framework drawn\n",
      "from the graph-neural-network literature to triangle meshes, and define\n",
      "convolutions on two types of graphs constructed from an input mesh. Our method\n",
      "takes features for both edges and faces of a 3D mesh as input and dynamically\n",
      "aggregates them using an attention mechanism. At the same time, we introduce a\n",
      "pooling operation with a precise geometric interpretation, that allows handling\n",
      "variations in the mesh connectivity by clustering mesh faces in a task-driven\n",
      "fashion. We provide theoretical insights of our approach using tools from the\n",
      "mesh-simplification literature. In addition, we validate experimentally our\n",
      "method in the tasks of shape classification and shape segmentation, where we\n",
      "obtain comparable or superior performance to the state of the art.\n",
      "\n",
      "**Paper Id :2006.13791 \n",
      "Title :Post-DAE: Anatomically Plausible Segmentation via Post-Processing with\n",
      "  Denoising Autoencoders\n",
      "  We introduce Post-DAE, a post-processing method based on denoising\n",
      "autoencoders (DAE) to improve the anatomical plausibility of arbitrary\n",
      "biomedical image segmentation algorithms. Some of the most popular segmentation\n",
      "methods (e.g. based on convolutional neural networks or random forest\n",
      "classifiers) incorporate additional post-processing steps to ensure that the\n",
      "resulting masks fulfill expected connectivity constraints. These methods\n",
      "operate under the hypothesis that contiguous pixels with similar aspect should\n",
      "belong to the same class. Even if valid in general, this assumption does not\n",
      "consider more complex priors like topological restrictions or convexity, which\n",
      "cannot be easily incorporated into these methods. Post-DAE leverages the latest\n",
      "developments in manifold learning via denoising autoencoders. First, we learn a\n",
      "compact and non-linear embedding that represents the space of anatomically\n",
      "plausible segmentations. Then, given a segmentation mask obtained with an\n",
      "arbitrary method, we reconstruct its anatomically plausible version by\n",
      "projecting it onto the learnt manifold. The proposed method is trained using\n",
      "unpaired segmentation mask, what makes it independent of intensity information\n",
      "and image modality. We performed experiments in binary and multi-label\n",
      "segmentation of chest X-ray and cardiac magnetic resonance images. We show how\n",
      "erroneous and noisy segmentation masks can be improved using Post-DAE. With\n",
      "almost no additional computation cost, our method brings erroneous\n",
      "segmentations back to a feasible space.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.12711 \n",
      "Title :On Convergence and Generalization of Dropout Training\n",
      "  We study dropout in two-layer neural networks with rectified linear unit\n",
      "(ReLU) activations. Under mild overparametrization and assuming that the\n",
      "limiting kernel can separate the data distribution with a positive margin, we\n",
      "show that dropout training with logistic loss achieves $\\epsilon$-suboptimality\n",
      "in test error in $O(1/\\epsilon)$ iterations.\n",
      "\n",
      "**Paper Id :2006.16540 \n",
      "Title :Associative Memory in Iterated Overparameterized Sigmoid Autoencoders\n",
      "  Recent work showed that overparameterized autoencoders can be trained to\n",
      "implement associative memory via iterative maps, when the trained input-output\n",
      "Jacobian of the network has all of its eigenvalue norms strictly below one.\n",
      "Here, we theoretically analyze this phenomenon for sigmoid networks by\n",
      "leveraging recent developments in deep learning theory, especially the\n",
      "correspondence between training neural networks in the infinite-width limit and\n",
      "performing kernel regression with the Neural Tangent Kernel (NTK). We find that\n",
      "overparameterized sigmoid autoencoders can have attractors in the NTK limit for\n",
      "both training with a single example and multiple examples under certain\n",
      "conditions. In particular, for multiple training examples, we find that the\n",
      "norm of the largest Jacobian eigenvalue drops below one with increasing input\n",
      "norm, leading to associative memory.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.12933 \n",
      "Title :Triclustering in Big Data Setting\n",
      "  In this paper, we describe versions of triclustering algorithms adapted for\n",
      "efficient calculations in distributed environments with MapReduce model or\n",
      "parallelisation mechanism provided by modern programming languages. OAC-family\n",
      "of triclustering algorithms shows good parallelisation capabilities due to the\n",
      "independent processing of triples of a triadic formal context. We provide the\n",
      "time and space complexity of the algorithms and justify their relevance. We\n",
      "also compare performance gain from using a distributed system and scalability.\n",
      "\n",
      "**Paper Id :1904.06517 \n",
      "Title :Improving detection of protein-ligand binding sites with 3D segmentation\n",
      "  In recent years machine learning (ML) took bio- and cheminformatics fields by\n",
      "storm, providing new solutions for a vast repertoire of problems related to\n",
      "protein sequence, structure, and interactions analysis. ML techniques, deep\n",
      "neural networks especially, were proven more effective than classical models\n",
      "for tasks like predicting binding affinity for molecular complex. In this work\n",
      "we investigated the earlier stage of drug discovery process - finding druggable\n",
      "pockets on protein surface, that can be later used to design active molecules.\n",
      "For this purpose we developed a 3D fully convolutional neural network capable\n",
      "of binding site segmentation. Our solution has high prediction accuracy and\n",
      "provides intuitive representations of the results, which makes it easy to\n",
      "incorporate into drug discovery projects. The model's source code, together\n",
      "with scripts for most common use-cases is freely available at\n",
      "http://gitlab.com/cheminfIBB/kalasanty\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.13032 \n",
      "Title :Byzantine Resilient Distributed Multi-Task Learning\n",
      "  Distributed multi-task learning provides significant advantages in\n",
      "multi-agent networks with heterogeneous data sources where agents aim to learn\n",
      "distinct but correlated models simultaneously. However, distributed algorithms\n",
      "for learning relatedness among tasks are not resilient in the presence of\n",
      "Byzantine agents. In this paper, we present an approach for Byzantine resilient\n",
      "distributed multi-task learning. We propose an efficient online weight\n",
      "assignment rule by measuring the accumulated loss using an agent's data and its\n",
      "neighbors' models. A small accumulated loss indicates a large similarity\n",
      "between the two tasks. In order to ensure the Byzantine resilience of the\n",
      "aggregation at a normal agent, we introduce a step for filtering out larger\n",
      "losses. We analyze the approach for convex models and show that normal agents\n",
      "converge resiliently towards their true targets. Further, an agent's learning\n",
      "performance using the proposed weight assignment rule is guaranteed to be at\n",
      "least as good as in the non-cooperative case as measured by the expected\n",
      "regret. Finally, we demonstrate the approach using three case studies,\n",
      "including regression and classification problems, and show that our method\n",
      "exhibits good empirical performance for non-convex models, such as\n",
      "convolutional neural networks.\n",
      "\n",
      "**Paper Id :1911.10120 \n",
      "Title :Multi-Agent Thompson Sampling for Bandit Applications with Sparse\n",
      "  Neighbourhood Structures\n",
      "  Multi-agent coordination is prevalent in many real-world applications.\n",
      "However, such coordination is challenging due to its combinatorial nature. An\n",
      "important observation in this regard is that agents in the real world often\n",
      "only directly affect a limited set of neighbouring agents. Leveraging such\n",
      "loose couplings among agents is key to making coordination in multi-agent\n",
      "systems feasible. In this work, we focus on learning to coordinate.\n",
      "Specifically, we consider the multi-agent multi-armed bandit framework, in\n",
      "which fully cooperative loosely-coupled agents must learn to coordinate their\n",
      "decisions to optimize a common objective. We propose multi-agent Thompson\n",
      "sampling (MATS), a new Bayesian exploration-exploitation algorithm that\n",
      "leverages loose couplings. We provide a regret bound that is sublinear in time\n",
      "and low-order polynomial in the highest number of actions of a single agent for\n",
      "sparse coordination graphs. Additionally, we empirically show that MATS\n",
      "outperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks,\n",
      "and a novel benchmark with Poisson distributions. An example of a\n",
      "loosely-coupled multi-agent system is a wind farm. Coordination within the wind\n",
      "farm is necessary to maximize power production. As upstream wind turbines only\n",
      "affect nearby downstream turbines, we can use MATS to efficiently learn the\n",
      "optimal control mechanism for the farm. To demonstrate the benefits of our\n",
      "method toward applications we apply MATS to a realistic wind farm control task.\n",
      "In this task, wind turbines must coordinate their alignments with respect to\n",
      "the incoming wind vector in order to optimize power production. Our results\n",
      "show that MATS improves significantly upon state-of-the-art coordination\n",
      "methods in terms of performance, demonstrating the value of using MATS in\n",
      "practical applications with sparse neighbourhood structures.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.13924 \n",
      "Title :Benchmarking Deep Learning Interpretability in Time Series Predictions\n",
      "  Saliency methods are used extensively to highlight the importance of input\n",
      "features in model predictions. These methods are mostly used in vision and\n",
      "language tasks, and their applications to time series data is relatively\n",
      "unexplored. In this paper, we set out to extensively compare the performance of\n",
      "various saliency-based interpretability methods across diverse neural\n",
      "architectures, including Recurrent Neural Network, Temporal Convolutional\n",
      "Networks, and Transformers in a new benchmark of synthetic time series data. We\n",
      "propose and report multiple metrics to empirically evaluate the performance of\n",
      "saliency methods for detecting feature importance over time using both\n",
      "precision (i.e., whether identified features contain meaningful signals) and\n",
      "recall (i.e., the number of features with signal identified as important).\n",
      "Through several experiments, we show that (i) in general, network architectures\n",
      "and saliency methods fail to reliably and accurately identify feature\n",
      "importance over time in time series data, (ii) this failure is mainly due to\n",
      "the conflation of time and feature domains, and (iii) the quality of saliency\n",
      "maps can be improved substantially by using our proposed two-step temporal\n",
      "saliency rescaling (TSR) approach that first calculates the importance of each\n",
      "time step before calculating the importance of each feature at a time step.\n",
      "\n",
      "**Paper Id :2007.14254 \n",
      "Title :Improving Robustness on Seasonality-Heavy Multivariate Time Series\n",
      "  Anomaly Detection\n",
      "  Robust Anomaly Detection (AD) on time series data is a key component for\n",
      "monitoring many complex modern systems. These systems typically generate\n",
      "high-dimensional time series that can be highly noisy, seasonal, and\n",
      "inter-correlated. This paper explores some of the challenges in such data, and\n",
      "proposes a new approach that makes inroads towards increased robustness on\n",
      "seasonal and contaminated data, while providing a better root cause\n",
      "identification of anomalies. In particular, we propose the use of Robust\n",
      "Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends\n",
      "recent advancements in GAN with the adoption of convolutional-LSTM layers and\n",
      "attention mechanisms to produce excellent performance on various settings. We\n",
      "conduct extensive experiments in which not only do this model displays more\n",
      "robust behavior on complex seasonality patterns, but also shows increased\n",
      "resistance to training data contamination. We compare it with existing\n",
      "classical and deep-learning AD models, and show that this architecture is\n",
      "associated with the lowest false positive rate and improves precision by 30%\n",
      "and 16% in real-world and synthetic data, respectively.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.13938 \n",
      "Title :Neural Unsigned Distance Fields for Implicit Function Learning\n",
      "  In this work we target a learnable output representation that allows\n",
      "continuous, high resolution outputs of arbitrary shape. Recent works represent\n",
      "3D surfaces implicitly with a Neural Network, thereby breaking previous\n",
      "barriers in resolution, and ability to represent diverse topologies. However,\n",
      "neural implicit representations are limited to closed surfaces, which divide\n",
      "the space into inside and outside. Many real world objects such as walls of a\n",
      "scene scanned by a sensor, clothing, or a car with inner structures are not\n",
      "closed. This constitutes a significant barrier, in terms of data pre-processing\n",
      "(objects need to be artificially closed creating artifacts), and the ability to\n",
      "output open surfaces. In this work, we propose Neural Distance Fields (NDF), a\n",
      "neural network based model which predicts the unsigned distance field for\n",
      "arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high\n",
      "resolutions as prior implicit models, but do not require closed surface data,\n",
      "and significantly broaden the class of representable shapes in the output. NDF\n",
      "allow to extract the surface as very dense point clouds and as meshes. We also\n",
      "show that NDF allow for surface normal calculation and can be rendered using a\n",
      "slight modification of sphere tracing. We find NDF can be used for multi-target\n",
      "regression (multiple outputs for one input) with techniques that have been\n",
      "exclusively used for rendering in graphics. Experiments on ShapeNet show that\n",
      "NDF, while simple, is the state-of-the art, and allows to reconstruct shapes\n",
      "with inner structures, such as the chairs inside a bus. Notably, we show that\n",
      "NDF are not restricted to 3D shapes, and can approximate more general open\n",
      "surfaces such as curves, manifolds, and functions. Code is available for\n",
      "research at https://virtualhumans.mpi-inf.mpg.de/ndf/.\n",
      "\n",
      "**Paper Id :2003.01456 \n",
      "Title :Implicit Functions in Feature Space for 3D Shape Reconstruction and\n",
      "  Completion\n",
      "  While many works focus on 3D reconstruction from images, in this paper, we\n",
      "focus on 3D shape reconstruction and completion from a variety of 3D inputs,\n",
      "which are deficient in some respect: low and high resolution voxels, sparse and\n",
      "dense point clouds, complete or incomplete. Processing of such 3D inputs is an\n",
      "increasingly important problem as they are the output of 3D scanners, which are\n",
      "becoming more accessible, and are the intermediate output of 3D computer vision\n",
      "algorithms. Recently, learned implicit functions have shown great promise as\n",
      "they produce continuous reconstructions. However, we identified two limitations\n",
      "in reconstruction from 3D inputs: 1) details present in the input data are not\n",
      "retained, and 2) poor reconstruction of articulated humans. To solve this, we\n",
      "propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,\n",
      "can handle multiple topologies, and complete shapes for missing or sparse input\n",
      "data retaining the nice properties of recent learned implicit functions, but\n",
      "critically they can also retain detail when it is present in the input data,\n",
      "and can reconstruct articulated humans. Our work differs from prior work in two\n",
      "crucial aspects. First, instead of using a single vector to encode a 3D shape,\n",
      "we extract a learnable 3-dimensional multi-scale tensor of deep features, which\n",
      "is aligned with the original Euclidean space embedding the shape. Second,\n",
      "instead of classifying x-y-z point coordinates directly, we classify deep\n",
      "features extracted from the tensor at a continuous query point. We show that\n",
      "this forces our model to make decisions based on global and local shape\n",
      "structure, as opposed to point coordinates, which are arbitrary under Euclidean\n",
      "transformations. Experiments demonstrate that IF-Nets clearly outperform prior\n",
      "work in 3D object reconstruction in ShapeNet, and obtain significantly more\n",
      "accurate 3D human reconstructions.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.14487 \n",
      "Title :Improved Guarantees for k-means++ and k-means++ Parallel\n",
      "  In this paper, we study k-means++ and k-means++ parallel, the two most\n",
      "popular algorithms for the classic k-means clustering problem. We provide novel\n",
      "analyses and show improved approximation and bi-criteria approximation\n",
      "guarantees for k-means++ and k-means++ parallel. Our results give a better\n",
      "theoretical justification for why these algorithms perform extremely well in\n",
      "practice. We also propose a new variant of k-means++ parallel algorithm\n",
      "(Exponential Race k-means++) that has the same approximation guarantees as\n",
      "k-means++.\n",
      "\n",
      "**Paper Id :1907.02929 \n",
      "Title :Improved local search for graph edit distance\n",
      "  The graph edit distance (GED) measures the dissimilarity between two graphs\n",
      "as the minimal cost of a sequence of elementary operations transforming one\n",
      "graph into another. This measure is fundamental in many areas such as\n",
      "structural pattern recognition or classification. However, exactly computing\n",
      "GED is NP-hard. Among different classes of heuristic algorithms that were\n",
      "proposed to compute approximate solutions, local search based algorithms\n",
      "provide the tightest upper bounds for GED. In this paper, we present K-REFINE\n",
      "and RANDPOST. K-REFINE generalizes and improves an existing local search\n",
      "algorithm and performs particularly well on small graphs. RANDPOST is a general\n",
      "warm start framework that stochastically generates promising initial solutions\n",
      "to be used by any local search based GED algorithm. It is particularly\n",
      "efficient on large graphs. An extensive empirical evaluation demonstrates that\n",
      "both K-REFINE and RANDPOST perform excellently in practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.15288 \n",
      "Title :Speech-Image Semantic Alignment Does Not Depend on Any Prior\n",
      "  Classification Tasks\n",
      "  Semantically-aligned $(speech, image)$ datasets can be used to explore\n",
      "\"visually-grounded speech\". In a majority of existing investigations, features\n",
      "of an image signal are extracted using neural networks \"pre-trained\" on other\n",
      "tasks (e.g., classification on ImageNet). In still others, pre-trained networks\n",
      "are used to extract audio features prior to semantic embedding. Without\n",
      "\"transfer learning\" through pre-trained initialization or pre-trained feature\n",
      "extraction, previous results have tended to show low rates of recall in $speech\n",
      "\\rightarrow image$ and $image \\rightarrow speech$ queries.\n",
      "  Choosing appropriate neural architectures for encoders in the speech and\n",
      "image branches and using large datasets, one can obtain competitive recall\n",
      "rates without any reliance on any pre-trained initialization or feature\n",
      "extraction: $(speech,image)$ semantic alignment and $speech \\rightarrow image$\n",
      "and $image \\rightarrow speech$ retrieval are canonical tasks worthy of\n",
      "independent investigation of their own and allow one to explore other\n",
      "questions---e.g., the size of the audio embedder can be reduced significantly\n",
      "with little loss of recall rates in $speech \\rightarrow image$ and $image\n",
      "\\rightarrow speech$ queries.\n",
      "\n",
      "**Paper Id :2010.04303 \n",
      "Title :How Can Self-Attention Networks Recognize Dyck-n Languages?\n",
      "  We focus on the recognition of Dyck-n ($\\mathcal{D}_n$) languages with\n",
      "self-attention (SA) networks, which has been deemed to be a difficult task for\n",
      "these networks. We compare the performance of two variants of SA, one with a\n",
      "starting symbol (SA$^+$) and one without (SA$^-$). Our results show that SA$^+$\n",
      "is able to generalize to longer sequences and deeper dependencies. For\n",
      "$\\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences\n",
      "whereas the accuracy of SA$^+$ is 58.82$\\%$. We find attention maps learned by\n",
      "$\\text{SA}{^+}$ to be amenable to interpretation and compatible with a\n",
      "stack-based language recognizer. Surprisingly, the performance of SA networks\n",
      "is at par with LSTMs, which provides evidence on the ability of SA to learn\n",
      "hierarchies without recursion.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.15969 \n",
      "Title :Greedy Optimization Provably Wins the Lottery: Logarithmic Number of\n",
      "  Winning Tickets is Enough\n",
      "  Despite the great success of deep learning, recent works show that large deep\n",
      "neural networks are often highly redundant and can be significantly reduced in\n",
      "size. However, the theoretical question of how much we can prune a neural\n",
      "network given a specified tolerance of accuracy drop is still open. This paper\n",
      "provides one answer to this question by proposing a greedy optimization based\n",
      "pruning method. The proposed method has the guarantee that the discrepancy\n",
      "between the pruned network and the original network decays with exponentially\n",
      "fast rate w.r.t. the size of the pruned network, under weak assumptions that\n",
      "apply for most practical settings. Empirically, our method improves prior arts\n",
      "on pruning various network architectures including ResNet, MobilenetV2/V3 on\n",
      "ImageNet.\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2010.16336 \n",
      "Title :Leveraging Extracted Model Adversaries for Improved Black Box Attacks\n",
      "  We present a method for adversarial input generation against black box models\n",
      "for reading comprehension based question answering. Our approach is composed of\n",
      "two steps. First, we approximate a victim black box model via model extraction\n",
      "(Krishna et al., 2020). Second, we use our own white box method to generate\n",
      "input perturbations that cause the approximate model to fail. These perturbed\n",
      "inputs are used against the victim. In experiments we find that our method\n",
      "improves on the efficacy of the AddAny---a white box attack---performed on the\n",
      "approximate model by 25% F1, and the AddSent attack---a black box attack---by\n",
      "11% F1 (Jia and Liang, 2017).\n",
      "\n",
      "**Paper Id :1904.09433 \n",
      "Title :Can Machine Learning Model with Static Features be Fooled: an\n",
      "  Adversarial Machine Learning Approach\n",
      "  The widespread adoption of smartphones dramatically increases the risk of\n",
      "attacks and the spread of mobile malware, especially on the Android platform.\n",
      "Machine learning-based solutions have been already used as a tool to supersede\n",
      "signature-based anti-malware systems. However, malware authors leverage\n",
      "features from malicious and legitimate samples to estimate statistical\n",
      "difference in-order to create adversarial examples. Hence, to evaluate the\n",
      "vulnerability of machine learning algorithms in malware detection, we propose\n",
      "five different attack scenarios to perturb malicious applications (apps). By\n",
      "doing this, the classification algorithm inappropriately fits the discriminant\n",
      "function on the set of data points, eventually yielding a higher\n",
      "misclassification rate. Further, to distinguish the adversarial examples from\n",
      "benign samples, we propose two defense mechanisms to counter attacks. To\n",
      "validate our attacks and solutions, we test our model on three different\n",
      "benchmark datasets. We also test our methods using various classifier\n",
      "algorithms and compare them with the state-of-the-art data poisoning method\n",
      "using the Jacobian matrix. Promising results show that generated adversarial\n",
      "samples can evade detection with a very high probability. Additionally, evasive\n",
      "variants generated by our attack models when used to harden the developed\n",
      "anti-malware system improves the detection rate up to 50% when using the\n",
      "Generative Adversarial Network (GAN) method.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.00359 \n",
      "Title :TartanVO: A Generalizable Learning-based VO\n",
      "  We present the first learning-based visual odometry (VO) model, which\n",
      "generalizes to multiple datasets and real-world scenarios and outperforms\n",
      "geometry-based methods in challenging scenes. We achieve this by leveraging the\n",
      "SLAM dataset TartanAir, which provides a large amount of diverse synthetic data\n",
      "in challenging environments. Furthermore, to make our VO model generalize\n",
      "across datasets, we propose an up-to-scale loss function and incorporate the\n",
      "camera intrinsic parameters into the model. Experiments show that a single\n",
      "model, TartanVO, trained only on synthetic data, without any finetuning, can be\n",
      "generalized to real-world datasets such as KITTI and EuRoC, demonstrating\n",
      "significant advantages over the geometry-based methods on challenging\n",
      "trajectories. Our code is available at https://github.com/castacks/tartanvo.\n",
      "\n",
      "**Paper Id :2008.02492 \n",
      "Title :Zero-Shot Multi-View Indoor Localization via Graph Location Networks\n",
      "  Indoor localization is a fundamental problem in location-based applications.\n",
      "Current approaches to this problem typically rely on Radio Frequency\n",
      "technology, which requires not only supporting infrastructures but human\n",
      "efforts to measure and calibrate the signal. Moreover, data collection for all\n",
      "locations is indispensable in existing methods, which in turn hinders their\n",
      "large-scale deployment. In this paper, we propose a novel neural network based\n",
      "architecture Graph Location Networks (GLN) to perform infrastructure-free,\n",
      "multi-view image based indoor localization. GLN makes location predictions\n",
      "based on robust location representations extracted from images through\n",
      "message-passing networks. Furthermore, we introduce a novel zero-shot indoor\n",
      "localization setting and tackle it by extending the proposed GLN to a dedicated\n",
      "zero-shot version, which exploits a novel mechanism Map2Vec to train\n",
      "location-aware embeddings and make predictions on novel unseen locations. Our\n",
      "extensive experiments show that the proposed approach outperforms\n",
      "state-of-the-art methods in the standard setting, and achieves promising\n",
      "accuracy even in the zero-shot setting where data for half of the locations are\n",
      "not available. The source code and datasets are publicly available at\n",
      "https://github.com/coldmanck/zero-shot-indoor-localization-release.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.01148 \n",
      "Title :SIMDive: Approximate SIMD Soft Multiplier-Divider for FPGAs with Tunable\n",
      "  Accuracy\n",
      "  The ever-increasing quest for data-level parallelism and variable precision\n",
      "in ubiquitous multimedia and Deep Neural Network (DNN) applications has\n",
      "motivated the use of Single Instruction, Multiple Data (SIMD) architectures. To\n",
      "alleviate energy as their main resource constraint, approximate computing has\n",
      "re-emerged,albeit mainly specialized for their Application-Specific Integrated\n",
      "Circuit (ASIC) implementations. This paper, presents for the first time, an\n",
      "SIMD architecture based on novel multiplier and divider with tunable accuracy,\n",
      "targeted for Field-Programmable Gate Arrays (FPGAs). The proposed hybrid\n",
      "architecture implements Mitchell's algorithms and supports precision\n",
      "variability from 8 to 32 bits. Experimental results obtained from Vivado,\n",
      "multimedia and DNN applications indicate superiority of proposed architecture\n",
      "(both SISD and SIMD) over accurate and state-of-the-art approximate\n",
      "counterparts. In particular, the proposed SISD divider outperforms the accurate\n",
      "Intellectual Property (IP) divider provided by Xilinx with 4x higher speed and\n",
      "4.6x less energy and tolerating only < 0.8% error. Moreover, the proposed SIMD\n",
      "multiplier-divider supersede accurate SIMD multiplier by achieving up to 26%,\n",
      "45%, 36%, and 56% improvement in area, throughput, power, and energy,\n",
      "respectively.\n",
      "\n",
      "**Paper Id :1909.09153 \n",
      "Title :Density Encoding Enables Resource-Efficient Randomly Connected Neural\n",
      "  Networks\n",
      "  The deployment of machine learning algorithms on resource-constrained edge\n",
      "devices is an important challenge from both theoretical and applied points of\n",
      "view. In this article, we focus on resource-efficient randomly connected neural\n",
      "networks known as Random Vector Functional Link (RVFL) networks since their\n",
      "simple design and extremely fast training time make them very attractive for\n",
      "solving many applied classification tasks. We propose to represent input\n",
      "features via the density-based encoding known in the area of stochastic\n",
      "computing and use the operations of binding and bundling from the area of\n",
      "hyperdimensional computing for obtaining the activations of the hidden neurons.\n",
      "Using a collection of 121 real-world datasets from the UCI Machine Learning\n",
      "Repository, we empirically show that the proposed approach demonstrates higher\n",
      "average accuracy than the conventional RVFL. We also demonstrate that it is\n",
      "possible to represent the readout matrix using only integers in a limited range\n",
      "with minimal loss in the accuracy. In this case, the proposed approach operates\n",
      "only on small n-bits integers, which results in a computationally efficient\n",
      "architecture. Finally, through hardware FPGA implementations, we show that such\n",
      "an approach consumes approximately eleven times less energy than that of the\n",
      "conventional RVFL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.01355 \n",
      "Title :Patch2Self: Denoising Diffusion MRI with Self-Supervised Learning\n",
      "  Diffusion-weighted magnetic resonance imaging (DWI) is the only noninvasive\n",
      "method for quantifying microstructure and reconstructing white-matter pathways\n",
      "in the living human brain. Fluctuations from multiple sources create\n",
      "significant additive noise in DWI data which must be suppressed before\n",
      "subsequent microstructure analysis. We introduce a self-supervised learning\n",
      "method for denoising DWI data, Patch2Self, which uses the entire volume to\n",
      "learn a full-rank locally linear denoiser for that volume. By taking advantage\n",
      "of the oversampled q-space of DWI data, Patch2Self can separate structure from\n",
      "noise without requiring an explicit model for either. We demonstrate the\n",
      "effectiveness of Patch2Self via quantitative and qualitative improvements in\n",
      "microstructure modeling, tracking (via fiber bundle coherency) and model\n",
      "estimation relative to other unsupervised methods on real and simulated data.\n",
      "\n",
      "**Paper Id :2004.09610 \n",
      "Title :Deep variational network for rapid 4D flow MRI reconstruction\n",
      "  Phase-contrast magnetic resonance imaging (MRI) provides time-resolved\n",
      "quantification of blood flow dynamics that can aid clinical diagnosis. Long in\n",
      "vivo scan times due to repeated three-dimensional (3D) volume sampling over\n",
      "cardiac phases and breathing cycles necessitate accelerated imaging techniques\n",
      "that leverage data correlations. Standard compressed sensing reconstruction\n",
      "methods require tuning of hyperparameters and are computationally expensive,\n",
      "which diminishes the potential reduction of examination times. We propose an\n",
      "efficient model-based deep neural reconstruction network and evaluate its\n",
      "performance on clinical aortic flow data. The network is shown to reconstruct\n",
      "undersampled 4D flow MRI data in under a minute on standard consumer hardware.\n",
      "Remarkably, the relatively low amounts of tunable parameters allowed the\n",
      "network to be trained on images from 11 reference scans while generalizing well\n",
      "to retrospective and prospective undersampled data for various acceleration\n",
      "factors and anatomies.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.01799 \n",
      "Title :Ensuring Dataset Quality for Machine Learning Certification\n",
      "  In this paper, we address the problem of dataset quality in the context of\n",
      "Machine Learning (ML)-based critical systems. We briefly analyse the\n",
      "applicability of some existing standards dealing with data and show that the\n",
      "specificities of the ML context are neither properly captured nor taken into\n",
      "ac-count. As a first answer to this concerning situation, we propose a dataset\n",
      "specification and verification process, and apply it on a signal recognition\n",
      "system from the railway domain. In addi-tion, we also give a list of\n",
      "recommendations for the collection and management of datasets. This work is one\n",
      "step towards the dataset engineering process that will be required for ML to be\n",
      "used on safety critical systems.\n",
      "\n",
      "**Paper Id :2010.11010 \n",
      "Title :Complex data labeling with deep learning methods: Lessons from fisheries\n",
      "  acoustics\n",
      "  Quantitative and qualitative analysis of acoustic backscattered signals from\n",
      "the seabed bottom to the sea surface is used worldwide for fish stocks\n",
      "assessment and marine ecosystem monitoring. Huge amounts of raw data are\n",
      "collected yet require tedious expert labeling. This paper focuses on a case\n",
      "study where the ground truth labels are non-obvious: echograms labeling, which\n",
      "is time-consuming and critical for the quality of fisheries and ecological\n",
      "analysis. We investigate how these tasks can benefit from supervised learning\n",
      "algorithms and demonstrate that convolutional neural networks trained with\n",
      "non-stationary datasets can be used to stress parts of a new dataset needing\n",
      "human expert correction. Further development of this approach paves the way\n",
      "toward a standardization of the labeling process in fisheries acoustics and is\n",
      "a good case study for non-obvious data labeling processes.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.01821 \n",
      "Title :Minimax Pareto Fairness: A Multi Objective Perspective\n",
      "  In this work we formulate and formally characterize group fairness as a\n",
      "multi-objective optimization problem, where each sensitive group risk is a\n",
      "separate objective. We propose a fairness criterion where a classifier achieves\n",
      "minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary\n",
      "harm, and can lead to the best zero-gap model if policy dictates so. We provide\n",
      "a simple optimization algorithm compatible with deep neural networks to satisfy\n",
      "these constraints. Since our method does not require test-time access to\n",
      "sensitive attributes, it can be applied to reduce worst-case classification\n",
      "errors between outcomes in unbalanced classification problems. We test the\n",
      "proposed methodology on real case-studies of predicting income, ICU patient\n",
      "mortality, skin lesions classification, and assessing credit risk,\n",
      "demonstrating how our framework compares favorably to other approaches.\n",
      "\n",
      "**Paper Id :1905.00424 \n",
      "Title :An ADMM Based Framework for AutoML Pipeline Configuration\n",
      "  We study the AutoML problem of automatically configuring machine learning\n",
      "pipelines by jointly selecting algorithms and their appropriate\n",
      "hyper-parameters for all steps in supervised learning pipelines. This black-box\n",
      "(gradient-free) optimization with mixed integer & continuous variables is a\n",
      "challenging problem. We propose a novel AutoML scheme by leveraging the\n",
      "alternating direction method of multipliers (ADMM). The proposed framework is\n",
      "able to (i) decompose the optimization problem into easier sub-problems that\n",
      "have a reduced number of variables and circumvent the challenge of mixed\n",
      "variable categories, and (ii) incorporate black-box constraints along-side the\n",
      "black-box optimization objective. We empirically evaluate the flexibility (in\n",
      "utilizing existing AutoML techniques), effectiveness (against open source\n",
      "AutoML toolkits),and unique capability (of executing AutoML with practically\n",
      "motivated black-box constraints) of our proposed scheme on a collection of\n",
      "binary classification data sets from UCI ML& OpenML repositories. We observe\n",
      "that on an average our framework provides significant gains in comparison to\n",
      "other AutoML frameworks (Auto-sklearn & TPOT), highlighting the practical\n",
      "advantages of this framework.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.01845 \n",
      "Title :Specialization in Hierarchical Learning Systems\n",
      "  Joining multiple decision-makers together is a powerful way to obtain more\n",
      "sophisticated decision-making systems, but requires to address the questions of\n",
      "division of labor and specialization. We investigate in how far information\n",
      "constraints in hierarchies of experts not only provide a principled method for\n",
      "regularization but also to enforce specialization. In particular, we devise an\n",
      "information-theoretically motivated on-line learning rule that allows\n",
      "partitioning of the problem space into multiple sub-problems that can be solved\n",
      "by the individual experts. We demonstrate two different ways to apply our\n",
      "method: (i) partitioning problems based on individual data samples and (ii)\n",
      "based on sets of data samples representing tasks. Approach (i) equips the\n",
      "system with the ability to solve complex decision-making problems by finding an\n",
      "optimal combination of local expert decision-makers. Approach (ii) leads to\n",
      "decision-makers specialized in solving families of tasks, which equips the\n",
      "system with the ability to solve meta-learning problems. We show the broad\n",
      "applicability of our approach on a range of problems including classification,\n",
      "regression, density estimation, and reinforcement learning problems, both in\n",
      "the standard machine learning setup and in a meta-learning setting.\n",
      "\n",
      "**Paper Id :2003.06005 \n",
      "Title :Model Agnostic Multilevel Explanations\n",
      "  In recent years, post-hoc local instance-level and global dataset-level\n",
      "explainability of black-box models has received a lot of attention. Much less\n",
      "attention has been given to obtaining insights at intermediate or group levels,\n",
      "which is a need outlined in recent works that study the challenges in realizing\n",
      "the guidelines in the General Data Protection Regulation (GDPR). In this paper,\n",
      "we propose a meta-method that, given a typical local explainability method, can\n",
      "build a multilevel explanation tree. The leaves of this tree correspond to the\n",
      "local explanations, the root corresponds to the global explanation, and\n",
      "intermediate levels correspond to explanations for groups of data points that\n",
      "it automatically clusters. The method can also leverage side information, where\n",
      "users can specify points for which they may want the explanations to be\n",
      "similar. We argue that such a multilevel structure can also be an effective\n",
      "form of communication, where one could obtain few explanations that\n",
      "characterize the entire dataset by considering an appropriate level in our\n",
      "explanation tree. Explanations for novel test points can be cost-efficiently\n",
      "obtained by associating them with the closest training points. When the local\n",
      "explainability technique is generalized additive (viz. LIME, GAMs), we develop\n",
      "a fast approximate algorithm for building the multilevel tree and study its\n",
      "convergence behavior. We validate the effectiveness of the proposed technique\n",
      "based on two human studies -- one with experts and the other with non-expert\n",
      "users -- on real world datasets, and show that we produce high fidelity sparse\n",
      "explanations on several other public datasets.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.02179 \n",
      "Title :Node-Centric Graph Learning from Data for Brain State Identification\n",
      "  Data-driven graph learning models a network by determining the strength of\n",
      "connections between its nodes. The data refers to a graph signal which\n",
      "associates a value with each graph node. Existing graph learning methods either\n",
      "use simplified models for the graph signal, or they are prohibitively expensive\n",
      "in terms of computational and memory requirements. This is particularly true\n",
      "when the number of nodes is high or there are temporal changes in the network.\n",
      "In order to consider richer models with a reasonable computational\n",
      "tractability, we introduce a graph learning method based on representation\n",
      "learning on graphs. Representation learning generates an embedding for each\n",
      "graph node, taking the information from neighbouring nodes into account. Our\n",
      "graph learning method further modifies the embeddings to compute the graph\n",
      "similarity matrix. In this work, graph learning is used to examine brain\n",
      "networks for brain state identification. We infer time-varying brain graphs\n",
      "from an extensive dataset of intracranial electroencephalographic (iEEG)\n",
      "signals from ten patients. We then apply the graphs as input to a classifier to\n",
      "distinguish seizure vs. non-seizure brain states. Using the binary\n",
      "classification metric of area under the receiver operating characteristic curve\n",
      "(AUC), this approach yields an average of 9.13 percent improvement when\n",
      "compared to two widely used brain network modeling methods.\n",
      "\n",
      "**Paper Id :1811.00821 \n",
      "Title :OrthoNet: Multilayer Network Data Clustering\n",
      "  Network data appears in very diverse applications, like biological, social,\n",
      "or sensor networks. Clustering of network nodes into categories or communities\n",
      "has thus become a very common task in machine learning and data mining. Network\n",
      "data comes with some information about the network edges. In some cases, this\n",
      "network information can even be given with multiple views or multiple layers,\n",
      "each one representing a different type of relationship between the network\n",
      "nodes. Increasingly often, network nodes also carry a feature vector. We\n",
      "propose in this paper to extend the node clustering problem, that commonly\n",
      "considers only the network information, to a problem where both the network\n",
      "information and the node features are considered together for learning a\n",
      "clustering-friendly representation of the feature space. Specifically, we\n",
      "design a generic two-step algorithm for multilayer network data clustering. The\n",
      "first step aggregates the different layers of network information into a graph\n",
      "representation given by the geometric mean of the network Laplacian matrices.\n",
      "The second step uses a neural net to learn a feature embedding that is\n",
      "consistent with the structure given by the network layers. We propose a novel\n",
      "algorithm for efficiently training the neural net via stochastic gradient\n",
      "descent, which encourages the neural net outputs to span the leading\n",
      "eigenvectors of the aggregated Laplacian matrix, in order to capture the\n",
      "pairwise interactions on the network, and provide a clustering-friendly\n",
      "representation of the feature space. We demonstrate with an extensive set of\n",
      "experiments on synthetic and real datasets that our method leads to a\n",
      "significant improvement w.r.t. state-of-the-art multilayer graph clustering\n",
      "algorithms, as it judiciously combines nodes features and network information\n",
      "in the node embedding algorithms.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.02195 \n",
      "Title :Correlation based Multi-phasal models for improved imagined speech EEG\n",
      "  recognition\n",
      "  Translation of imagined speech electroencephalogram(EEG) into human\n",
      "understandable commands greatly facilitates the design of naturalistic brain\n",
      "computer interfaces. To achieve improved imagined speech unit classification,\n",
      "this work aims to profit from the parallel information contained in\n",
      "multi-phasal EEG data recorded while speaking, imagining and performing\n",
      "articulatory movements corresponding to specific speech units. A bi-phase\n",
      "common representation learning module using neural networks is designed to\n",
      "model the correlation and reproducibility between an analysis phase and a\n",
      "support phase. The trained Correlation Network is then employed to extract\n",
      "discriminative features of the analysis phase. These features are further\n",
      "classified into five binary phonological categories using machine learning\n",
      "models such as Gaussian mixture based hidden Markov model and deep neural\n",
      "networks. The proposed approach further handles the non-availability of\n",
      "multi-phasal data during decoding. Topographic visualizations along with\n",
      "result-based inferences suggest that the multi-phasal correlation modelling\n",
      "approach proposed in the paper enhances imagined-speech EEG recognition\n",
      "performance.\n",
      "\n",
      "**Paper Id :2008.07092 \n",
      "Title :Understanding Brain Dynamics for Color Perception using Wearable EEG\n",
      "  headband\n",
      "  The perception of color is an important cognitive feature of the human brain.\n",
      "The variety of colors that impinge upon the human eye can trigger changes in\n",
      "brain activity which can be captured using electroencephalography (EEG). In\n",
      "this work, we have designed a multiclass classification model to detect the\n",
      "primary colors from the features of raw EEG signals. In contrast to previous\n",
      "research, our method employs spectral power features, statistical features as\n",
      "well as correlation features from the signal band power obtained from\n",
      "continuous Morlet wavelet transform instead of raw EEG, for the classification\n",
      "task. We have applied dimensionality reduction techniques such as Forward\n",
      "Feature Selection and Stacked Autoencoders to reduce the dimension of data\n",
      "eventually increasing the model's efficiency. Our proposed methodology using\n",
      "Forward Selection and Random Forest Classifier gave the best overall accuracy\n",
      "of 80.6\\% for intra-subject classification. Our approach shows promise in\n",
      "developing techniques for cognitive tasks using color cues such as controlling\n",
      "Internet of Thing (IoT) devices by looking at primary colors for individuals\n",
      "with restricted motor abilities.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.02280 \n",
      "Title :Physics-Informed Echo State Networks\n",
      "  We propose a physics-informed Echo State Network (ESN) to predict the\n",
      "evolution of chaotic systems. Compared to conventional ESNs, the\n",
      "physics-informed ESNs are trained to solve supervised learning tasks while\n",
      "ensuring that their predictions do not violate physical laws. This is achieved\n",
      "by introducing an additional loss function during the training, which is based\n",
      "on the system's governing equations. The additional loss function penalizes\n",
      "non-physical predictions without the need of any additional training data. This\n",
      "approach is demonstrated on a chaotic Lorenz system and a truncation of the\n",
      "Charney-DeVore system. Compared to the conventional ESNs, the physics-informed\n",
      "ESNs improve the predictability horizon by about two Lyapunov times. This\n",
      "approach is also shown to be robust with regard to noise. The proposed\n",
      "framework shows the potential of using machine learning combined with prior\n",
      "physical knowledge to improve the time-accurate prediction of chaotic dynamical\n",
      "systems.\n",
      "\n",
      "**Paper Id :1810.05547 \n",
      "Title :Physics-Driven Regularization of Deep Neural Networks for Enhanced\n",
      "  Engineering Design and Analysis\n",
      "  In this paper, we introduce a physics-driven regularization method for\n",
      "training of deep neural networks (DNNs) for use in engineering design and\n",
      "analysis problems. In particular, we focus on prediction of a physical system,\n",
      "for which in addition to training data, partial or complete information on a\n",
      "set of governing laws is also available. These laws often appear in the form of\n",
      "differential equations, derived from first principles, empirically-validated\n",
      "laws, or domain expertise, and are usually neglected in data-driven prediction\n",
      "of engineering systems. We propose a training approach that utilizes the known\n",
      "governing laws and regularizes data-driven DNN models by penalizing divergence\n",
      "from those laws. The first two numerical examples are synthetic examples, where\n",
      "we show that in constructing a DNN model that best fits the measurements from a\n",
      "physical system, the use of our proposed regularization results in DNNs that\n",
      "are more interpretable with smaller generalization errors, compared to other\n",
      "common regularization methods. The last two examples concern metamodeling for a\n",
      "random Burgers' system and for aerodynamic analysis of passenger vehicles,\n",
      "where we demonstrate that the proposed regularization provides superior\n",
      "generalization accuracy compared to other common alternatives.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.02598 \n",
      "Title :Binary classification with ambiguous training data\n",
      "  In supervised learning, we often face with ambiguous (A) samples that are\n",
      "difficult to label even by domain experts. In this paper, we consider a binary\n",
      "classification problem in the presence of such A samples. This problem is\n",
      "substantially different from semi-supervised learning since unlabeled samples\n",
      "are not necessarily difficult samples. Also, it is different from 3-class\n",
      "classification with the positive (P), negative (N), and A classes since we do\n",
      "not want to classify test samples into the A class. Our proposed method extends\n",
      "binary classification with reject option, which trains a classifier and a\n",
      "rejector simultaneously using P and N samples based on the 0-1-$c$ loss with\n",
      "rejection cost $c$. More specifically, we propose to train a classifier and a\n",
      "rejector under the 0-1-$c$-$d$ loss using P, N, and A samples, where $d$ is the\n",
      "misclassification penalty for ambiguous samples. In our practical\n",
      "implementation, we use a convex upper bound of the 0-1-$c$-$d$ loss for\n",
      "computational tractability. Numerical experiments demonstrate that our method\n",
      "can successfully utilize the additional information brought by such A training\n",
      "data.\n",
      "\n",
      "**Paper Id :2002.01136 \n",
      "Title :On Positive-Unlabeled Classification in GAN\n",
      "  This paper defines a positive and unlabeled classification problem for\n",
      "standard GANs, which then leads to a novel technique to stabilize the training\n",
      "of the discriminator in GANs. Traditionally, real data are taken as positive\n",
      "while generated data are negative. This positive-negative classification\n",
      "criterion was kept fixed all through the learning process of the discriminator\n",
      "without considering the gradually improved quality of generated data, even if\n",
      "they could be more realistic than real data at times. In contrast, it is more\n",
      "reasonable to treat the generated data as unlabeled, which could be positive or\n",
      "negative according to their quality. The discriminator is thus a classifier for\n",
      "this positive and unlabeled classification problem, and we derive a new\n",
      "Positive-Unlabeled GAN (PUGAN). We theoretically discuss the global optimality\n",
      "the proposed model will achieve and the equivalent optimization goal.\n",
      "Empirically, we find that PUGAN can achieve comparable or even better\n",
      "performance than those sophisticated discriminator stabilization methods.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.02834 \n",
      "Title :Augmenting Organizational Decision-Making with Deep Learning Algorithms:\n",
      "  Principles, Promises, and Challenges\n",
      "  The current expansion of theory and research on artificial intelligence in\n",
      "management and organization studies has revitalized the theory and research on\n",
      "decision-making in organizations. In particular, recent advances in deep\n",
      "learning (DL) algorithms promise benefits for decision-making within\n",
      "organizations, such as assisting employees with information processing, thereby\n",
      "augment their analytical capabilities and perhaps help their transition to more\n",
      "creative work.\n",
      "\n",
      "**Paper Id :1812.11794 \n",
      "Title :Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n",
      "  Challenges, Solutions and Applications\n",
      "  Reinforcement learning (RL) algorithms have been around for decades and\n",
      "employed to solve various sequential decision-making problems. These algorithms\n",
      "however have faced great challenges when dealing with high-dimensional\n",
      "environments. The recent development of deep learning has enabled RL methods to\n",
      "drive optimal policies for sophisticated and capable agents, which can perform\n",
      "efficiently in these challenging environments. This paper addresses an\n",
      "important aspect of deep RL related to situations that require multiple agents\n",
      "to communicate and cooperate to solve complex tasks. A survey of different\n",
      "approaches to problems related to multi-agent deep RL (MADRL) is presented,\n",
      "including non-stationarity, partial observability, continuous state and action\n",
      "spaces, multi-agent training schemes, multi-agent transfer learning. The merits\n",
      "and demerits of the reviewed methods will be analyzed and discussed, with their\n",
      "corresponding applications explored. It is envisaged that this review provides\n",
      "insights about various MADRL methods and can lead to future development of more\n",
      "robust and highly useful multi-agent learning methods for solving real-world\n",
      "problems.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.02838 \n",
      "Title :Real-time parameter inference in reduced-order flame models with\n",
      "  heteroscedastic Bayesian neural network ensembles\n",
      "  The estimation of model parameters with uncertainties from observed data is a\n",
      "ubiquitous inverse problem in science and engineering. In this paper, we\n",
      "suggest an inexpensive and easy to implement parameter estimation technique\n",
      "that uses a heteroscedastic Bayesian Neural Network trained using anchored\n",
      "ensembling. The heteroscedastic aleatoric error of the network models the\n",
      "irreducible uncertainty due to parameter degeneracies in our inverse problem,\n",
      "while the epistemic uncertainty of the Bayesian model captures uncertainties\n",
      "which may arise from an input observation's out-of-distribution nature. We use\n",
      "this tool to perform real-time parameter inference in a 6 parameter G-equation\n",
      "model of a ducted, premixed flame from observations of acoustically excited\n",
      "flames. We train our networks on a library of 2.1 million simulated flame\n",
      "videos. Results on the test dataset of simulated flames show that the network\n",
      "recovers flame model parameters, with the correlation coefficient between\n",
      "predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated\n",
      "uncertainty estimates. The trained neural networks are then used to infer model\n",
      "parameters from real videos of a premixed Bunsen flame captured using a\n",
      "high-speed camera in our lab. Re-simulation using inferred parameters shows\n",
      "excellent agreement between the real and simulated flames. Compared to Ensemble\n",
      "Kalman Filter-based tools that have been proposed for this problem in the\n",
      "combustion literature, our neural network ensemble achieves better\n",
      "data-efficiency and our sub-millisecond inference times represent a savings on\n",
      "computational costs by several orders of magnitude. This allows us to calibrate\n",
      "our reduced-order flame model in real-time and predict the thermoacoustic\n",
      "instability behaviour of the flame more accurately.\n",
      "\n",
      "**Paper Id :1907.06011 \n",
      "Title :Extracting Interpretable Physical Parameters from Spatiotemporal Systems\n",
      "  using Unsupervised Learning\n",
      "  Experimental data is often affected by uncontrolled variables that make\n",
      "analysis and interpretation difficult. For spatiotemporal systems, this problem\n",
      "is further exacerbated by their intricate dynamics. Modern machine learning\n",
      "methods are particularly well-suited for analyzing and modeling complex\n",
      "datasets, but to be effective in science, the result needs to be interpretable.\n",
      "We demonstrate an unsupervised learning technique for extracting interpretable\n",
      "physical parameters from noisy spatiotemporal data and for building a\n",
      "transferable model of the system. In particular, we implement a\n",
      "physics-informed architecture based on variational autoencoders that is\n",
      "designed for analyzing systems governed by partial differential equations\n",
      "(PDEs). The architecture is trained end-to-end and extracts latent parameters\n",
      "that parameterize the dynamics of a learned predictive model for the system. To\n",
      "test our method, we train our model on simulated data from a variety of PDEs\n",
      "with varying dynamical parameters that act as uncontrolled variables. Numerical\n",
      "experiments show that our method can accurately identify relevant parameters\n",
      "and extract them from raw and even noisy spatiotemporal data (tested with\n",
      "roughly 10% added noise). These extracted parameters correlate well (linearly\n",
      "with $R^2 > 0.95$) with the ground truth physical parameters used to generate\n",
      "the datasets. We then apply this method to nonlinear fiber propagation data,\n",
      "generated by an ab-initio simulation, to demonstrate its capabilities on a more\n",
      "realistic dataset. Our method for discovering interpretable latent parameters\n",
      "in spatiotemporal systems will allow us to better analyze and understand\n",
      "real-world phenomena and datasets, which often have unknown and uncontrolled\n",
      "variables that alter the system dynamics and cause varying behaviors that are\n",
      "difficult to disentangle.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.03479 \n",
      "Title :Massively Parallel Graph Drawing and Representation Learning\n",
      "  To fully exploit the performance potential of modern multi-core processors,\n",
      "machine learning and data mining algorithms for big data must be parallelized\n",
      "in multiple ways. Today's CPUs consist of multiple cores, each following an\n",
      "independent thread of control, and each equipped with multiple arithmetic units\n",
      "which can perform the same operation on a vector of multiple data objects.\n",
      "Graph embedding, i.e. converting the vertices of a graph into numerical vectors\n",
      "is a data mining task of high importance and is useful for graph drawing\n",
      "(low-dimensional vectors) and graph representation learning (high-dimensional\n",
      "vectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by\n",
      "Minimizing the Predictive Entropy), an information-theoretic method which can\n",
      "generate low and high-dimensional vectors. MulticoreGEMPE applies MIMD\n",
      "(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single\n",
      "Instructions Multiple Data, using AVX-512) parallelism. We propose general\n",
      "ideas applicable in other graph-based algorithms like \\emph{vectorized hashing}\n",
      "and \\emph{vectorized reduction}. Our experimental evaluation demonstrates the\n",
      "superiority of our approach.\n",
      "\n",
      "**Paper Id :2004.11204 \n",
      "Title :Classification using Hyperdimensional Computing: A Review\n",
      "  Hyperdimensional (HD) computing is built upon its unique data type referred\n",
      "to as hypervectors. The dimension of these hypervectors is typically in the\n",
      "range of tens of thousands. Proposed to solve cognitive tasks, HD computing\n",
      "aims at calculating similarity among its data. Data transformation is realized\n",
      "by three operations, including addition, multiplication and permutation. Its\n",
      "ultra-wide data representation introduces redundancy against noise. Since\n",
      "information is evenly distributed over every bit of the hypervectors, HD\n",
      "computing is inherently robust. Additionally, due to the nature of those three\n",
      "operations, HD computing leads to fast learning ability, high energy efficiency\n",
      "and acceptable accuracy in learning and classification tasks. This paper\n",
      "introduces the background of HD computing, and reviews the data representation,\n",
      "data transformation, and similarity measurement. The orthogonality in high\n",
      "dimensions presents opportunities for flexible computing. To balance the\n",
      "tradeoff between accuracy and efficiency, strategies include but are not\n",
      "limited to encoding, retraining, binarization and hardware acceleration.\n",
      "Evaluations indicate that HD computing shows great potential in addressing\n",
      "problems using data in the form of letters, signals and images. HD computing\n",
      "especially shows significant promise to replace machine learning algorithms as\n",
      "a light-weight classifier in the field of internet of things (IoTs).\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.04640 \n",
      "Title :Scaling Hidden Markov Language Models\n",
      "  The hidden Markov model (HMM) is a fundamental tool for sequence modeling\n",
      "that cleanly separates the hidden state from the emission structure. However,\n",
      "this separation makes it difficult to fit HMMs to large datasets in modern NLP,\n",
      "and they have fallen out of use due to very poor performance compared to fully\n",
      "observed models. This work revisits the challenge of scaling HMMs to language\n",
      "modeling datasets, taking ideas from recent approaches to neural modeling. We\n",
      "propose methods for scaling HMMs to massive state spaces while maintaining\n",
      "efficient exact inference, a compact parameterization, and effective\n",
      "regularization. Experiments show that this approach leads to models that are\n",
      "more accurate than previous HMM and n-gram-based methods, making progress\n",
      "towards the performance of state-of-the-art neural models.\n",
      "\n",
      "**Paper Id :2004.11714 \n",
      "Title :Residual Energy-Based Models for Text Generation\n",
      "  Text generation is ubiquitous in many NLP tasks, from summarization, to\n",
      "dialogue and machine translation. The dominant parametric approach is based on\n",
      "locally normalized models which predict one word at a time. While these work\n",
      "remarkably well, they are plagued by exposure bias due to the greedy nature of\n",
      "the generation process. In this work, we investigate un-normalized energy-based\n",
      "models (EBMs) which operate not at the token but at the sequence level. In\n",
      "order to make training tractable, we first work in the residual of a pretrained\n",
      "locally normalized language model and second we train using noise contrastive\n",
      "estimation. Furthermore, since the EBM works at the sequence level, we can\n",
      "leverage pretrained bi-directional contextual representations, such as BERT and\n",
      "RoBERTa. Our experiments on two large language modeling datasets show that\n",
      "residual EBMs yield lower perplexity compared to locally normalized baselines.\n",
      "Moreover, generation via importance sampling is very efficient and of higher\n",
      "quality than the baseline models according to human evaluation.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.04767 \n",
      "Title :An Analysis of Dataset Overlap on Winograd-Style Tasks\n",
      "  The Winograd Schema Challenge (WSC) and variants inspired by it have become\n",
      "important benchmarks for common-sense reasoning (CSR). Model performance on the\n",
      "WSC has quickly progressed from chance-level to near-human using neural\n",
      "language models trained on massive corpora. In this paper, we analyze the\n",
      "effects of varying degrees of overlap between these training corpora and the\n",
      "test instances in WSC-style tasks. We find that a large number of test\n",
      "instances overlap considerably with the corpora on which state-of-the-art\n",
      "models are (pre)trained, and that a significant drop in classification accuracy\n",
      "occurs when we evaluate models on instances with minimal overlap. Based on\n",
      "these results, we develop the KnowRef-60K dataset, which consists of over 60k\n",
      "pronoun disambiguation problems scraped from web data. KnowRef-60K is the\n",
      "largest corpus to date for WSC-style common-sense reasoning and exhibits a\n",
      "significantly lower proportion of overlaps with current pretraining corpora.\n",
      "\n",
      "**Paper Id :2002.12764 \n",
      "Title :Towards Learning a Universal Non-Semantic Representation of Speech\n",
      "  The ultimate goal of transfer learning is to reduce labeled data requirements\n",
      "by exploiting a pre-existing embedding model trained for different datasets or\n",
      "tasks. The visual and language communities have established benchmarks to\n",
      "compare embeddings, but the speech community has yet to do so. This paper\n",
      "proposes a benchmark for comparing speech representations on non-semantic\n",
      "tasks, and proposes a representation based on an unsupervised triplet-loss\n",
      "objective. The proposed representation outperforms other representations on the\n",
      "benchmark, and even exceeds state-of-the-art performance on a number of\n",
      "transfer learning tasks. The embedding is trained on a publicly available\n",
      "dataset, and it is tested on a variety of low-resource downstream tasks,\n",
      "including personalization tasks and medical domain. The benchmark, models, and\n",
      "evaluation code are publicly released.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.04798 \n",
      "Title :Learning identifiable and interpretable latent models of\n",
      "  high-dimensional neural activity using pi-VAE\n",
      "  The ability to record activities from hundreds of neurons simultaneously in\n",
      "the brain has placed an increasing demand for developing appropriate\n",
      "statistical techniques to analyze such data. Recently, deep generative models\n",
      "have been proposed to fit neural population responses. While these methods are\n",
      "flexible and expressive, the downside is that they can be difficult to\n",
      "interpret and identify. To address this problem, we propose a method that\n",
      "integrates key ingredients from latent models and traditional neural encoding\n",
      "models. Our method, pi-VAE, is inspired by recent progress on identifiable\n",
      "variational auto-encoder, which we adapt to make appropriate for neuroscience\n",
      "applications. Specifically, we propose to construct latent variable models of\n",
      "neural activity while simultaneously modeling the relation between the latent\n",
      "and task variables (non-neural variables, e.g. sensory, motor, and other\n",
      "externally observable states). The incorporation of task variables results in\n",
      "models that are not only more constrained, but also show qualitative\n",
      "improvements in interpretability and identifiability. We validate pi-VAE using\n",
      "synthetic data, and apply it to analyze neurophysiological datasets from rat\n",
      "hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits\n",
      "the data better, but also provides unexpected novel insights into the structure\n",
      "of the neural codes.\n",
      "\n",
      "**Paper Id :2006.14293 \n",
      "Title :Neural Decomposition: Functional ANOVA with Variational Autoencoders\n",
      "  Variational Autoencoders (VAEs) have become a popular approach for\n",
      "dimensionality reduction. However, despite their ability to identify latent\n",
      "low-dimensional structures embedded within high-dimensional data, these latent\n",
      "representations are typically hard to interpret on their own. Due to the\n",
      "black-box nature of VAEs, their utility for healthcare and genomics\n",
      "applications has been limited. In this paper, we focus on characterising the\n",
      "sources of variation in Conditional VAEs. Our goal is to provide a\n",
      "feature-level variance decomposition, i.e. to decompose variation in the data\n",
      "by separating out the marginal additive effects of latent variables z and fixed\n",
      "inputs c from their non-linear interactions. We propose to achieve this through\n",
      "what we call Neural Decomposition - an adaptation of the well-known concept of\n",
      "functional ANOVA variance decomposition from classical statistics to deep\n",
      "learning models. We show how identifiability can be achieved by training models\n",
      "subject to constraints on the marginal properties of the decoder networks. We\n",
      "demonstrate the utility of our Neural Decomposition on a series of synthetic\n",
      "examples as well as high-dimensional genomics data.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.04999 \n",
      "Title :Untangling Dense Knots by Learning Task-Relevant Keypoints\n",
      "  Untangling ropes, wires, and cables is a challenging task for robots due to\n",
      "the high-dimensional configuration space, visual homogeneity, self-occlusions,\n",
      "and complex dynamics. We consider dense (tight) knots that lack space between\n",
      "self-intersections and present an iterative approach that uses learned\n",
      "geometric structure in configurations. We instantiate this into an algorithm,\n",
      "HULK: Hierarchical Untangling from Learned Keypoints, which combines\n",
      "learning-based perception with a geometric planner into a policy that guides a\n",
      "bilateral robot to untangle knots. To evaluate the policy, we perform\n",
      "experiments both in a novel simulation environment modelling cables with varied\n",
      "knot types and textures and in a physical system using the da Vinci surgical\n",
      "robot. We find that HULK is able to untangle cables with dense figure-eight and\n",
      "overhand knots and generalize to varied textures and appearances. We compare\n",
      "two variants of HULK to three baselines and observe that HULK achieves 43.3%\n",
      "higher success rates on a physical system compared to the next best baseline.\n",
      "HULK successfully untangles a cable from a dense initial configuration\n",
      "containing up to two overhand and figure-eight knots in 97.9% of 378 simulation\n",
      "experiments with an average of 12.1 actions per trial. In physical experiments,\n",
      "HULK achieves 61.7% untangling success, averaging 8.48 actions per trial.\n",
      "Supplementary material, code, and videos can be found at\n",
      "https://tinyurl.com/y3a88ycu.\n",
      "\n",
      "**Paper Id :1905.13402 \n",
      "Title :Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep\n",
      "  Model-Based RL for Sparse Cost Robotic Tasks\n",
      "  Reinforcement learning (RL) for robotics is challenging due to the difficulty\n",
      "in hand-engineering a dense cost function, which can lead to unintended\n",
      "behavior, and dynamical uncertainty, which makes exploration and constraint\n",
      "satisfaction challenging. We address these issues with a new model-based\n",
      "reinforcement learning algorithm, Safety Augmented Value Estimation from\n",
      "Demonstrations (SAVED), which uses supervision that only identifies task\n",
      "completion and a modest set of suboptimal demonstrations to constrain\n",
      "exploration and learn efficiently while handling complex constraints. We then\n",
      "compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms\n",
      "on 6 standard simulation benchmarks involving navigation and manipulation and a\n",
      "physical knot-tying task on the da Vinci surgical robot. Results suggest that\n",
      "SAVED outperforms prior methods in terms of success rate, constraint\n",
      "satisfaction, and sample efficiency, making it feasible to safely learn a\n",
      "control policy directly on a real robot in less than an hour. For tasks on the\n",
      "robot, baselines succeed less than 5% of the time while SAVED has a success\n",
      "rate of over 75% in the first 50 training iterations. Code and supplementary\n",
      "material is available at https://tinyurl.com/saved-rl.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.05025 \n",
      "Title :Deep correction of breathing-related artifacts in MR-thermometry\n",
      "  Real-time MR-imaging has been clinically adapted for monitoring thermal\n",
      "therapies since it can provide on-the-fly temperature maps simultaneously with\n",
      "anatomical information. However, proton resonance frequency based thermometry\n",
      "of moving targets remains challenging since temperature artifacts are induced\n",
      "by the respiratory as well as physiological motion. If left uncorrected, these\n",
      "artifacts lead to severe errors in temperature estimates and impair therapy\n",
      "guidance. In this study, we evaluated deep learning for on-line correction of\n",
      "motion related errors in abdominal MR-thermometry. For this, a convolutional\n",
      "neural network (CNN) was designed to learn the apparent temperature\n",
      "perturbation from images acquired during a preparative learning stage prior to\n",
      "hyperthermia. The input of the designed CNN is the most recent magnitude image\n",
      "and no surrogate of motion is needed. During the subsequent hyperthermia\n",
      "procedure, the recent magnitude image is used as an input for the CNN-model in\n",
      "order to generate an on-line correction for the current temperature map. The\n",
      "method's artifact suppression performance was evaluated on 12 free breathing\n",
      "volunteers and was found robust and artifact-free in all examined cases.\n",
      "Furthermore, thermometric precision and accuracy was assessed for in vivo\n",
      "ablation using high intensity focused ultrasound. All calculations involved at\n",
      "the different stages of the proposed workflow were designed to be compatible\n",
      "with the clinical time constraints of a therapeutic procedure.\n",
      "\n",
      "**Paper Id :2002.05487 \n",
      "Title :End-to-end semantic segmentation of personalized deep brain structures\n",
      "  for non-invasive brain stimulation\n",
      "  Electro-stimulation or modulation of deep brain regions is commonly used in\n",
      "clinical procedures for the treatment of several nervous system disorders. In\n",
      "particular, transcranial direct current stimulation (tDCS) is widely used as an\n",
      "affordable clinical application that is applied through electrodes attached to\n",
      "the scalp. However, it is difficult to determine the amount and distribution of\n",
      "the electric field (EF) in the different brain regions due to anatomical\n",
      "complexity and high inter-subject variability. Personalized tDCS is an emerging\n",
      "clinical procedure that is used to tolerate electrode montage for accurate\n",
      "targeting. This procedure is guided by computational head models generated from\n",
      "anatomical images such as MRI. Distribution of the EF in segmented head models\n",
      "can be calculated through simulation studies. Therefore, fast, accurate, and\n",
      "feasible segmentation of different brain structures would lead to a better\n",
      "adjustment for customized tDCS studies. In this study, a single-encoder\n",
      "multi-decoders convolutional neural network is proposed for deep brain\n",
      "segmentation. The proposed architecture is trained to segment seven deep brain\n",
      "structures using T1-weighted MRI. Network generated models are compared with a\n",
      "reference model constructed using a semi-automatic method, and it presents a\n",
      "high matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate\n",
      "(DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution\n",
      "during tDCS in generated and reference models matched well each other,\n",
      "suggesting its potential usefulness in clinical practice.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.05516 \n",
      "Title :Probability-Density-Based Deep Learning Paradigm for the Fuzzy Design of\n",
      "  Functional Metastructures\n",
      "  In quantum mechanics, a norm squared wave function can be interpreted as the\n",
      "probability density that describes the likelihood of a particle to be measured\n",
      "in a given position or momentum. This statistical property is at the core of\n",
      "the fuzzy structure of microcosmos. Recently, hybrid neural structures raised\n",
      "intense attention, resulting in various intelligent systems with far-reaching\n",
      "influence. Here, we propose a probability-density-based deep learning paradigm\n",
      "for the fuzzy design of functional meta-structures. In contrast to other\n",
      "inverse design methods, our probability-density-based neural network can\n",
      "efficiently evaluate and accurately capture all plausible meta-structures in a\n",
      "high-dimensional parameter space. Local maxima in probability density\n",
      "distribution correspond to the most likely candidates to meet the desired\n",
      "performances. We verify this universally adaptive approach in but not limited\n",
      "to acoustics by designing multiple meta-structures for each targeted\n",
      "transmission spectrum, with experiments unequivocally demonstrating the\n",
      "effectiveness and generalization of the inverse design.\n",
      "\n",
      "**Paper Id :1911.07662 \n",
      "Title :Variational mean-field theory for training restricted Boltzmann machines\n",
      "  with binary synapses\n",
      "  Unsupervised learning requiring only raw data is not only a fundamental\n",
      "function of the cerebral cortex, but also a foundation for a next generation of\n",
      "artificial neural networks. However, a unified theoretical framework to treat\n",
      "sensory inputs, synapses and neural activity together is still lacking. The\n",
      "computational obstacle originates from the discrete nature of synapses, and\n",
      "complex interactions among these three essential elements of learning. Here, we\n",
      "propose a variational mean-field theory in which the distribution of synaptic\n",
      "weights is considered. The unsupervised learning can then be decomposed into\n",
      "two intertwined steps: a maximization step is carried out as a gradient ascent\n",
      "of the lower-bound on the data log-likelihood, in which the synaptic weight\n",
      "distribution is determined by updating variational parameters, and an\n",
      "expectation step is carried out as a message passing procedure on an equivalent\n",
      "or dual neural network whose parameter is specified by the variational\n",
      "parameters of the weight distribution. Therefore, our framework provides\n",
      "insights on how data (or sensory inputs), synapses and neural activities\n",
      "interact with each other to achieve the goal of extracting statistical\n",
      "regularities in sensory inputs. This variational framework is verified in\n",
      "restricted Boltzmann machines with planted synaptic weights and\n",
      "handwritten-digits learning.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.06306 \n",
      "Title :Analyzing Neural Discourse Coherence Models\n",
      "  In this work, we systematically investigate how well current models of\n",
      "coherence can capture aspects of text implicated in discourse organisation. We\n",
      "devise two datasets of various linguistic alterations that undermine coherence\n",
      "and test model sensitivity to changes in syntax and semantics. We furthermore\n",
      "probe discourse embedding space and examine the knowledge that is encoded in\n",
      "representations of coherence. We hope this study shall provide further insight\n",
      "into how to frame the task and improve models of coherence assessment further.\n",
      "Finally, we make our datasets publicly available as a resource for researchers\n",
      "to use to test discourse coherence models.\n",
      "\n",
      "**Paper Id :2007.05500 \n",
      "Title :Scientific Discovery by Generating Counterfactuals using Image\n",
      "  Translation\n",
      "  Model explanation techniques play a critical role in understanding the source\n",
      "of a model's performance and making its decisions transparent. Here we\n",
      "investigate if explanation techniques can also be used as a mechanism for\n",
      "scientific discovery. We make three contributions: first, we propose a\n",
      "framework to convert predictions from explanation techniques to a mechanism of\n",
      "discovery. Second, we show how generative models in combination with black-box\n",
      "predictors can be used to generate hypotheses (without human priors) that can\n",
      "be critically examined. Third, with these techniques we study classification\n",
      "models for retinal images predicting Diabetic Macular Edema (DME), where recent\n",
      "work showed that a CNN trained on these images is likely learning novel\n",
      "features in the image. We demonstrate that the proposed framework is able to\n",
      "explain the underlying scientific mechanism, thus bridging the gap between the\n",
      "model's performance and human understanding.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.07225 \n",
      "Title :Reinforced Molecular Optimization with Neighborhood-Controlled Grammars\n",
      "  A major challenge in the pharmaceutical industry is to design novel molecules\n",
      "with specific desired properties, especially when the property evaluation is\n",
      "costly. Here, we propose MNCE-RL, a graph convolutional policy network for\n",
      "molecular optimization with molecular neighborhood-controlled embedding\n",
      "grammars through reinforcement learning. We extend the original\n",
      "neighborhood-controlled embedding grammars to make them applicable to molecular\n",
      "graph generation and design an efficient algorithm to infer grammatical\n",
      "production rules from given molecules. The use of grammars guarantees the\n",
      "validity of the generated molecular structures. By transforming molecular\n",
      "graphs to parse trees with the inferred grammars, the molecular structure\n",
      "generation task is modeled as a Markov decision process where a policy gradient\n",
      "strategy is utilized. In a series of experiments, we demonstrate that our\n",
      "approach achieves state-of-the-art performance in a diverse range of molecular\n",
      "optimization tasks and exhibits significant superiority in optimizing molecular\n",
      "properties with a limited number of property evaluations.\n",
      "\n",
      "**Paper Id :2001.09382 \n",
      "Title :GraphAF: a Flow-based Autoregressive Model for Molecular Graph\n",
      "  Generation\n",
      "  Molecular graph generation is a fundamental problem for drug discovery and\n",
      "has been attracting growing attention. The problem is challenging since it\n",
      "requires not only generating chemically valid molecular structures but also\n",
      "optimizing their chemical properties in the meantime. Inspired by the recent\n",
      "progress in deep generative models, in this paper we propose a flow-based\n",
      "autoregressive model for graph generation called GraphAF. GraphAF combines the\n",
      "advantages of both autoregressive and flow-based approaches and enjoys: (1)\n",
      "high model flexibility for data density estimation; (2) efficient parallel\n",
      "computation for training; (3) an iterative sampling process, which allows\n",
      "leveraging chemical domain knowledge for valency checking. Experimental results\n",
      "show that GraphAF is able to generate 68% chemically valid molecules even\n",
      "without chemical knowledge rules and 100% valid molecules with chemical rules.\n",
      "The training process of GraphAF is two times faster than the existing\n",
      "state-of-the-art approach GCPN. After fine-tuning the model for goal-directed\n",
      "property optimization with reinforcement learning, GraphAF achieves\n",
      "state-of-the-art performance on both chemical property optimization and\n",
      "constrained property optimization.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.07451 \n",
      "Title :Coresets for Robust Training of Neural Networks against Noisy Labels\n",
      "  Modern neural networks have the capacity to overfit noisy labels frequently\n",
      "found in real-world datasets. Although great progress has been made, existing\n",
      "techniques are limited in providing theoretical guarantees for the performance\n",
      "of the neural networks trained with noisy labels. Here we propose a novel\n",
      "approach with strong theoretical guarantees for robust training of deep\n",
      "networks trained with noisy labels. The key idea behind our method is to select\n",
      "weighted subsets (coresets) of clean data points that provide an approximately\n",
      "low-rank Jacobian matrix. We then prove that gradient descent applied to the\n",
      "subsets do not overfit the noisy labels. Our extensive experiments corroborate\n",
      "our theory and demonstrate that deep networks trained on our subsets achieve a\n",
      "significantly superior performance compared to state-of-the art, e.g., 6%\n",
      "increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in\n",
      "accuracy on mini Webvision.\n",
      "\n",
      "**Paper Id :2002.07376 \n",
      "Title :Picking Winning Tickets Before Training by Preserving Gradient Flow\n",
      "  Overparameterization has been shown to benefit both the optimization and\n",
      "generalization of neural networks, but large networks are resource hungry at\n",
      "both training and test time. Network pruning can reduce test-time resource\n",
      "requirements, but is typically applied to trained networks and therefore cannot\n",
      "avoid the expensive training process. We aim to prune networks at\n",
      "initialization, thereby saving resources at training time as well.\n",
      "Specifically, we argue that efficient training requires preserving the gradient\n",
      "flow through the network. This leads to a simple but effective pruning\n",
      "criterion we term Gradient Signal Preservation (GraSP). We empirically\n",
      "investigate the effectiveness of the proposed method with extensive experiments\n",
      "on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet\n",
      "architectures. Our method can prune 80% of the weights of a VGG-16 network on\n",
      "ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover,\n",
      "our method achieves significantly better performance than the baseline at\n",
      "extreme sparsity levels.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.07516 \n",
      "Title :2CP: Decentralized Protocols to Transparently Evaluate Contributivity in\n",
      "  Blockchain Federated Learning Environments\n",
      "  Federated Learning harnesses data from multiple sources to build a single\n",
      "model. While the initial model might belong solely to the actor bringing it to\n",
      "the network for training, determining the ownership of the trained model\n",
      "resulting from Federated Learning remains an open question. In this paper we\n",
      "explore how Blockchains (in particular Ethereum) can be used to determine the\n",
      "evolving ownership of a model trained with Federated Learning.\n",
      "  Firstly, we use the step-by-step evaluation metric to assess the relative\n",
      "contributivities of participants in a Federated Learning process. Next, we\n",
      "introduce 2CP, a framework comprising two novel protocols for Blockchained\n",
      "Federated Learning, which both reward contributors with shares in the final\n",
      "model based on their relative contributivity. The Crowdsource Protocol allows\n",
      "an actor to bring a model forward for training, and use their own data to\n",
      "evaluate the contributions made to it. Potential trainers are guaranteed a fair\n",
      "share of the resulting model, even in a trustless setting. The Consortium\n",
      "Protocol gives trainers the same guarantee even when no party owns the initial\n",
      "model and no evaluator is available.\n",
      "  We conduct experiments with the MNIST dataset that reveal sound\n",
      "contributivity scores resulting from both Protocols by rewarding larger\n",
      "datasets with greater shares in the model. Our experiments also showed the\n",
      "necessity to pair 2CP with a robust model aggregation mechanism to discard low\n",
      "quality inputs coming from model poisoning attacks.\n",
      "\n",
      "**Paper Id :1703.02952 \n",
      "Title :A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n",
      "  Analytics\n",
      "  Internet of Things (IoT) devices and applications are being deployed in our\n",
      "homes and workplaces. These devices often rely on continuous data collection to\n",
      "feed machine learning models. However, this approach introduces several privacy\n",
      "and efficiency challenges, as the service operator can perform unwanted\n",
      "inferences on the available data. Recently, advances in edge processing have\n",
      "paved the way for more efficient, and private, data processing at the source\n",
      "for simple tasks and lighter models, though they remain a challenge for larger,\n",
      "and more complicated models. In this paper, we present a hybrid approach for\n",
      "breaking down large, complex deep neural networks for cooperative,\n",
      "privacy-preserving analytics. To this end, instead of performing the whole\n",
      "operation on the cloud, we let an IoT device to run the initial layers of the\n",
      "neural network, and then send the output to the cloud to feed the remaining\n",
      "layers and produce the final result. In order to ensure that the user's device\n",
      "contains no extra information except what is necessary for the main task and\n",
      "preventing any secondary inference on the data, we introduce Siamese\n",
      "fine-tuning. We evaluate the privacy benefits of this approach based on the\n",
      "information exposed to the cloud service. We also assess the local inference\n",
      "cost of different layers on a modern handset. Our evaluations show that by\n",
      "using Siamese fine-tuning and at a small processing cost, we can greatly reduce\n",
      "the level of unnecessary, potentially sensitive information in the personal\n",
      "data, and thus achieving the desired trade-off between utility, privacy, and\n",
      "performance.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.07923 \n",
      "Title :Quantum deep field: data-driven wave function, electron density\n",
      "  generation, and atomization energy prediction and extrapolation with machine\n",
      "  learning\n",
      "  Deep neural networks (DNNs) have been used to successfully predict molecular\n",
      "properties calculated based on the Kohn--Sham density functional theory\n",
      "(KS-DFT). Although this prediction is fast and accurate, we believe that a DNN\n",
      "model for KS-DFT must not only predict the properties but also provide the\n",
      "electron density of a molecule. This letter presents the quantum deep field\n",
      "(QDF), which provides the electron density with an unsupervised but end-to-end\n",
      "physics-informed modeling by learning the atomization energy on a large-scale\n",
      "dataset. QDF performed well at atomization energy prediction, generated valid\n",
      "electron density, and demonstrated extrapolation. Our QDF implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "**Paper Id :2011.07929 \n",
      "Title :On the equivalence of molecular graph convolution and molecular wave\n",
      "  function with poor basis set\n",
      "  In this study, we demonstrate that the linear combination of atomic orbitals\n",
      "(LCAO), an approximation of quantum physics introduced by Pauling and\n",
      "Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)\n",
      "for molecules. However, GCNs involve unnecessary nonlinearity and deep\n",
      "architecture. We also verify that molecular GCNs are based on a poor basis\n",
      "function set compared with the standard one used in theoretical calculations or\n",
      "quantum chemical simulations. From these observations, we describe the quantum\n",
      "deep field (QDF), a machine learning (ML) model based on an underlying quantum\n",
      "physics, in particular the density functional theory (DFT). We believe that the\n",
      "QDF model can be easily understood because it can be regarded as a single\n",
      "linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to\n",
      "learn an energy functional and a Hohenberg--Kohn map that have nonlinearities\n",
      "inherent in quantum physics and the DFT. For molecular energy prediction tasks,\n",
      "we demonstrated the viability of an ``extrapolation,'' in which we trained a\n",
      "QDF model with small molecules, tested it with large molecules, and achieved\n",
      "high extrapolation performance. This will lead to reliable and practical\n",
      "applications for discovering effective materials. The implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.07929 \n",
      "Title :On the equivalence of molecular graph convolution and molecular wave\n",
      "  function with poor basis set\n",
      "  In this study, we demonstrate that the linear combination of atomic orbitals\n",
      "(LCAO), an approximation of quantum physics introduced by Pauling and\n",
      "Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)\n",
      "for molecules. However, GCNs involve unnecessary nonlinearity and deep\n",
      "architecture. We also verify that molecular GCNs are based on a poor basis\n",
      "function set compared with the standard one used in theoretical calculations or\n",
      "quantum chemical simulations. From these observations, we describe the quantum\n",
      "deep field (QDF), a machine learning (ML) model based on an underlying quantum\n",
      "physics, in particular the density functional theory (DFT). We believe that the\n",
      "QDF model can be easily understood because it can be regarded as a single\n",
      "linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to\n",
      "learn an energy functional and a Hohenberg--Kohn map that have nonlinearities\n",
      "inherent in quantum physics and the DFT. For molecular energy prediction tasks,\n",
      "we demonstrated the viability of an ``extrapolation,'' in which we trained a\n",
      "QDF model with small molecules, tested it with large molecules, and achieved\n",
      "high extrapolation performance. This will lead to reliable and practical\n",
      "applications for discovering effective materials. The implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "**Paper Id :2011.07923 \n",
      "Title :Quantum deep field: data-driven wave function, electron density\n",
      "  generation, and atomization energy prediction and extrapolation with machine\n",
      "  learning\n",
      "  Deep neural networks (DNNs) have been used to successfully predict molecular\n",
      "properties calculated based on the Kohn--Sham density functional theory\n",
      "(KS-DFT). Although this prediction is fast and accurate, we believe that a DNN\n",
      "model for KS-DFT must not only predict the properties but also provide the\n",
      "electron density of a molecule. This letter presents the quantum deep field\n",
      "(QDF), which provides the electron density with an unsupervised but end-to-end\n",
      "physics-informed modeling by learning the atomization energy on a large-scale\n",
      "dataset. QDF performed well at atomization energy prediction, generated valid\n",
      "electron density, and demonstrated extrapolation. Our QDF implementation is\n",
      "available at https://github.com/masashitsubaki/QuantumDeepField_molecule.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.08073 \n",
      "Title :Analyzing Sustainability Reports Using Natural Language Processing\n",
      "  Climate change is a far-reaching, global phenomenon that will impact many\n",
      "aspects of our society, including the global stock market\n",
      "\\cite{dietz2016climate}. In recent years, companies have increasingly been\n",
      "aiming to both mitigate their environmental impact and adapt to the changing\n",
      "climate context. This is reported via increasingly exhaustive reports, which\n",
      "cover many types of climate risks and exposures under the umbrella of\n",
      "Environmental, Social, and Governance (ESG). However, given this abundance of\n",
      "data, sustainability analysts are obliged to comb through hundreds of pages of\n",
      "reports in order to find relevant information. We leveraged recent progress in\n",
      "Natural Language Processing (NLP) to create a custom model, ClimateQA, which\n",
      "allows the analysis of financial reports in order to identify climate-relevant\n",
      "sections based on a question answering approach. We present this tool and the\n",
      "methodology that we used to develop it in the present article.\n",
      "\n",
      "**Paper Id :2008.09043 \n",
      "Title :Considerations, Good Practices, Risks and Pitfalls in Developing AI\n",
      "  Solutions Against COVID-19\n",
      "  The COVID-19 pandemic has been a major challenge to humanity, with 12.7\n",
      "million confirmed cases as of July 13th, 2020 [1]. In previous work, we\n",
      "described how Artificial Intelligence can be used to tackle the pandemic with\n",
      "applications at the molecular, clinical, and societal scales [2]. In the\n",
      "present follow-up article, we review these three research directions, and\n",
      "assess the level of maturity and feasibility of the approaches used, as well as\n",
      "their potential for operationalization. We also summarize some commonly\n",
      "encountered risks and practical pitfalls, as well as guidelines and best\n",
      "practices for formulating and deploying AI applications at different scales.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.08543 \n",
      "Title :Structural and Functional Decomposition for Personality Image Captioning\n",
      "  in a Communication Game\n",
      "  Personality image captioning (PIC) aims to describe an image with a natural\n",
      "language caption given a personality trait. In this work, we introduce a novel\n",
      "formulation for PIC based on a communication game between a speaker and a\n",
      "listener. The speaker attempts to generate natural language captions while the\n",
      "listener encourages the generated captions to contain discriminative\n",
      "information about the input images and personality traits. In this way, we\n",
      "expect that the generated captions can be improved to naturally represent the\n",
      "images and express the traits. In addition, we propose to adapt the language\n",
      "model GPT2 to perform caption generation for PIC. This enables the speaker and\n",
      "listener to benefit from the language encoding capacity of GPT2. Our\n",
      "experiments show that the proposed model achieves the state-of-the-art\n",
      "performance for PIC.\n",
      "\n",
      "**Paper Id :1912.01834 \n",
      "Title :PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting\n",
      "  The latest methods based on deep learning have achieved amazing results\n",
      "regarding the complex work of inpainting large missing areas in an image. But\n",
      "this type of method generally attempts to generate one single \"optimal\" result,\n",
      "ignoring many other plausible results. Considering the uncertainty of the\n",
      "inpainting task, one sole result can hardly be regarded as a desired\n",
      "regeneration of the missing area. In view of this weakness, which is related to\n",
      "the design of the previous algorithms, we propose a novel deep generative model\n",
      "equipped with a brand new style extractor which can extract the style feature\n",
      "(latent vector) from the ground truth. Once obtained, the extracted style\n",
      "feature and the ground truth are both input into the generator. We also craft a\n",
      "consistency loss that guides the generated image to approximate the ground\n",
      "truth. After iterations, our generator is able to learn the mapping of styles\n",
      "corresponding to multiple sets of vectors. The proposed model can generate a\n",
      "large number of results consistent with the context semantics of the image.\n",
      "Moreover, we evaluated the effectiveness of our model on three datasets, i.e.,\n",
      "CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting\n",
      "methods, this model is able to offer desirable inpainting results with both\n",
      "better quality and higher diversity. The code and model will be made available\n",
      "on https://github.com/vivitsai/PiiGAN.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.08981 \n",
      "Title :RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object\n",
      "  Recognition\n",
      "  Millimeter-wave (mmW) radars are being increasingly integrated into\n",
      "commercial vehicles to support new advanced driver-assistance systems (ADAS) by\n",
      "enabling robust and high-performance object detection, localization, as well as\n",
      "recognition - a key component of new environmental perception. In this paper,\n",
      "we propose a novel radar multiple-perspectives convolutional neural network\n",
      "(RAMP-CNN) that extracts the location and class of objects based on further\n",
      "processing of the range-velocity-angle (RVA) heatmap sequences. To bypass the\n",
      "complexity of 4D convolutional neural networks (NN), we propose to combine\n",
      "several lower-dimension NN models within our RAMP-CNN model that nonetheless\n",
      "approaches the performance upper-bound with lower complexity. The extensive\n",
      "experiments show that the proposed RAMP-CNN model achieves better average\n",
      "recall (AR) and average precision (AP) than prior works in all testing\n",
      "scenarios (see Table. III). Besides, the RAMP-CNN model is validated to work\n",
      "robustly under the nighttime, which enables low-cost radars as a potential\n",
      "substitute for pure optical sensing under severe conditions.\n",
      "\n",
      "**Paper Id :2002.09821 \n",
      "Title :A Multi-view CNN-based Acoustic Classification System for Automatic\n",
      "  Animal Species Identification\n",
      "  Automatic identification of animal species by their vocalization is an\n",
      "important and challenging task. Although many kinds of audio monitoring system\n",
      "have been proposed in the literature, they suffer from several disadvantages\n",
      "such as non-trivial feature selection, accuracy degradation because of\n",
      "environmental noise or intensive local computation. In this paper, we propose a\n",
      "deep learning based acoustic classification framework for Wireless Acoustic\n",
      "Sensor Network (WASN). The proposed framework is based on cloud architecture\n",
      "which relaxes the computational burden on the wireless sensor node. To improve\n",
      "the recognition accuracy, we design a multi-view Convolution Neural Network\n",
      "(CNN) to extract the short-, middle-, and long-term dependencies in parallel.\n",
      "The evaluation on two real datasets shows that the proposed architecture can\n",
      "achieve high accuracy and outperforms traditional classification systems\n",
      "significantly when the environmental noise dominate the audio signal (low SNR).\n",
      "Moreover, we implement and deploy the proposed system on a testbed and analyse\n",
      "the system performance in real-world environments. Both simulation and\n",
      "real-world evaluation demonstrate the accuracy and robustness of the proposed\n",
      "acoustic classification system in distinguishing species of animals.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.09645 \n",
      "Title :Finding the Homology of Decision Boundaries with Active Learning\n",
      "  Accurately and efficiently characterizing the decision boundary of\n",
      "classifiers is important for problems related to model selection and\n",
      "meta-learning. Inspired by topological data analysis, the characterization of\n",
      "decision boundaries using their homology has recently emerged as a general and\n",
      "powerful tool. In this paper, we propose an active learning algorithm to\n",
      "recover the homology of decision boundaries. Our algorithm sequentially and\n",
      "adaptively selects which samples it requires the labels of. We theoretically\n",
      "analyze the proposed framework and show that the query complexity of our active\n",
      "learning algorithm depends naturally on the intrinsic complexity of the\n",
      "underlying manifold. We demonstrate the effectiveness of our framework in\n",
      "selecting best-performing machine learning models for datasets just using their\n",
      "respective homological summaries. Experiments on several standard datasets show\n",
      "the sample complexity improvement in recovering the homology and demonstrate\n",
      "the practical utility of the framework for model selection. Source code for our\n",
      "algorithms and experimental results is available at\n",
      "https://github.com/wayne0908/Active-Learning-Homology.\n",
      "\n",
      "**Paper Id :1911.02945 \n",
      "Title :J-MoDL: Joint Model-Based Deep Learning for Optimized Sampling and\n",
      "  Reconstruction\n",
      "  Modern MRI schemes, which rely on compressed sensing or deep learning\n",
      "algorithms to recover MRI data from undersampled multichannel Fourier\n",
      "measurements, are widely used to reduce scan time. The image quality of these\n",
      "approaches is heavily dependent on the sampling pattern. We introduce a\n",
      "continuous strategy to jointly optimize the sampling pattern and network\n",
      "parameters. We use a multichannel forward model, consisting of a non-uniform\n",
      "Fourier transform with continuously defined sampling locations, to realize the\n",
      "data consistency block within a model-based deep learning image reconstruction\n",
      "scheme. This approach facilitates the joint and continuous optimization of the\n",
      "sampling pattern and the CNN parameters to improve image quality. We observe\n",
      "that the joint optimization of the sampling patterns and the reconstruction\n",
      "module significantly improves the performance of most deep learning\n",
      "reconstruction algorithms. The source code of the proposed joint learning\n",
      "framework is available at https://github.com/hkaggarwal/J-MoDL.\n",
      "\n",
      "\n",
      "*********\n",
      "\n",
      "**Paper Id :2011.09801 \n",
      "Title :Novel Classification of Ischemic Heart Disease Using Artificial Neural\n",
      "  Network\n",
      "  Ischemic heart disease (IHD), particularly in its chronic stable form, is a\n",
      "subtle pathology due to its silent behavior before developing in unstable\n",
      "angina, myocardial infarction or sudden cardiac death. Machine learning\n",
      "techniques applied to parameters extracted form heart rate variability (HRV)\n",
      "signal seem to be a valuable support in the early diagnosis of some cardiac\n",
      "diseases. However, so far, IHD patients were identified using Artificial Neural\n",
      "Networks (ANNs) applied to a limited number of HRV parameters and only to very\n",
      "few subjects. In this study, we used several linear and non-linear HRV\n",
      "parameters applied to ANNs, in order to confirm these results on a large cohort\n",
      "of 965 sample of subjects and to identify which features could discriminate IHD\n",
      "patients with high accuracy. By using principal component analysis and stepwise\n",
      "regression, we reduced the original 17 parameters to five, used as inputs, for\n",
      "a series of ANNs. The highest accuracy of 82% was achieved using meanRR, LFn,\n",
      "SD1, gender and age parameters and two hidden neurons.\n",
      "\n",
      "**Paper Id :1803.05985 \n",
      "Title :EEG machine learning with Higuchi fractal dimension and Sample Entropy\n",
      "  as features for successful detection of depression\n",
      "  Reliable diagnosis of depressive disorder is essential for both optimal\n",
      "treatment and prevention of fatal outcomes. In this study, we aimed to\n",
      "elucidate the effectiveness of two non-linear measures, Higuchi Fractal\n",
      "Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders\n",
      "when applied on EEG. HFD and SampEn of EEG signals were used as features for\n",
      "seven machine learning algorithms including Multilayer Perceptron, Logistic\n",
      "Regression, Support Vector Machines with the linear and polynomial kernel,\n",
      "Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG\n",
      "between healthy control subjects and patients diagnosed with depression. We\n",
      "confirmed earlier observations that both non-linear measures can discriminate\n",
      "EEG signals of patients from healthy control subjects. The results suggest that\n",
      "good classification is possible even with a small number of principal\n",
      "components. Average accuracy among classifiers ranged from 90.24% to 97.56%.\n",
      "Among the two measures, SampEn had better performance. Using HFD and SampEn and\n",
      "a variety of machine learning techniques we can accurately discriminate\n",
      "patients diagnosed with depression vs controls which can serve as a highly\n",
      "sensitive, clinically relevant marker for the diagnosis of depressive\n",
      "disorders.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for scores in cosine_scores:\n",
    "    scores[count]=0.0\n",
    "    max_elements, max_indices = torch.max(scores,dim=0)\n",
    "    max_index = max_indices.item()\n",
    "    print(\"\\n*********\\n\")\n",
    "    print(\"**Paper Id :\"+ml_df.iloc[count]['id']+' '+'\\nTitle :'+ml_df.iloc[count]['title']\n",
    "          +'\\n'+ml_df.iloc[count]['abstract']+\n",
    "          '\\n**Paper Id :' +ml_df.iloc[max_index]['id']+' '+'\\nTitle :'+\n",
    "          ml_df.iloc[max_index]['title']+'\\n'+\n",
    "          ml_df.iloc[max_index]['abstract'])\n",
    "    count =count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1077, 768])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1077, 1077])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(cosine_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
